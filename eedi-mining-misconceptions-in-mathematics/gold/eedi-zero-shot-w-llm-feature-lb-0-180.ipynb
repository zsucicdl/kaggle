{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":6530547,"sourceType":"datasetVersion","datasetId":3775395},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":9405408,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":193175737,"sourceType":"kernelVersion"},{"sourceId":194017231,"sourceType":"kernelVersion"},{"sourceId":27644,"sourceType":"modelInstanceVersion","modelInstanceId":23286,"modelId":33601},{"sourceId":113573,"sourceType":"modelInstanceVersion","modelInstanceId":95303,"modelId":119502}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, numpy as np\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-16T13:18:14.962108Z","iopub.execute_input":"2024-09-16T13:18:14.962401Z","iopub.status.idle":"2024-09-16T13:18:14.973237Z","shell.execute_reply.started":"2024-09-16T13:18:14.962368Z","shell.execute_reply":"2024-09-16T13:18:14.972343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n!pip install --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T13:18:14.974972Z","iopub.execute_input":"2024-09-16T13:18:14.975336Z","iopub.status.idle":"2024-09-16T13:22:13.304983Z","shell.execute_reply.started":"2024-09-16T13:18:14.975276Z","shell.execute_reply":"2024-09-16T13:22:13.303894Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"%%writefile eedi_metrics.py\n\n# Credit: https://www.kaggle.com/code/abdullahmeda/eedi-map-k-metric\n\nimport numpy as np\ndef apk(actual, predicted, k=25):\n    \"\"\"\n    Computes the average precision at k.\n    \n    This function computes the average prescision at k between two lists of\n    items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    \n    if not actual:\n        return 0.0\n\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        # first condition checks whether it is valid prediction\n        # second condition checks if prediction is not repeated\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=25):\n    \"\"\"\n    Computes the mean average precision at k.\n    \n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:22:13.307525Z","iopub.execute_input":"2024-09-16T13:22:13.307955Z","iopub.status.idle":"2024-09-16T13:22:13.315934Z","shell.execute_reply.started":"2024-09-16T13:22:13.307905Z","shell.execute_reply":"2024-09-16T13:22:13.315079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare dataframe","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer\nimport pandas as pd\n\nIS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n\ndf_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).iloc[:500]\ndf_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/hugging-quants-meta-llama-3-1-8b-instruct-awq-int4\")\n\nPROMPT  = \"\"\"Question: {Question}\nIncorrect Answer: {IncorrectAnswer}\nCorrect Answer: {CorrectAnswer}\nConstruct Name: {ConstructName}\nSubject Name: {SubjectName}\n\nYour task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\nBefore answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\"\n\ndef apply_template(row, tokenizer, targetCol):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": PROMPT.format(\n                 ConstructName=row[\"ConstructName\"],\n                 SubjectName=row[\"SubjectName\"],\n                 Question=row[\"QuestionText\"],\n                 IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n                 CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"])\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndf = {}\nif not IS_SUBMISSION:\n    df_label = {}\n    for idx, row in df_train.iterrows():\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            if (row.CorrectAnswer!=option) & (row[f\"Misconception{option}Id\"]!=-1):\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n                df_label[f\"{row.QuestionId}_{option}\"] = [row[f\"Misconception{option}Id\"]]\n    df_label = pd.DataFrame([df_label]).T.reset_index()\n    df_label.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\n    df_label.to_parquet(\"label.parquet\", index=False)\nelse:\n    for idx, row in df_test.iterrows():\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            if row.CorrectAnswer!=option:\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\ndf = pd.DataFrame([df]).T.reset_index()\ndf.columns = [\"QuestionId_Answer\", \"text\"]\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:21:44.848047Z","iopub.execute_input":"2024-09-16T14:21:44.848863Z","iopub.status.idle":"2024-09-16T14:21:45.370564Z","shell.execute_reply.started":"2024-09-16T14:21:44.848821Z","shell.execute_reply":"2024-09-16T14:21:45.369784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LLM Reasoning","metadata":{}},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport re\nimport vllm\nimport pandas as pd\n\ndf = pd.read_parquet(\"submission.parquet\")\n\nllm = vllm.LLM(\n    \"/kaggle/input/hugging-quants-meta-llama-3-1-8b-instruct-awq-int4\",\n    quantization=\"awq\",\n    tensor_parallel_size=2, \n    gpu_memory_utilization=0.95, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    max_model_len=8192,\n    disable_log_stats=True\n)\ntokenizer = llm.get_tokenizer()\n\n\nresponses = llm.generate(\n    df[\"text\"].values,\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.\n        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n        temperature=0,  # randomness of the sampling\n        seed=777, # Seed for reprodicibility\n        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n        max_tokens=2048,  # Maximum number of tokens to generate per output sequence.\n    ),\n    use_tqdm = True\n)\n\nresponses = [x.outputs[0].text for x in responses]\ndf[\"fullLLMText\"] = responses\n\ndef extract_response(text):\n    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n\nresponses = [extract_response(x) for x in responses]\ndf[\"llmMisconception\"] = responses\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:21:45.372051Z","iopub.execute_input":"2024-09-16T14:21:45.372377Z","iopub.status.idle":"2024-09-16T14:21:45.37889Z","shell.execute_reply.started":"2024-09-16T14:21:45.372344Z","shell.execute_reply":"2024-09-16T14:21:45.378047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run_vllm.py","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:21:45.38017Z","iopub.execute_input":"2024-09-16T14:21:45.380565Z","iopub.status.idle":"2024-09-16T14:22:39.194605Z","shell.execute_reply.started":"2024-09-16T14:21:45.380522Z","shell.execute_reply":"2024-09-16T14:22:39.193602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_output = pd.read_parquet(\"submission.parquet\")\n\nfor idx, row in llm_output[-5:].iterrows():\n    print(row.fullLLMText)\n    print(\"---\"*3)\n    print(row.llmMisconception)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:22:39.197104Z","iopub.execute_input":"2024-09-16T14:22:39.197459Z","iopub.status.idle":"2024-09-16T14:22:39.211384Z","shell.execute_reply.started":"2024-09-16T14:22:39.197422Z","shell.execute_reply":"2024-09-16T14:22:39.210384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_output","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:22:39.212606Z","iopub.execute_input":"2024-09-16T14:22:39.212961Z","iopub.status.idle":"2024-09-16T14:22:39.226235Z","shell.execute_reply.started":"2024-09-16T14:22:39.212927Z","shell.execute_reply":"2024-09-16T14:22:39.225292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find similar Misconception","metadata":{}},{"cell_type":"code","source":"%%writefile run_similarity_search.py\n\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\ndf = pd.read_parquet(\"submission.parquet\")\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\nmodel = SentenceTransformer('/kaggle/input/bge-large-en-v1-5')\nPREFIX = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\ninput_features = df[\"text\"].str.lstrip(PREFIX).str.split(\"\\n\\nYour task:\").str[0]\n\nembedding_query = model.encode(input_features+ \"\\n----\\n\" + df[\"fullLLMText\"], convert_to_tensor=True)\nembedding_Misconception = model.encode(df_misconception_mapping.MisconceptionName.values, convert_to_tensor=True)\n\ntop25ids = util.semantic_search(embedding_query, embedding_Misconception, top_k=25)\n\ndf[\"MisconceptionId\"] = [\" \".join([str(x[\"corpus_id\"]) for x in top25id]) for top25id in top25ids]\n\ndf[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:22:39.227379Z","iopub.execute_input":"2024-09-16T14:22:39.227676Z","iopub.status.idle":"2024-09-16T14:22:39.237009Z","shell.execute_reply.started":"2024-09-16T14:22:39.227644Z","shell.execute_reply":"2024-09-16T14:22:39.236058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run_similarity_search.py","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:22:39.238166Z","iopub.execute_input":"2024-09-16T14:22:39.238531Z","iopub.status.idle":"2024-09-16T14:23:08.060994Z","shell.execute_reply.started":"2024-09-16T14:22:39.238488Z","shell.execute_reply":"2024-09-16T14:23:08.05988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sanity","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:23:08.063779Z","iopub.execute_input":"2024-09-16T14:23:08.064159Z","iopub.status.idle":"2024-09-16T14:23:08.080374Z","shell.execute_reply.started":"2024-09-16T14:23:08.06412Z","shell.execute_reply":"2024-09-16T14:23:08.079545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not IS_SUBMISSION:\n    import pandas as pd\n    from eedi_metrics import mapk\n    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n    label = pd.read_parquet(\"label.parquet\")[\"MisconceptionId\"]\n    print(\"Validation: \", mapk(label, predicted))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:23:08.081431Z","iopub.execute_input":"2024-09-16T14:23:08.081733Z","iopub.status.idle":"2024-09-16T14:23:08.113279Z","shell.execute_reply.started":"2024-09-16T14:23:08.081702Z","shell.execute_reply":"2024-09-16T14:23:08.112405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}