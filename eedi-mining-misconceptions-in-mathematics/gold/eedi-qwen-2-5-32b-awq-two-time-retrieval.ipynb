{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":9684003,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":9687536,"sourceType":"datasetVersion","datasetId":5922197},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":118192,"sourceType":"modelInstanceVersion","modelInstanceId":99392,"modelId":123481}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* The main idea of this notebook is using retrieval two times.\n  * The first time: Get the top-K1 relavent misconceptions to LLM as a reference(using ConstructName + SubjectName).\n  * The second time: Get the top-K2(K2 < K1) relavent misconceptions(using ConstructName + SubjectName + Question + Answer + LLM's output).\n  * Inference time: ~2 hours\n \n\nThanks to these great works:\n- [Zero-shot w/ LLM feature (LB: 0.180)](https://www.kaggle.com/code/ubamba98/eedi-zero-shot-w-llm-feature-lb-0-180)\n- [Infer BGE Synthetic Data](https://www.kaggle.com/code/minhnguyendichnhat/infer-bge-synthetic-data)\n- [Fine-tuning bge Train](https://www.kaggle.com/code/sinchir0/fine-tuning-bge-train)","metadata":{}},{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-22T01:11:54.072255Z","iopub.execute_input":"2024-10-22T01:11:54.072616Z","iopub.status.idle":"2024-10-22T01:15:24.723703Z","shell.execute_reply.started":"2024-10-22T01:11:54.072581Z","shell.execute_reply":"2024-10-22T01:15:24.7226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, math, numpy as np\nimport os\nfrom transformers import AutoTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\nimport re, gc\nimport torch\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\npd.set_option('display.max_rows', 300)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T01:15:29.495655Z","iopub.execute_input":"2024-10-22T01:15:29.496391Z","iopub.status.idle":"2024-10-22T01:15:32.827386Z","shell.execute_reply.started":"2024-10-22T01:15:29.496347Z","shell.execute_reply":"2024-10-22T01:15:32.826083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"%%writefile eedi_metrics.py\n\n# Credit: https://www.kaggle.com/code/abdullahmeda/eedi-map-k-metric\n\nimport numpy as np\ndef apk(actual, predicted, k=25):\n    \"\"\"\n    Computes the average precision at k.\n    \n    This function computes the average prescision at k between two lists of\n    items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    \n    if not actual:\n        return 0.0\n\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        # first condition checks whether it is valid prediction\n        # second condition checks if prediction is not repeated\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=25):\n    \"\"\"\n    Computes the mean average precision at k.\n    \n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:15:34.312701Z","iopub.execute_input":"2024-10-22T01:15:34.313243Z","iopub.status.idle":"2024-10-22T01:15:34.320411Z","shell.execute_reply.started":"2024-10-22T01:15:34.313203Z","shell.execute_reply":"2024-10-22T01:15:34.319559Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare dataframe","metadata":{}},{"cell_type":"code","source":"IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndf_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(100, random_state=42).reset_index(drop=True)\ndf_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:15:36.051559Z","iopub.execute_input":"2024-10-22T01:15:36.051929Z","iopub.status.idle":"2024-10-22T01:15:36.486153Z","shell.execute_reply.started":"2024-10-22T01:15:36.051896Z","shell.execute_reply":"2024-10-22T01:15:36.485082Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# first retrieval","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\nif not IS_SUBMISSION:\n    df_ret = df_train.copy()\nelse:\n    df_ret = df_test.copy()\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\nmodel = SentenceTransformer('/kaggle/input/eedi-finetuned-bge-public/Eedi-finetuned-bge')\ndf_ret.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:15:44.009381Z","iopub.execute_input":"2024-10-22T01:15:44.010083Z","iopub.status.idle":"2024-10-22T01:16:11.154128Z","shell.execute_reply.started":"2024-10-22T01:15:44.01004Z","shell.execute_reply":"2024-10-22T01:16:11.153201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(\"@\\w+\", '',x)      # Delete strings starting with @\n    #x = re.sub(\"'\\d+\", '',x)      # Delete Numbers\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\ndf_ret['input_features'] = df_ret[\"ConstructName\"] + \". \" + df_ret[\"SubjectName\"]\ndf_ret['input_features'] = df_ret['input_features'].apply(lambda x: preprocess_text(x))\n\nembedding_query = model.encode(df_ret['input_features'], convert_to_tensor=True)\nmisconceptions = df_misconception_mapping.MisconceptionName.values\nembedding_Misconception = model.encode(misconceptions, convert_to_tensor=True)\n\n# the first time retrieval for LLM prompt\nRet_topNids = util.semantic_search(embedding_query, embedding_Misconception, top_k=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:16:25.276611Z","iopub.execute_input":"2024-10-22T01:16:25.277383Z","iopub.status.idle":"2024-10-22T01:16:36.605733Z","shell.execute_reply.started":"2024-10-22T01:16:25.277343Z","shell.execute_reply":"2024-10-22T01:16:36.604825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retrivals = []\ndicts = {}\nfor idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n    top_ids = Ret_topNids[idx]\n    retrival = ''\n    dicts[str(row['QuestionId'])] = {}\n    for i, ids in enumerate(top_ids):\n        # serial number + misconceptions\n        retrival += f'{i+1}. ' + misconceptions[ids['corpus_id']] + '\\n'\n        # save retrieved misconceptions for each QuestionId.\n        dicts[str(row['QuestionId'])][str(i+1)] = misconceptions[ids['corpus_id']]\n    retrivals.append(retrival)\n\ndf_ret['Retrival'] = retrivals","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:16:46.147851Z","iopub.execute_input":"2024-10-22T01:16:46.148824Z","iopub.status.idle":"2024-10-22T01:16:46.241235Z","shell.execute_reply.started":"2024-10-22T01:16:46.14877Z","shell.execute_reply":"2024-10-22T01:16:46.24035Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(x):\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\nPROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\nQuestion: {Question}\nCorrect Answer: {CorrectAnswer}\nIncorrect Answer: {IncorrectAnswer}\n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the incorrect answer.\nNo need to give the reasoning process and do not use \"The misconception is\" to start your answers.\nThere are some relative and possible misconceptions below to help you make the decision:\n\n{Retrival}\n\"\"\"\n# just directly give your answers.\n\ndef apply_template(row, tokenizer, targetCol):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": preprocess_text(\n                PROMPT.format(\n                    ConstructName=row[\"ConstructName\"],\n                    SubjectName=row[\"SubjectName\"],\n                    Question=row[\"QuestionText\"],\n                    IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n                    CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"],\n                    Retrival=row[f\"Retrival\"]\n                )\n            )\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndf = {}\nif not IS_SUBMISSION:\n    df_label = {}\n    for idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            if (row.CorrectAnswer!=option) & (row[f\"Misconception{option}Id\"]!=-1):\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n                df_label[f\"{row.QuestionId}_{option}\"] = [row[f\"Misconception{option}Id\"]]\n                \n    df_label = pd.DataFrame([df_label]).T.reset_index()\n    df_label.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\n    df_label.to_parquet(\"label.parquet\", index=False)\nelse:\n    for idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            if row.CorrectAnswer!=option:\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n\ndf = pd.DataFrame([df]).T.reset_index()\ndf.columns = [\"QuestionId_Answer\", \"text\"]\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:18:12.52316Z","iopub.execute_input":"2024-10-22T01:18:12.523563Z","iopub.status.idle":"2024-10-22T01:18:12.919918Z","shell.execute_reply.started":"2024-10-22T01:18:12.523529Z","shell.execute_reply":"2024-10-22T01:18:12.919024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.loc[0, 'text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:18:17.082488Z","iopub.execute_input":"2024-10-22T01:18:17.083187Z","iopub.status.idle":"2024-10-22T01:18:17.088596Z","shell.execute_reply.started":"2024-10-22T01:18:17.083145Z","shell.execute_reply":"2024-10-22T01:18:17.087688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LLM Reasoning","metadata":{}},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport re\nimport vllm\nimport pandas as pd\n\ndf = pd.read_parquet(\"submission.parquet\")\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n\nllm = vllm.LLM(\n    model_path,\n    quantization=\"awq\",\n    tensor_parallel_size=2,\n    gpu_memory_utilization=0.90, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    max_model_len=5120,\n    disable_log_stats=True\n)\ntokenizer = llm.get_tokenizer()\n\n\nresponses = llm.generate(\n    df[\"text\"].values,\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.\n        top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.\n        temperature=0,  # randomness of the sampling\n        seed=777, # Seed for reprodicibility\n        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n        max_tokens=512,  # Maximum number of tokens to generate per output sequence.\n    ),\n    use_tqdm=True\n)\n\nresponses = [x.outputs[0].text for x in responses]\ndf[\"fullLLMText\"] = responses\n\ndef extract_response(text):\n    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n\ndf[\"llmMisconception\"] = responses\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:18:28.520688Z","iopub.execute_input":"2024-10-22T01:18:28.521408Z","iopub.status.idle":"2024-10-22T01:18:28.527481Z","shell.execute_reply.started":"2024-10-22T01:18:28.521365Z","shell.execute_reply":"2024-10-22T01:18:28.526602Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run_vllm.py","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:18:31.170437Z","iopub.execute_input":"2024-10-22T01:18:31.170812Z","iopub.status.idle":"2024-10-22T01:35:55.022196Z","shell.execute_reply.started":"2024-10-22T01:18:31.170779Z","shell.execute_reply":"2024-10-22T01:35:55.02089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm_output = pd.read_parquet(\"submission.parquet\")\n\nfor idx, row in llm_output[0:5].iterrows():\n    print(row.llmMisconception)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:08.195412Z","iopub.execute_input":"2024-10-22T01:36:08.195821Z","iopub.status.idle":"2024-10-22T01:36:08.302844Z","shell.execute_reply.started":"2024-10-22T01:36:08.19578Z","shell.execute_reply":"2024-10-22T01:36:08.301982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = llm_output.loc[0, 'text']\nPREFIX = \"<|im_start|>user\"\ntext = text.split(PREFIX)[1].split(\"You are a Mathematics teacher.\")[0].strip('\\n').split('Here is a question about')[-1].strip()\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:16.124759Z","iopub.execute_input":"2024-10-22T01:36:16.125156Z","iopub.status.idle":"2024-10-22T01:36:16.131355Z","shell.execute_reply.started":"2024-10-22T01:36:16.125118Z","shell.execute_reply":"2024-10-22T01:36:16.130479Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Post-processing for LLM output","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\ndf = pd.read_parquet(\"submission.parquet\")\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\nmodel = SentenceTransformer('/kaggle/input/eedi-finetuned-bge-public/Eedi-finetuned-bge')","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:24.771622Z","iopub.execute_input":"2024-10-22T01:36:24.772471Z","iopub.status.idle":"2024-10-22T01:36:35.248545Z","shell.execute_reply.started":"2024-10-22T01:36:24.772431Z","shell.execute_reply":"2024-10-22T01:36:35.247511Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def number2sentence(row):\n    \"\"\"\n    This is used for post-processing of LLM's output.\n    Since we give top-N retrieval to the LLM with serial number,\n    Sometimes the LLM will only output the serial number without any sentence.\n    We use the 'dicts' generated at the beginning to map the serial number with corresponding misconceptions.\n    \"\"\"\n    text = row['llmMisconception'].strip()\n    # potential is the most possible serial number in LLM output.\n    potential = re.search(r'^\\w+\\.{0,1}', text).group()\n    if '.' in potential:\n        sentence = text.replace(potential, '').strip()\n    # if the LLM output is only a serial number, we map it with corresponding misconceptions saved in the dict.\n    elif len(potential) == len(text):\n        qid_retrieval = dicts[row['QuestionId']]\n        try:\n            # qid_retrieval is the top-N misconceptions for an QuestionId,\n            # qid_retrieval[potential] is the most possible misconception.\n            sentence = qid_retrieval[potential]\n        except:\n            # If the mapping fails, we use the first one(the most possible one in the first retrieval).\n            sentence = qid_retrieval['1']\n    else:\n        sentence = text\n        \n    return sentence\n\n\ndf['QuestionId'] = df['QuestionId_Answer'].apply(lambda x: x.split('_')[0])\ndf['llmMisconception_clean'] = df.apply(number2sentence, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:39.243383Z","iopub.execute_input":"2024-10-22T01:36:39.243737Z","iopub.status.idle":"2024-10-22T01:36:39.257332Z","shell.execute_reply.started":"2024-10-22T01:36:39.243705Z","shell.execute_reply":"2024-10-22T01:36:39.256092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:41.732328Z","iopub.execute_input":"2024-10-22T01:36:41.732716Z","iopub.status.idle":"2024-10-22T01:36:41.746371Z","shell.execute_reply.started":"2024-10-22T01:36:41.73268Z","shell.execute_reply":"2024-10-22T01:36:41.745245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Second retrieval","metadata":{}},{"cell_type":"code","source":"def preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(r\"@\\w+\", '',x)      # Delete strings starting with @\n    #x = re.sub(r\"\\d+\", '',x)      # Delete Numbers\n    x = re.sub(r\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\nPREFIX = \"<|im_start|>user\"\ndf['input_features'] = df[\"text\"].apply(lambda x: x.split(PREFIX)[1].split(\"You are a Mathematics teacher.\")[0].strip('\\n').split('Here is a question about')[-1].strip())\n\ndf['input_features'] = df['input_features'].apply(lambda x: preprocess_text(x))\ndf['input_features'] = df[\"llmMisconception_clean\"] + \"\\n\\n\" + df['input_features']\n\nembedding_query = model.encode(df['input_features'], convert_to_tensor=True)\nembedding_Misconception = model.encode(df_misconception_mapping.MisconceptionName.values, convert_to_tensor=True)\ntop25ids = util.semantic_search(embedding_query, embedding_Misconception, top_k=25)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:36:48.943966Z","iopub.execute_input":"2024-10-22T01:36:48.944724Z","iopub.status.idle":"2024-10-22T01:37:06.539738Z","shell.execute_reply.started":"2024-10-22T01:36:48.944683Z","shell.execute_reply":"2024-10-22T01:37:06.538994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"MisconceptionId\"] = [\" \".join([str(x[\"corpus_id\"]) for x in top25id]) for top25id in top25ids]\n\ndf[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:37:15.018558Z","iopub.execute_input":"2024-10-22T01:37:15.019427Z","iopub.status.idle":"2024-10-22T01:37:15.048573Z","shell.execute_reply.started":"2024-10-22T01:37:15.019386Z","shell.execute_reply":"2024-10-22T01:37:15.047652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sanity","metadata":{}},{"cell_type":"code","source":"if not IS_SUBMISSION:\n    import pandas as pd\n    from eedi_metrics import mapk\n    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n    label = pd.read_parquet(\"label.parquet\")[\"MisconceptionId\"]\n    print(\"Validation: \", mapk(label, predicted))","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:37:56.77789Z","iopub.execute_input":"2024-10-22T01:37:56.778686Z","iopub.status.idle":"2024-10-22T01:37:56.833551Z","shell.execute_reply.started":"2024-10-22T01:37:56.778646Z","shell.execute_reply":"2024-10-22T01:37:56.832621Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}