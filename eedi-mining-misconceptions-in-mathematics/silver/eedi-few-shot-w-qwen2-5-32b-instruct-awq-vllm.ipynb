{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":196660105,"sourceType":"kernelVersion"},{"sourceId":196750896,"sourceType":"kernelVersion"},{"sourceId":122612,"sourceType":"modelInstanceVersion","modelInstanceId":103186,"modelId":127417},{"sourceId":124335,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":104643,"modelId":128845}],"dockerImageVersionId":30776,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n- Load AWQ Quantized Qwen2.5-32B-Instruct using vLLM on 2x T4\n- For each test question, we retrive a set of similar questions that \"support\" it using train data.\n    - Consider things like \"construct\" and \"subject\"\n- Using the set of similar questions, we create a conversation that can be fed to our model\n- We can then pass these messages into our quantized Qwen2.5-32B-Instruct. Towards the end of the conversation, we give it the question/answer pair we want to predict the misconception for.\n    - Use the recently supported `chat()` functionality on the conversation\n    - Generate `n` responses with a slightly higher temperature to create diverse outputs\n- For each question/answer pair, we now have `n` inferred misconceptions, for each, we retrieve the top 25 misconceptions using BGE embeddings. \n- The 25 nearest misconceptions for each of our `n` inferred misconceptions for each question/answer pair can now be combined using Borda Ranking. This is sort of like the simplest form of ensembling.\n- Submit :)\n\n### Note:\n- This notebook is a fork of the beautiful work by Rich Olson @ https://www.kaggle.com/code/richolson/eedi-phi-mini-3-5\n- Credits, Load 32B AWQ on 2x T4 by Chris Deotte @ https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport ctypes\nimport numpy as np\nimport pandas as pd\n\nfrom random import sample\nfrom tqdm.auto import tqdm\nfrom eedi_metrics import mapk, apk\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport torch\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"_uuid":"064535b3-4de6-4084-a4f0-ed21a4930d76","_cell_guid":"39fdb055-da68-4ba7-9576-5dfcbe798344","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-13T13:36:26.391005Z","iopub.execute_input":"2024-10-13T13:36:26.391664Z","iopub.status.idle":"2024-10-13T13:37:16.915365Z","shell.execute_reply.started":"2024-10-13T13:36:26.39162Z","shell.execute_reply":"2024-10-13T13:37:16.914258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndef clean_memory(deep=False):\n    gc.collect()\n    if deep:\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()","metadata":{"_uuid":"48bd86db-c8df-454c-b0e5-275b9f019a10","_cell_guid":"029b170b-c7d8-401c-aed2-ae41f5385f69","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-13T13:37:16.917113Z","iopub.execute_input":"2024-10-13T13:37:16.917786Z","iopub.status.idle":"2024-10-13T13:37:16.923371Z","shell.execute_reply.started":"2024-10-13T13:37:16.917747Z","shell.execute_reply":"2024-10-13T13:37:16.922202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"k = 3\n\ntrain_eval = True\nn_train_eval_rows = 100\n\ncomp_dir  = '/kaggle/input/eedi-mining-misconceptions-in-mathematics'\n\nllm_model_pth   = '/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1'\nembed_model_pth = '/kaggle/input/baai/transformers/bge-large-en-v1.5/1'\n\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    train_eval = False","metadata":{"_uuid":"7e9d6c3d-5dbe-4e17-90af-597deb8cac96","_cell_guid":"1875b764-6f35-4749-b9ad-04549659b4fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-13T13:37:16.924831Z","iopub.execute_input":"2024-10-13T13:37:16.925174Z","iopub.status.idle":"2024-10-13T13:37:16.934144Z","shell.execute_reply.started":"2024-10-13T13:37:16.925137Z","shell.execute_reply":"2024-10-13T13:37:16.933145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_eval:\n    test       = pd.read_csv(f'{comp_dir}/train.csv').sample(n_train_eval_rows, random_state=3)\n    test       = test.sort_values(['QuestionId'], ascending=True).reset_index(drop=True)\nelse:\n    test       = pd.read_csv(f'{comp_dir}/test.csv')\n\ntrain          = pd.read_csv(f'{comp_dir}/train.csv')\nsample_sub     = pd.read_csv(f'{comp_dir}/sample_submission.csv')\nmisconceptions = pd.read_csv(f'{comp_dir}/misconception_mapping.csv')\n\nlen(train), len(test), len(misconceptions)","metadata":{"_uuid":"a0a602f7-7a5f-4fb9-b783-97fc47802cda","_cell_guid":"0c03a53e-28bf-42b2-909b-7754e0a95cc7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-13T13:37:16.936014Z","iopub.execute_input":"2024-10-13T13:37:16.936462Z","iopub.status.idle":"2024-10-13T13:37:17.073962Z","shell.execute_reply.started":"2024-10-13T13:37:16.936407Z","shell.execute_reply":"2024-10-13T13:37:17.072923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spin up our `Qwen2.5-32B-Instruct-AWQ` using vLLM\n\nCredits: https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm\n\nThe model fits very comfortably even with a max model length of 4096. Even longer can could be experimented with. Incase of OOM errors, it might be helpful to decrease `max_num_seqs` to 4 or 8, or even 1! (Default is 256)","metadata":{}},{"cell_type":"code","source":"llm = LLM(\n    llm_model_pth,\n    trust_remote_code=True,\n    dtype=\"half\", max_model_len=4096,\n    tensor_parallel_size=2, gpu_memory_utilization=0.95, \n)\n\ntokenizer = llm.get_tokenizer()","metadata":{"_uuid":"d1945dd4-967b-4df0-af92-dcc6e7a7b8e4","_cell_guid":"f4967dbe-eb0d-425c-8c46-ccc3539b9175","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post process data","metadata":{}},{"cell_type":"code","source":"answer_cols         = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"]\nmisconception_cols  = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n\nkeep_cols           = [\"QuestionId\", \"CorrectAnswer\", \"ConstructName\", \"SubjectName\", \"QuestionText\" ]\n\ndef wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n    \n    # Melt the answer columns\n    answers_df = pd.melt(\n        id_vars=keep_cols,\n        frame=df[keep_cols + answer_cols],\n        var_name='Answer', value_name='Value'\n    ).sort_values([\"QuestionId\", \"Answer\"]).reset_index(drop=True)\n    \n    if misconception_cols[0] not in df.columns:  # If test set\n        return answers_df\n        \n    # Melt the misconception columns\n    misconceptions_df = pd.melt(\n        id_vars=keep_cols,\n        frame=df[keep_cols + misconception_cols],\n        var_name='Misconception', value_name='MisconceptionId'\n    ).sort_values([\"QuestionId\", \"Misconception\"]).reset_index(drop=True)\n\n    answers_df[['Misconception', 'MisconceptionId']] = misconceptions_df[['Misconception', 'MisconceptionId']]\n    \n    return answers_df\n\n\ntest  = wide_to_long(test)\ntrain = wide_to_long(train)\n\ntest['AnswerId']  = test.Answer.str.replace('Answer', '').str.replace('Text', '')\ntrain['AnswerId'] = train.Answer.str.replace('Answer', '').str.replace('Text', '')\n\ntrain = pd.merge(train, misconceptions, on='MisconceptionId', how='left')\nif train_eval:\n    test = pd.merge(test, misconceptions, on='MisconceptionId', how='left')","metadata":{"_uuid":"75ee62af-8be4-44bf-932e-f9c7350345ae","_cell_guid":"a7609fc8-f752-46d5-b866-3c14b62323d3","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.334161Z","iopub.execute_input":"2024-10-05T11:13:42.334942Z","iopub.status.idle":"2024-10-05T11:13:42.469814Z","shell.execute_reply.started":"2024-10-05T11:13:42.334891Z","shell.execute_reply":"2024-10-05T11:13:42.468734Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"_uuid":"80fb66c2-28fe-4826-8050-736356e5c298","_cell_guid":"45fe473b-f4ad-4425-b940-fd5fadb65e9c","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.471384Z","iopub.execute_input":"2024-10-05T11:13:42.471813Z","iopub.status.idle":"2024-10-05T11:13:42.529678Z","shell.execute_reply.started":"2024-10-05T11:13:42.471766Z","shell.execute_reply":"2024-10-05T11:13:42.528602Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"_uuid":"b0c18856-3a74-4c00-b46a-0280aeb3bf0b","_cell_guid":"54bea772-f8ca-4104-b527-38176e40a476","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.531209Z","iopub.execute_input":"2024-10-05T11:13:42.532152Z","iopub.status.idle":"2024-10-05T11:13:42.547686Z","shell.execute_reply.started":"2024-10-05T11:13:42.532108Z","shell.execute_reply":"2024-10-05T11:13:42.545356Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions\n\nCredits: https://www.kaggle.com/code/richolson/eedi-phi-mini-3-5","metadata":{}},{"cell_type":"markdown","source":"## Get the most similar question_ids' given the subject and construct\n\nThe following function return `top_k` number of question ids' first by checking for questions that have both construct and subject to be similar.\n\nIf that does not come up to `top_k`, questions with either similar subject or construct are chosen. If we are still short of question ids', we select random questions for the remainder of `top_k`","metadata":{}},{"cell_type":"code","source":"def get_topk_similar_rows(question_id: int, construct: str, subject: str, top_k: int) -> list[int]:\n    \"\"\" Gets the top n ids of questions that most similar to the given construct and subject \"\"\"\n    \n    # Rows with similar construct and subject\n    similar_cs_rows = train[(train.ConstructName == construct) & (train.SubjectName == subject)]\n    similar_cs_qids = list(set(similar_cs_rows.QuestionId.values.tolist()))\n    \n    if train_eval and question_id in similar_cs_qids:\n        similar_cs_qids.remove(question_id)\n        \n    if len(similar_cs_qids) >= top_k:\n        k_similar_cs_qids = sample(similar_cs_qids, top_k)\n        return k_similar_cs_qids\n    \n    \n    # Rows with similar construct or subject for remainder of top_k\n    similar_s_rows = train[(train.ConstructName != construct) & (train.SubjectName == subject)]\n    similar_c_rows = train[(train.ConstructName == construct) & (train.SubjectName != subject)]\n    similar_c_or_s_qids = list(set(similar_s_rows.QuestionId.values.tolist() + similar_c_rows.QuestionId.values.tolist()))\n    \n    if train_eval and question_id in similar_c_or_s_qids:\n        similar_c_or_s_qids.remove(question_id)\n    \n    if len(similar_c_or_s_qids) >= top_k - len(similar_cs_qids):\n        n_similar_c_or_s_qids = sample(similar_c_or_s_qids, top_k - len(similar_cs_qids))\n        return similar_cs_qids + n_similar_c_or_s_qids\n    \n    \n    # Random rows for remainder of top_k\n    not_so_similar_rows = train[(train.ConstructName != construct) & (train.SubjectName != subject)]\n    not_so_similar_rows_qids = list(set(not_so_similar_rows.QuestionId.values.tolist()))\n    \n    if train_eval and question_id in not_so_similar_rows_qids:\n        not_so_similar_rows_qids.remove(question_id)\n    \n    n_not_so_similar_rows_qids = sample(not_so_similar_rows_qids, top_k - len(similar_c_or_s_qids))\n    return similar_c_or_s_qids + n_not_so_similar_rows_qids","metadata":{"_uuid":"295d3bad-3a81-4ea2-8dd6-da8716f8aebd","_cell_guid":"3d201b2d-1b92-449f-b1a0-60e710474907","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.551062Z","iopub.execute_input":"2024-10-05T11:13:42.552642Z","iopub.status.idle":"2024-10-05T11:13:42.568279Z","shell.execute_reply.started":"2024-10-05T11:13:42.552214Z","shell.execute_reply":"2024-10-05T11:13:42.567318Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the chat conversation for each question","metadata":{}},{"cell_type":"code","source":"def get_conversation_msgs(question, correct_ans, incorrect_ans, misconception):\n    msgs = [\n        {'role': 'user',      'content': 'Question: ' + question.strip()},\n        {'role': 'assistant', 'content': 'Provide me with the correct answer for a baseline.'},\n        {'role': 'user',      'content': 'Correct Answer: ' + correct_ans.strip()},\n        {'role': 'assistant', 'content': 'Now provide the incorrect answer and I will anaylze the difference to infer the misconception.'},\n        {'role': 'user',      'content': 'Incorrect Answer: ' + incorrect_ans.strip()},\n    ]\n    \n    if misconception is not None:\n        msgs += [{'role': 'assistant', 'content': 'Misconception for incorrect answer: ' + misconception}]\n        \n    return msgs","metadata":{"_uuid":"2d0cb0cb-40e7-4345-8978-a71806171992","_cell_guid":"586cb3ea-6da3-406a-b159-2864b9314790","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.575444Z","iopub.execute_input":"2024-10-05T11:13:42.576448Z","iopub.status.idle":"2024-10-05T11:13:42.602553Z","shell.execute_reply.started":"2024-10-05T11:13:42.576393Z","shell.execute_reply":"2024-10-05T11:13:42.586325Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer the misconception using `llm.chat()`\n\nNote: `llm.chat()` has only been introduced recently and is available only in the later releases\n\nWe generate n outputs, with a higher temperature to create diverse representations of the outputs, which can then be used later to rank our results!","metadata":{}},{"cell_type":"code","source":"sampling_params = SamplingParams(\n    n=10,                     # Number of output sequences to return for each prompt.\n    # top_p=0.5,               # Float that controls the cumulative probability of the top tokens to consider.\n    temperature=0.7,          # randomness of the sampling\n    seed=1,                   # Seed for reprodicibility\n    skip_special_tokens=True, # Whether to skip special tokens in the output.\n    max_tokens=64,            # Maximum number of tokens to generate per output sequence.\n    stop=['\\n\\n', '. '],      # List of strings that stop the generation when they are generated.\n)","metadata":{"_uuid":"ca7a0849-8032-4fe2-b95b-3ea5775e1d99","_cell_guid":"8728080d-4c3a-472d-9b3c-f410021bba1e","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:13:42.60718Z","iopub.execute_input":"2024-10-05T11:13:42.607822Z","iopub.status.idle":"2024-10-05T11:13:42.618173Z","shell.execute_reply.started":"2024-10-05T11:13:42.607772Z","shell.execute_reply":"2024-10-05T11:13:42.616598Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\nfor idx, row in tqdm(test.iterrows(), total=len(test)):\n    \n    if idx % 50:\n        clean_memory()\n        clean_memory()\n    \n    if row['CorrectAnswer'] == row['AnswerId']: continue\n    if train_eval and not row['MisconceptionId'] >= 0: continue\n        \n    context_qids   = get_topk_similar_rows(row['QuestionId'], row['ConstructName'], row['SubjectName'], k)\n    correct_answer = test[(test.QuestionId == row['QuestionId']) & (test.CorrectAnswer == test.AnswerId)].Value.tolist()[0]\n    \n    messages = []\n    for qid in context_qids:\n        correct_option = train[(train.QuestionId == qid) & (train.CorrectAnswer == train.AnswerId)]\n        incorrect_options = train[(train.QuestionId == qid) & (train.CorrectAnswer != train.AnswerId)]\n\n        for idx, incorrect_option in incorrect_options.iterrows():\n            if type(incorrect_option['MisconceptionName']) == str: # Filter out NaNs\n                messages += get_conversation_msgs(\n                    question = correct_option.QuestionText.tolist()[0],\n                    correct_ans = correct_option.Value.tolist()[0],\n                    incorrect_ans = incorrect_option['Value'],\n                    misconception = incorrect_option['MisconceptionName'],\n                )\n                \n    # Coversation for Incorrect answer to get misconception for\n    messages += get_conversation_msgs(\n        question = row['QuestionText'],\n        correct_ans = correct_answer,\n        incorrect_ans = row['Value'],\n        misconception = None,\n    )\n    \n    output = llm.chat(messages, sampling_params, use_tqdm=False)\n    inferred_misconceptions = [imc.text.split(':')[-1].strip() for imc in output[0].outputs]\n    \n    if not train_eval:\n        submission.append([f\"{row['QuestionId']}_{row['AnswerId']}\", inferred_misconceptions])\n    else:\n        submission.append([\n            f\"{row['QuestionId']}_{row['AnswerId']}\", \n            inferred_misconceptions, \n            context_qids,\n            [int(row['MisconceptionId'])],\n            row['MisconceptionName']\n        ])\n\n\nsubmission = pd.DataFrame(submission, columns=['QuestionId_Answer', 'InferredMisconception', 'TopKQuestionIDs', \n                                               'MisconceptionIdGT', 'MisconceptionNameGT'][:len(submission[0])])\n\nlen(submission)","metadata":{"_uuid":"94b7e3dc-d1a5-480b-b1a3-948b6fe05fa9","_cell_guid":"5e971219-1c18-4a99-988a-c1da5a3f95c7","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:14:29.288896Z","iopub.execute_input":"2024-10-05T11:14:29.289351Z","iopub.status.idle":"2024-10-05T11:14:56.646844Z","shell.execute_reply.started":"2024-10-05T11:14:29.289309Z","shell.execute_reply":"2024-10-05T11:14:56.645664Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"_uuid":"ac6e657d-c08a-42ef-94b6-32bd4b3ffe8a","_cell_guid":"fe5db7db-7c29-4640-afb4-74407ffaf0d1","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:03.700178Z","iopub.execute_input":"2024-10-05T11:15:03.700602Z","iopub.status.idle":"2024-10-05T11:15:03.7129Z","shell.execute_reply.started":"2024-10-05T11:15:03.700561Z","shell.execute_reply":"2024-10-05T11:15:03.711934Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find most similar misconceptions\n\nDelete the model and clean memory to load up embedding model","metadata":{}},{"cell_type":"code","source":"del llm\n\nclean_memory(deep=True)\nclean_memory(deep=True)","metadata":{"_uuid":"bb488ea2-44d9-4fd4-8708-ac26b33c730a","_cell_guid":"b026875b-c132-46ad-bd37-714480a3eecc","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:04.653635Z","iopub.execute_input":"2024-10-05T11:15:04.654275Z","iopub.status.idle":"2024-10-05T11:15:05.638062Z","shell.execute_reply.started":"2024-10-05T11:15:04.654234Z","shell.execute_reply":"2024-10-05T11:15:05.637127Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer   = AutoTokenizer.from_pretrained(embed_model_pth, trust_remote_code=True)\nembed_model = AutoModel.from_pretrained(embed_model_pth, trust_remote_code=True).to(\"cuda:0\")","metadata":{"_uuid":"67f121a8-262f-4d86-ab35-7c7d044b4131","_cell_guid":"f0141914-5a34-4e31-be1a-a48fa39d1710","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:06.811666Z","iopub.execute_input":"2024-10-05T11:15:06.813353Z","iopub.status.idle":"2024-10-05T11:15:23.174912Z","shell.execute_reply.started":"2024-10-05T11:15:06.813177Z","shell.execute_reply":"2024-10-05T11:15:23.174033Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_embeddings(texts, batch_size=8):\n    all_embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024).to('cuda:0')\n        with torch.no_grad():\n            outputs = embed_model(**inputs)\n        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        all_embeddings.append(embeddings.cpu().numpy())\n        \n    return np.concatenate(all_embeddings, axis=0)","metadata":{"_uuid":"205d3c34-9fcf-4d61-a946-72c9ffd24bf9","_cell_guid":"0b6b47b0-b593-4214-8dd4-abcd4b09389c","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:23.176641Z","iopub.execute_input":"2024-10-05T11:15:23.176974Z","iopub.status.idle":"2024-10-05T11:15:23.184249Z","shell.execute_reply.started":"2024-10-05T11:15:23.17694Z","shell.execute_reply":"2024-10-05T11:15:23.183069Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_ctx_vector  = generate_embeddings(list(misconceptions.MisconceptionName.values))\n\nall_ctx_vector.shape","metadata":{"_uuid":"be5bc847-9c5c-4235-a855-f03ea8a1e7cb","_cell_guid":"99f37563-fba1-42e7-927d-b5ce07c4d4f3","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:23.185445Z","iopub.execute_input":"2024-10-05T11:15:23.185928Z","iopub.status.idle":"2024-10-05T11:15:37.938086Z","shell.execute_reply.started":"2024-10-05T11:15:23.185879Z","shell.execute_reply":"2024-10-05T11:15:37.937253Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_results = []\n\nfor results in tqdm(pd.DataFrame(submission.InferredMisconception.values.tolist()).T.values):\n    all_text_vector = generate_embeddings(list(results))\n    cosine_similarities = cosine_similarity(all_text_vector, all_ctx_vector)\n    test_sorted_indices = np.argsort(-cosine_similarities, axis=1)\n    n_results.append(test_sorted_indices)\n\nn_results = np.array(n_results)\nn_results.shape","metadata":{"_uuid":"75086bbc-4c8a-4b26-834c-4f507402c699","_cell_guid":"9cbd2e70-d9cb-44e0-9639-2a47349dde42","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:37.940437Z","iopub.execute_input":"2024-10-05T11:15:37.941167Z","iopub.status.idle":"2024-10-05T11:15:38.981268Z","shell.execute_reply.started":"2024-10-05T11:15:37.941112Z","shell.execute_reply":"2024-10-05T11:15:38.980146Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_results = np.transpose(n_results, (1, 0, 2))\nn_results.shape","metadata":{"_uuid":"dc8297b3-b1d7-45d0-9b67-2f122991283f","_cell_guid":"f304d958-0a24-4768-afba-4312e51020a5","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:38.982788Z","iopub.execute_input":"2024-10-05T11:15:38.984Z","iopub.status.idle":"2024-10-05T11:15:38.991717Z","shell.execute_reply.started":"2024-10-05T11:15:38.98395Z","shell.execute_reply":"2024-10-05T11:15:38.990563Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine ranking of each generated output for each question\n\nBorda count is a very simple ranking mechanism","metadata":{}},{"cell_type":"code","source":"def borda_count(rankings):\n    scores = {}\n    num_elements = len(next(iter(rankings)))\n    \n    for model_ranking in rankings:\n        for idx, item in enumerate(model_ranking):\n            points = num_elements - idx\n            scores[item] = scores.get(item, 0) + points\n            \n    # Sort the misconceptions based on total points\n    final_ranking = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    ranked_results = [r for r, score in final_ranking]\n    return ranked_results\n\n# Compute the final ranking\nfinal_rankings = np.array([borda_count(result) for result in n_results])\n\nfinal_rankings.shape","metadata":{"_uuid":"3b7bf00b-4c34-4ce6-9fce-d598c3a048e0","_cell_guid":"b1d9184a-055b-461e-9d54-6c1ad989d352","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:44.895153Z","iopub.execute_input":"2024-10-05T11:15:44.895929Z","iopub.status.idle":"2024-10-05T11:15:45.06506Z","shell.execute_reply.started":"2024-10-05T11:15:44.895887Z","shell.execute_reply":"2024-10-05T11:15:45.064097Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['MisconceptionId'] = final_rankings[:, :25].tolist()","metadata":{"_uuid":"2877b81b-2b3c-4694-a818-cdd40a111b2d","_cell_guid":"1249b3ad-8295-44a4-b8b9-3c4ec09cd62b","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:45.117098Z","iopub.execute_input":"2024-10-05T11:15:45.117434Z","iopub.status.idle":"2024-10-05T11:15:45.122366Z","shell.execute_reply.started":"2024-10-05T11:15:45.117395Z","shell.execute_reply":"2024-10-05T11:15:45.121375Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit :)","metadata":{}},{"cell_type":"code","source":"if train_eval:\n    submission['apk@25'] = submission.apply(lambda row: apk(row['MisconceptionIdGT'], row['MisconceptionId']), axis=1)\n    submission.to_csv('submission_debug.csv', index=False)\n    \n    print(submission['apk@25'].mean())","metadata":{"_uuid":"9ca8d18d-9f35-4160-9f79-d727e952f95d","_cell_guid":"93309060-9185-4a2e-8fc7-2ec398934160","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:45.493097Z","iopub.execute_input":"2024-10-05T11:15:45.49344Z","iopub.status.idle":"2024-10-05T11:15:45.498559Z","shell.execute_reply.started":"2024-10-05T11:15:45.493403Z","shell.execute_reply":"2024-10-05T11:15:45.497622Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"MisconceptionId\"] = submission[\"MisconceptionId\"].apply(lambda x: ' '.join(map(str, x)))\nsubmission[['QuestionId_Answer', 'MisconceptionId']].to_csv('submission.csv', index=False)","metadata":{"_uuid":"ea1b9622-0511-4912-bd58-3576a1195a1a","_cell_guid":"deb71ff1-2e1d-472e-be47-f92f4e8c5da6","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:46.078517Z","iopub.execute_input":"2024-10-05T11:15:46.078897Z","iopub.status.idle":"2024-10-05T11:15:46.103085Z","shell.execute_reply.started":"2024-10-05T11:15:46.07886Z","shell.execute_reply":"2024-10-05T11:15:46.102352Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(25)","metadata":{"_uuid":"6ed03390-4a94-43e5-bf53-74a9394ef1c0","_cell_guid":"b0e78b75-b055-4c5d-8b87-be54c4ebcb8f","collapsed":false,"execution":{"iopub.status.busy":"2024-10-05T11:15:46.458192Z","iopub.execute_input":"2024-10-05T11:15:46.459087Z","iopub.status.idle":"2024-10-05T11:15:46.47293Z","shell.execute_reply.started":"2024-10-05T11:15:46.459048Z","shell.execute_reply":"2024-10-05T11:15:46.471969Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}