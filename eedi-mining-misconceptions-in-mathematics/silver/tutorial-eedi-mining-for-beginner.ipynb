{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Aim of competition : \n\n* Predict the affinity between misconceptions and incorrect answers (distractors) in multiple-choice questions","metadata":{}},{"cell_type":"markdown","source":"* Before start, please use ml canvas to know the directions of the project ","metadata":{}},{"cell_type":"markdown","source":"# Install Library Gensim","metadata":{}},{"cell_type":"code","source":"%%time\n\n!pip install gensim","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:42:56.645561Z","iopub.execute_input":"2024-09-16T00:42:56.645946Z","iopub.status.idle":"2024-09-16T00:43:19.636774Z","shell.execute_reply.started":"2024-09-16T00:42:56.645913Z","shell.execute_reply":"2024-09-16T00:43:19.635592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Files","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\ntest = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\nmisconception = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')\nsample_submission = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-16T00:43:19.639194Z","iopub.execute_input":"2024-09-16T00:43:19.639738Z","iopub.status.idle":"2024-09-16T00:43:20.079524Z","shell.execute_reply.started":"2024-09-16T00:43:19.639691Z","shell.execute_reply":"2024-09-16T00:43:20.075775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Columns","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Defining the columns to keep for each dataset\ntrain_columns = ['QuestionId', 'QuestionText', 'AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\ntest_columns = ['QuestionId', 'QuestionText', 'AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n\n# Dropping other columns from train and test datasets\ntrain_filtered = train[train_columns]\ntest_filtered = test[test_columns]\n\ntrain_filtered\ntest_filtered","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:20.080896Z","iopub.execute_input":"2024-09-16T00:43:20.081155Z","iopub.status.idle":"2024-09-16T00:43:20.120579Z","shell.execute_reply.started":"2024-09-16T00:43:20.081125Z","shell.execute_reply":"2024-09-16T00:43:20.119465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Missing Values","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Checking for missing values in the combined dataset\nmissing_values = train_filtered.isnull().sum()\n\nmissing_values","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:20.123474Z","iopub.execute_input":"2024-09-16T00:43:20.124106Z","iopub.status.idle":"2024-09-16T00:43:20.139044Z","shell.execute_reply.started":"2024-09-16T00:43:20.124046Z","shell.execute_reply":"2024-09-16T00:43:20.137562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Text Except Formula / Format Mathematics","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport re\n\n# Function to clean text but keep mathematical symbols and formatting\ndef clean_text(text):\n    # Remove unnecessary symbols but retain mathematical expressions\n    cleaned_text = re.sub(r'[^A-Za-z0-9\\s\\(\\)\\[\\]\\+\\-\\*/\\\\]', '', text)\n    return cleaned_text\n\n# Apply this function to the Answer and Question text columns\ntrain_filtered['QuestionText_Clean'] = train_filtered['QuestionText'].apply(lambda x: clean_text(x))\ntrain_filtered['AnswerAText_Clean'] = train_filtered['AnswerAText'].apply(lambda x: clean_text(x))\ntrain_filtered['AnswerBText_Clean'] = train_filtered['AnswerBText'].apply(lambda x: clean_text(x))\ntrain_filtered['AnswerCText_Clean'] = train_filtered['AnswerCText'].apply(lambda x: clean_text(x))\ntrain_filtered['AnswerDText_Clean'] = train_filtered['AnswerDText'].apply(lambda x: clean_text(x))\n\ntrain_filtered[['QuestionText', 'QuestionText_Clean', 'AnswerAText_Clean', 'AnswerBText_Clean', 'AnswerCText_Clean', 'AnswerDText_Clean']].head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:20.140712Z","iopub.execute_input":"2024-09-16T00:43:20.141168Z","iopub.status.idle":"2024-09-16T00:43:20.20282Z","shell.execute_reply.started":"2024-09-16T00:43:20.14111Z","shell.execute_reply":"2024-09-16T00:43:20.201965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Text","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\nimport string\n\n# Instantiate the lemmatizer\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T00:43:20.204162Z","iopub.execute_input":"2024-09-16T00:43:20.204548Z","iopub.status.idle":"2024-09-16T00:43:21.400708Z","shell.execute_reply.started":"2024-09-16T00:43:20.204506Z","shell.execute_reply":"2024-09-16T00:43:21.399797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Download necessary resources from NLTK\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:21.402037Z","iopub.execute_input":"2024-09-16T00:43:21.40279Z","iopub.status.idle":"2024-09-16T00:43:22.967285Z","shell.execute_reply.started":"2024-09-16T00:43:21.402742Z","shell.execute_reply":"2024-09-16T00:43:22.966083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Define stopwords and punctuation to remove\nstop_words = set(stopwords.words('english'))\npunctuation = set(string.punctuation)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:22.96907Z","iopub.execute_input":"2024-09-16T00:43:22.969866Z","iopub.status.idle":"2024-09-16T00:43:22.981167Z","shell.execute_reply.started":"2024-09-16T00:43:22.969817Z","shell.execute_reply":"2024-09-16T00:43:22.98025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Combined function to clean and preprocess the text\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Tokenize the text\n    words = word_tokenize(text)\n    \n    # Remove punctuation and stop words, and lemmatize the words\n    cleaned_words = [\n        lemmatizer.lemmatize(word) \n        for word in words \n        if word not in stop_words and word not in punctuation\n    ]\n    \n    # Join the cleaned words back into a single string\n    cleaned_text = ' '.join(cleaned_words)\n    \n    return cleaned_text\n\n# Applying this function to a column in the dataframe\ntrain_filtered['cleaned_QuestionText'] = train_filtered['QuestionText'].apply(preprocess_text)\ntrain_filtered['cleaned_AnswerAText'] = train_filtered['AnswerAText'].apply(preprocess_text)\ntrain_filtered['cleaned_AnswerBText'] = train_filtered['AnswerBText'].apply(preprocess_text)\ntrain_filtered['cleaned_AnswerCText'] = train_filtered['AnswerCText'].apply(preprocess_text)\ntrain_filtered['cleaned_AnswerDText'] = train_filtered['AnswerDText'].apply(preprocess_text)\n\ntrain_filtered[['QuestionText', 'cleaned_QuestionText', 'cleaned_AnswerAText', 'cleaned_AnswerBText', 'cleaned_AnswerCText', 'cleaned_AnswerDText']].head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:22.982251Z","iopub.execute_input":"2024-09-16T00:43:22.98255Z","iopub.status.idle":"2024-09-16T00:43:27.291974Z","shell.execute_reply.started":"2024-09-16T00:43:22.982519Z","shell.execute_reply":"2024-09-16T00:43:27.291013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Categorical Columns","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Using label encoding for the categorical columns \nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize the LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Identify categorical columns in train and test datasets\ncategorical_columns_train = train_filtered.select_dtypes(include=['object']).columns\ncategorical_columns_test = test_filtered.select_dtypes(include=['object']).columns\n\n# Apply One-Hot Encoding for categorical columns in train and test datasets\ntrain_encoded = pd.get_dummies(train_filtered, columns=categorical_columns_train, drop_first=True)\ntest_encoded = pd.get_dummies(test_filtered, columns=categorical_columns_test, drop_first=True)\n\n# Align columns by adding missing columns to the test or train dataset\ntrain_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n\ntrain_encoded.head(), test_encoded.head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:27.295385Z","iopub.execute_input":"2024-09-16T00:43:27.295728Z","iopub.status.idle":"2024-09-16T00:43:27.48663Z","shell.execute_reply.started":"2024-09-16T00:43:27.295677Z","shell.execute_reply":"2024-09-16T00:43:27.485688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize WordCloud","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Create the corpus using the original 'QuestionText' column after basic cleaning\ncorpus = ' '.join(train_filtered['QuestionText'].astype(str).tolist())\n\n# Generate the word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(corpus)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud for Question Text')\nplt.show()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:27.48782Z","iopub.execute_input":"2024-09-16T00:43:27.488128Z","iopub.status.idle":"2024-09-16T00:43:28.808751Z","shell.execute_reply.started":"2024-09-16T00:43:27.488095Z","shell.execute_reply":"2024-09-16T00:43:28.807855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Count Vocab","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Count the vocabulary (unique words) in the corpus\n\nwords = corpus.split()\nvocabulary_count = len(set(words))\n\nvocabulary_count","metadata":{"execution":{"iopub.status.busy":"2024-09-16T00:43:28.810101Z","iopub.execute_input":"2024-09-16T00:43:28.810933Z","iopub.status.idle":"2024-09-16T00:43:28.828789Z","shell.execute_reply.started":"2024-09-16T00:43:28.810885Z","shell.execute_reply":"2024-09-16T00:43:28.82788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Most Common Words","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom collections import Counter\nimport pandas as pd\n\n# Split the corpus into individual words\nwords = corpus.split()\n\n# Count the frequency of each word\nword_counts = Counter(words)\n\n# Create a DataFrame with the most common words\ncommon_words_df = pd.DataFrame(word_counts.most_common(20), columns=['Word', 'Frequency'])\n\n# Plot the most common words\nplt.figure(figsize=(10, 6))\ncommon_words_df.plot(kind='bar', x='Word', y='Frequency', legend=False)\nplt.title('Most Common Words in Question Text')\nplt.ylabel('Frequency')\nplt.xlabel('Words')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:28.829876Z","iopub.execute_input":"2024-09-16T00:43:28.830209Z","iopub.status.idle":"2024-09-16T00:43:29.351268Z","shell.execute_reply.started":"2024-09-16T00:43:28.830176Z","shell.execute_reply":"2024-09-16T00:43:29.350305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform LDA Topic Modelling","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Prepare the text data for LDA by vectorizing the corpus\nvectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\nX = vectorizer.fit_transform(train_filtered['QuestionText'].astype(str))\n\n# Perform LDA to find 5 topics\nlda = LatentDirichletAllocation(n_components=5, random_state=42)\nlda.fit(X)\n\n# Get the top words for each topic\nn_top_words = 10\nfeature_names = vectorizer.get_feature_names_out()\n\ntopics = []\nfor topic_idx, topic in enumerate(lda.components_):\n    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n    top_features = [feature_names[i] for i in top_features_ind]\n    topics.append(f\"Topic {topic_idx+1}: {' '.join(top_features)}\")\n\ntopics","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:29.35226Z","iopub.execute_input":"2024-09-16T00:43:29.352552Z","iopub.status.idle":"2024-09-16T00:43:32.924425Z","shell.execute_reply.started":"2024-09-16T00:43:29.352521Z","shell.execute_reply":"2024-09-16T00:43:32.92344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create BoW & TF-IDF","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# 'QuestionText' as the text column to convert corpus\ncorpus = train_filtered['QuestionText'].astype(str).tolist()\n\n# Bag of Words (BoW) conversion\nvectorizer_bow = CountVectorizer(max_features=1000)  # You can limit the number of features if needed\nX_bow = vectorizer_bow.fit_transform(corpus)\n\n# TF-IDF conversion\nvectorizer_tfidf = TfidfVectorizer(max_features=1000)\nX_tfidf = vectorizer_tfidf.fit_transform(corpus)\n\n# Display the shape of the resulting matrices\nX_bow.shape, X_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-16T00:43:32.925983Z","iopub.execute_input":"2024-09-16T00:43:32.926625Z","iopub.status.idle":"2024-09-16T00:43:33.070223Z","shell.execute_reply.started":"2024-09-16T00:43:32.926578Z","shell.execute_reply":"2024-09-16T00:43:33.069341Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Tokenized & Vectors","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom gensim.models import Word2Vec\n\n# Tokenize the text \ntokenized_corpus = [question.split() for question in corpus]\n\n# Train a Word2Vec model on the tokenized text\nword2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n\n# Get the vector for a word\nword_vector = word2vec_model.wv['What']\n\nword_vector","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:33.071462Z","iopub.execute_input":"2024-09-16T00:43:33.071836Z","iopub.status.idle":"2024-09-16T00:43:44.489812Z","shell.execute_reply.started":"2024-09-16T00:43:33.071795Z","shell.execute_reply":"2024-09-16T00:43:44.488934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nterms = ['triangle', 'rectangle', 'function', 'equation', 'graph']\n\n# Get word vectors for each term in the list\nword_vectors = {term: word2vec_model.wv[term] for term in terms if term in word2vec_model.wv}\n\nword_vectors","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:44.491152Z","iopub.execute_input":"2024-09-16T00:43:44.491875Z","iopub.status.idle":"2024-09-16T00:43:44.505979Z","shell.execute_reply.started":"2024-09-16T00:43:44.49184Z","shell.execute_reply":"2024-09-16T00:43:44.504808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Convert the entire text into vectors (average word vectors for each document)\ndocument_vectors = []\nfor tokens in tokenized_corpus:\n    vector = sum([word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]) / len(tokens)\n    document_vectors.append(vector)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:44.507054Z","iopub.execute_input":"2024-09-16T00:43:44.507314Z","iopub.status.idle":"2024-09-16T00:43:44.72117Z","shell.execute_reply.started":"2024-09-16T00:43:44.507285Z","shell.execute_reply":"2024-09-16T00:43:44.720375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Means Clustering","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Normalize the data using StandardScaler, setting with_mean=False for sparse data\nscaler = StandardScaler(with_mean=False)\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-Means with an arbitrary number of clusters (e.g., 5 clusters)\nkmeans = KMeans(n_clusters=5, random_state=42)\ntrain_filtered['Cluster'] = kmeans.fit_predict(X_scaled)\n\ntrain_filtered[['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText', 'Cluster']].head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:44.722517Z","iopub.execute_input":"2024-09-16T00:43:44.722892Z","iopub.status.idle":"2024-09-16T00:43:45.132376Z","shell.execute_reply.started":"2024-09-16T00:43:44.722851Z","shell.execute_reply":"2024-09-16T00:43:45.13149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize K-Means Clustering For Answer","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib.pyplot as plt\n\n# Reduce the dimensionality of the data to 2D for visualization using TruncatedSVD\nsvd = TruncatedSVD(n_components=2)\nX_svd = svd.fit_transform(X_scaled)\n\n# Plot the clusters\nplt.scatter(X_svd[:, 0], X_svd[:, 1], c=train_filtered['Cluster'], cmap='viridis', marker='o')\nplt.title('K-Means Clustering of Answer Texts')\nplt.xlabel('SVD Component 1')\nplt.ylabel('SVD Component 2')\nplt.show()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:45.134376Z","iopub.execute_input":"2024-09-16T00:43:45.135211Z","iopub.status.idle":"2024-09-16T00:43:45.478198Z","shell.execute_reply.started":"2024-09-16T00:43:45.135159Z","shell.execute_reply":"2024-09-16T00:43:45.477264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The most similar questions","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\n# Create a TF-IDF vectorizer to convert 'QuestionText' into numerical vectors\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the 'QuestionText' column to get the TF-IDF matrix\nquestion_vectors = vectorizer.fit_transform(train_filtered['QuestionText'].values)\n\n# Find the vector for the first question (query vector)\nquery_vector = question_vectors[0]\n\n# Compute cosine similarities between the query and all other questions\ncosine_similarities = cosine_similarity(query_vector, question_vectors)\n\n# Get the indices of the most similar questions\nmost_similar_indices = cosine_similarities[0].argsort()[::-1][1:6]  # Top 5 most similar\n\n# The most similar questions \nmost_similar_questions = train_filtered.iloc[most_similar_indices][['QuestionText']]\nmost_similar_questions","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T00:43:45.479568Z","iopub.execute_input":"2024-09-16T00:43:45.479948Z","iopub.status.idle":"2024-09-16T00:43:45.558305Z","shell.execute_reply.started":"2024-09-16T00:43:45.479904Z","shell.execute_reply":"2024-09-16T00:43:45.557533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The most similar answers","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\n# Create a TF-IDF vectorizer to convert text into numerical vectors\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the 'AnswerAText' column to get the TF-IDF matrix\nanswer_vectors = vectorizer.fit_transform(train_filtered['AnswerAText'].values)\n\n# Find the vector for the first answer (query vector)\nquery_vector = answer_vectors[0]\n\n# Compute cosine similarities between the query and all other answers\ncosine_similarities = cosine_similarity(query_vector, answer_vectors)\n\n# Get the indices of the most similar answers\nmost_similar_indices = cosine_similarities[0].argsort()[::-1][1:6]  # Top 5 most similar\n\n# The most similar answers\nmost_similar_answers = train_filtered.iloc[most_similar_indices][['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']]\nmost_similar_answers","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-09-16T00:43:45.559254Z","iopub.execute_input":"2024-09-16T00:43:45.559573Z","iopub.status.idle":"2024-09-16T00:43:45.593816Z","shell.execute_reply.started":"2024-09-16T00:43:45.559538Z","shell.execute_reply":"2024-09-16T00:43:45.593011Z"},"trusted":true},"execution_count":null,"outputs":[]}]}