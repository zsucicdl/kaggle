{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":196660105,"sourceType":"kernelVersion"},{"sourceId":196655110,"sourceType":"kernelVersion"},{"sourceId":27644,"sourceType":"modelInstanceVersion","modelInstanceId":23286,"modelId":33601},{"sourceId":113573,"sourceType":"modelInstanceVersion","modelInstanceId":95303,"modelId":119502}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install llama-index llama-index-retrievers-bm25 llama-index-embeddings-huggingface xformers -q","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:44:33.991803Z","iopub.execute_input":"2024-09-14T17:44:33.992694Z","iopub.status.idle":"2024-09-14T17:44:33.99671Z","shell.execute_reply.started":"2024-09-14T17:44:33.992651Z","shell.execute_reply":"2024-09-14T17:44:33.995721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.utils import io\nfrom types import SimpleNamespace\nfrom eedi_metrics import apk, mapk\nfrom IPython.display import display\nfrom typing import Any, Dict, List, Optional, ClassVar\n\nfrom llama_index.retrievers.bm25 import BM25Retriever\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.core.evaluation.retrieval.metrics import *\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, MetadataMode, TextNode\nfrom llama_index.core.evaluation.retrieval.metrics_base import BaseRetrievalMetric, RetrievalMetricResult\nfrom llama_index.core.evaluation import RetrieverEvaluator, EmbeddingQAFinetuneDataset as RetrievalDataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-14T17:44:34.026041Z","iopub.execute_input":"2024-09-14T17:44:34.026311Z","iopub.status.idle":"2024-09-14T17:45:25.248408Z","shell.execute_reply.started":"2024-09-14T17:44:34.026282Z","shell.execute_reply":"2024-09-14T17:45:25.247534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MAP25(BaseRetrievalMetric):\n    \"\"\"\n    Computes the average precision at k\n    \n    Useful resources:\n      - stackoverflow.com/questions/55748792\n      - kaggle.com/code/nandeshwar/mean-average-precision-map-k-metric-explained-code\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"map@25\"\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\" Computes the average precision at k \"\"\"\n        \n        # Checking for the required arguments\n        if retrieved_ids is None or expected_ids is None or not retrieved_ids or not expected_ids:\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n            \n        map25 = apk(expected_ids, retrieved_ids, k=25)\n        \n        return RetrievalMetricResult(score=map25)\n    \n\nclass HybridRetriever(BaseRetriever):\n    \"\"\" Custom retriever \"\"\"\n\n    def __init__(\n        self,\n        bm25_retriever: BM25Retriever,\n        vector_retriever: VectorIndexRetriever,\n    ) -> None:\n        \n        self._bm25_retriever = bm25_retriever\n        self._vector_retriever = vector_retriever\n        \n        super().__init__()\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n\n        bm25_nodes    = self._bm25_retriever.retrieve(query_bundle)\n        vector_nodes  = self._vector_retriever.retrieve(query_bundle)\n\n        bm25_ids      = {n.node.node_id for n in bm25_nodes}\n        vector_ids    = {n.node.node_id for n in vector_nodes}\n\n        combined_dict = {n.node.node_id: n for n in bm25_nodes}\n        combined_dict.update({n.node.node_id: n for n in vector_nodes})\n\n        return list(combined_dict.values())","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:25.25007Z","iopub.execute_input":"2024-09-14T17:45:25.25072Z","iopub.status.idle":"2024-09-14T17:45:25.263988Z","shell.execute_reply.started":"2024-09-14T17:45:25.250685Z","shell.execute_reply":"2024-09-14T17:45:25.261978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = SimpleNamespace(\n    run_eval = True,\n    comp_dir = '/kaggle/input/eedi-mining-misconceptions-in-mathematics',\n    \n    similarity_top_k = 15,\n    metrics = [HitRate(), MRR(), Recall(), AveragePrecision(), NDCG(), MAP25()],\n    \n    embed_batch_size=64,\n    embed_model_pth=\"/kaggle/input/bge-small-en-v1.5/transformers/bge/2\",\n    # embed_model_pth=\"/kaggle/input/stella-en-embedding-400m-v5/transformers/default/1\",\n)\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=config.embed_model_pth, device='cuda:0', \n    embed_batch_size=config.embed_batch_size, trust_remote_code=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:25.265279Z","iopub.execute_input":"2024-09-14T17:45:25.265677Z","iopub.status.idle":"2024-09-14T17:45:29.463573Z","shell.execute_reply.started":"2024-09-14T17:45:25.265629Z","shell.execute_reply":"2024-09-14T17:45:29.462685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test           = pd.read_csv(f'{config.comp_dir}/test.csv')\ntrain          = pd.read_csv(f'{config.comp_dir}/train.csv')\ns_submission   = pd.read_csv(f'{config.comp_dir}/sample_submission.csv')\nmisconceptions = pd.read_csv(f'{config.comp_dir}/misconception_mapping.csv')\n\ntest[\"AllQuestionText\"]  = test[\"SubjectName\"]  + \"\\n\\n\" + test[\"ConstructName\"] + \"\\n\\n\" + test[\"QuestionText\"]\ntrain[\"AllQuestionText\"] = train[\"SubjectName\"] + \"\\n\\n\" + train[\"ConstructName\"] + \"\\n\\n\" + train[\"QuestionText\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:29.466052Z","iopub.execute_input":"2024-09-14T17:45:29.466381Z","iopub.status.idle":"2024-09-14T17:45:29.545165Z","shell.execute_reply.started":"2024-09-14T17:45:29.46633Z","shell.execute_reply":"2024-09-14T17:45:29.544181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misconception_nodes = [TextNode(text=name, id_=f\"{id_}\") for id_, name in misconceptions.values]\n\nwith io.capture_output() as captured:\n    \n    # BM25 Retriever\n    bm25_retriever = BM25Retriever.from_defaults(\n        nodes=misconception_nodes,\n        similarity_top_k=config.similarity_top_k,\n    )\n    \n    # Embedding Retriever\n    semantic_retriever = VectorStoreIndex(\n        nodes=misconception_nodes\n    ).as_retriever(similarity_top_k=config.similarity_top_k)\n    \n    hybrid_retriever = HybridRetriever(bm25_retriever=bm25_retriever, vector_retriever=semantic_retriever)\n\n    \nlen(misconception_nodes)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:29.546598Z","iopub.execute_input":"2024-09-14T17:45:29.54728Z","iopub.status.idle":"2024-09-14T17:45:36.952929Z","shell.execute_reply.started":"2024-09-14T17:45:29.547234Z","shell.execute_reply":"2024-09-14T17:45:36.95195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keep_cols           = [\"QuestionId\", \"AllQuestionText\", \"CorrectAnswer\"]\nanswer_cols         = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"]\nmisconception_cols  = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n\ndef wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n    # Melt the answer columns\n    answers_df = pd.melt(\n        id_vars=keep_cols,\n        frame=df[keep_cols + answer_cols],\n        var_name='Answer', value_name='Value'\n    ).sort_values([\"QuestionId\", \"Answer\"]).reset_index(drop=True)\n    \n    # If NOT test set\n    if misconception_cols[0] in df.columns:\n        \n        # Melt the misconception columns\n        misconceptions_df = pd.melt(\n            id_vars=keep_cols,\n            frame=df[keep_cols + misconception_cols],\n            var_name='Misconception', value_name='MisconceptionId'\n        ).sort_values([\"QuestionId\", \"Misconception\"]).reset_index(drop=True)\n\n        answers_df[['Misconception', 'MisconceptionId']] = misconceptions_df[['Misconception', 'MisconceptionId']]\n    \n    return answers_df\n\ntest = wide_to_long(test)\ntrain = wide_to_long(train)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:36.954152Z","iopub.execute_input":"2024-09-14T17:45:36.954508Z","iopub.status.idle":"2024-09-14T17:45:37.010359Z","shell.execute_reply.started":"2024-09-14T17:45:36.954473Z","shell.execute_reply":"2024-09-14T17:45:37.009592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"AllText\"]  = test[\"AllQuestionText\"]  + \"\\n\\n\" + test[\"Value\"]\ntrain[\"AllText\"] = train[\"AllQuestionText\"] + \"\\n\\n\" + train[\"Value\"]\n\ntest['AnswerId'] = test.Answer.str.replace('Answer', '').str.replace('Text', '')\ntrain['AnswerId'] = train.Answer.str.replace('Answer', '').str.replace('Text', '')\n\ntest.drop(['AllQuestionText', 'Answer'], axis=1, inplace=True)\ntrain.drop(['AllQuestionText', 'Answer', 'Misconception'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:37.011491Z","iopub.execute_input":"2024-09-14T17:45:37.011766Z","iopub.status.idle":"2024-09-14T17:45:37.03438Z","shell.execute_reply.started":"2024-09-14T17:45:37.011735Z","shell.execute_reply":"2024-09-14T17:45:37.033412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dropna().head()","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:37.035568Z","iopub.execute_input":"2024-09-14T17:45:37.035958Z","iopub.status.idle":"2024-09-14T17:45:37.082187Z","shell.execute_reply.started":"2024-09-14T17:45:37.035914Z","shell.execute_reply":"2024-09-14T17:45:37.081304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"queries, relevant_docs = {}, {}\ncorpus = { node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in misconception_nodes }\n\nfor q_id, m_id, all_text in train[['QuestionId', 'MisconceptionId', 'AllText']].dropna().values:\n    question_id = f'{q_id}'\n    queries[question_id] = all_text\n    relevant_docs[question_id] = [f'{int(m_id)}']\n    \nr_dataset = RetrievalDataset(queries=queries, corpus=corpus, relevant_docs=relevant_docs)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:37.083285Z","iopub.execute_input":"2024-09-14T17:45:37.083616Z","iopub.status.idle":"2024-09-14T17:45:37.109232Z","shell.execute_reply.started":"2024-09-14T17:45:37.083585Z","shell.execute_reply":"2024-09-14T17:45:37.108543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_results(name, eval_results):\n    \"\"\" Display results from retrieval evaluation \"\"\"\n\n    metric_names = [m.metric_name for m in config.metrics]\n    results = [eval_result.metric_vals_dict for eval_result in eval_results]\n    return pd.DataFrame({ \"desc\": [name], **{k: [pd.DataFrame(results)[k].mean()] for k in metric_names} })","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:37.111713Z","iopub.execute_input":"2024-09-14T17:45:37.112018Z","iopub.status.idle":"2024-09-14T17:45:37.117642Z","shell.execute_reply.started":"2024-09-14T17:45:37.111975Z","shell.execute_reply":"2024-09-14T17:45:37.116573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever_evaluator = RetrieverEvaluator(\n    retriever=hybrid_retriever, metrics=config.metrics\n)\n\nif config.run_eval and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    with io.capture_output():\n        eval_results = await retriever_evaluator.aevaluate_dataset(r_dataset, workers=4)\n\n    display(display_results(\"hybrid eval\", eval_results))","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:45:37.118663Z","iopub.execute_input":"2024-09-14T17:45:37.118978Z","iopub.status.idle":"2024-09-14T17:50:43.669442Z","shell.execute_reply.started":"2024-09-14T17:45:37.118945Z","shell.execute_reply":"2024-09-14T17:50:43.668556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\n\nfor q_id, a, all_text, a_id in test[['QuestionId', 'CorrectAnswer', 'AllText', 'AnswerId']].values:\n    if a.strip() == a_id.strip(): continue\n    \n    with io.capture_output():\n        ids = \" \".join([node.id_ for node in hybrid_retriever.retrieve(all_text)][:25]) # Only the first 25\n    \n    data.append([f'{q_id}_{a_id}', ids])","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:50:43.670838Z","iopub.execute_input":"2024-09-14T17:50:43.671234Z","iopub.status.idle":"2024-09-14T17:50:45.102786Z","shell.execute_reply.started":"2024-09-14T17:50:43.671189Z","shell.execute_reply":"2024-09-14T17:50:45.102022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(data, columns=s_submission.columns)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:50:45.104076Z","iopub.execute_input":"2024-09-14T17:50:45.104522Z","iopub.status.idle":"2024-09-14T17:50:45.121684Z","shell.execute_reply.started":"2024-09-14T17:50:45.104475Z","shell.execute_reply":"2024-09-14T17:50:45.120761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:50:45.122747Z","iopub.execute_input":"2024-09-14T17:50:45.123024Z","iopub.status.idle":"2024-09-14T17:50:45.13343Z","shell.execute_reply.started":"2024-09-14T17:50:45.122994Z","shell.execute_reply":"2024-09-14T17:50:45.13253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}