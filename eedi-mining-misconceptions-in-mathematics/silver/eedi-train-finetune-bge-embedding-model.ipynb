{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetune Embeddings\n\nIn this notebook, we finetune the `bge-large-en-v1.5` model. A lot of code and text excerpts here were taken from the [official documentation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune)","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/FlagOpen/FlagEmbedding.git faiss-gpu -q","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:42:41.05169Z","iopub.execute_input":"2024-09-21T09:42:41.052073Z","iopub.status.idle":"2024-09-21T09:43:06.237515Z","shell.execute_reply.started":"2024-09-21T09:42:41.052037Z","shell.execute_reply":"2024-09-21T09:43:06.236282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, re, json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer\n\ntqdm.pandas()\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-21T09:43:32.646082Z","iopub.execute_input":"2024-09-21T09:43:32.646541Z","iopub.status.idle":"2024-09-21T09:43:32.653298Z","shell.execute_reply.started":"2024-09-21T09:43:32.6465Z","shell.execute_reply":"2024-09-21T09:43:32.65229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = 'BAAI/bge-large-en-v1.5'\n\ncomp_dir = '/kaggle/input/eedi-mining-misconceptions-in-mathematics'","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:32.848706Z","iopub.execute_input":"2024-09-21T09:43:32.84913Z","iopub.status.idle":"2024-09-21T09:43:32.854035Z","shell.execute_reply.started":"2024-09-21T09:43:32.849088Z","shell.execute_reply":"2024-09-21T09:43:32.852988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train          = pd.read_csv(f'{comp_dir}/train.csv')\nmisconceptions = pd.read_csv(f'{comp_dir}/misconception_mapping.csv')\n\ntrain[\"AllQuestionText\"] = train[\"SubjectName\"] + \"\\n\\n\" + train[\"ConstructName\"] + \"\\n\\n\" + train[\"QuestionText\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:33.099306Z","iopub.execute_input":"2024-09-21T09:43:33.100541Z","iopub.status.idle":"2024-09-21T09:43:33.137625Z","shell.execute_reply.started":"2024-09-21T09:43:33.100495Z","shell.execute_reply":"2024-09-21T09:43:33.136709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keep_cols           = [\"QuestionId\", \"AllQuestionText\", \"CorrectAnswer\"]\nanswer_cols         = [\"AnswerAText\", \"AnswerBText\", \"AnswerCText\", \"AnswerDText\"]\nmisconception_cols  = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n\ndef wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n    # Melt the answer columns\n    answers_df = pd.melt(\n        id_vars=keep_cols,\n        frame=df[keep_cols + answer_cols],\n        var_name='Answer', value_name='Value'\n    ).sort_values([\"QuestionId\", \"Answer\"]).reset_index(drop=True)\n    \n    # If NOT test set\n    if misconception_cols[0] in df.columns:\n        \n        # Melt the misconception columns\n        misconceptions_df = pd.melt(\n            id_vars=keep_cols,\n            frame=df[keep_cols + misconception_cols],\n            var_name='Misconception', value_name='MisconceptionId'\n        ).sort_values([\"QuestionId\", \"Misconception\"]).reset_index(drop=True)\n\n        answers_df[['Misconception', 'MisconceptionId']] = misconceptions_df[['Misconception', 'MisconceptionId']]\n    \n    return answers_df\n\ntrain = wide_to_long(train)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:34.665383Z","iopub.execute_input":"2024-09-21T09:43:34.665862Z","iopub.status.idle":"2024-09-21T09:43:34.6965Z","shell.execute_reply.started":"2024-09-21T09:43:34.66582Z","shell.execute_reply":"2024-09-21T09:43:34.695404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/pshikk/similarity-preprocessing\n\ndef preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(\"@\\w+\", '',x)      # Delete strings starting with @\n    x = re.sub(\"'\\d+\", '',x)      # Delete Numbers\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\s+\", \" \", x)    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:35.501814Z","iopub.execute_input":"2024-09-21T09:43:35.502768Z","iopub.status.idle":"2024-09-21T09:43:35.50972Z","shell.execute_reply.started":"2024-09-21T09:43:35.502725Z","shell.execute_reply":"2024-09-21T09:43:35.508599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"AllText\"] = train[\"AllQuestionText\"] + \"\\n\\n\" + train[\"Value\"]\ntrain['AnswerId'] = train.Answer.str.replace('Answer', '').str.replace('Text', '')\n\ntrain = train[train.AnswerId != train.CorrectAnswer].reset_index(drop=True)\ntrain.drop(['AllQuestionText', 'Answer', 'Misconception'], axis=1, inplace=True)\n\ntrain = pd.merge(train, misconceptions, on='MisconceptionId', how='left')\n\ntrain = train.dropna()\n\ntrain[\"AllText\"] = train[\"AllText\"].apply(preprocess_text)\ntrain[\"MisconceptionName\"] = train[\"MisconceptionName\"].apply(preprocess_text)\n\nlen(train)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:36.57421Z","iopub.execute_input":"2024-09-21T09:43:36.57463Z","iopub.status.idle":"2024-09-21T09:43:36.957666Z","shell.execute_reply.started":"2024-09-21T09:43:36.574591Z","shell.execute_reply":"2024-09-21T09:43:36.956719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:37.233603Z","iopub.execute_input":"2024-09-21T09:43:37.234381Z","iopub.status.idle":"2024-09-21T09:43:37.248543Z","shell.execute_reply.started":"2024-09-21T09:43:37.234336Z","shell.execute_reply":"2024-09-21T09:43:37.247598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer optimal max_lengths for query and context\n\nLooking at the grphs, 256 for query and 64 for misconceptions seem like a good limit","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\n\nall_texts_len = train['AllText'].progress_apply(lambda x: len(tokenizer(x)['input_ids']))\nmisconceptions_len = misconceptions['MisconceptionName'].progress_apply(lambda x: len(tokenizer(x)['input_ids']))\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\n_ = all_texts_len.sort_values().reset_index(drop=True).plot.line(ax=ax1)\n_ = misconceptions_len.sort_values().reset_index(drop=True).plot.line(ax=ax2)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:40.68167Z","iopub.execute_input":"2024-09-21T09:43:40.682128Z","iopub.status.idle":"2024-09-21T09:43:43.664589Z","shell.execute_reply.started":"2024-09-21T09:43:40.682081Z","shell.execute_reply":"2024-09-21T09:43:43.663538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data format\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str]}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts.\nIf you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune/toy_finetune_data.jsonl) for a toy data file.","metadata":{}},{"cell_type":"code","source":"pretrain_data = [{'text': preprocess_text(misconception)} for misconception in list(misconceptions.MisconceptionName.values)]\n\nfinetune_data = [\n    {\n        'query': query.strip(),\n        'pos': [misconception.strip()],\n        'neg': []    # Leave empty, to be populated by hard mining algorithm below\n    } for query, misconception in train[['AllText', 'MisconceptionName']].values\n]\n\nlen(pretrain_data), len(finetune_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:57.096606Z","iopub.execute_input":"2024-09-21T09:43:57.097029Z","iopub.status.idle":"2024-09-21T09:43:57.307573Z","shell.execute_reply.started":"2024-09-21T09:43:57.096991Z","shell.execute_reply":"2024-09-21T09:43:57.306522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finetune_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:43:57.355715Z","iopub.execute_input":"2024-09-21T09:43:57.356434Z","iopub.status.idle":"2024-09-21T09:43:57.363281Z","shell.execute_reply.started":"2024-09-21T09:43:57.356376Z","shell.execute_reply":"2024-09-21T09:43:57.362228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('pretrain_data.jsonl', 'w') as f:\n    for entry in pretrain_data:\n        json.dump(entry, f)\n        f.write('\\n')\n        \nwith open('finetune_data.jsonl', 'w') as f:\n    for entry in finetune_data:\n        json.dump(entry, f)\n        f.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:44:00.913847Z","iopub.execute_input":"2024-09-21T09:44:00.914212Z","iopub.status.idle":"2024-09-21T09:44:01.057591Z","shell.execute_reply.started":"2024-09-21T09:44:00.914179Z","shell.execute_reply":"2024-09-21T09:44:01.056723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hard Negatives\nHard negatives is a widely used method to improve the quality of sentence embedding. You can mine hard negatives following this command:\n\n- `input_file`: json data for finetuning. This script will retrieve top-k documents for each query, \nand random sample negatives from the top-k documents (not including the positive documents).\n- `output_file`: path to save JSON data with mined hard negatives for finetuning\n- `negative_number`: the number of sampled negatives \n- `range_for_sampling`: where to sample negative. For example, `2-100` means sampling `negative_number` negatives from top2-top200 documents. **You can set larger value to reduce the difficulty of negatives (e.g., set it `60-300` to sample negatives from top60-300 passages)**\n- `candidate_pool`: The pool to retrieval. The default value is None, and this script will retrieve from the combination of all `neg` in `input_file`. \nThe format of this file is the same as [pretrain data](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain#2-data-format). If input a candidate_pool, this script will retrieve negatives from this file.\n- `use_gpu_for_searching`: whether to use faiss-gpu to retrieve negatives.\n","metadata":{}},{"cell_type":"code","source":"!python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\n    --model_name_or_path {model_id} \\\n    --input_file finetune_data.jsonl \\\n    --candidate_pool pretrain_data.jsonl \\\n    --output_file finetune_data_minedHN.jsonl \\\n    --range_for_sampling 1-100 \\\n    --negative_number 15 \\\n    --use_gpu_for_searching\n\n\nfinetune_data_minedHN = []\nwith open('finetune_data_minedHN.jsonl', 'r') as file:\n    for line in file:\n        finetune_data_minedHN.append(json.loads(line))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:46:02.16888Z","iopub.execute_input":"2024-09-21T09:46:02.169581Z","iopub.status.idle":"2024-09-21T09:46:40.643712Z","shell.execute_reply.started":"2024-09-21T09:46:02.169531Z","shell.execute_reply":"2024-09-21T09:46:40.642472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finetune_data_minedHN[0]    # negs are now populated","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:46:40.645941Z","iopub.execute_input":"2024-09-21T09:46:40.646359Z","iopub.status.idle":"2024-09-21T09:46:40.654304Z","shell.execute_reply.started":"2024-09-21T09:46:40.646302Z","shell.execute_reply":"2024-09-21T09:46:40.653352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train\n\n**some important arguments**:\n- `per_device_train_batch_size`: batch size in training. In most of cases, larger batch size will bring stronger performance. You can expand it by enabling `--fp16`, `--deepspeed ./df_config.json` (df_config.json can refer to [ds_config.json](./ds_config.json)), `--gradient_checkpointing`, etc. \n- `train_group_size`: the number of positive and negatives for a query in training.\nThere are always one positive, so this argument will control the number of negatives (#negatives=train_group_size-1).\nNoted that the number of negatives should not be larger than the numbers of negatives in data `\"neg\":List[str]`.\nBesides the negatives in this group, the in-batch negatives also will be used in fine-tuning.\n- `negatives_cross_device`: share the negatives across all GPUs. This argument will extend the number of negatives.\n- `learning_rate`: select a appropriate for your model. Recommend 1e-5/2e-5/3e-5 for large/base/small-scale. \n- `temperature`: It will influence the distribution of similarity scores. **Recommended value: 0.01-0.1.**\n- `query_max_len`: max length for query. Please set it according the average length of queries in your data.\n- `passage_max_len`: max length for passage. Please set it according the average length of passages in your data.\n- `query_instruction_for_retrieval`: instruction for query, which will be added to each query. You also can set it `\"\"` to add nothing to query.\n- `use_inbatch_neg`: use passages in the same batch as negatives. Default value is True. \n- `save_steps`: for setting how many training steps to save a checkpoint.\n\nFor more training arguments please refer to [transformers.TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)","metadata":{}},{"cell_type":"code","source":"!torchrun --nproc_per_node 2 -m FlagEmbedding.baai_general_embedding.finetune.run \\\n    --output_dir eedi_model \\\n    --model_name_or_path {model_id} \\\n    --train_data finetune_data_minedHN.jsonl \\\n    --learning_rate 1e-5 \\\n    --fp16 \\\n    --temperature 0.03 \\\n    --num_train_epochs 3 \\\n    --per_device_train_batch_size 8 \\\n    --query_max_len 256 \\\n    --passage_max_len 64 \\\n    --logging_steps 100 \\\n    --query_instruction_for_retrieval \"\" \\\n    --report_to none \\\n    --save_steps 250","metadata":{"execution":{"iopub.status.busy":"2024-09-21T09:52:07.984219Z","iopub.execute_input":"2024-09-21T09:52:07.98468Z","iopub.status.idle":"2024-09-21T09:55:08.777004Z","shell.execute_reply.started":"2024-09-21T09:52:07.984637Z","shell.execute_reply":"2024-09-21T09:55:08.775771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}