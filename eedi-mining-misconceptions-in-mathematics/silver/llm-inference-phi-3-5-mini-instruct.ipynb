{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":118141,"sourceType":"modelInstanceVersion","modelInstanceId":99348,"modelId":123513},{"sourceId":64394,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":53694,"modelId":73698},{"sourceId":27644,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":23286,"modelId":33601}],"dockerImageVersionId":30776,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-24T08:32:25.795535Z","iopub.execute_input":"2024-09-24T08:32:25.795829Z","iopub.status.idle":"2024-09-24T08:32:27.694094Z","shell.execute_reply.started":"2024-09-24T08:32:25.79579Z","shell.execute_reply":"2024-09-24T08:32:27.692944Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\ntest_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:32:45.639657Z","iopub.execute_input":"2024-09-24T08:32:45.640195Z","iopub.status.idle":"2024-09-24T08:32:45.709082Z","shell.execute_reply.started":"2024-09-24T08:32:45.640156Z","shell.execute_reply":"2024-09-24T08:32:45.708124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misconception_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:32:46.573322Z","iopub.execute_input":"2024-09-24T08:32:46.574105Z","iopub.status.idle":"2024-09-24T08:32:46.595177Z","shell.execute_reply.started":"2024-09-24T08:32:46.574063Z","shell.execute_reply":"2024-09-24T08:32:46.594481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleanTrainData(df):\n\n    options = ['A','B','C','D']\n    originalCols = list(df.columns)\n\n    newCols = originalCols\n    newCols.append('AnswerText')\n    newCols.append('MisconceptionId')\n    \n    \n    newDf = pd.DataFrame(columns=newCols)\n\n    for i in range(df.shape[0]):\n    \n        for option in options:\n            new_row = df.iloc[i,:].copy()\n            new_row['QuestionId'] = f\"{new_row['QuestionId']}_{option}\"\n            new_row['AnswerText'] = new_row[f\"Answer{option}Text\"]\n            new_row['MisconceptionId'] = new_row[f\"Misconception{option}Id\"]\n            newDf = pd.concat([newDf, new_row.to_frame().T], ignore_index=True)\n\n    newDf = newDf.drop(['ConstructId','SubjectId','AnswerAText','AnswerBText','AnswerCText','AnswerDText','MisconceptionAId','MisconceptionBId','MisconceptionCId','MisconceptionDId'],axis=1)\n\n    return newDf.dropna()\n            \n            \ndef cleanTestData(df):\n    options = ['A', 'B', 'C', 'D']\n    \n    # Prepare a list to collect new rows\n    new_rows = []\n\n    for i in range(df.shape[0]):\n        for option in options:\n            if df.iloc[i]['CorrectAnswer'] == option:\n                continue\n            \n            new_row = df.iloc[i].copy()\n            new_row['QuestionId'] = f\"{new_row['QuestionId']}_{option}\"\n            new_row['AnswerText'] = new_row[f\"Answer{option}Text\"]\n            new_row['All'] = f\"Describe the misconception in this answer. Question : {new_row['ConstructName']} {new_row['SubjectName']} {new_row['QuestionText']}, Answer: {new_row['AnswerText']}.\"\n            \n            # Append the new_row to the list\n            new_rows.append(new_row)\n\n    # Create a DataFrame from the list of new rows\n    newDf = pd.DataFrame(new_rows)\n\n    # Drop unnecessary columns\n    newDf = newDf.drop(['ConstructId', 'SubjectId', 'AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText'], axis=1)\n\n    return newDf.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:32:47.24704Z","iopub.execute_input":"2024-09-24T08:32:47.247661Z","iopub.status.idle":"2024-09-24T08:32:47.260078Z","shell.execute_reply.started":"2024-09-24T08:32:47.24761Z","shell.execute_reply":"2024-09-24T08:32:47.25923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_df = cleanTrainData(train_df)\nclean_test_df = cleanTestData(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:32:48.377228Z","iopub.execute_input":"2024-09-24T08:32:48.378052Z","iopub.status.idle":"2024-09-24T08:33:03.346839Z","shell.execute_reply.started":"2024-09-24T08:32:48.378012Z","shell.execute_reply":"2024-09-24T08:33:03.34594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_df","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:33:10.387412Z","iopub.execute_input":"2024-09-24T08:33:10.388271Z","iopub.status.idle":"2024-09-24T08:33:10.409633Z","shell.execute_reply.started":"2024-09-24T08:33:10.388233Z","shell.execute_reply":"2024-09-24T08:33:10.408759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_test_df","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:33:11.314443Z","iopub.execute_input":"2024-09-24T08:33:11.314859Z","iopub.status.idle":"2024-09-24T08:33:11.328656Z","shell.execute_reply.started":"2024-09-24T08:33:11.314821Z","shell.execute_reply":"2024-09-24T08:33:11.327663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n\nmodel_id = \"/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\npipe = pipeline(\n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n    max_new_tokens=256, \n    device_map=\"auto\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:33:12.733248Z","iopub.execute_input":"2024-09-24T08:33:12.733626Z","iopub.status.idle":"2024-09-24T08:34:28.783341Z","shell.execute_reply.started":"2024-09-24T08:33:12.733588Z","shell.execute_reply":"2024-09-24T08:34:28.782512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_response(pipeline,message):\n    \n    terminators = [\n        pipeline.tokenizer.eos_token_id,\n        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n    outputs = pipeline(\n        message,\n        max_new_tokens=256,\n        do_sample=False,\n        eos_token_id=terminators,  # Ensure EOS token is used for stopping generation\n        return_full_text=False  # Prevent the model from repeating the input prompt\n    )\n    # Extract and return the generated text\n    return outputs[0][\"generated_text\"].strip()","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:34:40.254941Z","iopub.execute_input":"2024-09-24T08:34:40.255917Z","iopub.status.idle":"2024-09-24T08:34:40.261731Z","shell.execute_reply.started":"2024-09-24T08:34:40.255864Z","shell.execute_reply":"2024-09-24T08:34:40.260757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result = []\n\nfor i in range(clean_test_df.shape[0]):\n    \n    row = clean_test_df.iloc[i,:]\n    prompt = row['All']\n    \n    answer = get_response(pipe,prompt)\n    test_result.append(answer)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:34:41.389788Z","iopub.execute_input":"2024-09-24T08:34:41.390424Z","iopub.status.idle":"2024-09-24T08:38:19.214351Z","shell.execute_reply.started":"2024-09-24T08:34:41.390378Z","shell.execute_reply":"2024-09-24T08:38:19.213531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer_mis = AutoTokenizer.from_pretrained('/kaggle/input/bge-small-en-v1.5/transformers/bge/2')\nmodel_mis = AutoModel.from_pretrained('/kaggle/input/bge-small-en-v1.5/transformers/bge/2').to(device)\nmodel_mis.eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:22.355268Z","iopub.execute_input":"2024-09-24T08:38:22.356142Z","iopub.status.idle":"2024-09-24T08:38:24.491369Z","shell.execute_reply.started":"2024-09-24T08:38:22.356102Z","shell.execute_reply":"2024-09-24T08:38:24.490454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare inputs\ndef prepare_inputs(texts, tokenizer, device):\n    encoded = tokenizer.batch_encode_plus(\n        texts,\n        padding=True,\n        truncation=True,\n        return_tensors='pt',\n        max_length=520\n    )\n    return {key: value.to(device) for key, value in encoded.items()}","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:26.754334Z","iopub.execute_input":"2024-09-24T08:38:26.755257Z","iopub.status.idle":"2024-09-24T08:38:26.760616Z","shell.execute_reply.started":"2024-09-24T08:38:26.755215Z","shell.execute_reply":"2024-09-24T08:38:26.759682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Compute embeddings\ndef compute_embeddings(texts, tokenizer, model, device, batch_size):\n    all_embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i + batch_size]\n        inputs = prepare_inputs(batch_texts, tokenizer, device)\n        with torch.no_grad():\n            embeddings = model(**inputs).last_hidden_state[:, 0]\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        all_embeddings.append(embeddings.cpu().numpy())\n    return np.concatenate(all_embeddings, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:27.619866Z","iopub.execute_input":"2024-09-24T08:38:27.62052Z","iopub.status.idle":"2024-09-24T08:38:27.62741Z","shell.execute_reply.started":"2024-09-24T08:38:27.620468Z","shell.execute_reply":"2024-09-24T08:38:27.626443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misconception = list(misconception_df['MisconceptionName'].values)\n\nembeddings_misconception = compute_embeddings(misconception,tokenizer_mis,model_mis,device,8);\n\n# test = list(clean_test_df['All'].values)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:28.442692Z","iopub.execute_input":"2024-09-24T08:38:28.443424Z","iopub.status.idle":"2024-09-24T08:38:31.987578Z","shell.execute_reply.started":"2024-09-24T08:38:28.443381Z","shell.execute_reply":"2024-09-24T08:38:31.986535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_test = compute_embeddings(test_result,tokenizer_mis,model_mis,device,8);","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:31.989795Z","iopub.execute_input":"2024-09-24T08:38:31.99029Z","iopub.status.idle":"2024-09-24T08:38:32.041233Z","shell.execute_reply.started":"2024-09-24T08:38:31.990241Z","shell.execute_reply":"2024-09-24T08:38:32.04036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cosine_sim = cosine_similarity(embeddings_test,embeddings_misconception)\ntest_sorted_indices = np.argsort(-cosine_sim, axis=1)\ntest_sorted_indices","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:33.146245Z","iopub.execute_input":"2024-09-24T08:38:33.14665Z","iopub.status.idle":"2024-09-24T08:38:33.180022Z","shell.execute_reply.started":"2024-09-24T08:38:33.146612Z","shell.execute_reply":"2024-09-24T08:38:33.177441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_n = 25\n\ntop_misconception_ids = []\n\nfor i in range(cosine_sim.shape[0]):\n    # Get the indices of the top_n highest similarity values\n    top_indices = np.argsort(cosine_sim[i])[::-1][:top_n]  # Sort in descending order and take the top n\n\n    # Get the corresponding MisconceptionIds from your DataFrame\n    top_ids = misconception_df.iloc[top_indices]['MisconceptionId'].tolist()\n    \n    top_misconception_ids.append(top_ids)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:33.876512Z","iopub.execute_input":"2024-09-24T08:38:33.876883Z","iopub.status.idle":"2024-09-24T08:38:33.886288Z","shell.execute_reply.started":"2024-09-24T08:38:33.876846Z","shell.execute_reply":"2024-09-24T08:38:33.885328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(columns=['QuestionId_Answer','MisconceptionId'])\nsubmission_df['QuestionId_Answer'] = clean_test_df['QuestionId']","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:34.784748Z","iopub.execute_input":"2024-09-24T08:38:34.785487Z","iopub.status.idle":"2024-09-24T08:38:34.795279Z","shell.execute_reply.started":"2024-09-24T08:38:34.785433Z","shell.execute_reply":"2024-09-24T08:38:34.794263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(clean_test_df.shape[0]):\n    row = clean_test_df.iloc[i, :]\n    \n    # Retrieve the top 25 closest misconceptions\n    top_25_misconception_ids = top_misconception_ids[i]\n    result = ' '.join(map(str,top_25_misconception_ids))\n    submission_df.loc[submission_df['QuestionId_Answer'] == row['QuestionId'], 'MisconceptionId'] = result","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:36.132335Z","iopub.execute_input":"2024-09-24T08:38:36.133328Z","iopub.status.idle":"2024-09-24T08:38:36.146099Z","shell.execute_reply.started":"2024-09-24T08:38:36.133275Z","shell.execute_reply":"2024-09-24T08:38:36.144967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:36.993505Z","iopub.execute_input":"2024-09-24T08:38:36.994421Z","iopub.status.idle":"2024-09-24T08:38:37.005362Z","shell.execute_reply.started":"2024-09-24T08:38:36.994376Z","shell.execute_reply":"2024-09-24T08:38:37.004091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T08:38:38.266867Z","iopub.execute_input":"2024-09-24T08:38:38.267514Z","iopub.status.idle":"2024-09-24T08:38:38.286749Z","shell.execute_reply.started":"2024-09-24T08:38:38.267462Z","shell.execute_reply":"2024-09-24T08:38:38.285681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}