{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":9463773,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":193175737,"sourceType":"kernelVersion"},{"sourceId":194017231,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, numpy as np\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-24T08:59:27.250884Z","iopub.execute_input":"2024-09-24T08:59:27.251552Z","iopub.status.idle":"2024-09-24T08:59:27.265826Z","shell.execute_reply.started":"2024-09-24T08:59:27.251503Z","shell.execute_reply":"2024-09-24T08:59:27.264453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n!pip install --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:01:12.03692Z","iopub.execute_input":"2024-09-24T09:01:12.037616Z","iopub.status.idle":"2024-09-24T09:04:03.977265Z","shell.execute_reply.started":"2024-09-24T09:01:12.037571Z","shell.execute_reply":"2024-09-24T09:04:03.975978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer\nimport pandas as pd\n\n\ndf_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/hugging-quants-meta-llama-3-1-8b-instruct-awq-int4\")\n\nPROMPT  = \"\"\"Question: {Question}\nIncorrect Answer: {IncorrectAnswer}\nCorrect Answer: {CorrectAnswer}\nConstruct Name: {ConstructName}\nSubject Name: {SubjectName}\n\nYour task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\nBefore answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\"\n\ndef apply_template(row, tokenizer, targetCol):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": PROMPT.format(\n                 ConstructName=row[\"ConstructName\"],\n                 SubjectName=row[\"SubjectName\"],\n                 Question=row[\"QuestionText\"],\n                 IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n                 CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"])\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:04:03.979743Z","iopub.execute_input":"2024-09-24T09:04:03.980777Z","iopub.status.idle":"2024-09-24T09:04:07.794786Z","shell.execute_reply.started":"2024-09-24T09:04:03.980706Z","shell.execute_reply":"2024-09-24T09:04:07.793881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_correct_answer_text(row):\n    if row['CorrectAnswer'] == 'A':\n        return row['AnswerAText']\n    elif row['CorrectAnswer'] == 'B':\n        return row['AnswerBText']\n    elif row['CorrectAnswer'] == 'C':\n        return row['AnswerCText']\n    elif row['CorrectAnswer'] == 'D':\n        return row['AnswerDText']\n    else:\n        return None\n\ndf_train['CorrectAnswerText'] = df_train.apply(get_correct_answer_text, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:08:09.750524Z","iopub.execute_input":"2024-09-24T09:08:09.750932Z","iopub.status.idle":"2024-09-24T09:08:09.802759Z","shell.execute_reply.started":"2024-09-24T09:08:09.750894Z","shell.execute_reply":"2024-09-24T09:08:09.801679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nselect_column = [\"QuestionId\", \"ConstructName\", \"SubjectName\", \"CorrectAnswer\", \"QuestionText\", \"CorrectAnswerText\"]\n\ndf_answer = pd.melt(df_train, \n                    id_vars=select_column,\n                    value_vars=[f\"Answer{ans}Text\" for ans in [\"A\", \"B\", \"C\", \"D\"]],\n                    var_name=\"Option\",\n                    value_name=\"AnswerText\").sort_values(\"QuestionId\")\n\ndf_misconception = pd.melt(df_train, \n                    id_vars=select_column,\n                    value_vars=[f\"Misconception{ans}Id\" for ans in [\"A\", \"B\", \"C\", \"D\"]],\n                    var_name=\"Option\",\n                    value_name=\"Misconception\").sort_values(\"QuestionId\")\n\ndf_answer['Option'] = df_answer['Option'].apply(lambda x: re.search(r'Answer([A-D])', x).group(1) if re.search(r'Answer([A-D])', x) else None)\ndf_misconception['Option'] = df_misconception['Option'].apply(lambda x: re.search(r'Misconception([A-D])', x).group(1) if re.search(r'Misconception([A-D])', x) else None)\n\ndf_merged = pd.merge(df_answer, df_misconception, \n                     on=[\"QuestionId\", \"Option\", \"ConstructName\", \"SubjectName\", \"CorrectAnswer\", \"QuestionText\", \"CorrectAnswerText\"], \n                     how=\"inner\", \n                     suffixes=('', '_y'))\n\n# Drop any extra duplicated columns that were suffixed with '_y'\ndf_merged.drop(df_merged.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:09:55.146671Z","iopub.execute_input":"2024-09-24T09:09:55.147642Z","iopub.status.idle":"2024-09-24T09:09:55.250493Z","shell.execute_reply.started":"2024-09-24T09:09:55.147598Z","shell.execute_reply":"2024-09-24T09:09:55.24937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:09:55.719844Z","iopub.execute_input":"2024-09-24T09:09:55.72023Z","iopub.status.idle":"2024-09-24T09:09:55.739929Z","shell.execute_reply.started":"2024-09-24T09:09:55.720192Z","shell.execute_reply":"2024-09-24T09:09:55.738854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer\n\n\nPROMPT  = \"\"\"Question: {Question}\nIncorrect Answer: {IncorrectAnswer}\nCorrect Answer: {CorrectAnswer}\nConstruct Name: {ConstructName}\nSubject Name: {SubjectName}\n\nYour task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\nBefore answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\"\n\ndef apply_template(row, tokenizer):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": PROMPT.format(\n                ConstructName=row[\"ConstructName\"],\n                SubjectName=row[\"SubjectName\"],\n                Question=row[\"QuestionText\"],\n                IncorrectAnswer=row[\"AnswerText\"],\n                CorrectAnswer=row[\"CorrectAnswerText\"]\n            )\n        }\n    ]\n    \n\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndf_merged[\"Prompt\"] = df_merged.apply(lambda row: apply_template(row, tokenizer), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:09:59.492045Z","iopub.execute_input":"2024-09-24T09:09:59.492439Z","iopub.status.idle":"2024-09-24T09:10:00.228644Z","shell.execute_reply.started":"2024-09-24T09:09:59.4924Z","shell.execute_reply":"2024-09-24T09:10:00.227795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged.to_parquet(\"prompt.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:14:50.743245Z","iopub.execute_input":"2024-09-24T09:14:50.744056Z","iopub.status.idle":"2024-09-24T09:14:50.888688Z","shell.execute_reply.started":"2024-09-24T09:14:50.744012Z","shell.execute_reply":"2024-09-24T09:14:50.887616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport re\nimport vllm\nimport pandas as pd\n\ndf = pd.read_parquet(\"prompt.parquet\")\n\nllm = vllm.LLM(\n    \"/kaggle/input/hugging-quants-meta-llama-3-1-8b-instruct-awq-int4\",\n    quantization=\"awq\",\n    tensor_parallel_size=2, \n    gpu_memory_utilization=0.95, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    max_model_len=8192,\n    disable_log_stats=True\n)\ntokenizer = llm.get_tokenizer()\n\n\nresponses = llm.generate(\n    df[\"Prompt\"].values,\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.\n        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n        temperature=0,  # randomness of the sampling\n        seed=777, # Seed for reprodicibility\n        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n        max_tokens=2048,  # Maximum number of tokens to generate per output sequence.\n    ),\n    use_tqdm = True\n)\n\nresponses = [x.outputs[0].text for x in responses]\ndf[\"FullResponse\"] = responses\n\ndef extract_response(text):\n    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n\nresponses = [extract_response(x) for x in responses]\ndf[\"Misconception\"] = responses\ndf.to_parquet(\"output.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:15:04.957941Z","iopub.execute_input":"2024-09-24T09:15:04.95836Z","iopub.status.idle":"2024-09-24T09:15:04.965901Z","shell.execute_reply.started":"2024-09-24T09:15:04.958321Z","shell.execute_reply":"2024-09-24T09:15:04.96491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run_vllm.py\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T09:16:35.53729Z","iopub.execute_input":"2024-09-24T09:16:35.538143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}