{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":9449548,"sourceType":"datasetVersion","datasetId":5743552},{"sourceId":9600626,"sourceType":"datasetVersion","datasetId":5856973},{"sourceId":9612614,"sourceType":"datasetVersion","datasetId":5744154},{"sourceId":9625498,"sourceType":"datasetVersion","datasetId":5875516},{"sourceId":198206164,"sourceType":"kernelVersion"},{"sourceId":118141,"sourceType":"modelInstanceVersion","modelInstanceId":99348,"modelId":123513}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Thanks to [Rich Olson](https://www.kaggle.com/richolson) for the Phi 3.5 Mini Instruct and [sinchir0](https://www.kaggle.com/sinchir0) for the BGE model.\n\nThis is a baseline hybrid search approach, where-\n\n1) a vector search is being done directly on the dataset question + misconception\n\n2) a keyword search is being done on Phi's response to us asking what the misconception is","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/rank-bm25/rank_bm25-0.2.2-py3-none-any.whl\n!pip install /kaggle/input/sentence-transformers/sentence_transformers-3.2.0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T09:38:53.754518Z","iopub.execute_input":"2024-10-16T09:38:53.7548Z","iopub.status.idle":"2024-10-16T09:39:58.445018Z","shell.execute_reply.started":"2024-10-16T09:38:53.754769Z","shell.execute_reply":"2024-10-16T09:39:58.443986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom rank_bm25 import BM25Okapi\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nimport os\nimport gc\n\nfrom sentence_transformers import SentenceTransformer","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:39:58.447387Z","iopub.execute_input":"2024-10-16T09:39:58.447803Z","iopub.status.idle":"2024-10-16T09:40:19.07257Z","shell.execute_reply.started":"2024-10-16T09:39:58.447756Z","shell.execute_reply":"2024-10-16T09:40:19.071821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nBATCH_SIZE = 8\nMAX_NEW_TOKENS = 55\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:40:19.073651Z","iopub.execute_input":"2024-10-16T09:40:19.074251Z","iopub.status.idle":"2024-10-16T09:40:19.137752Z","shell.execute_reply.started":"2024-10-16T09:40:19.074217Z","shell.execute_reply":"2024-10-16T09:40:19.136597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load models\nphi_model_name = '/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1'\n\nBGE_MODEL_PATH = '/kaggle/input/train-bge-synthetic-data/trained_model'\nbge_model = SentenceTransformer(BGE_MODEL_PATH)\nbge_model = bge_model.to(DEVICE)\n\nphi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name)\nphi_model = AutoModelForCausalLM.from_pretrained(\n    phi_model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nphi_pipe = pipeline(\"text-generation\", model=phi_model, tokenizer=phi_tokenizer, trust_remote_code=True, max_new_tokens=MAX_NEW_TOKENS)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:40:19.140603Z","iopub.execute_input":"2024-10-16T09:40:19.141188Z","iopub.status.idle":"2024-10-16T09:41:37.217702Z","shell.execute_reply.started":"2024-10-16T09:40:19.141142Z","shell.execute_reply":"2024-10-16T09:41:37.216706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\nmisconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.218957Z","iopub.execute_input":"2024-10-16T09:41:37.219354Z","iopub.status.idle":"2024-10-16T09:41:37.274795Z","shell.execute_reply.started":"2024-10-16T09:41:37.219309Z","shell.execute_reply":"2024-10-16T09:41:37.274045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_filtered_df(df, question, min_rows=5, max_rows=7):\n    construct_id = question['ConstructId']\n    subject_id = question['SubjectId']\n    \n    # Filter by ConstructId\n    filtered_df = df[df['ConstructId'] == construct_id]\n    \n    # If not enough rows, add by SubjectId\n    if len(filtered_df) < min_rows:\n        subject_df = df[(df['SubjectId'] == subject_id) & (df['ConstructId'] != construct_id)]\n        filtered_df = pd.concat([filtered_df, subject_df])\n    \n    # If still not enough, add random rows\n    if len(filtered_df) < min_rows:\n        random_df = df[~df.index.isin(filtered_df.index)].sample(n=min(min_rows - len(filtered_df), len(df) - len(filtered_df)))\n        filtered_df = pd.concat([filtered_df, random_df])\n    \n    # Limit to max_rows\n    return filtered_df.sample(n=min(max_rows, len(filtered_df)))\n\n# Function to generate example sequences\ndef get_example_sequences(filtered_train_df, num_examples=3):\n    examples = []\n    for _, row in filtered_train_df.sample(n=min(num_examples, len(filtered_train_df))).iterrows():\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != row['CorrectAnswer']:\n                misconception_id = row[f'Misconception{answer_choice}Id']\n                if not pd.isna(misconception_id):\n                    examples.append({\n                        'question': f\"{row['ConstructName']}: {row['QuestionText']}\",\n                        'correct_answer': row[f'Answer{row[\"CorrectAnswer\"]}Text'],\n                        'incorrect_answer': row[f'Answer{answer_choice}Text'],\n                        'misconception': misconception_mapping.loc[int(misconception_id), 'MisconceptionName']\n                    })\n                    break  # Only use one incorrect answer per question\n    return examples","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.275946Z","iopub.execute_input":"2024-10-16T09:41:37.276309Z","iopub.status.idle":"2024-10-16T09:41:37.287407Z","shell.execute_reply.started":"2024-10-16T09:41:37.276267Z","shell.execute_reply":"2024-10-16T09:41:37.286421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(x):\n    x = x.lower()\n    x = re.sub(r\"[^\\w\\s]\", '', x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    return x.strip()","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.288537Z","iopub.execute_input":"2024-10-16T09:41:37.288836Z","iopub.status.idle":"2024-10-16T09:41:37.301298Z","shell.execute_reply.started":"2024-10-16T09:41:37.288802Z","shell.execute_reply":"2024-10-16T09:41:37.3004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_embeddings(texts, model, batch_size=BATCH_SIZE):\n    texts = [preprocess_text(text) for text in texts]\n    return model.encode(texts, batch_size=batch_size, show_progress_bar=True, normalize_embeddings=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.302471Z","iopub.execute_input":"2024-10-16T09:41:37.302859Z","iopub.status.idle":"2024-10-16T09:41:37.311722Z","shell.execute_reply.started":"2024-10-16T09:41:37.302817Z","shell.execute_reply":"2024-10-16T09:41:37.311029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_question_embeddings(questions, bge_model):\n    texts = []\n    for q in questions:\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != q['CorrectAnswer']:\n                text = f\"{q['ConstructName']}: {q['QuestionText']} {q[f'Answer{answer_choice}Text']}\"\n                texts.append(text)\n    return generate_embeddings(texts, bge_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.312821Z","iopub.execute_input":"2024-10-16T09:41:37.313111Z","iopub.status.idle":"2024-10-16T09:41:37.324146Z","shell.execute_reply.started":"2024-10-16T09:41:37.31308Z","shell.execute_reply":"2024-10-16T09:41:37.323376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_misconception(questions, phi_pipe):\n    all_prompts = []\n    for q in questions:\n        correct_answer_key = f\"Answer{q['CorrectAnswer']}Text\"\n        correct_answer = q[correct_answer_key]\n        \n        filtered_df = generate_filtered_df(train, q)\n        examples = get_example_sequences(filtered_df)\n        \n        messages = []\n        \n        # Add example messages\n        for example in examples:\n            messages.extend([\n                {\"role\": \"user\", \"content\": f\"Question: {example['question']}\"},\n                {\"role\": \"assistant\", \"content\": \"Provide me with the correct answer for a baseline.\"},\n                {\"role\": \"user\", \"content\": f\"Correct Answer: {example['correct_answer']}\"},\n                {\"role\": \"assistant\", \"content\": \"Now - provide the incorrect answer and I will analyze the difference to infer the misconception.\"},\n                {\"role\": \"user\", \"content\": f\"Incorrect Answer: {example['incorrect_answer']}\"},\n                {\"role\": \"assistant\", \"content\": f\"Misconception for incorrect answer: {example['misconception']}\"}\n            ])\n        \n        # Add the current question\n        messages.extend([\n            {\"role\": \"user\", \"content\": f\"Question: {q['ConstructName']}: {q['QuestionText']}\"},\n            {\"role\": \"assistant\", \"content\": \"Provide me with the correct answer for a baseline.\"},\n            {\"role\": \"user\", \"content\": f\"Correct Answer: {correct_answer}\"},\n            {\"role\": \"assistant\", \"content\": \"Now - provide the incorrect answer and I will analyze the difference to infer the misconception.\"},\n        ])\n        \n        # Add each incorrect answer as a separate prompt\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != q['CorrectAnswer']:\n                incorrect_answer_key = f\"Answer{answer_choice}Text\"\n                incorrect_answer = q[incorrect_answer_key]\n                \n                prompt_messages = messages.copy()\n                prompt_messages.append({\"role\": \"user\", \"content\": f\"Incorrect Answer: {incorrect_answer}\"})\n                \n                all_prompts.append(prompt_messages)\n    \n    responses = phi_pipe(all_prompts, batch_size=BATCH_SIZE)\n    \n    # Updated response processing\n    processed_responses = []\n    for response in responses:\n        if isinstance(response, list) and len(response) > 0:\n            generated_text = response[0].get('generated_text', [])\n            if isinstance(generated_text, list) and len(generated_text) > 0:\n                last_message = generated_text[-1]\n                if isinstance(last_message, dict) and 'content' in last_message:\n                    content = last_message['content'].strip()\n                    # Find the start of the misconception text\n                    start_index = content.find(\"Misconception for incorrect answer:\")\n                    if start_index != -1:\n                        # Extract text after the prefix\n                        misconception = content[start_index + len(\"Misconception for incorrect answer:\"):].strip()\n                        # Find the first full stop\n                        end_index = misconception.find('.')\n                        if end_index != -1:\n                            misconception = misconception[:end_index + 1].strip()\n                        processed_responses.append(misconception)\n                    else:\n                        processed_responses.append(content)\n                else:\n                    processed_responses.append(str(last_message))\n            else:\n                processed_responses.append(str(generated_text))\n        else:\n            processed_responses.append(str(response))\n\n    return processed_responses","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.326978Z","iopub.execute_input":"2024-10-16T09:41:37.327349Z","iopub.status.idle":"2024-10-16T09:41:37.342103Z","shell.execute_reply.started":"2024-10-16T09:41:37.327297Z","shell.execute_reply":"2024-10-16T09:41:37.341324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bm25_search(queries, documents, top_k=50):\n    tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n    bm25 = BM25Okapi(tokenized_corpus)\n    \n    results = []\n    scores = []\n    for query in queries:\n        tokenized_query = word_tokenize(query.lower())\n        doc_scores = bm25.get_scores(tokenized_query)\n        top_indices = np.argsort(doc_scores)[::-1][:top_k]\n        top_scores = np.sort(doc_scores)[::-1][:top_k]\n        results.append(top_indices)\n        scores.append(top_scores)\n    return results, scores\n\ndef semantic_search(embeddings, misc_embeddings, top_k=50):\n    similarities = cosine_similarity(embeddings, misc_embeddings)\n    top_indices = np.argsort(-similarities, axis=1)[:, :top_k]\n    top_scores = np.sort(similarities, axis=1)[:, ::-1][:, :top_k]\n    return top_indices, top_scores\n\ndef combined_search(semantic_results, semantic_scores, keyword_results, keyword_scores, top_k=25, alpha=0.5):\n    combined_results = []\n    for sem_res, sem_scores, key_res, key_scores in zip(semantic_results, semantic_scores, keyword_results, keyword_scores):\n        combined_scores = np.zeros(len(misconception_mapping))\n        \n        # Normalize scores using min-max normalization\n        sem_scores_norm = (sem_scores - np.min(sem_scores)) / (np.max(sem_scores) - np.min(sem_scores))\n        key_scores_norm = (key_scores - np.min(key_scores)) / (np.max(key_scores) - np.min(key_scores))\n        \n        # Combine scores by taking the weighted sum\n        for idx, score in zip(sem_res, sem_scores_norm):\n            combined_scores[idx] += alpha * score\n        \n        for idx, score in zip(key_res, key_scores_norm):\n            combined_scores[idx] += (1 - alpha) * score\n        \n        # Get top combined results\n        top_combined = np.argsort(combined_scores)[::-1][:top_k]\n        combined_results.append(top_combined)\n    \n    return combined_results","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.342995Z","iopub.execute_input":"2024-10-16T09:41:37.343295Z","iopub.status.idle":"2024-10-16T09:41:37.356871Z","shell.execute_reply.started":"2024-10-16T09:41:37.343266Z","shell.execute_reply":"2024-10-16T09:41:37.35598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_questions_batch(questions, misc_embeddings, bge_model, phi_pipe):\n    llm_responses = predict_misconception(questions, phi_pipe)\n    question_embeddings = generate_question_embeddings(questions, bge_model)\n    \n    semantic_results, semantic_scores = semantic_search(question_embeddings, misc_embeddings)\n    keyword_results, keyword_scores = bm25_search(llm_responses, misconception_mapping['MisconceptionName'].tolist())\n    \n    combined_results = combined_search(semantic_results, semantic_scores, keyword_results, keyword_scores)\n    \n    results = []\n    result_index = 0\n    for question in questions:\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != question['CorrectAnswer']:\n                top_misconceptions = combined_results[result_index]\n                results.append({\n                    'QuestionId_Answer': f\"{question['QuestionId']}_{answer_choice}\",\n                    'MisconceptionId': ' '.join(map(str, top_misconceptions))\n                })\n                result_index += 1\n    \n    return results  # Ensure we're returning the results","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.357922Z","iopub.execute_input":"2024-10-16T09:41:37.358243Z","iopub.status.idle":"2024-10-16T09:41:37.370506Z","shell.execute_reply.started":"2024-10-16T09:41:37.358211Z","shell.execute_reply":"2024-10-16T09:41:37.36977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate embeddings for all misconceptions\nmisc_embeddings = generate_embeddings(misconception_mapping['MisconceptionName'].tolist(), bge_model)\n\nresults = []\nfor i in range(0, len(test), BATCH_SIZE):\n    batch = test.iloc[i:i+BATCH_SIZE].to_dict('records')\n    batch_results = process_questions_batch(batch, misc_embeddings, bge_model, phi_pipe)\n    if batch_results:  # Add a check to ensure batch_results is not None\n        results.extend(batch_results)\n    else:\n        print(f\"Warning: No results for batch starting at index {i}\")\n    \n    # Clear cache and collect garbage\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"Processed {i+len(batch)} out of {len(test)} questions\")\n\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T09:41:37.37154Z","iopub.execute_input":"2024-10-16T09:41:37.371827Z","iopub.status.idle":"2024-10-16T09:42:22.690866Z","shell.execute_reply.started":"2024-10-16T09:41:37.371797Z","shell.execute_reply":"2024-10-16T09:42:22.68985Z"},"trusted":true},"execution_count":null,"outputs":[]}]}