{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8897601,"sourceType":"datasetVersion","datasetId":5297895},{"sourceId":9094368,"sourceType":"datasetVersion","datasetId":5251603},{"sourceId":9688062,"sourceType":"datasetVersion","datasetId":5920031},{"sourceId":9734430,"sourceType":"datasetVersion","datasetId":5957531},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft accelerate \\\n    -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nimport gc\nimport pandas as pd\nimport pickle\nimport sys\nimport numpy as np\nfrom tqdm.autonotebook import trange\nfrom sklearn.model_selection import GroupKFold\nimport json\nimport torch\nfrom numpy.linalg import norm\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n)\nimport json\nimport copy\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef apk(actual, predicted, k=25):\n    \"\"\"\n    Computes the average precision at k.\n    \n    This function computes the average prescision at k between two lists of\n    items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    \n    if not actual:\n        return 0.0\n\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        # first condition checks whether it is valid prediction\n        # second condition checks if prediction is not repeated\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=25):\n    \"\"\"\n    Computes the mean average precision at k.\n    \n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n\ndef batch_to_device(batch, target_device):\n    \"\"\"\n    send a pytorch batch to a device (CPU/GPU)\n    \"\"\"\n    for key in batch:\n        if isinstance(batch[key], Tensor):\n            batch[key] = batch[key].to(target_device)\n    return batch\n\ndef last_token_pool(last_hidden_states: Tensor,\n                    attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\ndef inference(df, model, tokenizer, device):\n    batch_size = 16\n    max_length = 512\n    sentences = list(df['query_text'].values)\n    pids = list(df['order_index'].values)\n    all_embeddings = []\n    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n                             return_tensors=\"pt\")\n        features = batch_to_device(features, device)\n        with torch.no_grad():\n            outputs = model.model(**features)\n            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n            embeddings = embeddings.detach().cpu().numpy().tolist()\n        all_embeddings.extend(embeddings)\n\n    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n\n    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n    result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\nmodel_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\nlora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\ndevice='cuda:0'\nVALID = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\nbnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16\n        )\nmodel = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\nconfig = LoraConfig(\n        r=64,\n        lora_alpha=128,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        bias=\"none\",\n        lora_dropout=0.05,  # Conventional\n        task_type=\"CAUSAL_LM\",\n    )\nmodel = get_peft_model(model, config)\nd = torch.load(lora_path, map_location=model.device)\nmodel.load_state_dict(d, strict=False)\nmodel = model.eval()\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 输出模型的参数名和参数值\n# for name, param in model.named_parameters():\n#     if \"base_model.model.layers.12.input_layernorm.weight\"  in name:\n#         print(f\"参数名: {name}\")\n#         print(f\"参数值: {param}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if VALID:\n    tra = pd.read_parquet(\"/kaggle/input/v1-parquet/v1_val.parquet\")\n    print(tra.shape)\nelse:\n    tra = pd.read_csv(f\"{path_prefix}/test.csv\")\n    print(tra.shape)\nmisconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\nif tra.shape[0]<10:\n    misconception_mapping = misconception_mapping.sample(n=5,random_state=2023)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if VALID:\n    train_data = []\n    for _,row in tra.iterrows():\n        for c in ['A','B','C','D']:\n            if str(row[f\"Misconception{c}Id\"])!=\"nan\":\n                # print(row[f\"Misconception{c}Id\"])\n                real_answer_id = row['CorrectAnswer']\n                real_text = row[f'Answer{real_answer_id}Text']\n                query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{c}.{row[f'Answer{c}Text']}\"\n                row['query_text'] = get_detailed_instruct(task_description,query_text)\n                row['answer_id'] = row[f\"Misconception{c}Id\"]\n                train_data.append(copy.deepcopy(row))\n    train_df = pd.DataFrame(train_data)\n    train_df['order_index'] = list(range(len(train_df)))\nelse:\n    train_data = []\n    for _,row in tra.iterrows():\n        for c in ['A','B','C','D']:\n            if c ==row['CorrectAnswer']:\n                continue\n            if f'Answer{c}Text' not in row:\n                continue\n            real_answer_id = row['CorrectAnswer']\n            real_text = row[f'Answer{real_answer_id}Text']\n            query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{c}.{row[f'Answer{c}Text']}\"\n            row['query_text'] = get_detailed_instruct(task_description,query_text)\n            row['answer_name'] = c\n            train_data.append(copy.deepcopy(row))\n    train_df = pd.DataFrame(train_data)\n    train_df['order_index'] = list(range(len(train_df)))\ntrain_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings = inference(train_df, model, tokenizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\nmisconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\ndoc_embeddings = inference(misconception_mapping, model, tokenizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\nindex_text_embeddings_index = {index: paper_id for index, paper_id in\n                                         enumerate(list(doc_embeddings.keys()))}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts_test = []\nfor _, row in tqdm(train_df.iterrows()):\n    query_id = row['order_index']\n    query_em = train_embeddings[query_id].reshape(1, -1)\n    \n    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n    \n    sort_index = np.argsort(-cosine_similarity)[:25]\n    pids = [index_text_embeddings_index[index] for index in sort_index]\n    predicts_test.append(pids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if VALID:\n    train_df['recall_ids'] = predicts_test\n    print(mapk([[data] for data in train_df['answer_id'].values],train_df['recall_ids'].values))\nelse:\n    train_df['MisconceptionId'] = [' '.join(map(str,c)) for c in predicts_test]\n    sub = []\n    for _,row in train_df.iterrows():\n        sub.append(\n            {\n                \"QuestionId_Answer\":f\"{row['QuestionId']}_{row['answer_name']}\",\n                \"MisconceptionId\":row['MisconceptionId']\n            }\n        )\n    submission_df = pd.DataFrame(sub)\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"Submission file created successfully!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}