{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":9063357,"sourceType":"datasetVersion","datasetId":5465816},{"sourceId":9387960,"sourceType":"datasetVersion","datasetId":5688622}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install datasets sentencepiece peft -q\n!pip install git+https://github.com/huggingface/transformers.git -qq\n!pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q\n!pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":121.512757,"end_time":"2023-11-04T12:34:07.401258","exception":false,"start_time":"2023-11-04T12:32:05.888501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:17:41.940558Z","iopub.execute_input":"2024-09-26T23:17:41.940861Z","iopub.status.idle":"2024-09-26T23:20:13.118954Z","shell.execute_reply.started":"2024-09-26T23:17:41.940835Z","shell.execute_reply":"2024-09-26T23:20:13.11791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n)\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom spmd_util import partition_module\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.metrics import roc_auc_score\n\n!export USE_TORCH=True\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"\nos.environ.pop('TPU_PROCESS_ADDRESSES')\nhf_logging.set_verbosity_error()\n\n\nMAX_INPUT=512\nMODEL = \"/kaggle/input/llama-3-1-8b-instruct\"","metadata":{"papermill":{"duration":13.608021,"end_time":"2023-11-04T12:34:21.013846","exception":false,"start_time":"2023-11-04T12:34:07.405825","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:20:13.120647Z","iopub.execute_input":"2024-09-26T23:20:13.120913Z","iopub.status.idle":"2024-09-26T23:20:24.661534Z","shell.execute_reply.started":"2024-09-26T23:20:13.120886Z","shell.execute_reply":"2024-09-26T23:20:24.66016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:20:24.663367Z","iopub.execute_input":"2024-09-26T23:20:24.664144Z","iopub.status.idle":"2024-09-26T23:20:24.689759Z","shell.execute_reply.started":"2024-09-26T23:20:24.664113Z","shell.execute_reply":"2024-09-26T23:20:24.688947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"external_df = pd.read_csv('/kaggle/input/eedi-external-dataset/train_external.csv')\nexternal_df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:42:13.502132Z","iopub.execute_input":"2024-09-26T23:42:13.502511Z","iopub.status.idle":"2024-09-26T23:42:13.545196Z","shell.execute_reply.started":"2024-09-26T23:42:13.502482Z","shell.execute_reply":"2024-09-26T23:42:13.544402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/eedi-external-dataset/all_train.csv\").fillna(-1)\ntest_ds = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-3-1-8b-instruct\")\n\nPROMPT  = \"\"\"Question: {Question}\nIncorrect Answer: {IncorrectAnswer}\nCorrect Answer: {CorrectAnswer}\nConstruct Name: {ConstructName}\nSubject Name: {SubjectName}\n\nYour task is to identify the misconception behind Incorrect Answer. Answer concisely and generically. Output misconception only.\n\"\"\"\n\ndef apply_template(row, tokenizer, targetCol):\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": PROMPT.format(\n                 ConstructName=row[\"ConstructName\"],\n                 SubjectName=row[\"SubjectName\"],\n                 Question=row[\"QuestionText\"],\n                 IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n                 CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"])\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": row[f\"Misconception{targetCol}Name\"]\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndf = {}\ndf_label = {}\nfor idx, row in tqdm(train_df.iterrows()):\n    for option in [\"A\", \"B\", \"C\", \"D\"]:\n        try:\n            if (row.CorrectAnswer!=option) & (row[f\"Misconception{option}Name\"]!=-1):\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n                df_label[f\"{row.QuestionId}_{option}\"] = [row[f\"Misconception{option}Name\"]]\n        except Exception as e:\n            pass\n\ndf_label = pd.DataFrame([df_label]).T.reset_index()\ndf_label.columns = [\"QuestionId_Answer\", \"MisconceptionName\"]\n\ndf = pd.DataFrame([df]).T.reset_index()\ndf.columns = [\"QuestionId_Answer\", \"text\"]","metadata":{"papermill":{"duration":3.87932,"end_time":"2023-11-04T12:34:24.897696","exception":false,"start_time":"2023-11-04T12:34:21.018376","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:45:51.050391Z","iopub.execute_input":"2024-09-26T23:45:51.050756Z","iopub.status.idle":"2024-09-26T23:45:53.542393Z","shell.execute_reply.started":"2024-09-26T23:45:51.050712Z","shell.execute_reply":"2024-09-26T23:45:53.541237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = []\nfor sample in df['text']:\n    lengths.append(len(sample.split(' ')))\n\nmax(lengths), min(lengths), sum(lengths) / len(lengths)","metadata":{"papermill":{"duration":0.581213,"end_time":"2023-11-04T12:34:25.482941","exception":false,"start_time":"2023-11-04T12:34:24.901728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:46:20.041308Z","iopub.execute_input":"2024-09-26T23:46:20.041658Z","iopub.status.idle":"2024-09-26T23:46:20.116114Z","shell.execute_reply.started":"2024-09-26T23:46:20.041631Z","shell.execute_reply":"2024-09-26T23:46:20.115146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"papermill":{"duration":0.547357,"end_time":"2023-11-04T12:34:26.034497","exception":false,"start_time":"2023-11-04T12:34:25.48714","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:46:20.909373Z","iopub.execute_input":"2024-09-26T23:46:20.909695Z","iopub.status.idle":"2024-09-26T23:46:21.466431Z","shell.execute_reply.started":"2024-09-26T23:46:20.909669Z","shell.execute_reply":"2024-09-26T23:46:21.465049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True) # It's slightly excessive","metadata":{"papermill":{"duration":0.01077,"end_time":"2023-11-04T12:34:26.049685","exception":false,"start_time":"2023-11-04T12:34:26.038915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:46:21.469158Z","iopub.execute_input":"2024-09-26T23:46:21.469545Z","iopub.status.idle":"2024-09-26T23:46:21.474654Z","shell.execute_reply.started":"2024-09-26T23:46:21.469506Z","shell.execute_reply":"2024-09-26T23:46:21.473771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = Dataset.from_pandas(df).train_test_split(test_size=0.15)\n\nds['train'] = ds['train'].map(preprocess_function, batched=False, num_proc=96, remove_columns=['text', 'QuestionId_Answer'])\nds['test'] = ds['test'].map(preprocess_function, batched=False, num_proc=96, remove_columns=['text', 'QuestionId_Answer'])\nds","metadata":{"papermill":{"duration":57.083722,"end_time":"2023-11-04T12:35:23.137022","exception":false,"start_time":"2023-11-04T12:34:26.0533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:46:21.927287Z","iopub.execute_input":"2024-09-26T23:46:21.927566Z","iopub.status.idle":"2024-09-26T23:48:09.250429Z","shell.execute_reply.started":"2024-09-26T23:46:21.927541Z","shell.execute_reply":"2024-09-26T23:48:09.249433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.bfloat16)","metadata":{"papermill":{"duration":300.034782,"end_time":"2023-11-04T12:40:23.192281","exception":false,"start_time":"2023-11-04T12:35:23.157499","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:48:09.252062Z","iopub.execute_input":"2024-09-26T23:48:09.252348Z","iopub.status.idle":"2024-09-26T23:48:10.500685Z","shell.execute_reply.started":"2024-09-26T23:48:09.252312Z","shell.execute_reply":"2024-09-26T23:48:10.499847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': 512,\n         'LOGGING_STEPS': 1,\n         'NUM_EPOCHS': 1,\n         'BATCH_SIZE': 4,\n          'NUM_STEPS': len(ds['train'])} ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:48:10.501661Z","iopub.execute_input":"2024-09-26T23:48:10.501926Z","iopub.status.idle":"2024-09-26T23:48:10.506022Z","shell.execute_reply.started":"2024-09-26T23:48:10.501904Z","shell.execute_reply":"2024-09-26T23:48:10.505251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\ntraining_loader = torch.utils.data.DataLoader(ds['train'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=128))\ntesting_loader = torch.utils.data.DataLoader(ds['test'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=128))\n\ndevice = xm.xla_device()","metadata":{"papermill":{"duration":0.027766,"end_time":"2023-11-04T12:40:23.286172","exception":false,"start_time":"2023-11-04T12:40:23.258406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:48:10.507635Z","iopub.execute_input":"2024-09-26T23:48:10.50797Z","iopub.status.idle":"2024-09-26T23:48:10.562203Z","shell.execute_reply.started":"2024-09-26T23:48:10.507948Z","shell.execute_reply":"2024-09-26T23:48:10.56152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(MODEL)\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh)","metadata":{"papermill":{"duration":29.992732,"end_time":"2023-11-04T12:40:53.298154","exception":false,"start_time":"2023-11-04T12:40:23.305422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:48:37.916328Z","iopub.execute_input":"2024-09-26T23:48:37.91675Z","iopub.status.idle":"2024-09-26T23:48:49.45599Z","shell.execute_reply.started":"2024-09-26T23:48:37.916701Z","shell.execute_reply":"2024-09-26T23:48:49.454861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor param in model.parameters():\n    cnt += 1\n    param.requires_grad = False\n    if cnt > 0:\n        param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-09-26T23:48:53.475355Z","iopub.execute_input":"2024-09-26T23:48:53.475661Z","iopub.status.idle":"2024-09-26T23:48:53.483702Z","shell.execute_reply.started":"2024-09-26T23:48:53.47563Z","shell.execute_reply":"2024-09-26T23:48:53.482998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\n\ndef train_loop(training_loader, epoch, optimizer, scheduler, num_actual_steps):\n    model.train()\n    print('Epoch {} train begin {} for {} steps'.format(epoch, test_utils.now(), num_actual_steps))\n    for step, batch in tqdm(enumerate(training_loader), total=num_actual_steps):\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch.input_ids.to(device), batch.attention_mask.to(device), batch.labels.to(device)\n\n        xs.mark_sharding(input_ids, mesh, (0, 1))\n        xs.mark_sharding(attention_mask, mesh, (0, 1))\n        xs.mark_sharding(labels, mesh, (0, 1))\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        xm.mark_step()\n        scheduler.step()\n    print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n        \ndef eval_loop(testing_loader, epoch):\n    model.eval()\n    total_loss = 0.0\n    total_steps = 0\n    with torch.no_grad():\n        for step, batch in enumerate(testing_loader):\n            input_ids, attention_mask, labels = batch.input_ids.to(device), batch.attention_mask.to(device), batch.labels.to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            total_steps += 1\n    average_loss = total_loss / total_steps\n    print('Epoch {} test end {}, TEST LOSS={:.2f}'.format(epoch, test_utils.now(), average_loss))\n    \ndef save_model(model, tokenizer, dir_name):\n    model = model.cpu()\n    model.save_pretrained(dir_name)\n    tokenizer.save_pretrained(dir_name)\n\ndef train(FLAGS):\n    num_actual_steps = FLAGS['NUM_STEPS'] // FLAGS['BATCH_SIZE']\n    lr = 1e-5\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters = num_actual_steps)\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n\n        eval_loop(testing_loader, epoch)\n        train_loop(training_loader, epoch, optimizer, scheduler, num_actual_steps)\n\n    eval_loop(testing_loader, epoch)\n    save_model(model, tokenizer, 'trained_model')","metadata":{"papermill":{"duration":1.912133,"end_time":"2023-11-04T12:40:55.230144","exception":false,"start_time":"2023-11-04T12:40:53.318011","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:48:51.495464Z","iopub.execute_input":"2024-09-26T23:48:51.495766Z","iopub.status.idle":"2024-09-26T23:48:53.473267Z","shell.execute_reply.started":"2024-09-26T23:48:51.495735Z","shell.execute_reply":"2024-09-26T23:48:53.47194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"papermill":{"duration":2070.358248,"end_time":"2023-11-04T13:15:25.607979","exception":false,"start_time":"2023-11-04T12:40:55.249731","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-26T23:48:53.484564Z","iopub.execute_input":"2024-09-26T23:48:53.484792Z"},"trusted":true},"execution_count":null,"outputs":[]}]}