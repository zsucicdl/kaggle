{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":5250,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":4035,"modelId":2195}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # There was a warning message\n\n# Qwen requirements\n!pip install torch transformers datasets -q\n!pip install transformers -q \n!pip install transformers_stream_generator einops -q\n!pip install tiktoken -q\n!pip install gradio-client -q","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:01:47.64351Z","iopub.execute_input":"2024-09-30T08:01:47.643855Z","iopub.status.idle":"2024-09-30T08:02:51.730419Z","shell.execute_reply.started":"2024-09-30T08:01:47.64382Z","shell.execute_reply":"2024-09-30T08:02:51.729225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom transformers.generation import GenerationConfig\n\nimport torch\nimport re\n\nfrom gradio_client import Client, handle_file\nimport json\n\nfrom scipy.spatial.distance import cosine","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:02:51.732492Z","iopub.execute_input":"2024-09-30T08:02:51.73284Z","iopub.status.idle":"2024-09-30T08:02:56.137078Z","shell.execute_reply.started":"2024-09-30T08:02:51.732806Z","shell.execute_reply":"2024-09-30T08:02:56.136132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\ntest_df = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n\nmisconceptions = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')\n\nX = train_df.copy()\nX_test = test_df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:02:56.138536Z","iopub.execute_input":"2024-09-30T08:02:56.139389Z","iopub.status.idle":"2024-09-30T08:02:56.210783Z","shell.execute_reply.started":"2024-09-30T08:02:56.139344Z","shell.execute_reply":"2024-09-30T08:02:56.209936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:02:56.212787Z","iopub.execute_input":"2024-09-30T08:02:56.213107Z","iopub.status.idle":"2024-09-30T08:02:56.240091Z","shell.execute_reply.started":"2024-09-30T08:02:56.213075Z","shell.execute_reply":"2024-09-30T08:02:56.239213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misconceptions.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:02:56.241271Z","iopub.execute_input":"2024-09-30T08:02:56.241584Z","iopub.status.idle":"2024-09-30T08:02:56.249868Z","shell.execute_reply.started":"2024-09-30T08:02:56.241553Z","shell.execute_reply":"2024-09-30T08:02:56.24888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example use of LLMs on a sample\n\n[OpenAI's gpt-4o](https://chatgpt.com/share/66f257f5-1a74-800a-8f87-e3ffc91bfd62)\n\n[Qwen2.5 7B and MetaMath Mistral 7B](https://colab.research.google.com/drive/1zGwcMO5cSmKylPwIUkBQnEXJ1WMTOCpn?usp=sharing)","metadata":{}},{"cell_type":"markdown","source":"# Outline\n---\n1. Get embeddings for each misconception\n2. Store them in a vector database\n3. Let the model describe student's mistake in one sentence\n4. Using the vector database query and output top 25 similar misconceptions\n","metadata":{}},{"cell_type":"markdown","source":"#### Qwen2.5 14B \n    @article{yang2024qwen25mathtechnicalreportmathematical,\n    title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement}, \n    author={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},\n    journal={arXiv preprint arXiv:2409.12122},\n    year={2024}","metadata":{}},{"cell_type":"markdown","source":"# Instantiate the base model","metadata":{}},{"cell_type":"code","source":"# I will be using API as it is requires less compute\nclient = Client(\"Qwen/Qwen2.5-Math-Demo\")\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-72B\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:02:56.25111Z","iopub.execute_input":"2024-09-30T08:02:56.251711Z","iopub.status.idle":"2024-09-30T08:03:03.648566Z","shell.execute_reply.started":"2024-09-30T08:02:56.251658Z","shell.execute_reply":"2024-09-30T08:03:03.647631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings for each misconception","metadata":{}},{"cell_type":"code","source":"# a small utility function as tokenizer was outputing input_ids and other not needed details\ndef return_only_vector_embeddings(text):\n    full_output = tokenizer(text, return_tensors='pt')\n    return full_output['input_ids']  # Extract only 'input_ids'\n\nembeddings = {}\n\nfor idx, name in misconceptions.values:\n    embedding = return_only_vector_embeddings(name)\n    embeddings[idx] = embedding","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:03:03.649925Z","iopub.execute_input":"2024-09-30T08:03:03.650346Z","iopub.status.idle":"2024-09-30T08:03:04.162216Z","shell.execute_reply.started":"2024-09-30T08:03:03.650303Z","shell.execute_reply":"2024-09-30T08:03:04.161413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Qwen2.5 to Describe Mistakes","metadata":{}},{"cell_type":"code","source":"# Example on one sample\nquestion = X['QuestionText'][0]\nanswer = X['AnswerDText'][0]\ncorrect_answer = X['AnswerAText'][0]\n\nprompt = f\"\"\"\n    Given a question and an incorrect answer to that question. \n    Describe the nature of the mistake the incorrect answer makes.\n    question: ({question}).\n    incorrect answer: ({answer}). \n    correct answer: ({correct_answer}).\n    Put your final explanation in one simple sentence (for example: \"Does not know that angles in a triangle sum to 180 degrees\") and in parentheses().\n\"\"\"\n\nresult = client.predict(\n    image=None,\n    sketchpad=None,\n    question=prompt,\n    api_name=\"/math_chat_bot\"\n)\nprint(result)\n\n\n# a utility function to do the above\ndef ask_the_model(question, answer, correct_answer):\n    prompt = f\"\"\"\n        Given a question and an incorrect answer to that question. \n        Describe the nature of the mistake the incorrect answer makes.\n        question: ({question}).\n        incorrect answer: ({answer}). \n        correct answer: ({correct_answer}).\n        Put your final explanation in one simple sentence (for example: \"Does not know that angles in a triangle sum to 180 degrees\") and in parentheses().\n    \"\"\"\n    result = client.predict(\n        image=None,\n        sketchpad=None,\n        question=prompt,\n        api_name=\"/math_chat_bot\"\n    )\n    \n    # Extract the last sentence\n    result = extract_boxed_text(result)\n    \n    return result\n\n# a function to extract the final sentence from the model\ndef extract_boxed_text(text):\n    pattern = r'\\\\boxed{(.*?)}'\n    result = re.findall(pattern, text)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:03:04.163347Z","iopub.execute_input":"2024-09-30T08:03:04.163683Z","iopub.status.idle":"2024-09-30T08:03:56.36809Z","shell.execute_reply.started":"2024-09-30T08:03:04.163649Z","shell.execute_reply":"2024-09-30T08:03:56.367129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_the_model(question, answer, correct_answer)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:03:56.369498Z","iopub.execute_input":"2024-09-30T08:03:56.370343Z","iopub.status.idle":"2024-09-30T08:04:04.817194Z","shell.execute_reply.started":"2024-09-30T08:03:56.370296Z","shell.execute_reply":"2024-09-30T08:04:04.816139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a dataset like the submission.csv to see zero-shot model performance","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef get_answer(letter, row):\n    answer_column = 'Answer' + letter + 'Text'\n    return row[answer_column]\n\n# Initialize a list to store rows\nrows = []\n\nfor index, row in X.iterrows():\n    q_id = row['QuestionId']\n    question = row['QuestionText']\n    \n    # Get the correct answer based on the letter\n    correct_answer_letter = row['CorrectAnswer']\n    correct_answer = get_answer(correct_answer_letter, row)\n    \n    for letter in ['A', 'B', 'C', 'D']:\n        q_letter = f\"{q_id}_{letter}\"\n        \n        # Get the misconception ID\n        misconception_letter = 'Misconception' + letter + 'Id'\n        m_id = row[misconception_letter]\n        \n        # Get the answer text\n        answer = get_answer(letter, row)\n        \n        # Append the new row as a tuple\n        rows.append({\n            'q_id': q_id,\n            'q_letter': q_letter,\n            'm_id': m_id,\n            'question': question,\n            'answer': answer,\n            'correct_answer': correct_answer\n        })\n\n# Create a DataFrame from the list of rows\ntraining_data = pd.DataFrame(rows)\n\n# Drop nulls\ntraining_data = training_data.dropna(subset=['m_id'])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:04:04.820093Z","iopub.execute_input":"2024-09-30T08:04:04.820403Z","iopub.status.idle":"2024-09-30T08:04:05.045345Z","shell.execute_reply.started":"2024-09-30T08:04:04.820372Z","shell.execute_reply":"2024-09-30T08:04:05.044529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:04:05.046502Z","iopub.execute_input":"2024-09-30T08:04:05.04685Z","iopub.status.idle":"2024-09-30T08:04:05.058514Z","shell.execute_reply.started":"2024-09-30T08:04:05.046816Z","shell.execute_reply":"2024-09-30T08:04:05.057424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function (MAP@25)","metadata":{}},{"cell_type":"code","source":"def rel(prediction_25, m_id):\n    if m_id not in prediction_25:\n        return 0\n    else:\n        return 1 / (prediction_25.index(m_id) + 1)\n\ndef map_at_25(predictions, logits):\n    \"\"\"\n    Calculate the MAP@25 scores for predictions, logits\n    \n    Args:\n        predictions: list of 25 ints (25 per sample)\n        logits: list of int\n        \n    Returns:\n        float: The Mean Average Precision at 25 (Cutoff) [0-1]\n    \"\"\"\n    total = 0\n    \n    for prediction_25, m_id in zip(predictions, logits):\n        total += rel(prediction_25, m_id)\n        \n    score = total / len(logits)\n            \n    return score","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:04:05.060057Z","iopub.execute_input":"2024-09-30T08:04:05.06045Z","iopub.status.idle":"2024-09-30T08:04:05.066692Z","shell.execute_reply.started":"2024-09-30T08:04:05.060408Z","shell.execute_reply":"2024-09-30T08:04:05.065765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eternity is a long time, so I will be using first 100 samples to evaluate the model","metadata":{}},{"cell_type":"code","source":"rows = []\nfor index, row in training_data[:50].iterrows():\n    question = row['question']\n    answer = row['answer']\n    correct_answer = row['correct_answer']\n    \n    model_response = ask_the_model(question, answer, correct_answer)\n    \n    rows.append({\n        'q_letter': row['q_letter'],\n        'model_response': model_response\n    })\n    \n    if index % 10 == 0:\n        print(f'processed {index} question_letter pairs')\n    \n# Create a DataFrame from the list of rows\nquestion_response_df = pd.DataFrame(rows)\n\nquestion_response_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:04:05.067782Z","iopub.execute_input":"2024-09-30T08:04:05.068098Z","iopub.status.idle":"2024-09-30T08:04:56.200777Z","shell.execute_reply.started":"2024-09-30T08:04:05.068059Z","shell.execute_reply":"2024-09-30T08:04:56.199749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize the models outputs","metadata":{}},{"cell_type":"code","source":"question_response_df['embedding'] = question_response_df['model_response'].apply(lambda x: return_only_vector_embeddings(x))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:04:56.201902Z","iopub.execute_input":"2024-09-30T08:04:56.202208Z","iopub.status.idle":"2024-09-30T08:04:56.208859Z","shell.execute_reply.started":"2024-09-30T08:04:56.202177Z","shell.execute_reply":"2024-09-30T08:04:56.208068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## For each explanation (embedding from the model) get 25 most similar embeddings from the misconception","metadata":{}},{"cell_type":"code","source":"def normalize_token_length(embedding, token_length=100):\n    \"\"\"\n    Normalize the token length of an embedding tensor to a fixed size.\n    \n    Parameters:\n    embedding (torch.Tensor): The input tensor with variable token length.\n    token_length (int): The desired token length for normalization.\n    \n    Returns:\n    torch.Tensor: A tensor with the specified fixed length.\n    \"\"\"\n    current_length = embedding.shape[1]  # Get the current token length\n\n    # If current length is less than the target, pad with zeros\n    if current_length < token_length:\n        padding_length = token_length - current_length\n        # Pad tensor with zeros on the right\n        padding = torch.zeros((embedding.shape[0], padding_length))\n        embedding = torch.cat((embedding, padding), dim=1)\n\n    # If current length is more, truncate to the target length\n    elif current_length > token_length:\n        embedding = embedding[:, :token_length]\n\n    return embedding\n\n\ndef get_25_most_similar(query_embedding, embeddings):\n    \"\"\"\n    Find the 25 most similar vectors based on cosine similarity.\n\n    Parameters:\n    query_embedding (torch.Tensor): The embedding to compare against.\n    embeddings (dict): A dictionary of index and their corresponding embedding tensors.\n    token_length (int): The fixed length for token normalization.\n\n    Returns:\n    List[int]: A list of top 25 most similar indices.\n    \"\"\"\n    similarities = []\n\n    # Normalize the query embedding length\n    query_embedding = normalize_token_length(query_embedding).flatten()\n\n    # Calculate cosine similarity for each vector in the dictionary\n    for idx, embedding in embeddings.items():\n        # Normalize each embedding in the dictionary to the same token length\n        normalized_embedding = normalize_token_length(embedding).flatten()\n\n        # Compute cosine similarity (1 - cosine distance)\n        similarity = 1 - cosine(query_embedding.detach().numpy(), normalized_embedding.detach().numpy())\n        similarities.append((idx, similarity))\n\n    # Sort by similarity score in descending order\n    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n\n    # Return only the indices of the top 25 most similar items\n    top_25_indices = [idx for idx, _ in sorted_similarities[:25]]\n    \n    return top_25_indices\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:11:36.406199Z","iopub.execute_input":"2024-09-30T08:11:36.406935Z","iopub.status.idle":"2024-09-30T08:11:36.415419Z","shell.execute_reply.started":"2024-09-30T08:11:36.406895Z","shell.execute_reply":"2024-09-30T08:11:36.414451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_25_most_similar(question_response_df['embedding'][1], embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:11:39.391752Z","iopub.execute_input":"2024-09-30T08:11:39.39224Z","iopub.status.idle":"2024-09-30T08:11:39.536387Z","shell.execute_reply.started":"2024-09-30T08:11:39.392202Z","shell.execute_reply":"2024-09-30T08:11:39.535346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_response_df['top_25'] = question_response_df['embedding'].apply(lambda x: get_25_most_similar(x, embeddings))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:11:43.443586Z","iopub.execute_input":"2024-09-30T08:11:43.444284Z","iopub.status.idle":"2024-09-30T08:11:43.719649Z","shell.execute_reply.started":"2024-09-30T08:11:43.444245Z","shell.execute_reply":"2024-09-30T08:11:43.718618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Performance Evaluation","metadata":{}},{"cell_type":"code","source":"# Merge based on 'q_letter' and get 'm_id' column\nquestion_response_df = question_response_df.merge(training_data[['q_letter', 'm_id']], on='q_letter', how='left')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:16:49.524611Z","iopub.execute_input":"2024-09-30T08:16:49.525487Z","iopub.status.idle":"2024-09-30T08:16:49.542006Z","shell.execute_reply.started":"2024-09-30T08:16:49.52545Z","shell.execute_reply":"2024-09-30T08:16:49.540947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_response_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:16:51.816638Z","iopub.execute_input":"2024-09-30T08:16:51.817535Z","iopub.status.idle":"2024-09-30T08:16:51.823534Z","shell.execute_reply.started":"2024-09-30T08:16:51.817495Z","shell.execute_reply":"2024-09-30T08:16:51.822579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = question_response_df['top_25'].values\nlogits = question_response_df['m_id'].values\n\nperformance = map_at_25(predictions, logits)\n    \nprint(f'Zero-shot model performance: {performance:.5f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T08:16:54.677847Z","iopub.execute_input":"2024-09-30T08:16:54.678604Z","iopub.status.idle":"2024-09-30T08:16:54.684695Z","shell.execute_reply.started":"2024-09-30T08:16:54.678556Z","shell.execute_reply":"2024-09-30T08:16:54.683524Z"},"trusted":true},"execution_count":null,"outputs":[]}]}