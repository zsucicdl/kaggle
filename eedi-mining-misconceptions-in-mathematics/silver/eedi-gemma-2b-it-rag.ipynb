{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82695,"databundleVersionId":9551816,"sourceType":"competition"},{"sourceId":7715453,"sourceType":"datasetVersion","datasetId":4505960},{"sourceId":7715470,"sourceType":"datasetVersion","datasetId":4505971},{"sourceId":196505597,"sourceType":"kernelVersion"},{"sourceId":11358,"sourceType":"modelInstanceVersion","modelInstanceId":5383,"modelId":3301},{"sourceId":27644,"sourceType":"modelInstanceVersion","modelInstanceId":23286,"modelId":33601}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> EEDI: <span style='color:#F1A424'>Gemma 2b-it + RAG </span><span style='color:#ABABAB'> [Inference]</span></b>\n\n***\n\nIn this competition, youâ€™ll develop an NLP model driven by ML to accurately predict the affinity between misconceptions and incorrect answers (distractors) in multiple-choice questions. This solution will suugest candidate misconceptions for distractors, making it easier for expert human teachers to tag distractors with misconceptions.\n\n### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n    <li> <a href=\"#introduction\">Introduction</a></li>\n    <li> <a href=\"#install_libraries\">Install libraries</a></li>\n    <li><a href=\"#import_libraries\">Import Libraries</a></li>\n    <li><a href=\"#configuration\">Configuration</a></li>\n    <li><a href=\"#load_data\">Load Data</a></li>\n    <li><a href=\"#utils\">Utils</a></li>\n    <li><a href=\"#model\">Model</a></li>\n    <li><a href=\"#prompt_gemma\">Prompt Gemma</a></li>\n    <li><a href=\"#preprocessing\">Pre-processing</a></li>\n    <li><a href=\"#dataset\">Dataset</a></li>\n    <li><a href=\"#embedding_function\">Embedding Function</a></li>\n    <li><a href=\"#create_embeddings\">Create Embeddings</a></li>\n    <li><a href=\"#retrieval\">Retrieval</a></li>\n    <li><a href=\"#submission\">Submission</a></li>\n</div>\n\n\n# <b><span style='color:#F1A424'>|</span> Introduction</b><a class='anchor' id='introduction'></a> [â†‘](#top)\n\n***\n\nIn this notebook, we will create embeddings using Hugging Face ðŸ¤— models from all misconceptions texts present in the `misconception_mapping.csv` file. Then, we will pass test Q&A pairs to a Gemma model and ask it to predict a misconception for that pair. We will later embed these predictions and retrieve the top-25 most similar misconceptions. These will be our predictions.\n\n\n### <b><span style='color:#F1A424'>References</span></b>\n\n\n- The Hugging Face models that we use here are models directly downloaded from Hugging Face (in notebook [here][1]).\n- This notebook is based on @cdeotte work from the AES-2 competition [here][2]\n- Nischay Dhankhar's [Gemma-Asking LLM to generate prompt][3]\n- [Open LLM Leaderboard][4]\n\n[1]: https://www.kaggle.com/code/cdeotte/download-huggingface-models\n[2]: https://www.kaggle.com/code/cdeotte/rapids-svr-starter-cv-0-830-lb-0-804\n[3]: https://www.kaggle.com/code/nischaydnk/gemma-asking-llm-to-generate-prompt\n[4]: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [â†‘](#top)\n\n***\n\nInstall all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/gemma/\n!cp /kaggle/input/gemma-pytorch/gemma_pytorch-main/gemma/* /kaggle/working/gemma/\n!pip install --no-index --no-deps /kaggle/input/immutabledict/immutabledict-4.1.0-py3-none-any.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-16T21:28:11.675808Z","iopub.execute_input":"2024-09-16T21:28:11.676707Z","iopub.status.idle":"2024-09-16T21:28:16.115027Z","shell.execute_reply.started":"2024-09-16T21:28:11.676656Z","shell.execute_reply":"2024-09-16T21:28:16.113781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys \n\nsys.path.append(\"/kaggle/working/\") ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-16T21:28:16.117113Z","iopub.execute_input":"2024-09-16T21:28:16.117448Z","iopub.status.idle":"2024-09-16T21:28:16.122082Z","shell.execute_reply.started":"2024-09-16T21:28:16.117415Z","shell.execute_reply":"2024-09-16T21:28:16.121095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [â†‘](#top)\n\n***\n\nImport all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"import contextlib\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport re\nimport torch\nimport torch.nn.functional as F\nimport transformers\n\n\nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nfrom glob import glob\nfrom sklearn.manifold import TSNE\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import AutoModel,AutoTokenizer\nfrom typing import Any, Dict, List\n\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Current device is: {device}\")\nprint('pandas version' ,pd.__version__)\nprint('transformers version', transformers.__version__)\n!mkdir output","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:16.123479Z","iopub.execute_input":"2024-09-16T21:28:16.123859Z","iopub.status.idle":"2024-09-16T21:28:22.693851Z","shell.execute_reply.started":"2024-09-16T21:28:16.123796Z","shell.execute_reply":"2024-09-16T21:28:22.692512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [â†‘](#top)\n\n***\n\nCentral repository for this notebook's hyperparameters.","metadata":{}},{"cell_type":"code","source":"class config:\n    BATCH_SIZE = 128\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MAX_LEN = 128\n    MAX_LEN_QA = 512\n    MODEL = \"intfloat/e5-base-v2\"\n    \n\nclass paths:\n    HF_MODELS = \"/kaggle/input/download-and-save-huggingface-models/\"\n    MISCONCEPTION_CSV = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n    OUTPUT = \"/kaggle/working/output/\"\n    TRAIN_CSV = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\"\n    TEST_CSV = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\"\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:22.6973Z","iopub.execute_input":"2024-09-16T21:28:22.698677Z","iopub.status.idle":"2024-09-16T21:28:22.708467Z","shell.execute_reply.started":"2024-09-16T21:28:22.698628Z","shell.execute_reply":"2024-09-16T21:28:22.707401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Load Data</b><a class='anchor' id='load_data'></a> [â†‘](#top)\n\n***\n\nLoad data.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(paths.TRAIN_CSV)\ndf_test = pd.read_csv(paths.TEST_CSV)\nmisconception_mapping = pd.read_csv(paths.MISCONCEPTION_CSV)\nprint(f\"Train dataframe shape: {df_train.shape}\")\nprint(f\"Test dataframe shape: {df_test.shape}\")\nprint(f\"Misconception mapping dataframe shape: {misconception_mapping.shape}\")\ndisplay(df_train.head())\ndisplay(df_test.head())\ndisplay(misconception_mapping.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:22.709853Z","iopub.execute_input":"2024-09-16T21:28:22.710249Z","iopub.status.idle":"2024-09-16T21:28:22.815323Z","shell.execute_reply.started":"2024-09-16T21:28:22.710188Z","shell.execute_reply":"2024-09-16T21:28:22.814228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Utils</b><a class='anchor' id='utils'></a> [â†‘](#top)\n\n***\n\nUtility functions.","metadata":{}},{"cell_type":"code","source":"def create_example(df: pd.DataFrame, idx: int) -> str:\n    \"\"\"\n    Create examples for RAG.\n    \"\"\"\n    QuestionText = df.loc[idx, \"QuestionText\"]\n    AnswerAText = df.loc[idx, \"AnswerAText\"]\n    AnswerBText = df.loc[idx, \"AnswerBText\"]\n    AnswerCText = df.loc[idx, \"AnswerCText\"]\n    AnswerDText = df.loc[idx, \"AnswerDText\"]\n    SubjectName = df.loc[idx, \"SubjectName\"]\n    ConstructName = df.loc[idx, \"ConstructName\"]\n    text = f\"\"\"\n    ###\n    ConstructName: {ConstructName}\n    ###\n    Subject: {SubjectName}\n    ###\n    Question: {QuestionText} \\n\n    ###\n    Answers:\n    (A) - {AnswerAText} \\n\n    (B) - {AnswerBText} \\n\n    (C) - {AnswerCText} \\n\n    (D) - {AnswerDText} \\n\n    \"\"\"\n    return text\n\n\ndef create_misconception(df: pd.DataFrame, idx: int) -> str:\n    \"\"\"\n    Create misconception for RAG.\n    \"\"\"\n    QuestionText = df.loc[idx, \"QuestionText\"]\n    AnswerAText = df.loc[idx, \"AnswerAText\"]\n    AnswerBText = df.loc[idx, \"AnswerBText\"]\n    AnswerCText = df.loc[idx, \"AnswerCText\"]\n    AnswerDText = df.loc[idx, \"AnswerDText\"]\n    SubjectName = df.loc[idx, \"SubjectName\"]\n    ConstructName = df.loc[idx, \"ConstructName\"]\n    MisconceptionNameA = df.loc[idx, \"MisconceptionNameA\"]\n    MisconceptionNameB = df.loc[idx, \"MisconceptionNameB\"]\n    MisconceptionNameC = df.loc[idx, \"MisconceptionNameC\"]\n    MisconceptionNameD = df.loc[idx, \"MisconceptionNameD\"]\n    answer_list = [AnswerAText, AnswerBText, AnswerCText, AnswerDText]\n    misc_list = [MisconceptionNameA, MisconceptionNameB, MisconceptionNameC, MisconceptionNameD]\n    filtered_data = filtered_data = [(index, item) for index, item in enumerate(misc_list) if not isinstance(item, float) or not np.isnan(item)]\n    random_index, random_value = random.choice(filtered_data)\n    text = f\"\"\"\n    ###\n    ConstructName: {ConstructName}\n    ###\n    Subject: {SubjectName}\n    ###\n    Question: {QuestionText}\n    ###\n    Wrong Answer:\n    - {answer_list[random_index]}\n    ###\n    Misconception:\n    {random_value}\n    \"\"\"\n    return text\n\n\ndef create_text(df: pd.DataFrame, idx: int, letter: str) -> str:\n    \"\"\"\n    Joins all available text information into a single string\n    \"\"\"\n    QuestionText = df.loc[idx, \"QuestionText\"]\n    WrongAnswer = df.loc[idx, f\"Answer{letter}Text\"]\n    SubjectName = df.loc[idx, \"SubjectName\"]\n    ConstructName = df.loc[idx, \"ConstructName\"]\n    text = f\"\"\"\n    ###\n    ConstructName: {ConstructName}\n    ###\n    Subject: {SubjectName}\n    ###\n    Question: {QuestionText} \\n\n    ###\n    Wrong Answer:\n    - {WrongAnswer} \\n\n    ###\n    Misconception:\n    ???\n    \"\"\"\n    return text\n\n\ndef ensure_row_vector(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Ensures that the input array is a row vector.\n    If the input array is one-dimensional, it reshapes it into a row vector (1, n).\n    If the array is already two-dimensional, it returns the array as is.\n    :param arr: Input array, which can be 1D or 2D.\n    :return arr: numpy.ndarray: A 2D array with shape (1, n) if the input was 1D, or the original\n    array if it was already 2D.\n    \"\"\"\n    if arr.ndim == 1:  # Check if the array has one dimension\n        return arr.reshape(1, -1)  # Reshape to row vector\n    else:\n        return arr\n    \n\ndef most_similar_embeddings_train(embeddings: np.ndarray, index: int, K: int = 5) -> List[int]:\n    \"\"\"\n    Finds the K most similar embeddings to the embedding at the given index based on cosine similarity.\n\n    :param embeddings: An array of shape (N, M) where N is the number of embeddings and M is the embedding dimension.\n    :param index: The index of the embedding to compare against the others.\n    K (int): The number of most similar embeddings to return (default is 5).\n\n    Returns:\n    numpy.ndarray: An array of indices of the K most similar embeddings.\n    \"\"\"\n    # Get the embedding for the given index\n    query_embedding = embeddings[index]\n    \n    # Compute the cosine similarity between the query embedding and all other embeddings\n    dot_product = np.dot(embeddings, query_embedding)\n    norm_query = np.linalg.norm(query_embedding)\n    norms = np.linalg.norm(embeddings, axis=1)\n    cosine_similarity = dot_product / (norm_query * norms)\n    \n    # Sort the indices by similarity in descending order, exclude the index itself\n    most_similar_indices = np.argsort(-cosine_similarity)  # Sort in descending order\n    most_similar_indices = most_similar_indices[most_similar_indices != index]  # Exclude the input index\n    \n    # Return the top K most similar indices\n    return most_similar_indices[:K]\n\n\ndef most_similar_embeddings_test(embed_test: np.ndarray, embed_train: np.ndarray, index: int, K=5) -> np.ndarray:\n    \"\"\"\n    Finds the K most similar embeddings from array B to the embedding at the given index in array A using cosine similarity.\n\n    Parameters:\n    :param embed_test: An array of shape (N, M) where N is the number of embeddings in A and M is the embedding dimension.\n    :param embed_train: An array of shape (P, M) where P is the number of embeddings in B and M is the embedding dimension.\n    :param index: The index of the embedding in A to compare against the embeddings in B.\n    :param K: The number of most similar embeddings to return from B (default is 5).\n    :return most_similar_indices: An array of indices of the K most similar embeddings from B.\n    \"\"\"\n    # Get the embedding for the given index from array embed_test\n    query_embedding = embed_test[index]\n    \n    # Compute the cosine similarity between the query embedding from embed_test and all embeddings in embed_train\n    dot_product = np.dot(embed_train, query_embedding)\n    norm_query = np.linalg.norm(query_embedding)\n    norms = np.linalg.norm(embed_train, axis=1)\n    cosine_similarity = dot_product / (norm_query * norms)\n    \n    # Sort the indices of embed_train by similarity in descending order\n    most_similar_indices = np.argsort(-cosine_similarity)  # Sort in descending order\n    \n    # Return the top K most similar indices from embed_train\n    return most_similar_indices[:K]\n\n\ndef sep():\n    print(\"-\"*100)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-16T21:28:22.816786Z","iopub.execute_input":"2024-09-16T21:28:22.81711Z","iopub.status.idle":"2024-09-16T21:28:22.844568Z","shell.execute_reply.started":"2024-09-16T21:28:22.817074Z","shell.execute_reply":"2024-09-16T21:28:22.843467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Pre-processing</b><a class='anchor' id='preprocessing'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"for letter in [\"A\", \"B\", \"C\", \"D\"]:\n    # === Preprocess Train ===\n    df_train = df_train.merge(\n        misconception_mapping[['MisconceptionId', 'MisconceptionName']].rename(\n            columns={'MisconceptionName': f'MisconceptionName{letter}'}\n        ),\n        left_on=f\"Misconception{letter}Id\", right_on='MisconceptionId', how='left'\n    )\n    df_train.drop(\"MisconceptionId\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:22.845988Z","iopub.execute_input":"2024-09-16T21:28:22.846372Z","iopub.status.idle":"2024-09-16T21:28:22.890658Z","shell.execute_reply.started":"2024-09-16T21:28:22.846328Z","shell.execute_reply":"2024-09-16T21:28:22.889565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"# Load the model\nVARIANT = \"2b-it\" \nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/2b-it/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Model Config.\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n# model_config.quant = \"quant\" in VARIANT\n\n# Model.\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n    model.load_weights(ckpt_path)\n    model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:22.892253Z","iopub.execute_input":"2024-09-16T21:28:22.892679Z","iopub.status.idle":"2024-09-16T21:28:47.525043Z","shell.execute_reply.started":"2024-09-16T21:28:22.892632Z","shell.execute_reply":"2024-09-16T21:28:47.523981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"def mean_pooling(model_output: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    last_hidden_state = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded =  attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n    sum_mask = input_mask_expanded.sum(1)\n    sum_mask = torch.clamp(sum_mask, min=1e-9)\n    mean_embeddings = sum_embeddings / sum_mask\n    return  mean_embeddings\n\n\nclass EmbedQA(Dataset):\n    def __init__(self, df, tokenizer, config):\n        self.config = config\n        self.df = df\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, item):\n        text = create_example(self.df, item)\n        tokens = self.tokenizer(\n            text, None, add_special_tokens=True,\n            padding='max_length', truncation=True,\n            max_length=self.config.MAX_LEN_QA, return_tensors=\"pt\"\n        )\n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n\n\nclass EmbedMisconception(Dataset):\n    def __init__(self, df, tokenizer, config):\n        self.config = config\n        self.texts = df['MisconceptionName'].values\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = self.texts[item]\n        tokens = self.tokenizer(\n            text, None, add_special_tokens=True,\n            padding='max_length', truncation=True,\n            max_length=self.config.MAX_LEN, return_tensors=\"pt\"\n        )\n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:47.526437Z","iopub.execute_input":"2024-09-16T21:28:47.526876Z","iopub.status.idle":"2024-09-16T21:28:47.539355Z","shell.execute_reply.started":"2024-09-16T21:28:47.52683Z","shell.execute_reply":"2024-09-16T21:28:47.538293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Embedding Function</b><a class='anchor' id='embedding_function'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"def get_embeddings_qa(\n        config = config,\n        model_name: str = '',\n        batch_size: int = 32\n    ):\n\n    global train, test\n    \n    MODEL_PATH = paths.HF_MODELS + config.MODEL.replace(\"/\",\"_\")\n    \n    # === Load Model ===\n    model = AutoModel.from_pretrained(MODEL_PATH , trust_remote_code=True)\n    model = model.to(config.DEVICE)\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH , trust_remote_code=True)\n    \n    # === Dataset & DataLoader ===\n    train_dataset = EmbedQA(df_train, tokenizer, config)\n    test_dataset = EmbedQA(df_test, tokenizer, config)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    # === Compute Train Embeddings ===\n    all_train_embeddings = []\n    for batch in tqdm(train_loader, total=len(train_loader)):\n        input_ids = batch[\"input_ids\"].to(config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Normalize the embeddings\n        sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        sentence_embeddings = ensure_row_vector(sentence_embeddings)\n        all_train_embeddings.extend(sentence_embeddings)\n    all_train_embeddings = np.array(all_train_embeddings)\n\n    # === Compute Test Embeddings ===\n    all_test_embeddings = []\n    for batch in tqdm(test_loader, total=len(test_loader)):\n        input_ids = batch[\"input_ids\"].to(config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Normalize the embeddings\n        sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        sentence_embeddings = ensure_row_vector(sentence_embeddings)\n        all_test_embeddings.extend(sentence_embeddings)\n    all_test_embeddings = np.array(all_test_embeddings)\n\n    # === Clean memory ===\n    del train_dataset, test_dataset\n    del train_loader, test_loader\n    del model, tokenizer\n    del model_output, sentence_embeddings, input_ids, attention_mask\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return all_train_embeddings, all_test_embeddings\n\n\n\ndef get_embeddings(\n        config = config,\n        model_name: str = '',\n        batch_size: int = 128,\n    ):\n\n    global train, test\n    \n    MODEL_PATH = paths.HF_MODELS + config.MODEL.replace(\"/\",\"_\")\n    \n    # === Load Model ===\n    model = AutoModel.from_pretrained(MODEL_PATH , trust_remote_code=True)\n    model = model.to(config.DEVICE)\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH , trust_remote_code=True)\n    \n    # === Dataset & DataLoader ===\n    misconception_mapping_dataset = EmbedMisconception(misconception_mapping, tokenizer, config)\n    misconception_preds_dataset = EmbedMisconception(misconception_preds, tokenizer, config)\n    misconception_mapping_loader = DataLoader(misconception_mapping_dataset, batch_size=batch_size, shuffle=False)\n    misconception_preds_loader = DataLoader(misconception_preds_dataset, batch_size=batch_size, shuffle=False)\n    \n    # === Compute Misconceptions Embeddings ===\n    all_misconception_mapping_embeddings = []\n    for batch in tqdm(misconception_mapping_loader, total=len(misconception_mapping_loader)):\n        input_ids = batch[\"input_ids\"].to(config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Normalize the embeddings\n        sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        sentence_embeddings = ensure_row_vector(sentence_embeddings)\n        all_misconception_mapping_embeddings.extend(sentence_embeddings)\n    all_misconception_mapping_embeddings = np.array(all_misconception_mapping_embeddings)\n\n    # === Compute Predictions Embeddings ===\n    all_misconception_preds_embeddings = []\n    for batch in tqdm(misconception_preds_loader, total=len(misconception_preds_loader)):\n        input_ids = batch[\"input_ids\"].to(config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=True):\n                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Normalize the embeddings\n        sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        sentence_embeddings = ensure_row_vector(sentence_embeddings)\n        all_misconception_preds_embeddings.extend(sentence_embeddings)\n    all_misconception_preds_embeddings = np.array(all_misconception_preds_embeddings)\n\n    # === Clean memory ===\n    del misconception_mapping_dataset, misconception_preds_dataset\n    del misconception_mapping_loader, misconception_preds_loader\n    del model, tokenizer\n    del model_output, sentence_embeddings, input_ids, attention_mask\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return all_misconception_mapping_embeddings, all_misconception_preds_embeddings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-16T21:28:47.542549Z","iopub.execute_input":"2024-09-16T21:28:47.542858Z","iopub.status.idle":"2024-09-16T21:28:47.569774Z","shell.execute_reply.started":"2024-09-16T21:28:47.542827Z","shell.execute_reply":"2024-09-16T21:28:47.568763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Create Q&A Embeddings</b><a class='anchor' id='create_embeddings_qa'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"train_embeddings = []\ntest_embeddings = []\n\n# === Compute Embeddings ===\nname = paths.OUTPUT + config.MODEL.replace(\"/\",\"_\") + \".npy\"\nprint(f\"Computing embeddings for {config.MODEL}\")\ntrain_embeddings, test_embeddings = get_embeddings_qa(\n    model_name=config.MODEL, config=config, batch_size=32\n)\n\ngc.collect()\nprint('Train embeddings have shape', train_embeddings.shape )\nprint('Test embeddings have shape', test_embeddings.shape )\n\n# === Compute t-SNE projections ===\ntsne = TSNE(n_components=2, perplexity=5, random_state=42)\nX = tsne.fit_transform(train_embeddings)\ndf_train[\"x1\"] = X[:, 0]\ndf_train[\"x2\"] = X[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:28:47.570934Z","iopub.execute_input":"2024-09-16T21:28:47.571221Z","iopub.status.idle":"2024-09-16T21:29:38.786062Z","shell.execute_reply.started":"2024-09-16T21:28:47.571191Z","shell.execute_reply":"2024-09-16T21:29:38.784982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Visualize</span></b>\n","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode the 'SubjectName' column\ncol = \"ConstructName\"\nlabel_encoder = LabelEncoder()\ndf_train[f'{col}_encoded'] = label_encoder.fit_transform(df_train[col])\n\n# Create scatter plot with color based on 'SubjectName_encoded'\nfig = px.scatter(\n    df_train, x='x1', y='x2', color=f'{col}_encoded', \n    hover_data={col: True, \"QuestionId\": True},\n    color_continuous_scale=px.colors.sequential.Viridis\n)\nfig.update_layout(\n    title=f'{col} | t-SNE projections of embeddings',\n    width=1200,  # Set the width of the plot\n    height=800  # Set the height of the plot\n)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-16T21:29:38.787455Z","iopub.execute_input":"2024-09-16T21:29:38.787981Z","iopub.status.idle":"2024-09-16T21:29:40.893741Z","shell.execute_reply.started":"2024-09-16T21:29:38.787944Z","shell.execute_reply":"2024-09-16T21:29:40.89279Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Check one Q&A train example</span></b>\n","metadata":{}},{"cell_type":"code","source":"original_index = np.random.choice(df_train.index)\nsimilar_index = most_similar_embeddings_train(train_embeddings, original_index, 5)\nprint(f\"Original index: {original_index} | Similar Index: {similar_index}\")\nprint(create_example(df_train, original_index)), sep()\nprint(create_example(df_train, similar_index[0]))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:29:40.895002Z","iopub.execute_input":"2024-09-16T21:29:40.89534Z","iopub.status.idle":"2024-09-16T21:29:40.923248Z","shell.execute_reply.started":"2024-09-16T21:29:40.895306Z","shell.execute_reply":"2024-09-16T21:29:40.921281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Check one Q&A test example</span></b>\n","metadata":{}},{"cell_type":"code","source":"original_index = np.random.choice(df_test.index)\nsimilar_index = most_similar_embeddings_test(test_embeddings, train_embeddings, original_index, 5)\nprint(f\"Original index: {original_index} | Similar Index: {similar_index}\")\nprint(f\"Prediction:\\n {create_example(df_test, original_index)}\"), sep()\nprint(f\"Most similar:\\n {create_example(df_train, similar_index[1])}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:29:40.925228Z","iopub.execute_input":"2024-09-16T21:29:40.925992Z","iopub.status.idle":"2024-09-16T21:29:40.940276Z","shell.execute_reply.started":"2024-09-16T21:29:40.925947Z","shell.execute_reply":"2024-09-16T21:29:40.939011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Prompt Gemma</b><a class='anchor' id='prompt_gemma'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"def create_prompt(question: str, index: int, verbose: bool = False) -> str:\n    similar_index = most_similar_embeddings_test(test_embeddings, train_embeddings, index, 5)\n    example = create_misconception(df_train, similar_index[0])\n    \n    if verbose:\n        print(example)\n        \n    prompt_for_llm = (\n        \"<start_of_turn>user\\nYou are an expert mathematician. You are presented with a math problem question and a wrong answer. \"\n        \"Your task is to say which common misconceptions is associated to the incorrect answer. \"\n        \"It is important that your answer should be more of a generic error and not tailored to the particular question you are presented. \"\n        \"Your answer should be formated as: ###Misconception: <YOUR-ANSWER-HERE>###\"\n        f\"Here is an example sample: {example}\\n\"\n        f\"Here is the question you must answer: {question}\\n\"\n        \"<end_of_turn>\\n<start_of_turn>model\\n\"\n    )\n    response = model.generate(\n        prompt_for_llm,\n        device=device,\n        output_len=128,\n    )\n    return response\n\noutput = []\nfor index in tqdm(df_test.index):\n    question_id = df_test.loc[index, \"QuestionId\"]\n    correct_answer = df_test.loc[index, \"CorrectAnswer\"]\n    for letter in [\"A\", \"B\", \"C\", \"D\"]:\n        if letter != correct_answer:\n            question_id_answer = str(question_id) + \"_\" + letter\n            question = create_text(df_test, index, letter)\n            response = create_prompt(question, index, False)\n            output.append([question_id_answer, response])\n        else: \n            pass\n        \nmisconception_preds = pd.DataFrame(output)\nmisconception_preds.columns = [\"QuestionId_Answer\", \"MisconceptionName\"]\nmisconception_preds[\"MisconceptionName\"] = misconception_preds[\"MisconceptionName\"].apply(lambda x: x.replace(\"###Misconception:\", \"\"))\nmisconception_preds[\"MisconceptionName\"] = misconception_preds[\"MisconceptionName\"].apply(lambda x: x.replace(\"\\n\", \"\"))\nmisconception_preds[\"len\"] = misconception_preds[\"MisconceptionName\"].apply(lambda x: len(x.split()))\nmisconception_preds.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:29:40.94239Z","iopub.execute_input":"2024-09-16T21:29:40.942886Z","iopub.status.idle":"2024-09-16T21:30:14.18183Z","shell.execute_reply.started":"2024-09-16T21:29:40.942828Z","shell.execute_reply":"2024-09-16T21:30:14.180862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Create Misconception Embeddings</b><a class='anchor' id='create_embeddings_misc'></a> [â†‘](#top)\n\n***\n\n    ","metadata":{}},{"cell_type":"code","source":"all_misconception_mapping_embeddings = []\nall_misconception_preds_embeddings = []\n\n# === Compute Embeddings ===\nname = paths.OUTPUT + config.MODEL.replace(\"/\",\"_\") + \".npy\"\nprint(f\"Computing embeddings for {config.MODEL}\")\nall_misconception_mapping_embeddings, all_misconception_preds_embeddings = get_embeddings(\n    model_name=config.MODEL, config=config, batch_size=config.BATCH_SIZE\n)\n\ngc.collect()\nprint('Misconception mapping embeddings have shape', all_misconception_mapping_embeddings.shape)\nprint('Misconception predictions embeddings have shape', all_misconception_preds_embeddings.shape)\n\n# === Compute t-SNE projections ===\ntsne = TSNE(n_components=2, perplexity=5, random_state=42)\nX = tsne.fit_transform(all_misconception_mapping_embeddings)\nmisconception_mapping[\"x1\"] = X[:, 0]\nmisconception_mapping[\"x2\"] = X[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:33:19.560292Z","iopub.execute_input":"2024-09-16T21:33:19.561337Z","iopub.status.idle":"2024-09-16T21:33:42.69669Z","shell.execute_reply.started":"2024-09-16T21:33:19.561291Z","shell.execute_reply":"2024-09-16T21:33:42.695727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Visualize</span></b>\n","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\n\ncolumns = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\nall_identifiers = df_train[columns].melt()['value']\nidentifier_counts = all_identifiers.value_counts(ascending=True)\ntrain_ids = [int(idx) for idx in identifier_counts.index]\nmisconception_mapping[\"unseen\"] = 1\nidx = misconception_mapping[misconception_mapping[\"MisconceptionId\"].isin(train_ids)].index\nmisconception_mapping.loc[idx, \"unseen\"] = 0\n\nfig = px.scatter(\n    misconception_mapping, x='x1', y='x2', \n    color='unseen',\n    hover_data={\"MisconceptionName\": True},\n#     color_continuous_scale=px.colors.sequential.Viridis\n)\nfig.update_layout(\n    title=f'MisconceptionName | t-SNE projections of embeddings',\n    width=1200,  # Set the width of the plot\n    height=800  # Set the height of the plot\n)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-16T21:33:42.698953Z","iopub.execute_input":"2024-09-16T21:33:42.699437Z","iopub.status.idle":"2024-09-16T21:33:42.798765Z","shell.execute_reply.started":"2024-09-16T21:33:42.699384Z","shell.execute_reply":"2024-09-16T21:33:42.797749Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Check one train example</span></b>\n","metadata":{}},{"cell_type":"code","source":"original_index = np.random.choice(misconception_mapping.index)\nsimilar_index = most_similar_embeddings_train(all_misconception_mapping_embeddings, original_index, 5)\nprint(f\"Original index: {original_index} | Similar Index: {similar_index}\")\nprint(misconception_mapping.loc[original_index, \"MisconceptionName\"]), sep()\nprint(misconception_mapping.loc[similar_index[0], \"MisconceptionName\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:30:26.584428Z","iopub.execute_input":"2024-09-16T21:30:26.584777Z","iopub.status.idle":"2024-09-16T21:30:26.597564Z","shell.execute_reply.started":"2024-09-16T21:30:26.584742Z","shell.execute_reply":"2024-09-16T21:30:26.596318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Check one test example</span></b>\n","metadata":{}},{"cell_type":"code","source":"original_index = np.random.choice(misconception_preds.index)\nsimilar_index = most_similar_embeddings_test(all_misconception_preds_embeddings, all_misconception_mapping_embeddings, original_index, 5)\nprint(f\"Original index: {original_index} | Similar Index: {similar_index}\")\nprint(f\"Prediction:\\n {misconception_preds.loc[original_index, 'MisconceptionName']}\"), sep()\nprint(f\"Most similar:\\n {misconception_mapping.loc[similar_index[0], 'MisconceptionName']}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:30:26.59904Z","iopub.execute_input":"2024-09-16T21:30:26.599557Z","iopub.status.idle":"2024-09-16T21:30:26.624878Z","shell.execute_reply.started":"2024-09-16T21:30:26.599493Z","shell.execute_reply":"2024-09-16T21:30:26.623502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Retrieval</b><a class='anchor' id='retrieval'></a> [â†‘](#top)\n\n***\n\nFor each predicted misconception we will retrieve the top-25 most similar misconceptions from `misconception_mapping.csv`.","metadata":{"execution":{"iopub.status.busy":"2024-09-13T14:25:33.331342Z","iopub.execute_input":"2024-09-13T14:25:33.331742Z","iopub.status.idle":"2024-09-13T14:25:33.344715Z","shell.execute_reply.started":"2024-09-13T14:25:33.331703Z","shell.execute_reply":"2024-09-13T14:25:33.343347Z"}}},{"cell_type":"code","source":"misconception_cols = [\"MisconceptionAId\", \"MisconceptionBId\", \"MisconceptionCId\", \"MisconceptionDId\"]\n\ndef remove_duplicates(lst: List[Any]) -> List[Any]:\n    seen = set()\n    result = []\n    for item in lst:\n        if item not in seen:\n            result.append(item)\n            seen.add(item)\n    return result\n\ndef get_misconceptions_test(idx: int, df: pd.DataFrame, embeddings_test: np.ndarray, embeddings_train: np.ndarray) -> str:\n    similar_index = most_similar_embeddings_test(embeddings_test, embeddings_train, idx, 100)\n    misconception_ids = misconception_mapping.loc[similar_index, \"MisconceptionId\"]\n    misconception_ids = misconception_ids[:25] # get top 25\n    assert len(misconception_ids) == 25\n    misconception_ids = ' '.join(map(str, misconception_ids))\n    return misconception_ids\n\nmisconception_preds[\"pred\"] = \"\"\n    \nfor idx in tqdm(misconception_preds.index):\n    misconception_preds.loc[idx, \"pred\"] = get_misconceptions_test(\n        idx, misconception_mapping, all_misconception_preds_embeddings, all_misconception_mapping_embeddings\n    )\n    \ndisplay(misconception_preds)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:30:26.626714Z","iopub.execute_input":"2024-09-16T21:30:26.6286Z","iopub.status.idle":"2024-09-16T21:30:26.718313Z","shell.execute_reply.started":"2024-09-16T21:30:26.628535Z","shell.execute_reply":"2024-09-16T21:30:26.71708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Submission</b><a class='anchor' id='retrieval'></a> [â†‘](#top)\n\n***\n\nStructure our predictions in the required format for our `submission.csv` file.","metadata":{}},{"cell_type":"code","source":"submission = misconception_preds[[\"QuestionId_Answer\", \"pred\"]].copy()\nsubmission.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:30:26.72075Z","iopub.execute_input":"2024-09-16T21:30:26.721759Z","iopub.status.idle":"2024-09-16T21:30:26.742891Z","shell.execute_reply.started":"2024-09-16T21:30:26.721693Z","shell.execute_reply":"2024-09-16T21:30:26.741645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T21:30:26.748702Z","iopub.execute_input":"2024-09-16T21:30:26.751987Z","iopub.status.idle":"2024-09-16T21:30:26.764425Z","shell.execute_reply.started":"2024-09-16T21:30:26.751922Z","shell.execute_reply":"2024-09-16T21:30:26.763176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}