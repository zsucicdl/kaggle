{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":9502328,"sourceType":"datasetVersion","datasetId":5755831},{"sourceId":9503395,"sourceType":"datasetVersion","datasetId":5750064},{"sourceId":9534632,"sourceType":"datasetVersion","datasetId":5784947},{"sourceId":9600626,"sourceType":"datasetVersion","datasetId":5856973},{"sourceId":9625498,"sourceType":"datasetVersion","datasetId":5875516},{"sourceId":199269375,"sourceType":"kernelVersion"},{"sourceId":118141,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":99348,"modelId":123513}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rank-bm25/rank_bm25-0.2.2-py3-none-any.whl\n!python -m pip install -qq --no-index --find-links=/kaggle/input/eedi-library-from-sinchiro \\\nsentence-transformers\\\nfaiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:35:25.552166Z","iopub.execute_input":"2024-10-19T13:35:25.552467Z","iopub.status.idle":"2024-10-19T13:36:13.279006Z","shell.execute_reply.started":"2024-10-19T13:35:25.552435Z","shell.execute_reply":"2024-10-19T13:36:13.277725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom rank_bm25 import BM25Okapi\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nimport os\nimport gc\nimport faiss\nfrom sentence_transformers import SentenceTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:40:58.718223Z","iopub.execute_input":"2024-10-19T13:40:58.719187Z","iopub.status.idle":"2024-10-19T13:40:58.724913Z","shell.execute_reply.started":"2024-10-19T13:40:58.719145Z","shell.execute_reply":"2024-10-19T13:40:58.724009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\nBATCH_SIZE = 8\nMAX_NEW_TOKENS = 55\nK = 50  # For initial FAISS search\nFINAL_K = 25  # For final hybrid search result\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nD = 1024","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:40:59.068201Z","iopub.execute_input":"2024-10-19T13:40:59.068508Z","iopub.status.idle":"2024-10-19T13:40:59.07444Z","shell.execute_reply.started":"2024-10-19T13:40:59.068476Z","shell.execute_reply":"2024-10-19T13:40:59.073051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\nDATA_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\nBGE_MODEL_PATH = \"/kaggle/input/bge-weights-version1/bge_trained_model_version3\"\nGTE_BASE_MODEL_PATH = \"/kaggle/input/mod-gte-base-weights/gte-base-weights/gte-base_trained_model_version2\"\nMPNETV2_MODEL_PATH = \"/kaggle/input/mpnet-weights-version1/mpnetV2_trained_model_version3\"\nPHI_MODEL_PATH = '/kaggle/input/phi-3.5-mini-instruct/pytorch/default/1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:40:59.621787Z","iopub.execute_input":"2024-10-19T13:40:59.622567Z","iopub.status.idle":"2024-10-19T13:40:59.626791Z","shell.execute_reply.started":"2024-10-19T13:40:59.622526Z","shell.execute_reply":"2024-10-19T13:40:59.625893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntest = pd.read_csv(f\"{DATA_PATH}/test.csv\")\ntrain = pd.read_csv(f\"{DATA_PATH}/train.csv\")\nmisconception_mapping = pd.read_csv(f\"{DATA_PATH}/misconception_mapping.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:41:00.070682Z","iopub.execute_input":"2024-10-19T13:41:00.071029Z","iopub.status.idle":"2024-10-19T13:41:00.103977Z","shell.execute_reply.started":"2024-10-19T13:41:00.070996Z","shell.execute_reply":"2024-10-19T13:41:00.103078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load models\nbge_model = SentenceTransformer(BGE_MODEL_PATH, trust_remote_code=True, local_files_only=True)\ngte_model = SentenceTransformer(GTE_BASE_MODEL_PATH, trust_remote_code=True, local_files_only=True)\nmpnetv2_model = SentenceTransformer(MPNETV2_MODEL_PATH, trust_remote_code=True, local_files_only=True)\nbge_model.to(DEVICE)\ngte_model.to(DEVICE)\nmpnetv2_model.to(DEVICE)\n\nphi_tokenizer = AutoTokenizer.from_pretrained(PHI_MODEL_PATH)\nphi_model = AutoModelForCausalLM.from_pretrained(\n    PHI_MODEL_PATH,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nphi_pipe = pipeline(\"text-generation\", model=phi_model, tokenizer=phi_tokenizer, trust_remote_code=True, max_new_tokens=MAX_NEW_TOKENS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T13:41:00.480685Z","iopub.execute_input":"2024-10-19T13:41:00.481288Z","iopub.status.idle":"2024-10-19T13:41:05.454541Z","shell.execute_reply.started":"2024-10-19T13:41:00.48125Z","shell.execute_reply":"2024-10-19T13:41:05.453598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_embeddings(texts, model, batch_size=BATCH_SIZE):\n    return model.encode(texts, batch_size=batch_size, show_progress_bar=True, normalize_embeddings=True)\n\ndef generate_question_embeddings(questions, model):\n    texts = [f\"<Construct> {q['ConstructName']} <Subject> {q['SubjectName']} <Question> {q['QuestionText']} <Answer> {q[f'Answer{answer_choice}Text']}\"\n             for q in questions\n             for answer_choice in ['A', 'B', 'C', 'D']\n             if answer_choice != q['CorrectAnswer']]\n    return generate_embeddings(texts, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:55.875708Z","iopub.execute_input":"2024-10-19T14:07:55.876154Z","iopub.status.idle":"2024-10-19T14:07:55.882815Z","shell.execute_reply.started":"2024-10-19T14:07:55.876113Z","shell.execute_reply":"2024-10-19T14:07:55.881831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_filtered_df(df, question, min_rows=5, max_rows=7):\n    construct_id = question['ConstructId']\n    subject_id = question['SubjectId']\n    \n    filtered_df = df[df['ConstructId'] == construct_id]\n    \n    if len(filtered_df) < min_rows:\n        subject_df = df[(df['SubjectId'] == subject_id) & (df['ConstructId'] != construct_id)]\n        filtered_df = pd.concat([filtered_df, subject_df])\n    \n    if len(filtered_df) < min_rows:\n        random_df = df[~df.index.isin(filtered_df.index)].sample(n=min(min_rows - len(filtered_df), len(df) - len(filtered_df)))\n        filtered_df = pd.concat([filtered_df, random_df])\n    \n    return filtered_df.sample(n=min(max_rows, len(filtered_df)))\n\ndef get_example_sequences(filtered_train_df, num_examples=3):\n    examples = []\n    for _, row in filtered_train_df.sample(n=min(num_examples, len(filtered_train_df))).iterrows():\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != row['CorrectAnswer']:\n                misconception_id = row[f'Misconception{answer_choice}Id']\n                if not pd.isna(misconception_id):\n                    examples.append({\n                        'question': f\"{row['ConstructName']}: {row['QuestionText']}\",\n                        'correct_answer': row[f'Answer{row[\"CorrectAnswer\"]}Text'],\n                        'incorrect_answer': row[f'Answer{answer_choice}Text'],\n                        'misconception': misconception_mapping.loc[int(misconception_id), 'MisconceptionName']\n                    })\n                    break\n    return examples\n\ndef predict_misconception(questions, phi_pipe):\n    all_prompts = []\n    for q in questions:\n        correct_answer_key = f\"Answer{q['CorrectAnswer']}Text\"\n        correct_answer = q[correct_answer_key]\n        \n        filtered_df = generate_filtered_df(train, q)\n        examples = get_example_sequences(filtered_df)\n        \n        messages = []\n        \n        for example in examples:\n            messages.extend([\n                {\"role\": \"user\", \"content\": f\"Question: {example['question']}\"},\n                {\"role\": \"assistant\", \"content\": \"Provide me with the correct answer for a baseline.\"},\n                {\"role\": \"user\", \"content\": f\"Correct Answer: {example['correct_answer']}\"},\n                {\"role\": \"assistant\", \"content\": \"Now - provide the incorrect answer and I will analyze the difference to infer the misconception.\"},\n                {\"role\": \"user\", \"content\": f\"Incorrect Answer: {example['incorrect_answer']}\"},\n                {\"role\": \"assistant\", \"content\": f\"Misconception for incorrect answer: {example['misconception']}\"}\n            ])\n        \n        messages.extend([\n            {\"role\": \"user\", \"content\": f\"Question: {q['ConstructName']}: {q['QuestionText']}\"},\n            {\"role\": \"assistant\", \"content\": \"Provide me with the correct answer for a baseline.\"},\n            {\"role\": \"user\", \"content\": f\"Correct Answer: {correct_answer}\"},\n            {\"role\": \"assistant\", \"content\": \"Now - provide the incorrect answer and I will analyze the difference to infer the misconception.\"},\n        ])\n        \n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != q['CorrectAnswer']:\n                incorrect_answer_key = f\"Answer{answer_choice}Text\"\n                incorrect_answer = q[incorrect_answer_key]\n                \n                prompt_messages = messages.copy()\n                prompt_messages.append({\"role\": \"user\", \"content\": f\"Incorrect Answer: {incorrect_answer}\"})\n                \n                all_prompts.append(prompt_messages)\n    \n    responses = phi_pipe(all_prompts, batch_size=BATCH_SIZE)\n    \n    processed_responses = []\n    for response in responses:\n        if isinstance(response, list) and len(response) > 0:\n            generated_text = response[0].get('generated_text', [])\n            if isinstance(generated_text, list) and len(generated_text) > 0:\n                last_message = generated_text[-1]\n                if isinstance(last_message, dict) and 'content' in last_message:\n                    content = last_message['content'].strip()\n                    start_index = content.find(\"Misconception for incorrect answer:\")\n                    if start_index != -1:\n                        misconception = content[start_index + len(\"Misconception for incorrect answer:\"):].strip()\n                        end_index = misconception.find('.')\n                        if end_index != -1:\n                            misconception = misconception[:end_index + 1].strip()\n                        processed_responses.append(misconception)\n                    else:\n                        processed_responses.append(content)\n                else:\n                    processed_responses.append(str(last_message))\n            else:\n                processed_responses.append(str(generated_text))\n        else:\n            processed_responses.append(str(response))\n\n    return processed_responses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:56.28526Z","iopub.execute_input":"2024-10-19T14:07:56.28565Z","iopub.status.idle":"2024-10-19T14:07:56.30748Z","shell.execute_reply.started":"2024-10-19T14:07:56.285617Z","shell.execute_reply":"2024-10-19T14:07:56.30629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bm25_search(queries, documents, top_k=K):\n    tokenized_corpus = [word_tokenize(doc.lower()) for doc in documents]\n    bm25 = BM25Okapi(tokenized_corpus)\n    \n    results = []\n    scores = []\n    for query in queries:\n        tokenized_query = word_tokenize(query.lower())\n        doc_scores = bm25.get_scores(tokenized_query)\n        top_indices = np.argsort(doc_scores)[::-1][:top_k]\n        top_scores = np.sort(doc_scores)[::-1][:top_k]\n        results.append(top_indices)\n        scores.append(top_scores)\n    return results, scores\n\ndef semantic_search(embeddings, misc_embeddings, top_k=K):\n    d = embeddings.shape[1]  # This will now always be 1024 (768 + 256 padding)\n    index = faiss.IndexFlatL2(d)\n    index.add(misc_embeddings)\n    distances, indices = index.search(embeddings, top_k)\n    print(indices)\n    print(indices)\n    return indices, distances\n\ndef combined_search(semantic_results, semantic_scores, keyword_results, keyword_scores, top_k=FINAL_K, alpha=0.8):\n    combined_results = []\n    for sem_res, sem_scores, key_res, key_scores in zip(semantic_results, semantic_scores, keyword_results, keyword_scores):\n        combined_scores = np.zeros(len(misconception_mapping))\n        \n        # Reverse the order of semantic results (closest matches first)\n        sem_res = sem_res[::-1]\n        sem_scores = sem_scores[::-1]\n        \n        # Normalize semantic scores (now smaller is better)\n        sem_scores_norm = (sem_scores - np.min(sem_scores)) / (np.max(sem_scores) - np.min(sem_scores))\n        sem_scores_norm = 1 - sem_scores_norm  # Invert so that smaller distances get higher scores\n        \n        # Normalize keyword scores\n        key_scores_norm = (key_scores - np.min(key_scores)) / (np.max(key_scores) - np.min(key_scores))\n\n        for idx, score in zip(sem_res, sem_scores_norm):\n            combined_scores[idx] += alpha * score\n        \n        for idx, score in zip(key_res, key_scores_norm):\n            combined_scores[idx] += (1 - alpha) * score\n        \n        top_combined = np.argsort(combined_scores)[::-1][:top_k]\n        combined_results.append(top_combined)\n\n    print(f\"combined res: {combined_results}\")\n    \n    return combined_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:56.456702Z","iopub.execute_input":"2024-10-19T14:07:56.457098Z","iopub.status.idle":"2024-10-19T14:07:56.470612Z","shell.execute_reply.started":"2024-10-19T14:07:56.457061Z","shell.execute_reply":"2024-10-19T14:07:56.469582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pad_embeddings(embeddings):\n    return np.pad(embeddings, ((0, 0), (0, 256)), mode='constant')\n\ndef process_questions_batch(questions, misc_embeddings):\n    # Ensemble embeddings\n    bge_embeddings = generate_question_embeddings(questions, bge_model)\n    gte_embeddings = generate_question_embeddings(questions, gte_model)\n    mpnetv2_embeddings = generate_question_embeddings(questions, mpnetv2_model)\n    \n    # Pad embeddings\n    bge_embeddings_padded = bge_embeddings\n    gte_embeddings_padded = pad_embeddings(gte_embeddings)\n    mpnetv2_embeddings_padded = pad_embeddings(mpnetv2_embeddings)\n    \n    weight1, weight2, weight3 = 0.5, 0.29, 0.35\n    ensemble_embeddings = (weight1 * gte_embeddings_padded + weight2 * bge_embeddings_padded + weight3 * mpnetv2_embeddings_padded)\n\n    print(ensemble_embeddings.shape)\n    \n    # Semantic search\n    semantic_results, semantic_scores = semantic_search(ensemble_embeddings, misc_embeddings)\n    \n    # Keyword search\n    llm_responses = predict_misconception(questions, phi_pipe)\n    keyword_results, keyword_scores = bm25_search(llm_responses, misconception_mapping['MisconceptionName'].tolist())\n    \n    # Hybrid search\n    combined_results = combined_search(semantic_results, semantic_scores, keyword_results, keyword_scores)\n    \n    results = []\n    result_index = 0\n    for question in questions:\n        for answer_choice in ['A', 'B', 'C', 'D']:\n            if answer_choice != question['CorrectAnswer']:\n                top_misconceptions = combined_results[result_index]\n                results.append({\n                    'QuestionId_Answer': f\"{question['QuestionId']}_{answer_choice}\",\n                    'MisconceptionId': ' '.join(map(str, top_misconceptions))\n                })\n                result_index += 1\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:56.606111Z","iopub.execute_input":"2024-10-19T14:07:56.606723Z","iopub.status.idle":"2024-10-19T14:07:56.617665Z","shell.execute_reply.started":"2024-10-19T14:07:56.606681Z","shell.execute_reply":"2024-10-19T14:07:56.616692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_misc_embeddings(misconceptions):\n    bge_embeddings = generate_embeddings(misconceptions, bge_model)\n    gte_embeddings = generate_embeddings(misconceptions, gte_model)\n    mpnetv2_embeddings = generate_embeddings(misconceptions, mpnetv2_model)\n    \n    # Pad embeddings\n    bge_embeddings_padded = bge_embeddings\n    gte_embeddings_padded = pad_embeddings(gte_embeddings)\n    mpnetv2_embeddings_padded = pad_embeddings(mpnetv2_embeddings)\n    \n    # Use the same weights as in the question embedding ensemble\n    weight1, weight2, weight3 = 0.5, 0.29, 0.35\n    ensemble_embeddings = (weight1 * gte_embeddings_padded + weight2 * mpnetv2_embeddings_padded + weight3 * bge_embeddings_padded)\n    \n    return ensemble_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:56.754222Z","iopub.execute_input":"2024-10-19T14:07:56.755005Z","iopub.status.idle":"2024-10-19T14:07:56.7608Z","shell.execute_reply.started":"2024-10-19T14:07:56.754966Z","shell.execute_reply":"2024-10-19T14:07:56.759908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate misconception embeddings using the ensemble approach\nmisconceptions = misconception_mapping['MisconceptionName'].tolist()\nmisc_embeddings = generate_misc_embeddings(misconceptions)\nprint(f\"Ensemble misconception embeddings shape: {misc_embeddings.shape}\")\nprint(misc_embeddings)\n\n# Update the main execution loop\nresults = []\nfor i in range(0, len(test), BATCH_SIZE):\n    batch = test.iloc[i:i+BATCH_SIZE].to_dict('records')\n    batch_results = process_questions_batch(batch, misc_embeddings)\n    if batch_results:\n        results.extend(batch_results)\n    else:\n        print(f\"Warning: No results for batch starting at index {i}\")\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(f\"Processed {i+len(batch)} out of {len(test)} questions\")\n\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:07:56.932978Z","iopub.execute_input":"2024-10-19T14:07:56.933374Z","iopub.status.idle":"2024-10-19T14:08:57.059059Z","shell.execute_reply.started":"2024-10-19T14:07:56.933335Z","shell.execute_reply":"2024-10-19T14:08:57.05812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T14:08:57.243095Z","iopub.execute_input":"2024-10-19T14:08:57.243481Z","iopub.status.idle":"2024-10-19T14:08:57.255293Z","shell.execute_reply.started":"2024-10-19T14:08:57.243434Z","shell.execute_reply":"2024-10-19T14:08:57.254193Z"}},"outputs":[],"execution_count":null}]}