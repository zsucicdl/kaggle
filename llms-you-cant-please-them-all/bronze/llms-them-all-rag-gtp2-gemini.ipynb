{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOO/YEJHITz/7JZjroRavy+"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>LLMs You Can't Please Them All - RAG GTP Gemini</b></div>\n\n<div align=\"center\">\n    <img src=\"https://img.freepik.com/vetores-gratis/banner-abstrato-com-design-de-comunicacao-de-rede-poli-plexo_1048-12914.jpg?t=st=1733785035~exp=1733788635~hmac=c21820351902dbef6335cac51924f0043011b6675ba20c6ea631f0b35706df17&w=740\" />\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 1 - Business Problem</b></div>","metadata":{}},{"cell_type":"markdown","source":"\r\n\r\n**Business Problem: Improving Customer Service with LLMs in an E-commerce Platform**\r\n\r\n**Description:**\r\nOur e-commerce platform is receiving a large volume of customer service inquiries every day, ranging from questions about order status to inquiries about product availability. The current customer support team is overwhelmed, leading to delays in response times and potential customer dissatisfaction. \r\n\r\nWe want to use LLMs, specificaGTP, Geminiith Gemma RAG, to automate and optimize the response process. By combining a large pre-trained language model with a retrieval system that can access a knowledge base of common queries and responses, we aim to create a more efficient and effective customer support system.\r\n\r\nThe goal is to build a system where the LLM can intelligently retrieve relevant information from the knowledge base and generate responses that are contextually appropriate, clear, and helpful. This will reduce the workload of the support team and improve the overall customer experience by providing instant responses to commt the scenario!","metadata":{}},{"cell_type":"code","source":"# Import the necessary libraries\nimport faiss\n\n# Import additional libraries\nimport numpy as np\nimport pandas as pd\n\n# Import deep learning libraries\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"id":"C6mnZcfZUGZ9","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:51:50.534842Z","iopub.execute_input":"2024-12-09T22:51:50.535255Z","iopub.status.idle":"2024-12-09T22:51:50.540542Z","shell.execute_reply.started":"2024-12-09T22:51:50.535221Z","shell.execute_reply":"2024-12-09T22:51:50.539471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 2 - Kaggle secrets</b></div>","metadata":{}},{"cell_type":"code","source":"# Import the library to access Kaggle secrets and configure the API key\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n# Retrieve the secret API key and configure the Gemini API\nsecret_value_0 = user_secrets.get_secret(\"Gemeni\")\ngenai.configure(api_key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:36:20.710736Z","iopub.execute_input":"2024-12-09T22:36:20.71132Z","iopub.status.idle":"2024-12-09T22:36:20.898015Z","shell.execute_reply.started":"2024-12-09T22:36:20.711262Z","shell.execute_reply":"2024-12-09T22:36:20.896836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking available models\nfor m in genai.list_models():\n    if 'generateContent' in m.supported_generation_methods:\n        print(m.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:36:40.423147Z","iopub.execute_input":"2024-12-09T22:36:40.423518Z","iopub.status.idle":"2024-12-09T22:36:40.872486Z","shell.execute_reply.started":"2024-12-09T22:36:40.423486Z","shell.execute_reply":"2024-12-09T22:36:40.871455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 3 - Database</b></div>","metadata":{}},{"cell_type":"code","source":"# Load test data\ntest_data = pd.read_csv(\"/kaggle/input/llms-you-cant-please-them-all/test.csv\")\ntest_data","metadata":{"id":"37eOujGoU37W","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:11.686262Z","iopub.execute_input":"2024-12-09T22:27:11.687598Z","iopub.status.idle":"2024-12-09T22:27:11.703217Z","shell.execute_reply.started":"2024-12-09T22:27:11.687557Z","shell.execute_reply":"2024-12-09T22:27:11.702287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first rows of test_data\ntest_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:11.941699Z","iopub.execute_input":"2024-12-09T22:27:11.942503Z","iopub.status.idle":"2024-12-09T22:27:11.951157Z","shell.execute_reply.started":"2024-12-09T22:27:11.942466Z","shell.execute_reply":"2024-12-09T22:27:11.950153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the last rows of test_data\ntest_data.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:12.38714Z","iopub.execute_input":"2024-12-09T22:27:12.387532Z","iopub.status.idle":"2024-12-09T22:27:12.396929Z","shell.execute_reply.started":"2024-12-09T22:27:12.387497Z","shell.execute_reply":"2024-12-09T22:27:12.395834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get general information about the test_data DataFrame\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:12.729808Z","iopub.execute_input":"2024-12-09T22:27:12.730202Z","iopub.status.idle":"2024-12-09T22:27:12.74299Z","shell.execute_reply.started":"2024-12-09T22:27:12.73017Z","shell.execute_reply":"2024-12-09T22:27:12.741869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the data types of test_data\ntest_data.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:13.157453Z","iopub.execute_input":"2024-12-09T22:27:13.158594Z","iopub.status.idle":"2024-12-09T22:27:13.165696Z","shell.execute_reply.started":"2024-12-09T22:27:13.158555Z","shell.execute_reply":"2024-12-09T22:27:13.16463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 4 - RAG model GTP 2</b></div>","metadata":{}},{"cell_type":"code","source":"# Load a retrieval model\nretriever_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')","metadata":{"id":"xVWwNjfpUOYF","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:13.604589Z","iopub.execute_input":"2024-12-09T22:27:13.605025Z","iopub.status.idle":"2024-12-09T22:27:15.011719Z","shell.execute_reply.started":"2024-12-09T22:27:13.604991Z","shell.execute_reply":"2024-12-09T22:27:15.010183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example corpus for retrieval\n\ndocuments = [\"The importance of self-reliance in healthcare.\",\n             \"Consulting management to address marketing conflicts.\",\n             \"The role of self-reliance in software engineering success.\"]\n\n# Generate embeddings\ndocument_embeddings = retriever_model.encode(documents)\n\n# Create FAISS index\nindex = faiss.IndexFlatL2(document_embeddings.shape[1])\nindex.add(np.array(document_embeddings))","metadata":{"id":"6CsD3QdbUdM5","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:15.014309Z","iopub.execute_input":"2024-12-09T22:27:15.014801Z","iopub.status.idle":"2024-12-09T22:27:15.07386Z","shell.execute_reply.started":"2024-12-09T22:27:15.014747Z","shell.execute_reply":"2024-12-09T22:27:15.07283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieval function\n\ndef retrieve(query, top_k=2):\n    query_embedding = retriever_model.encode([query])\n    distances, indices = index.search(np.array(query_embedding), top_k)\n    results = [documents[i] for i in indices[0]]\n    return results","metadata":{"id":"Uqwk8C_AUlvG","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:21.349281Z","iopub.execute_input":"2024-12-09T22:27:21.34997Z","iopub.status.idle":"2024-12-09T22:27:21.358504Z","shell.execute_reply.started":"2024-12-09T22:27:21.349921Z","shell.execute_reply":"2024-12-09T22:27:21.357314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 5 - Model GTP 2</b></div>","metadata":{}},{"cell_type":"code","source":"# Load pre-trained LLM\n\n## Replace with your preferred model\nllm_model_name = \"gpt2\"","metadata":{"id":"NipqFhCmUuS3","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:24.382148Z","iopub.execute_input":"2024-12-09T22:27:24.38331Z","iopub.status.idle":"2024-12-09T22:27:24.387272Z","shell.execute_reply.started":"2024-12-09T22:27:24.383271Z","shell.execute_reply":"2024-12-09T22:27:24.386379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the language model\nllm_model = AutoModelForCausalLM.from_pretrained(llm_model_name)","metadata":{"id":"k2RGptw5Uv0C","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:25.241393Z","iopub.execute_input":"2024-12-09T22:27:25.242166Z","iopub.status.idle":"2024-12-09T22:27:25.855421Z","shell.execute_reply.started":"2024-12-09T22:27:25.242127Z","shell.execute_reply":"2024-12-09T22:27:25.854452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model's tokenizer\ntokenizer = AutoTokenizer.from_pretrained(llm_model_name)","metadata":{"id":"tdgAW3ZTUxz6","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:25.857312Z","iopub.execute_input":"2024-12-09T22:27:25.858201Z","iopub.status.idle":"2024-12-09T22:27:26.160064Z","shell.execute_reply.started":"2024-12-09T22:27:25.858151Z","shell.execute_reply":"2024-12-09T22:27:26.158939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Essay generation using retrieved context\n\ndef generate_essay(topic):\n    # Retrieve contextual documents\n    context = retrieve(topic)\n    \n    # Join the context documents\n    context_text = \"\\n\".join(context)\n\n    # Construct input for the LLM\n    input_text = f\"Topic: {topic}\\nEssay:\\n{context_text}\"\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n    # Define attention mask and pad_token_id\n    attention_mask = torch.ones(inputs.shape, device=inputs.device)\n    pad_token_id = tokenizer.eos_token_id\n\n    # Generate output with better control\n    outputs = llm_model.generate(inputs,\n                                 attention_mask=attention_mask,\n                                 max_length=150,\n                                 num_return_sequences=1,\n                                 temperature=0.7,\n                                 do_sample=True,  # Enable sampling for more varied output\n                                 top_p=0.9,  # Nucleus sampling\n                                 top_k=50,  # Limit the sampling to top-k candidates\n                                 no_repeat_ngram_size=2,  # Avoid repeated n-grams\n                                 pad_token_id=pad_token_id  # Ensure proper padding\n                                 )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"id":"Pbl3rTxMUzfL","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:26.405017Z","iopub.execute_input":"2024-12-09T22:27:26.405421Z","iopub.status.idle":"2024-12-09T22:27:26.414051Z","shell.execute_reply.started":"2024-12-09T22:27:26.405388Z","shell.execute_reply":"2024-12-09T22:27:26.41303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 6 - Viewing RAG results</b></div>","metadata":{}},{"cell_type":"code","source":"# Generate essays for each topic\n\nresults = []\nfor _, row in test_data.iterrows():\n    essay = generate_essay(row[\"topic\"])\n    results.append({\"id\": row[\"id\"], \"essay\": essay})\n\n# Results\nfor result in results:\n    print(f\"ID: {result['id']}\\nEssay: {result['essay']}\\n\")","metadata":{"id":"JDJizzVHW4Xi","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:27:27.126639Z","iopub.execute_input":"2024-12-09T22:27:27.127073Z","iopub.status.idle":"2024-12-09T22:27:28.170292Z","shell.execute_reply.started":"2024-12-09T22:27:27.127034Z","shell.execute_reply":"2024-12-09T22:27:28.169146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 7 - RAG model Gemini</b></div>","metadata":{}},{"cell_type":"code","source":"# Load data from the CSV\ndata_test = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')['topic'].tolist()\ndata_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:30:46.677272Z","iopub.execute_input":"2024-12-09T22:30:46.677709Z","iopub.status.idle":"2024-12-09T22:30:46.688153Z","shell.execute_reply.started":"2024-12-09T22:30:46.67766Z","shell.execute_reply":"2024-12-09T22:30:46.686974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 8 - Model Gemini</b></div>","metadata":{}},{"cell_type":"code","source":"# Initialize the Gemini model\nmodel_gemini = \"gemini-1.5-pro\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:30:56.911977Z","iopub.execute_input":"2024-12-09T22:30:56.912366Z","iopub.status.idle":"2024-12-09T22:30:56.917009Z","shell.execute_reply.started":"2024-12-09T22:30:56.912332Z","shell.execute_reply":"2024-12-09T22:30:56.915996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Config generation\ngeneration_config = {\"temperature\": 0.7,\n                     \"top_p\": 0.9,\n                     \"top_k\": 50,\n                     \"max_output_tokens\": 8192,\n                     \"response_mime_type\": \"text/plain\",}\n\n# System instruction\nsystem_instruction = \"\"\"\n# System Prompt: You are an AI Research Assistant. Understand and summarize data. Answer briefly, referring only to the context.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:43:26.024232Z","iopub.execute_input":"2024-12-09T22:43:26.025022Z","iopub.status.idle":"2024-12-09T22:43:26.029679Z","shell.execute_reply.started":"2024-12-09T22:43:26.024983Z","shell.execute_reply":"2024-12-09T22:43:26.028665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 9 - RAG Gemini</b></div>","metadata":{}},{"cell_type":"code","source":"# Function to generate answers using the Gemini model\ndef generate_with_rag(query):\n    \n    ### Step 1: Retrieve relevant documents\n\n    # Adjust the number of documents as needed\n    context = retrieve(query, top_k=3)  \n    context_text = \"\\n\".join(context)\n    \n    ### Step 2: Concatenate context and query for the Gemini model\n    prompt = f\"Context:\\n{context_text}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    # Create the chat session with the Gemini model\n    model = genai.GenerativeModel(model_name=model_gemini,\n                                  generation_config=generation_config, \n                                  system_instruction=system_instruction)\n    \n    # Send the message to the Gemini model\n    chat_session = model.start_chat(history=[{'role': 'user', 'parts': [prompt]}])\n    \n    # Get the response from the model\n    response = chat_session.send_message(prompt)\n    \n    return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:43:52.538652Z","iopub.execute_input":"2024-12-09T22:43:52.539042Z","iopub.status.idle":"2024-12-09T22:43:52.545991Z","shell.execute_reply.started":"2024-12-09T22:43:52.53901Z","shell.execute_reply":"2024-12-09T22:43:52.544908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 9 - Viewing RAG results Gemini</b></div>","metadata":{}},{"cell_type":"code","source":"# Generate 5 topics related to self-reliance and success in Data Science\nprompt = \"Generate 5 topics related to self-reliance and success in Data Science.\"\nresponse = generate_with_rag(prompt)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:33:32.119899Z","iopub.execute_input":"2024-12-09T22:33:32.120312Z","iopub.status.idle":"2024-12-09T22:33:33.961966Z","shell.execute_reply.started":"2024-12-09T22:33:32.120275Z","shell.execute_reply":"2024-12-09T22:33:33.960822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate 5 topics related to self-reliance and success in generative artificial intelligence\nprompt = \"Generate 5 topics related to self-reliance and success in generative artificial intelligence.\"\nresponse = generate_with_rag(prompt)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:37:44.48156Z","iopub.execute_input":"2024-12-09T22:37:44.48197Z","iopub.status.idle":"2024-12-09T22:37:46.545402Z","shell.execute_reply.started":"2024-12-09T22:37:44.481936Z","shell.execute_reply":"2024-12-09T22:37:46.544312Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 10 - Submission</b></div>","metadata":{}},{"cell_type":"code","source":"# Create submission file\nsubmission2 = pd.DataFrame(results)\nsubmission2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:38:41.672295Z","iopub.execute_input":"2024-12-09T22:38:41.67272Z","iopub.status.idle":"2024-12-09T22:38:41.68247Z","shell.execute_reply.started":"2024-12-09T22:38:41.672684Z","shell.execute_reply":"2024-12-09T22:38:41.681336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first rows of the essay\nsubmission2.essay.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:38:42.73439Z","iopub.execute_input":"2024-12-09T22:38:42.734925Z","iopub.status.idle":"2024-12-09T22:38:42.742132Z","shell.execute_reply.started":"2024-12-09T22:38:42.734887Z","shell.execute_reply":"2024-12-09T22:38:42.741083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the submission2 DataFrame to a CSV file called submission1.csv\nsubmission2.to_csv(\"submission1.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:38:46.800565Z","iopub.execute_input":"2024-12-09T22:38:46.801587Z","iopub.status.idle":"2024-12-09T22:38:46.807032Z","shell.execute_reply.started":"2024-12-09T22:38:46.801549Z","shell.execute_reply":"2024-12-09T22:38:46.806055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b>Part 11 - Conclusion</b></div>","metadata":{}},{"cell_type":"markdown","source":"- This project demonstrated the effectiveness of using Large-Scale Language Models (LLMs) such as GPT-2 Gemini with RAG to generate essays that cause disagreement among multiple automated judges, addressing the challenge posed by the \"LLMs - You Can't Please Them All\" competition. By exploring text generation strategies that focused on topic diversity and ambiguity, we were able to maximize the variance in scores provided by the LLM-judge models. In addition, the use of the Retrieval-Augmented Generation (RAG) model allowed us to incorporate external data and aggregate different perspectives, enriching the response generation and increasing the complexity of the texts created. This was crucial to generate responses that were sufficiently distinct for the judges to assign varying scores.\n\n- The main contribution of this project was to demonstrate how it is possible to manipulate the interaction between LLMs and automated assessment systems, creating a robust approach to identify and exploit potential biases and limitations of these systems. This study helps provide a deeper understanding of how LLMs can be used efficiently and robustly in large-scale subjective assessment tasks. Throughout the project, we also identified areas of opportunity for future improvements, such as exploring different strategies for adjusting text style and structure, as well as combining multiple LLM models to improve the robustness and variability of scores.\n\n**The next step will be to test new ways of generating more sophisticated texts and further exploit the specific biases of each model, in order to maximize the discrepancy in assessments and contribute to the understanding of the limits and challenges of using LLMs in automated assessment processes.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}