{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Gemma 7b-V3 + Prompt Engineering Strategy:\n\nTrying to randomize prompts to Gemma 7b-V3 so they become more relevant for the challenge's goal","metadata":{"execution":{"iopub.status.busy":"2024-12-03T21:56:09.723409Z","iopub.execute_input":"2024-12-03T21:56:09.724226Z","iopub.status.idle":"2024-12-03T21:56:09.727911Z","shell.execute_reply.started":"2024-12-03T21:56:09.724195Z","shell.execute_reply":"2024-12-03T21:56:09.727076Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport random\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-12-04T11:41:34.103167Z","iopub.execute_input":"2024-12-04T11:41:34.103869Z","iopub.status.idle":"2024-12-04T11:41:35.085487Z","shell.execute_reply.started":"2024-12-04T11:41:34.103813Z","shell.execute_reply":"2024-12-04T11:41:35.084636Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load models\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = '/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:44:27.591608Z","iopub.execute_input":"2024-12-04T11:44:27.591975Z","iopub.status.idle":"2024-12-04T11:45:10.771511Z","shell.execute_reply.started":"2024-12-04T11:44:27.591944Z","shell.execute_reply":"2024-12-04T11:45:10.770836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_prompt(topic):\n    \"\"\"\n    Generate a prompt for the GEMMA model based on the given topic. The prompt strategy involves:\n    - Introducing the topic in an engaging manner.\n    - Including instructions to guide the model's response.\n    - Adding context or constraints for better specificity.\n    \"\"\"\n    introduction = random.choice([\n        f\"Write a detailed essay on the topic '{topic}', exploring its various dimensions.\",\n        f\"Craft an insightful and balanced essay about '{topic}', considering multiple perspectives.\",\n        f\"Discuss the significance of '{topic}' in contemporary society and its implications.\"\n    ])\n\n    instructions = random.choice([\n        \"Make sure to include examples, counterarguments, and a conclusion.\",\n        \"Incorporate historical context, current trends, and potential future developments.\",\n        \"Use a mix of rhetorical questions, facts, analogies, and quotes to enhance the essay.\"\n    ])\n\n    constraints = random.choice([\n        \"Limit the essay to approximately 100 words.\",\n        \"Focus on clarity and coherence while maintaining an engaging tone.\",\n        \"Ensure the essay is suitable for a general audience with diverse backgrounds.\"\n    ])\n\n    prompt = f\"{introduction} {instructions} {constraints}\"\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:57:45.880655Z","iopub.execute_input":"2024-12-04T11:57:45.880991Z","iopub.status.idle":"2024-12-04T11:57:45.886604Z","shell.execute_reply.started":"2024-12-04T11:57:45.880962Z","shell.execute_reply":"2024-12-04T11:57:45.885566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission(test_df, model, tokenizer):\n    essays = []\n    for _, row in test_df.iterrows():\n        # Generate the prompt\n        prompt = generate_prompt(row['topic'])\n        \n        # Tokenize the input prompt\n        input_ids = tokenizer(prompt, return_tensors=\"pt\")\n        input_ids = {k: v.to('cuda') for k, v in input_ids.items()}\n        \n        # Generate the essay\n        outputs = model.generate(**input_ids, max_new_tokens=100)\n        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        essays.append(output_text)\n    \n    # Add the essays to the test DataFrame\n    test_df['essay'] = essays\n    \n    # Return only the required columns\n    return test_df[['id', 'essay']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:57:46.363934Z","iopub.execute_input":"2024-12-04T11:57:46.364321Z","iopub.status.idle":"2024-12-04T11:57:46.370219Z","shell.execute_reply.started":"2024-12-04T11:57:46.364292Z","shell.execute_reply":"2024-12-04T11:57:46.369274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(file_path):\n    return pd.read_csv(file_path)\n    \ndef save_submission(submission_df, output_file):\n    submission_df.to_csv(output_file, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:57:46.982312Z","iopub.execute_input":"2024-12-04T11:57:46.982667Z","iopub.status.idle":"2024-12-04T11:57:46.987678Z","shell.execute_reply.started":"2024-12-04T11:57:46.982638Z","shell.execute_reply":"2024-12-04T11:57:46.986612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_file = \"/kaggle/input/llms-you-cant-please-them-all/test.csv\"\ntest_data = load_data(test_file)\n\nsubmission_df = create_submission(test_data, model, tokenizer)\n\noutput_file = \"submission.csv\"\nsave_submission(submission_df, output_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:57:47.811731Z","iopub.execute_input":"2024-12-04T11:57:47.812469Z","iopub.status.idle":"2024-12-04T11:58:03.469794Z","shell.execute_reply.started":"2024-12-04T11:57:47.812434Z","shell.execute_reply":"2024-12-04T11:58:03.469131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:58:13.342171Z","iopub.execute_input":"2024-12-04T11:58:13.342513Z","iopub.status.idle":"2024-12-04T11:58:13.354663Z","shell.execute_reply.started":"2024-12-04T11:58:13.342484Z","shell.execute_reply":"2024-12-04T11:58:13.3537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}