{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"},{"sourceId":120005,"sourceType":"modelInstanceVersion","modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:46:04.259623Z","iopub.execute_input":"2024-12-06T14:46:04.259899Z","iopub.status.idle":"2024-12-06T14:46:05.497221Z","shell.execute_reply.started":"2024-12-06T14:46:04.259872Z","shell.execute_reply":"2024-12-06T14:46:05.496173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nimport torch\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:46:52.438167Z","iopub.execute_input":"2024-12-06T14:46:52.438942Z","iopub.status.idle":"2024-12-06T14:46:54.545075Z","shell.execute_reply.started":"2024-12-06T14:46:52.438907Z","shell.execute_reply":"2024-12-06T14:46:54.544165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up the environment for efficient processing\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:46:56.945638Z","iopub.execute_input":"2024-12-06T14:46:56.94613Z","iopub.status.idle":"2024-12-06T14:46:56.950697Z","shell.execute_reply.started":"2024-12-06T14:46:56.946098Z","shell.execute_reply":"2024-12-06T14:46:56.949723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model path and pipeline setup for LLaMA 3.2 3B Instruct\nmodel_id = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:47:00.831398Z","iopub.execute_input":"2024-12-06T14:47:00.832187Z","iopub.status.idle":"2024-12-06T14:47:00.835831Z","shell.execute_reply.started":"2024-12-06T14:47:00.832152Z","shell.execute_reply":"2024-12-06T14:47:00.834926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"cuda\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:47:07.238365Z","iopub.execute_input":"2024-12-06T14:47:07.238901Z","iopub.status.idle":"2024-12-06T14:48:10.794175Z","shell.execute_reply.started":"2024-12-06T14:47:07.238869Z","shell.execute_reply":"2024-12-06T14:48:10.793447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prompt template to maximize disagreement\nprompt_template = \"\"\"\nYou are an argumentative writer who thrives on exploring controversial and conflicting ideas. Your task is to write a short essay on the topic: \"{topic_name}\".\nThe essay should:\n- Present arguments that directly oppose each other.\n- Explore the topic from polarized perspectives, emphasizing contradictions.\n- Highlight flaws in all arguments, leaving room for interpretation.\n- Be unique, bold, and thought-provoking.\n\nWrite the essay below:\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:48:41.739264Z","iopub.execute_input":"2024-12-06T14:48:41.739911Z","iopub.status.idle":"2024-12-06T14:48:41.744468Z","shell.execute_reply.started":"2024-12-06T14:48:41.739874Z","shell.execute_reply":"2024-12-06T14:48:41.743458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_file_path = \"/kaggle/input/llms-you-cant-please-them-all/test.csv\"  # Adjust this path as needed\ntest = pd.read_csv(test_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:48:45.924249Z","iopub.execute_input":"2024-12-06T14:48:45.925009Z","iopub.status.idle":"2024-12-06T14:48:45.939236Z","shell.execute_reply.started":"2024-12-06T14:48:45.924973Z","shell.execute_reply":"2024-12-06T14:48:45.93826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process topics and generate essays in batches\nbatch_size = 3  # Adjust based on available GPU memory\npredictions = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:48:49.439825Z","iopub.execute_input":"2024-12-06T14:48:49.440533Z","iopub.status.idle":"2024-12-06T14:48:49.444428Z","shell.execute_reply.started":"2024-12-06T14:48:49.440501Z","shell.execute_reply":"2024-12-06T14:48:49.443465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0, len(test), batch_size):\n    # Prepare batch data\n    batch_topics = test.loc[i:i + batch_size - 1, \"topic\"].tolist()\n    batch_prompts = [prompt_template.format(topic_name=topic) for topic in batch_topics]\n\n    # Generate predictions for the batch\n    outputs = pipeline(\n        batch_prompts,  # Input prompts directly\n        max_new_tokens=1024,  # Longer generation to allow more depth\n        temperature=1.7,  # Higher temperature to encourage diverse outputs\n        top_p=0.9,  # Nucleus sampling for creative diversity\n        top_k=50,  # Limits to the top 50 token probabilities\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:03:47.364785Z","iopub.execute_input":"2024-12-06T15:03:47.365141Z","iopub.status.idle":"2024-12-06T15:05:46.21294Z","shell.execute_reply.started":"2024-12-06T15:03:47.365112Z","shell.execute_reply":"2024-12-06T15:05:46.21215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_essays = [output[\"generated_text\"] if isinstance(output, dict) else output for output in outputs]\npredictions.append(batch_essays)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:06:02.276353Z","iopub.execute_input":"2024-12-06T15:06:02.277237Z","iopub.status.idle":"2024-12-06T15:06:02.28134Z","shell.execute_reply.started":"2024-12-06T15:06:02.2772Z","shell.execute_reply":"2024-12-06T15:06:02.280424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# if i < 3 * batch_size:\n#         for topic, essay in zip(batch_topics, batch_essays):\n#             print(\"Topic: \", topic)\n            \n#             print(\"Generated Essay: \", essay)\n#             print(\"\\n\\n***********************\\n\\n\")\n#             print(type(essay))\nnew_predictions=[]\nif i < 3 * batch_size:\n    for topic, essay in zip(batch_topics, batch_essays):\n        print(\"Topic: \", topic)\n        # Extract the essay text from the dictionary\n        if isinstance(essay, list) and len(essay) > 0 and 'generated_text' in essay[0]:\n            essay_text = essay[0]['generated_text']\n        else:\n            essay_text = \"Invalid format for essay\"\n        \n        print(\"Generated Essay: \", essay_text)\n        print(\"\\n\\n***********************\\n\\n\")\n        print(type(essay_text))\n        new_predictions.append(essay_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:21:22.239916Z","iopub.execute_input":"2024-12-06T15:21:22.2404Z","iopub.status.idle":"2024-12-06T15:21:22.249029Z","shell.execute_reply.started":"2024-12-06T15:21:22.240355Z","shell.execute_reply":"2024-12-06T15:21:22.248113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(new_predictions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:22:25.527118Z","iopub.execute_input":"2024-12-06T15:22:25.527828Z","iopub.status.idle":"2024-12-06T15:22:25.531724Z","shell.execute_reply.started":"2024-12-06T15:22:25.527795Z","shell.execute_reply":"2024-12-06T15:22:25.530905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:22:45.362779Z","iopub.execute_input":"2024-12-06T15:22:45.363544Z","iopub.status.idle":"2024-12-06T15:22:45.374834Z","shell.execute_reply.started":"2024-12-06T15:22:45.36351Z","shell.execute_reply":"2024-12-06T15:22:45.373861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if len(predictions) > len(submission):\n#     predictions = predictions[:len(submission)]\nsubmission['essay'] = new_predictions\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:23:01.516358Z","iopub.execute_input":"2024-12-06T15:23:01.516956Z","iopub.status.idle":"2024-12-06T15:23:01.524839Z","shell.execute_reply.started":"2024-12-06T15:23:01.516927Z","shell.execute_reply":"2024-12-06T15:23:01.523924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv',index = False)\nprint(\"submission file saved succesfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:23:06.865754Z","iopub.execute_input":"2024-12-06T15:23:06.866524Z","iopub.status.idle":"2024-12-06T15:23:06.872378Z","shell.execute_reply.started":"2024-12-06T15:23:06.866474Z","shell.execute_reply":"2024-12-06T15:23:06.871514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission[\"essay\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:23:12.384972Z","iopub.execute_input":"2024-12-06T15:23:12.385511Z","iopub.status.idle":"2024-12-06T15:23:12.391608Z","shell.execute_reply.started":"2024-12-06T15:23:12.385478Z","shell.execute_reply":"2024-12-06T15:23:12.390622Z"}},"outputs":[],"execution_count":null}]}