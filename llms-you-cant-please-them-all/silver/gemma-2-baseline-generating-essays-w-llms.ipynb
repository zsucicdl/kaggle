{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"},{"sourceId":7715470,"sourceType":"datasetVersion","datasetId":4505971},{"sourceId":8754872,"sourceType":"datasetVersion","datasetId":5259309},{"sourceId":10092657,"sourceType":"datasetVersion","datasetId":4505960},{"sourceId":85979,"sourceType":"modelInstanceVersion","modelInstanceId":72240,"modelId":76277}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":64.136685,"end_time":"2024-02-27T20:52:18.400218","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-27T20:51:14.263533","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"3b15ea24-20c2-49ba-af81-0b6a6a40d416","cell_type":"markdown","source":"# Gemma 2 Baseline PyTorch\n## Generate Random Essays for given Topics with Gemma Models\n\n##### This is a baseline notebook in which we will be using Gemma 2 models inference in PyTorch for generating essays. \n##### You can check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n\n##### You can also use other Gemma Variants including 9B, 27B parameter models. You just need to add different variant from Kaggle models and update the parameters \"GEMMA_MODEL\" , \"MODEL_CONFIG\" & \"MODEL_DIR\".\n\n### Note: Since the competition is more than just generating an essay for a given topic, this notebook is just to provide a solution with the help of LLMs.","metadata":{}},{"id":"ac4b1c7a-4cc9-4ac2-83c6-8aca3210c05d","cell_type":"markdown","source":"## 1.  Installing additional dependencies ","metadata":{}},{"id":"c933ae50-4e38-44be-a34a-df663b26d361","cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/immutabledict/immutabledict-4.1.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/sentencepiece-0-2-0-cp310-cp310-manylinux/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n\n!mkdir /kaggle/working/gemma/\n!cp /kaggle/input/gemma-pytorch/gemma_pytorch-main/gemma/* /kaggle/working/gemma/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"edba1fe9-7d5a-48d8-97cd-a3c369239a23","cell_type":"code","source":"import sys \nsys.path.append(\"/kaggle/working/\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e31c174c-e82f-48b4-a27a-d909f7e1130a","cell_type":"markdown","source":"# 2.  Import Gemma Modules","metadata":{}},{"id":"ce9ec0c7-e76f-4b34-a169-a35867bac9b1","cell_type":"code","source":"from gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nfrom transformers import AutoTokenizer\nimport contextlib\nimport os\nimport torch\nimport random\nimport pandas as pd\nrandom.seed(0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ffc31e46-56b6-4548-bfa9-707783a95b42","cell_type":"markdown","source":"# 3. Select Gemma Model Variant","metadata":{}},{"id":"ae719c59-a16f-4e85-be1b-a04e0453a5e8","cell_type":"code","source":"GEMMA_MODEL = '2b-it'\nDEVICE = 'cuda' \nMODEL_CONFIG = '2b-v2'\nMODEL_DIR = \"/kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1\"\nCKPT_PATH = os.path.join(MODEL_DIR, f'model.ckpt')\nTOKENIZER_PATH = os.path.join(MODEL_DIR, f'tokenizer.model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"89ad76bd-8be9-442a-82b7-d67809671f1e","cell_type":"markdown","source":"# 4. Loading Model & Model Config","metadata":{}},{"id":"7c8ef756-bb97-47e3-94d4-4f999cb0eed2","cell_type":"code","source":"# Set up model config.\nCONFIG = get_model_config(MODEL_CONFIG)\nCONFIG.quant = 'quant' in GEMMA_MODEL\nCONFIG.tokenizer = TOKENIZER_PATH\ntorch.set_default_dtype(CONFIG.get_dtype())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e7505662-4df7-49b4-ab2c-a92314124125","cell_type":"code","source":"# Initiialize the model and load the weights.\ndevice = torch.device(DEVICE)\nmodel = GemmaForCausalLM(CONFIG)\nmodel.load_weights(CKPT_PATH)\nmodel = model.to(device).eval()\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a895eb3f-f94a-472d-8db9-9bbbc5553a59","cell_type":"markdown","source":"# 5. Defining Gemma Chat Template","metadata":{}},{"id":"9d331b78-5edb-4f2b-94ef-410a5d82cadc","cell_type":"code","source":"# This is the prompt format the model expects\nUSER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ddd86ca6-4ac7-4b2b-b295-7d041b957d8a","cell_type":"markdown","source":"# 6. Sample Generation","metadata":{}},{"id":"89fec907-4de4-4532-999d-96bd409aaade","cell_type":"code","source":"max_seq_length = 1024\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, max_seq_length=max_seq_length)\nEOS_TOKEN = tokenizer.eos_token\n\ninput_text = \"What is Kaggle?\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9876d710-30d9-4105-9444-d851ed7f246b","cell_type":"code","source":"input_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0b4477ba-13f1-4687-a615-dca774f9a9cd","cell_type":"code","source":"print('Chat prompt:\\n', USER_CHAT_TEMPLATE.format(prompt=input_text))\n\nresults = model.generate(\n    USER_CHAT_TEMPLATE.format(prompt=input_text),\n    device=DEVICE,\n    output_len=128,\n)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b2ddda61-9024-4f68-8a44-8aa405b2acd9","cell_type":"markdown","source":"# 7. Design your own Prompt","metadata":{}},{"id":"881d202c","cell_type":"code","source":"prompt_for_llm = (\n    \"<start_of_turn>user\\nGenerate an essay for the following topic with no more than 100 words: {topic_name}.\"\n    \"<end_of_turn>\\n<start_of_turn>model\\n\"\n)\n","metadata":{"papermill":{"duration":0.016016,"end_time":"2024-02-27T20:52:12.209684","exception":false,"start_time":"2024-02-27T20:52:12.193668","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"07033533","cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')","metadata":{"papermill":{"duration":0.046216,"end_time":"2024-02-27T20:52:12.264417","exception":false,"start_time":"2024-02-27T20:52:12.218201","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"803b5f19-51c2-47c9-b2cf-b4a3219c6715","cell_type":"code","source":"test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ba2d955","cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')","metadata":{"papermill":{"duration":0.023977,"end_time":"2024-02-27T20:52:12.296966","exception":false,"start_time":"2024-02-27T20:52:12.272989","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d076bffe","cell_type":"code","source":"test.loc[0,'topic']","metadata":{"papermill":{"duration":0.022089,"end_time":"2024-02-27T20:52:12.327421","exception":false,"start_time":"2024-02-27T20:52:12.305332","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"197d1f7d","cell_type":"code","source":"sample_sub","metadata":{"papermill":{"duration":0.019735,"end_time":"2024-02-27T20:52:12.354767","exception":false,"start_time":"2024-02-27T20:52:12.335032","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a58bd72a-c764-4c38-9cc3-9c7365da6350","cell_type":"markdown","source":"# 7. Basic Baseline\n#### Here we will try to use Gemma model for generating essays relevant to the provided topics in test csv.","metadata":{}},{"id":"2bf64714","cell_type":"code","source":"\n\npredictions = []\n\nfor i in range(len(test)):\n    topic = test.loc[i, 'topic']\n    gen_essay = model.generate(\n        prompt_for_llm.format(topic_name=topic),\n        device=device,\n        output_len=32, ### smaller essay\n    )\n    predictions.append(gen_essay) ### V1\n    # predictions.append(topic + ' ' + gen_essay) ### V3\n\n    \n    if i<=2:\n        print('Topic: ', topic)\n        print('Generated Essay: ', gen_essay)\n        print('\\n\\n***********************\\n\\n')","metadata":{"papermill":{"duration":4.806177,"end_time":"2024-02-27T20:52:17.169674","exception":false,"start_time":"2024-02-27T20:52:12.363497","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"7065f278","cell_type":"code","source":"predictions[0]","metadata":{"papermill":{"duration":0.015209,"end_time":"2024-02-27T20:52:17.192106","exception":false,"start_time":"2024-02-27T20:52:17.176897","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"43e6d366","cell_type":"code","source":"sample_sub['essay'] = predictions","metadata":{"papermill":{"duration":0.013597,"end_time":"2024-02-27T20:52:17.212446","exception":false,"start_time":"2024-02-27T20:52:17.198849","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e2421095","cell_type":"code","source":"sample_sub","metadata":{"papermill":{"duration":0.016846,"end_time":"2024-02-27T20:52:17.236046","exception":false,"start_time":"2024-02-27T20:52:17.2192","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"6b90b72b","cell_type":"code","source":"sample_sub.to_csv('submission.csv',index=False)","metadata":{"papermill":{"duration":0.016296,"end_time":"2024-02-27T20:52:17.259257","exception":false,"start_time":"2024-02-27T20:52:17.242961","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"57d7e06f","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006777,"end_time":"2024-02-27T20:52:17.27334","exception":false,"start_time":"2024-02-27T20:52:17.266563","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}