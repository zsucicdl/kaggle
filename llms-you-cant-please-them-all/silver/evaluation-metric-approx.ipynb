{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"},{"sourceId":211084264,"sourceType":"kernelVersion"},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277},{"sourceId":120005,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100936,"modelId":121027},{"sourceId":166247,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141460,"modelId":164048}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h2 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Evaluation Metric (Approx)</h2>\n<br>\n\n- LLM-as-a-judge\n- English Confidence Score\n- Sequence Similarity Matcher","metadata":{}},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">LLM-as-a-judge</h1></span>\n\n#### This is an attempt to understand the evaluation metric using any 3 open-source LLMs as judge\n\n![](https://cdn.prod.website-files.com/64bd90bdba579d6cce245aec/66d3fdd1f10fc3992b6c9d75_66d3fd227a958b870c333da4_llm-judge-metric.png)","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:55:59.867064Z","iopub.execute_input":"2024-12-04T04:55:59.867829Z","iopub.status.idle":"2024-12-04T04:56:04.171018Z","shell.execute_reply.started":"2024-12-04T04:55:59.867789Z","shell.execute_reply":"2024-12-04T04:56:04.170324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"judge_model_list = [\n    \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\",\n    \"/kaggle/input/qwen2.5/transformers/3b-instruct/1\",\n    \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2/\",    \n]\n\nmodels = [AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.bfloat16) for model_name in judge_model_list]\ntokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in judge_model_list]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:56:04.172227Z","iopub.execute_input":"2024-12-04T04:56:04.172594Z","iopub.status.idle":"2024-12-04T04:58:30.84837Z","shell.execute_reply.started":"2024-12-04T04:56:04.172569Z","shell.execute_reply":"2024-12-04T04:58:30.847436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def llm_judge(prompt, response, criteria, model, tokenizer):\n    \"\"\"\n    Evaluate a list of responses with scores (0-9) using a Hugging Face Transformer model.\n\n    Args:\n        prompt (str): The initial task or question given to the respondents.\n        response (str): Response to evaluate.\n        criteria (str): Evaluation criteria to judge the response.\n        model: Huggingface model to use.\n        tokenizer: Huggingface tokenizer to use.\n\n    Returns:\n        score: Score for the response.\n    \"\"\"\n    # Build the evaluation prompt\n    evaluation_prompt = f\"\"\"\nYou are an expert judge scoring responses to the following prompt:\n\nPrompt: {prompt}\n\nEvaluation Criteria: {criteria}\n\nProvide a score between 0 and 9 (inclusive) for the response. Do not provide any explanation.\n\nHere is the response to evaluate:\n\"\"\"\n    evaluation_prompt += f\"\\nResponse: {response}\\nScore:\"\n\n    # Tokenize the input\n    inputs = tokenizer(evaluation_prompt, return_tensors=\"pt\")\n    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n\n    # Generate output\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=5,\n        num_return_sequences=1,\n        temperature=0,\n        do_sample=False\n    )\n\n    # Decode the output\n    evaluation_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    try:\n        score_line = evaluation_output.split(f\"Response:\")[1].split(\"\\nScore:\")[1].strip()\n        score = float(score_line.split()[0])  # Extract the numeric score\n    except Exception:\n        score = 0\n\n    return score\n\n\njudging_criteria = \"Clarity, relevance to the topic, and strength of the argument.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:30.849578Z","iopub.execute_input":"2024-12-04T04:58:30.85009Z","iopub.status.idle":"2024-12-04T04:58:30.85677Z","shell.execute_reply.started":"2024-12-04T04:58:30.850062Z","shell.execute_reply":"2024-12-04T04:58:30.855895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv(\"/kaggle/input/gemma-2-naive-submission/submission.csv\")\nsubmission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:30.858501Z","iopub.execute_input":"2024-12-04T04:58:30.858786Z","iopub.status.idle":"2024-12-04T04:58:30.889307Z","shell.execute_reply.started":"2024-12-04T04:58:30.858762Z","shell.execute_reply":"2024-12-04T04:58:30.888474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"avg_qs = []\navg_variances = []\n\nfor i, row in submission_df.iterrows():\n    task_prompt = f\"Write an essay on the topic {row['topic']}\"\n    results = [llm_judge(task_prompt, row['essay'], judging_criteria, model, tokenizer) for (model, tokenizer) in zip(models, tokenizers)]\n    avg_qs.append(np.mean(results))\n    avg_variances.append(np.var(results))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:30.890489Z","iopub.execute_input":"2024-12-04T04:58:30.890856Z","iopub.status.idle":"2024-12-04T04:58:37.877284Z","shell.execute_reply.started":"2024-12-04T04:58:30.8908Z","shell.execute_reply":"2024-12-04T04:58:37.876341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df['avg_q'] = avg_qs\nsubmission_df['avg_variance'] = avg_variances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:37.878469Z","iopub.execute_input":"2024-12-04T04:58:37.878735Z","iopub.status.idle":"2024-12-04T04:58:37.883928Z","shell.execute_reply.started":"2024-12-04T04:58:37.878711Z","shell.execute_reply":"2024-12-04T04:58:37.883033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">English Language Confidence</h1></span>","metadata":{}},{"cell_type":"code","source":"# Install the lingua-language-detector package\n!pip install lingua-language-detector\n\n# Import necessary modules\nfrom lingua import Language, LanguageDetectorBuilder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:37.88478Z","iopub.execute_input":"2024-12-04T04:58:37.885027Z","iopub.status.idle":"2024-12-04T04:58:48.502732Z","shell.execute_reply.started":"2024-12-04T04:58:37.885004Z","shell.execute_reply":"2024-12-04T04:58:48.501641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the language detector\ndetector = LanguageDetectorBuilder.from_all_languages().build()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.504329Z","iopub.execute_input":"2024-12-04T04:58:48.505059Z","iopub.status.idle":"2024-12-04T04:58:48.509465Z","shell.execute_reply.started":"2024-12-04T04:58:48.505017Z","shell.execute_reply":"2024-12-04T04:58:48.508618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"english_confidence = []\n\nfor i, row in submission_df.iterrows():\n    # Compute language confidence values\n    results = detector.compute_language_confidence_values(row['essay'])\n    confidence = next((result.value for result in results if result.language == Language.ENGLISH), 0.0)\n    english_confidence.append(confidence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.510764Z","iopub.execute_input":"2024-12-04T04:58:48.511465Z","iopub.status.idle":"2024-12-04T04:58:48.942653Z","shell.execute_reply.started":"2024-12-04T04:58:48.511416Z","shell.execute_reply":"2024-12-04T04:58:48.941626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df['avg_e'] = english_confidence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.946217Z","iopub.execute_input":"2024-12-04T04:58:48.946535Z","iopub.status.idle":"2024-12-04T04:58:48.952251Z","shell.execute_reply.started":"2024-12-04T04:58:48.946493Z","shell.execute_reply":"2024-12-04T04:58:48.951456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Sequence Similarity Score</h1></span>","metadata":{}},{"cell_type":"markdown","source":"#### Based on this [comment](https://www.kaggle.com/competitions/llms-you-cant-please-them-all/discussion/549809#3062886) I assume that sequence similarity refers to similarity between different essays in the test data. Feel free to correct me if this isn't the case","metadata":{}},{"cell_type":"code","source":"import difflib\nfrom itertools import combinations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.953305Z","iopub.execute_input":"2024-12-04T04:58:48.954081Z","iopub.status.idle":"2024-12-04T04:58:48.963135Z","shell.execute_reply.started":"2024-12-04T04:58:48.954044Z","shell.execute_reply":"2024-12-04T04:58:48.962417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"essays = submission_df['essay'].values\nsimilarities = [\n        difflib.SequenceMatcher(a=essay1, b=essay2).ratio() for essay1, essay2 in combinations(essays, 2)\n    ]\navg_s = sum(similarities) / len(similarities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.964255Z","iopub.execute_input":"2024-12-04T04:58:48.964878Z","iopub.status.idle":"2024-12-04T04:58:48.976481Z","shell.execute_reply.started":"2024-12-04T04:58:48.96484Z","shell.execute_reply":"2024-12-04T04:58:48.97561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MIN_S = 0.2\navg_s_clipped = max(avg_s, MIN_S)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.977494Z","iopub.execute_input":"2024-12-04T04:58:48.977803Z","iopub.status.idle":"2024-12-04T04:58:48.985192Z","shell.execute_reply.started":"2024-12-04T04:58:48.977766Z","shell.execute_reply":"2024-12-04T04:58:48.984361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Final Score</h1></span>","metadata":{}},{"cell_type":"markdown","source":"![](https://imgur.com/VRlKKgq.png)","metadata":{}},{"cell_type":"code","source":"MAX_Q = 9\nfinal_score = (submission_df['avg_variance'].mean() / (MAX_Q - submission_df['avg_q'].mean())) * (submission_df['avg_e'].mean() / avg_s_clipped)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.986039Z","iopub.execute_input":"2024-12-04T04:58:48.98629Z","iopub.status.idle":"2024-12-04T04:58:48.994766Z","shell.execute_reply.started":"2024-12-04T04:58:48.986267Z","shell.execute_reply":"2024-12-04T04:58:48.994095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T04:58:48.995838Z","iopub.execute_input":"2024-12-04T04:58:48.996432Z","iopub.status.idle":"2024-12-04T04:58:49.007289Z","shell.execute_reply.started":"2024-12-04T04:58:48.996369Z","shell.execute_reply":"2024-12-04T04:58:49.006453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","metadata":{}}]}