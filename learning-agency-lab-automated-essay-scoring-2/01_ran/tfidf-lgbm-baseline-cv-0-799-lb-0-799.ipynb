{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8070853,
     "sourceType": "datasetVersion",
     "datasetId": 4762179
    },
    {
     "sourceId": 8078642,
     "sourceType": "datasetVersion",
     "datasetId": 4767886
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "#### Thank you for exploring my notebook.\nIn this notebook, I used TfidfVectorizer and Polars to generate features, and used LightGBM as the scoring model.\n\nIn addition, I used both Chinese and English as code comments.\n\n##### update: version 2 \n1. Add more data Preprocessing function\n2. LGBMClassifier  -->  LGBMRegressor\n3. CV:  0.7646  --> 0.7871\n4. LB:  0.772 --> 0.786\n\n##### update: version 4\n1. Add code comments\n2. Using kappa as the early stop metric\n3. CV:  0.7889\n4. LB:  0.787\n\n##### update: version 5\n1. Using kappa as LGBMRegressor objective\n2. CV:  0.7990",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Import",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import re\nimport copy\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm,trange\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:25.486026Z",
     "iopub.execute_input": "2024-04-09T07:56:25.486812Z",
     "iopub.status.idle": "2024-04-09T07:56:30.342215Z",
     "shell.execute_reply.started": "2024-04-09T07:56:25.486777Z",
     "shell.execute_reply": "2024-04-09T07:56:30.341078Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:52:57.820855Z",
     "start_time": "2024-05-18T06:52:56.747196Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "columns = [  \n",
    "    (\n",
    "        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n",
    "    ),\n",
    "]\n",
    "PATH = \"/home/zvonimir/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n",
    "# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\n",
    "train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n",
    "test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n",
    "# 显示训练集中的第一个样本数据\n",
    "# Display the first sample data in the training set\n",
    "train.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:30.344235Z",
     "iopub.execute_input": "2024-04-09T07:56:30.344775Z",
     "iopub.status.idle": "2024-04-09T07:56:31.067402Z",
     "shell.execute_reply.started": "2024-04-09T07:56:30.34474Z",
     "shell.execute_reply": "2024-04-09T07:56:31.066369Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:52:57.928779Z",
     "start_time": "2024-05-18T06:52:57.821664Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Features engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 1.Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # 将单词转化为小写\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    # 移除html\n    x = removeHTML(x)\n    # 删除以@作为首字母的字符串\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # 删除数字\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # 删除网址\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # 将连续空白符替换为一个空格字符\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # 替换连续的句号和逗号为一个\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # 去除开头结尾的空白符\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:31.068681Z",
     "iopub.execute_input": "2024-04-09T07:56:31.069117Z",
     "iopub.status.idle": "2024-04-09T07:56:31.078698Z",
     "shell.execute_reply.started": "2024-04-09T07:56:31.069079Z",
     "shell.execute_reply": "2024-04-09T07:56:31.077541Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:52:57.931444Z",
     "start_time": "2024-05-18T06:52:57.929322Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.Paragraph Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# 获取特征名称\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:31.080499Z",
     "iopub.execute_input": "2024-04-09T07:56:31.081189Z",
     "iopub.status.idle": "2024-04-09T07:56:39.738877Z",
     "shell.execute_reply.started": "2024-04-09T07:56:31.081155Z",
     "shell.execute_reply": "2024-04-09T07:56:39.737577Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:52:59.843834Z",
     "start_time": "2024-05-18T06:52:57.932040Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.Sentence Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:39.744544Z",
     "iopub.execute_input": "2024-04-09T07:56:39.744918Z",
     "iopub.status.idle": "2024-04-09T07:56:48.078384Z",
     "shell.execute_reply.started": "2024-04-09T07:56:39.744889Z",
     "shell.execute_reply": "2024-04-09T07:56:48.077563Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:53:01.742848Z",
     "start_time": "2024-05-18T06:52:59.844812Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.Word Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:56:48.079702Z",
     "iopub.execute_input": "2024-04-09T07:56:48.080779Z",
     "iopub.status.idle": "2024-04-09T07:57:04.320546Z",
     "shell.execute_reply.started": "2024-04-09T07:56:48.080745Z",
     "shell.execute_reply": "2024-04-09T07:57:04.319246Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:53:04.796050Z",
     "start_time": "2024-05-18T06:53:01.743240Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.Tf-idf features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# 将全部数据集都填充进TfidfVectorizer里，这可能会造成泄露和过于乐观的CV分数\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n# 转换为数组\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# 转换为dataframe\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# 重命名特征\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:57:04.322087Z",
     "iopub.execute_input": "2024-04-09T07:57:04.322438Z",
     "iopub.status.idle": "2024-04-09T07:58:46.872477Z",
     "shell.execute_reply.started": "2024-04-09T07:57:04.322408Z",
     "shell.execute_reply": "2024-04-09T07:58:46.871183Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:53:21.196478Z",
     "start_time": "2024-05-18T06:53:04.796560Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Train\n* I have trained and saved the model\n* you can choose to retrain or load the model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:46.873961Z",
     "iopub.execute_input": "2024-04-09T07:58:46.874395Z",
     "iopub.status.idle": "2024-04-09T07:58:46.88386Z",
     "shell.execute_reply.started": "2024-04-09T07:58:46.874365Z",
     "shell.execute_reply": "2024-04-09T07:58:46.882575Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:53:21.199107Z",
     "start_time": "2024-05-18T06:53:21.196957Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "LOAD = False\n",
    "models = []\n",
    "if LOAD:\n",
    "    for i in range(5):\n",
    "        models.append(lgb.Booster(model_file=f'../input/lal-lgb-baseline-4/fold_{i}.txt'))\n",
    "else:\n",
    "    # oof用于存储每一次模型对验证集的预测结果\n",
    "    # OOF is used to store the prediction results of each model on the validation set\n",
    "    oof = []\n",
    "    x= train_feats\n",
    "    y= train_feats['score'].values\n",
    "    # 5 fold\n",
    "    kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n",
    "            # 创建模型\n",
    "            # create model\n",
    "            model = lgb.LGBMRegressor(\n",
    "                objective = qwk_obj,\n",
    "                metrics = 'None',\n",
    "                learning_rate = 0.1,\n",
    "                max_depth = 5,\n",
    "                num_leaves = 10,\n",
    "                colsample_bytree=0.5,\n",
    "                reg_alpha = 0.1,\n",
    "                reg_lambda = 0.8,\n",
    "                n_estimators=1024,\n",
    "                random_state=42,\n",
    "                verbosity = - 1)\n",
    "            # 分别取出5 kfold分割的训练集和验证集\n",
    "            # Take out the training and validation sets for 5 kfold segmentation separately\n",
    "            X_train = train_feats.iloc[trn_idx][feature_names]\n",
    "            Y_train = train_feats.iloc[trn_idx]['score'] - a\n",
    "\n",
    "            X_val = train_feats.iloc[val_idx][feature_names]\n",
    "            Y_val = train_feats.iloc[val_idx]['score'] - a\n",
    "            print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "            # 训练模型\n",
    "            # Training model\n",
    "            lgb_model = model.fit(X_train,\n",
    "                                  Y_train,\n",
    "                                  eval_names=['train', 'valid'],\n",
    "                                  eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                                  eval_metric=quadratic_weighted_kappa,\n",
    "                                  callbacks=callbacks,)\n",
    "            # 使用训练完成的模型对验证集进行预测\n",
    "            # Use the trained model to predict the validation set\n",
    "            pred_val = lgb_model.predict(\n",
    "                X_val, num_iteration=lgb_model.best_iteration_)\n",
    "            df_tmp = train_feats.iloc[val_idx][['essay_id', 'score']].copy()\n",
    "            df_tmp['pred'] = pred_val + a\n",
    "            oof.append(df_tmp)\n",
    "            # 保存模型参数\n",
    "            # Save model parameters\n",
    "            models.append(model.booster_)\n",
    "            lgb_model.booster_.save_model(f'fold_{fold_id}.txt')\n",
    "    df_oof = pd.concat(oof)\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:46.885746Z",
     "iopub.execute_input": "2024-04-09T07:58:46.886781Z",
     "iopub.status.idle": "2024-04-09T07:58:47.039463Z",
     "shell.execute_reply.started": "2024-04-09T07:58:46.886746Z",
     "shell.execute_reply": "2024-04-09T07:58:47.03821Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:54:15.090463Z",
     "start_time": "2024-05-18T06:53:21.199539Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### CV",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if LOAD:\n    print('acc: ',0.6275495464263015)\n    print('kappa: ',0.7990509565910948)\nelse:\n    acc = accuracy_score(df_oof['score'], df_oof['pred'].clip(1, 6).round())\n    kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n    print('acc: ',acc)\n    print('kappa: ',kappa)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:47.041049Z",
     "iopub.execute_input": "2024-04-09T07:58:47.041533Z",
     "iopub.status.idle": "2024-04-09T07:58:47.05337Z",
     "shell.execute_reply.started": "2024-04-09T07:58:47.041496Z",
     "shell.execute_reply": "2024-04-09T07:58:47.052122Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:54:15.103079Z",
     "start_time": "2024-05-18T06:54:15.090909Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:47.054733Z",
     "iopub.execute_input": "2024-04-09T07:58:47.055224Z",
     "iopub.status.idle": "2024-04-09T07:58:47.303949Z",
     "shell.execute_reply.started": "2024-04-09T07:58:47.055162Z",
     "shell.execute_reply": "2024-04-09T07:58:47.302486Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:54:15.203978Z",
     "start_time": "2024-05-18T06:54:15.103450Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "prediction = test_feats[['essay_id']].copy()\nprediction['score'] = 0\npred_test = models[0].predict(test_feats[feature_names]) + a\nfor i in range(4):\n    pred_now = models[i+1].predict(test_feats[feature_names]) + a\n    pred_test = np.add(pred_test,pred_now)\n# 最终预测结果需要除以5，因为使用了5个模型的预测结果相加\n# The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\npred_test = pred_test/5\nprint(pred_test)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:47.305725Z",
     "iopub.execute_input": "2024-04-09T07:58:47.306253Z",
     "iopub.status.idle": "2024-04-09T07:58:47.401326Z",
     "shell.execute_reply.started": "2024-04-09T07:58:47.306221Z",
     "shell.execute_reply": "2024-04-09T07:58:47.400047Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:54:15.257320Z",
     "start_time": "2024-05-18T06:54:15.204572Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 将预测结果四舍五入为整数，限定范围为1-6（文章评分范围）\n# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\npred_test = pred_test.clip(1, 6).round()\nprediction['score'] = pred_test\nprediction.to_csv('submission.csv', index=False)\nprediction.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-09T07:58:47.403128Z",
     "iopub.execute_input": "2024-04-09T07:58:47.403575Z",
     "iopub.status.idle": "2024-04-09T07:58:47.421226Z",
     "shell.execute_reply.started": "2024-04-09T07:58:47.40353Z",
     "shell.execute_reply": "2024-04-09T07:58:47.420101Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:54:15.262840Z",
     "start_time": "2024-05-18T06:54:15.258552Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Reference Notebook\n#### I would like to give thanks to the authors of these public notebooks. I have learned a lot from you.\n* https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n* https://www.kaggle.com/code/yunsuxiaozi/aes2-0-baseline-naivebayesclassifier\n* https://www.kaggle.com/code/finlay/llm-detect-0-to-1\n* https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features\n* https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features\n* https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective",
   "metadata": {}
  }
 ]
}
