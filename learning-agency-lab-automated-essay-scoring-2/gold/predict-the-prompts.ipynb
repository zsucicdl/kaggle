{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 7017419,
     "sourceType": "datasetVersion",
     "datasetId": 3937250
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### ‚≠êÔ∏è What this notebook is about\nI've been experiencing the discrepancy between CV and LB. For example in my previous [notebook](https://www.kaggle.com/code/emiz6413/cv-0-825-lb-0-803-deberta-v3-small-with-huber-loss) I used 5 fold `StratifiedKFold` split and obtained cv=0.825 while LB was 0.803. As many reported, CV has been constantly higher than LB (by around 0.2~0.3). <br>\nThere is [a great discussion](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/499959) by @ragnar123, that points out the potential problem of using vanilla stratified k-fold split if the test set contains essays given by different prompt(s) than the training set. <br>\n@ragnar123 suggested using `GroupKFold` to seperate essays of different prompts in the training and evaluation set to reduce the discrepancy between CV and LB. <br>\nThis notebook is my attempt to predict the prompts from the essay.\n\n### ‚úíÔ∏è Method\n1. Extract essays that exist in [persuade 2.0 corpus](https://www.kaggle.com/datasets/nbroad/persaude-corpus-2). We get 12871 duplicates (intersection set) and 4436 that don't exist in persuade (difference set).\n2. Persuade corpus contains prompt_name. We train a prompt classifier using the intersection set. We can easily achieve very high accuracy (>0.99) even with small models like `deberta-v3-small` trained for 1 epoch.\n3. Predict the prompt on the difference set using the classifier trained in step 2. And save the predicted prompts as csv.\n\n### üõë Credit\nI implemented the method mentioned in [the discussion](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/499959) by @ragnar123. I highly recommend checking the liked discussion if you haven't.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport copy\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EvalPrediction\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, ConfusionMatrixDisplay",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:00.590844Z",
     "iopub.execute_input": "2024-05-06T02:35:00.591219Z",
     "iopub.status.idle": "2024-05-06T02:35:19.533516Z",
     "shell.execute_reply.started": "2024-05-06T02:35:00.591191Z",
     "shell.execute_reply": "2024-05-06T02:35:19.532726Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\npersuade = pd.read_csv(\"/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:19.535412Z",
     "iopub.execute_input": "2024-05-06T02:35:19.536557Z",
     "iopub.status.idle": "2024-05-06T02:35:21.718838Z",
     "shell.execute_reply.started": "2024-05-06T02:35:19.53652Z",
     "shell.execute_reply": "2024-05-06T02:35:21.718064Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Find the number of train $|S_{train}|$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "len(train_df)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:21.72008Z",
     "iopub.execute_input": "2024-05-06T02:35:21.72042Z",
     "iopub.status.idle": "2024-05-06T02:35:21.727627Z",
     "shell.execute_reply.started": "2024-05-06T02:35:21.720391Z",
     "shell.execute_reply": "2024-05-06T02:35:21.726677Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Find intersection $S_{train} \\cap S_{persuade}$ and difference $S_{train} \\setminus S_{persuade}$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "intersection = pd.merge(train_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"score\", \"prompt_name\"]].reset_index(drop=True)\ndifference = train_df[~train_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\nprint(\"len(intersection):\", len(intersection))\nprint(\"len(difference):\", len(difference))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:40.811442Z",
     "iopub.execute_input": "2024-05-06T02:35:40.811821Z",
     "iopub.status.idle": "2024-05-06T02:35:40.929156Z",
     "shell.execute_reply.started": "2024-05-06T02:35:40.811791Z",
     "shell.execute_reply": "2024-05-06T02:35:40.928139Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Plot the histogram of prompts colored by score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "px.histogram(intersection, x=\"prompt_name\", color=\"score\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:43.408164Z",
     "iopub.execute_input": "2024-05-06T02:35:43.408538Z",
     "iopub.status.idle": "2024-05-06T02:35:44.926677Z",
     "shell.execute_reply.started": "2024-05-06T02:35:43.408507Z",
     "shell.execute_reply": "2024-05-06T02:35:44.925798Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Split the intersection\nI will split the intersection into 2 folds with StratifiedKFold based on the combination of their score and the prompt.<br>\nTo do that, I first create a new column called `score_and_prompt` which is a concatenation of score and prompt. Then we can target the column to split.",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-04T05:37:11.497241Z",
     "iopub.execute_input": "2024-05-04T05:37:11.498096Z",
     "iopub.status.idle": "2024-05-04T05:37:11.511369Z",
     "shell.execute_reply.started": "2024-05-04T05:37:11.498057Z",
     "shell.execute_reply": "2024-05-04T05:37:11.510171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "intersection[\"score_and_prompt\"] = intersection[\"score\"].astype(str) + \"-\" + intersection[\"prompt_name\"]\nintersection.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:48.343699Z",
     "iopub.execute_input": "2024-05-06T02:35:48.344075Z",
     "iopub.status.idle": "2024-05-06T02:35:48.378065Z",
     "shell.execute_reply.started": "2024-05-06T02:35:48.344045Z",
     "shell.execute_reply": "2024-05-06T02:35:48.376884Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "intersection[\"fold\"] = None\nsplitter = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\nfolds = list(splitter.split(X=np.zeros(len(intersection)), y=intersection[\"score_and_prompt\"].values))\nfor fold_idx, (_, val_idx) in enumerate(folds):\n    intersection.loc[val_idx, \"fold\"] = fold_idx",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:48.445048Z",
     "iopub.execute_input": "2024-05-06T02:35:48.446272Z",
     "iopub.status.idle": "2024-05-06T02:35:48.493149Z",
     "shell.execute_reply.started": "2024-05-06T02:35:48.446234Z",
     "shell.execute_reply": "2024-05-06T02:35:48.492297Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "intersection.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:48.910158Z",
     "iopub.execute_input": "2024-05-06T02:35:48.910872Z",
     "iopub.status.idle": "2024-05-06T02:35:48.922863Z",
     "shell.execute_reply.started": "2024-05-06T02:35:48.91084Z",
     "shell.execute_reply": "2024-05-06T02:35:48.921622Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualize the split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "px.histogram(intersection, x=\"prompt_name\", color=\"score\", facet_row=\"fold\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:50.223617Z",
     "iopub.execute_input": "2024-05-06T02:35:50.22448Z",
     "iopub.status.idle": "2024-05-06T02:35:50.428923Z",
     "shell.execute_reply.started": "2024-05-06T02:35:50.224448Z",
     "shell.execute_reply": "2024-05-06T02:35:50.427903Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Convert prompt to prompt id\nWe're going to train a model to classify prompt. Currently, the target column `prompt_name` is a string, so we need to convert it to id and name it `label`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "intersection[\"label\"] =  intersection[\"prompt_name\"].astype(\"category\").cat.codes\nintersection.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:53.228256Z",
     "iopub.execute_input": "2024-05-06T02:35:53.228706Z",
     "iopub.status.idle": "2024-05-06T02:35:53.246282Z",
     "shell.execute_reply.started": "2024-05-06T02:35:53.228674Z",
     "shell.execute_reply": "2024-05-06T02:35:53.245398Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "prompt_to_id = intersection.drop_duplicates(subset=(\"prompt_name\", \"label\"))[[\"prompt_name\", \"label\"]]\nlabel2id = {row[\"prompt_name\"]: row[\"label\"] for _, row in prompt_to_id.iterrows()}\nid2label = {row[\"label\"]: row[\"prompt_name\"] for _, row in prompt_to_id.iterrows()}\nprompt_to_id",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:53.369957Z",
     "iopub.execute_input": "2024-05-06T02:35:53.370772Z",
     "iopub.status.idle": "2024-05-06T02:35:53.387996Z",
     "shell.execute_reply.started": "2024-05-06T02:35:53.37074Z",
     "shell.execute_reply": "2024-05-06T02:35:53.387057Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Instantiate the model & tokenizer\nI will use `deberta-v3-xsmall` here for good trade-off of performance and speed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "checkpoint = \"microsoft/deberta-v3-xsmall\"\n\nclass ModelInit:\n    def __init__(self, checkpoint, label2id, id2label):\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            checkpoint, \n            num_labels=len(label2id),\n            label2id=label2id,\n            id2label=id2label,\n        )\n        self.state_dict = copy.deepcopy(self.model.state_dict())\n        \n    def __call__(self):\n        self.model.load_state_dict(self.state_dict)\n        return self.model\n    \ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel_init = ModelInit(checkpoint, label2id=label2id, id2label=id2label)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:35:55.28687Z",
     "iopub.execute_input": "2024-05-06T02:35:55.28772Z",
     "iopub.status.idle": "2024-05-06T02:35:59.849508Z",
     "shell.execute_reply.started": "2024-05-06T02:35:55.287687Z",
     "shell.execute_reply": "2024-05-06T02:35:59.84839Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Tokenize the dataset\nI will use `max_length=1024` here.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "intersection_ds = Dataset.from_pandas(intersection)\ndiff_ds = Dataset.from_pandas(difference)\nintersection_ds = intersection_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)\ndiff_ds = diff_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:36:00.710805Z",
     "iopub.execute_input": "2024-05-06T02:36:00.711703Z",
     "iopub.status.idle": "2024-05-06T02:36:14.802319Z",
     "shell.execute_reply.started": "2024-05-06T02:36:00.711668Z",
     "shell.execute_reply": "2024-05-06T02:36:14.801396Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Train prompt classifier",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(eval_pred: EvalPrediction) -> dict[str, float]:\n    predictions = eval_pred.predictions\n    y_true = eval_pred.label_ids\n    y_pred = predictions.argmax(-1)\n    acc = accuracy_score(y_true, y_pred)\n    return {\"acc\": acc}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:36:14.80383Z",
     "iopub.execute_input": "2024-05-06T02:36:14.80418Z",
     "iopub.status.idle": "2024-05-06T02:36:14.809454Z",
     "shell.execute_reply.started": "2024-05-06T02:36:14.804135Z",
     "shell.execute_reply": "2024-05-06T02:36:14.808386Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "args = TrainingArguments(\n    output_dir=\"output\",\n    report_to=\"none\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=8,\n    learning_rate=1e-5,\n    lr_scheduler_type=\"constant\",\n    warmup_ratio=0.0,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    optim=\"adamw_torch\",\n    fp16=torch.cuda.is_available()\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:36:14.810802Z",
     "iopub.execute_input": "2024-05-06T02:36:14.811258Z",
     "iopub.status.idle": "2024-05-06T02:36:14.920054Z",
     "shell.execute_reply.started": "2024-05-06T02:36:14.811223Z",
     "shell.execute_reply": "2024-05-06T02:36:14.919146Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "predictions = []\n\nfor fold_idx in np.unique(intersection_ds[\"fold\"]):\n    args.output_dir = os.path.join(\"output\", f\"fold_{fold_idx}\")\n    train_ds = intersection_ds.select([i for i, f in enumerate(intersection_ds[\"fold\"]) if f != fold_idx])\n    eval_ds = intersection_ds.select([i for i, f in enumerate(intersection_ds[\"fold\"]) if f == fold_idx])\n    trainer = Trainer(\n        args=args, \n        model_init=model_init,\n        train_dataset=train_ds, \n        eval_dataset=eval_ds, \n        tokenizer=tokenizer, \n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    # predict on eval dataset to visualize the result\n    preds = trainer.predict(eval_ds)\n    fig, ax = plt.subplots()\n    ConfusionMatrixDisplay.from_predictions(\n        y_true=preds.label_ids, \n        y_pred=preds.predictions.argmax(-1),\n        ax=ax\n    )\n    acc = accuracy_score(y_true=preds.label_ids, y_pred=preds.predictions.argmax(-1))\n    ax.set_title(f\"fold-{fold_idx} acc: {acc:.3f}\")\n    fig.show()\n    # predict on difference dataset\n    test_preds = trainer.predict(diff_ds)\n    predictions.append(test_preds.predictions)\n    \npredictions = np.stack(predictions, axis=0).mean(axis=0)  # average the result of 2 folds",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:36:14.922245Z",
     "iopub.execute_input": "2024-05-06T02:36:14.922996Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Save the result",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "essay_id = diff_ds[\"essay_id\"]\nprompt_name = [id2label[i] for i in predictions.argmax(-1)]  # convert prompt id back to prompt name\nresult_df = pd.DataFrame({\"essay_id\": essay_id, \"prompt_name\": prompt_name, \"predicted\": [True] * len(essay_id)})\nresult_df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-04T07:14:20.403432Z",
     "iopub.execute_input": "2024-05-04T07:14:20.403828Z",
     "iopub.status.idle": "2024-05-04T07:14:20.429421Z",
     "shell.execute_reply.started": "2024-05-04T07:14:20.403795Z",
     "shell.execute_reply": "2024-05-04T07:14:20.428473Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "intersection_df = intersection[[\"essay_id\", \"prompt_name\"]].copy()\nintersection_df[\"predicted\"] = False\nfinal_df = pd.concat([result_df, intersection_df])\nfinal_df.to_csv(\"predicted_prompt.csv\", index=False)\nfinal_df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-04T07:15:38.004228Z",
     "iopub.execute_input": "2024-05-04T07:15:38.004602Z",
     "iopub.status.idle": "2024-05-04T07:15:38.070369Z",
     "shell.execute_reply.started": "2024-05-04T07:15:38.004572Z",
     "shell.execute_reply": "2024-05-04T07:15:38.069443Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
