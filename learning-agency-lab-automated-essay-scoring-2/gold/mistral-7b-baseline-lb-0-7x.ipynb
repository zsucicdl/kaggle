{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 7923821,
     "sourceType": "datasetVersion",
     "datasetId": 4656352
    },
    {
     "sourceId": 8159746,
     "sourceType": "datasetVersion",
     "datasetId": 4827409
    },
    {
     "sourceId": 8159856,
     "sourceType": "datasetVersion",
     "datasetId": 4827483
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Mistral Instruct 7B Starter - CV 0.7X - LB 0.7X\nHi everyone. I'm learning to finetune large decoder language models like Mistral 7B. \n\nThis Kaggle competition is a perfect opportunity to explore finetuning LLM like `mistralai/Mistral-7B-Instruct-v0.2` [here][1]. Nowadays, large LLM (like Mistral, Llama, Claude, Gemini, Vicuna, etc) are popular and I'm curious to see how they compare to \"smaller\" models like DeBERTa on basic tasks like classification and regression! \n\nThe following notebook finetunes `Mistral Instruct 7B` to predict the essay score in Kaggle's essay comp. This code is most likely **not** the best way of doing everything because i'm just learning. We have much opportunity to improve this notebook! Let's discuss ways to improve this notebook in the comments!\n\n### Notes:\nI use HuggingFace `Trainer` to train Mistral instead of `SFTTrainer`. Note that SFTTrainer has more conveinent features to make our life easier like assisting us in PEFT and dataloading. Furthermore, I load and train Mistral in `fp16` because we have enough memory to do so on Kaggle 2xT4. Perhaps using `4bit` quantization will speed up training but it could effect accuracy. Certainly if we want to try larger LLM, we should use `4bit` quantization.\n\nI only train with 3072 rows of train data for 1 epoch for speed, but training with more train data will certainly improve our CV score and LB score. We can improve the PEFT parameters and LR schedule for better performance too.\n\nWe can use tricks (to help with training) like `gradient accumulation` and `gradient checkpointing` but i do not in this notebook.\n\nWhen I try to train on 2xT4 in Kaggle notebook, I get an error. I'm not sure why :( So, this notebook loads a model that I trained locally on 2xV100 GPU (using this notebook).\n\nEnjoy!\n\n[1]: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Load Data and Model\nWe load `Mistral-7B-Instruct-v0.2` in `fp16` unto both 2xT4 Kaggle GPU. I change tokenizer padding to right because it makes more sense to me. However it might not be best way. And I update the pad token.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#import os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nVER=1\n# USE FIRST K ROWS FOR VALIDATION\nVAL_SET = 1024\n# MAX SEQ LENGTH FOR MISTRAL TRAINING\nMAX_LEN = 1024\n# BATCH SIZE PER DEVICE\nBATCH_SIZE = 1 \n# NUMBER OF EPOCHS\nEPOCHS = 1\n# USE NEXT K ROWS FOR TRAIN\nTRAIN_SET = 3072\n# IF LOAD PATH IS NOT NONE, WE LOAD INSTEAD OF TRAIN\nLOAD_PATH = '/kaggle/input/mistral-v0/'",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-19T00:26:52.558736Z",
     "iopub.execute_input": "2024-04-19T00:26:52.55954Z",
     "iopub.status.idle": "2024-04-19T00:26:52.571532Z",
     "shell.execute_reply.started": "2024-04-19T00:26:52.559498Z",
     "shell.execute_reply": "2024-04-19T00:26:52.570396Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd, numpy as np\ndf = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\nprint('Train shape:', df.shape )\ndf.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:26:52.573285Z",
     "iopub.execute_input": "2024-04-19T00:26:52.573603Z",
     "iopub.status.idle": "2024-04-19T00:26:54.221445Z",
     "shell.execute_reply.started": "2024-04-19T00:26:52.57358Z",
     "shell.execute_reply": "2024-04-19T00:26:54.220579Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n#model_name = 'mistralai/Mistral-7B-Instruct-v0.2' # WHEN INTERNET IS TURNED ON\nmodel_name = '/kaggle/input/mistral-7b-instruct-v02-fp16'\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='right') \ntokenizer.pad_token = tokenizer.eos_token",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:26:54.223076Z",
     "iopub.execute_input": "2024-04-19T00:26:54.223449Z",
     "iopub.status.idle": "2024-04-19T00:29:50.782968Z",
     "shell.execute_reply.started": "2024-04-19T00:26:54.223416Z",
     "shell.execute_reply": "2024-04-19T00:29:50.781972Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Data Loader\nWe create a data loader using HuggingFace's Dataset. We could also make a dataloader with PyTorch's Dataset. Both are equally useful. We use the Mistral Instruct template to train and infer our model. It is shown below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import Dataset\ndataset_v = Dataset.from_pandas(df.iloc[:VAL_SET])\ndataset_t = Dataset.from_pandas(df.iloc[VAL_SET:VAL_SET+TRAIN_SET])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:29:50.784143Z",
     "iopub.execute_input": "2024-04-19T00:29:50.784594Z",
     "iopub.status.idle": "2024-04-19T00:29:51.277973Z",
     "shell.execute_reply.started": "2024-04-19T00:29:50.784568Z",
     "shell.execute_reply": "2024-04-19T00:29:51.276968Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def preprocess(sample, text=False, infer_mode=False, max_seq=MAX_LEN, return_tensors=None):\n    \n    sys_prompt = \"Please read the following essay and assign a score of 1,2,3,4,5,6 where 6 is the best. Output only a single number with no explanation.\\n\\n\"\n    prompt = sample[\"full_text\"]\n    if infer_mode: answer = \"\"\n    else: answer = str(sample[\"score\"])\n        \n    messages = [\n        {\"role\": \"user\", \"content\": sys_prompt + prompt},\n        {\"role\": \"assistant\", \"content\": f\"\\n\\nThe score is: \" + answer}\n    ]\n    formatted_sample = tokenizer.apply_chat_template(messages, tokenize=False)\n    if infer_mode: formatted_sample = formatted_sample.replace(\"</s>\",\"\")\n        \n    tokenized_sample = tokenizer(formatted_sample, padding=True, return_tensors=return_tensors, \n                                 truncation=True, add_special_tokens=False, max_length=max_seq) \n    \n    if return_tensors==\"pt\":\n        tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].clone()\n    else:\n        tokenized_sample[\"labels\"] = tokenized_sample[\"input_ids\"].copy()\n    \n    if text: return formatted_sample\n    else: return tokenized_sample",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:52:46.881646Z",
     "iopub.execute_input": "2024-04-19T00:52:46.882068Z",
     "iopub.status.idle": "2024-04-19T00:52:46.891616Z",
     "shell.execute_reply.started": "2024-04-19T00:52:46.882036Z",
     "shell.execute_reply": "2024-04-19T00:52:46.890514Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print( preprocess(df.iloc[0], text=True) )",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:29:51.289772Z",
     "iopub.execute_input": "2024-04-19T00:29:51.290104Z",
     "iopub.status.idle": "2024-04-19T00:29:51.340574Z",
     "shell.execute_reply.started": "2024-04-19T00:29:51.290069Z",
     "shell.execute_reply": "2024-04-19T00:29:51.339767Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tokenized_dataset_v = dataset_v.map(preprocess, num_proc=4, \n                            remove_columns=['essay_id', 'full_text', 'score'])\ntokenized_dataset_t = dataset_t.map(preprocess, num_proc=4, \n                            remove_columns=['essay_id', 'full_text', 'score'])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:29:51.341604Z",
     "iopub.execute_input": "2024-04-19T00:29:51.342029Z",
     "iopub.status.idle": "2024-04-19T00:29:55.885576Z",
     "shell.execute_reply.started": "2024-04-19T00:29:51.341981Z",
     "shell.execute_reply": "2024-04-19T00:29:55.884542Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Train Mistral Instruct\nWe train with HuggingFace `Trainer`. For more conveinence, we could use HuggingFace new `SFTTrainer` but i'm ok with more manual steps. We use PEFT for more efficient training. Note that I have **not** found the best hyperparameters for PEFT.\n\nWe use `DataCollatorForSeq2Seq`. This doesn't make a difference if we use `BATCH_SIZE = 1`, but if we train offline with `BATCH_SIZE > 1` this will speed up training. For each batch, it will keep the sequence length as short as possible. It will add padding so that both samples have length equal to the larger of the two samples. This is faster than padding every sample in train dataset to a maximum 1024.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    logging_dir = f'./logs_v{VER}',\n    output_dir = f'./output_v{VER}',\n    logging_steps=125,\n    save_steps=250,\n    logging_first_step=True,\n    fp16=True,\n    overwrite_output_dir=True,\n    warmup_ratio=0.0,\n    learning_rate=5e-5,\n    lr_scheduler_type='constant',\n    weight_decay=0.01,\n    eval_steps=None,\n    evaluation_strategy='no',\n    save_total_limit=2,\n    report_to='none',\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:29:55.887138Z",
     "iopub.execute_input": "2024-04-19T00:29:55.887463Z",
     "iopub.status.idle": "2024-04-19T00:30:06.18826Z",
     "shell.execute_reply.started": "2024-04-19T00:29:55.887434Z",
     "shell.execute_reply": "2024-04-19T00:30:06.187397Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip install /kaggle/input/llm-peft-pkg/peft-0.10.0-py3-none-any.whl",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:06.189389Z",
     "iopub.execute_input": "2024-04-19T00:30:06.189947Z",
     "iopub.status.idle": "2024-04-19T00:30:19.636437Z",
     "shell.execute_reply.started": "2024-04-19T00:30:06.189921Z",
     "shell.execute_reply": "2024-04-19T00:30:19.63523Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from peft import LoraConfig, get_peft_model\npeft_config = LoraConfig(\n    lora_alpha=16, # regularization\n    lora_dropout=0.1, \n    r=32, # attention heads\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"], \n)\nif not LOAD_PATH:\n    model = get_peft_model(model, peft_config)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:19.638099Z",
     "iopub.execute_input": "2024-04-19T00:30:19.638439Z",
     "iopub.status.idle": "2024-04-19T00:30:19.696317Z",
     "shell.execute_reply.started": "2024-04-19T00:30:19.638409Z",
     "shell.execute_reply": "2024-04-19T00:30:19.695483Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import DataCollatorForSeq2Seq\ncollator = DataCollatorForSeq2Seq(tokenizer, padding='longest')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:19.697401Z",
     "iopub.execute_input": "2024-04-19T00:30:19.697667Z",
     "iopub.status.idle": "2024-04-19T00:30:19.702347Z",
     "shell.execute_reply.started": "2024-04-19T00:30:19.697644Z",
     "shell.execute_reply": "2024-04-19T00:30:19.701408Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset_t,\n    eval_dataset=tokenized_dataset_v,\n    data_collator=collator,\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:19.703531Z",
     "iopub.execute_input": "2024-04-19T00:30:19.703825Z",
     "iopub.status.idle": "2024-04-19T00:30:19.753394Z",
     "shell.execute_reply.started": "2024-04-19T00:30:19.703799Z",
     "shell.execute_reply": "2024-04-19T00:30:19.752367Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from peft import PeftModel\n\nif not LOAD_PATH:\n    trainer.train()\n    model.save_pretrained(f\"mistral_v{VER}\")\nelse:\n    model = PeftModel.from_pretrained(model, LOAD_PATH)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:19.754569Z",
     "iopub.execute_input": "2024-04-19T00:30:19.754853Z",
     "iopub.status.idle": "2024-04-19T00:30:24.719268Z",
     "shell.execute_reply.started": "2024-04-19T00:30:19.754828Z",
     "shell.execute_reply": "2024-04-19T00:30:24.718427Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Validation Score\nWe trained with `MAX_LEN = 1024` but we can infer with any size. We infer with `size 2048` for the 3% of texts that are longer than 1024.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nimport warnings\nwarnings.filterwarnings('ignore')\npreds = []\n\nfor i,row in df.iloc[:VAL_SET].iterrows():\n\n    if i%100==0: print(i,', ',end='')\n        \n    tokenized_sample = preprocess(row, infer_mode=True, max_seq=2048, return_tensors=\"pt\")\n    generated_ids = model.generate(**tokenized_sample, \n                                    max_new_tokens=1,\n                                    pad_token_id=tokenizer.eos_token_id,\n                                    do_sample=False)\n    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    \n    try:\n        answer = decoded[0].rsplit(\"The score is: \", 1)[1]\n        preds.append( int(answer) )\n    except:\n        preds.append( 3 )\n        \n    if i==7: print(f'preds[:8]={preds}, ',end='')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:30:24.722552Z",
     "iopub.execute_input": "2024-04-19T00:30:24.722824Z",
     "iopub.status.idle": "2024-04-19T00:38:20.4009Z",
     "shell.execute_reply.started": "2024-04-19T00:30:24.722801Z",
     "shell.execute_reply": "2024-04-19T00:38:20.399849Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\ncm = confusion_matrix(df.score.values[:VAL_SET], preds)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n            xticklabels=[f\"Class {i}\" for i in range(1,7)], \n            yticklabels=[f\"Class {i}\" for i in range(1,7)])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:38:20.40218Z",
     "iopub.execute_input": "2024-04-19T00:38:20.402463Z",
     "iopub.status.idle": "2024-04-19T00:38:21.021454Z",
     "shell.execute_reply.started": "2024-04-19T00:38:20.402438Z",
     "shell.execute_reply": "2024-04-19T00:38:21.020455Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import cohen_kappa_score\nqwk = cohen_kappa_score(df.score.values[:VAL_SET], preds, weights=\"quadratic\")\nprint(f'Validation QWK Score = {qwk}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:38:21.022635Z",
     "iopub.execute_input": "2024-04-19T00:38:21.023248Z",
     "iopub.status.idle": "2024-04-19T00:38:21.033894Z",
     "shell.execute_reply.started": "2024-04-19T00:38:21.02322Z",
     "shell.execute_reply": "2024-04-19T00:38:21.032862Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Infer Test Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\nprint('Test shape',test.shape)\ntest.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:52:10.884793Z",
     "iopub.execute_input": "2024-04-19T00:52:10.885194Z",
     "iopub.status.idle": "2024-04-19T00:52:10.916448Z",
     "shell.execute_reply.started": "2024-04-19T00:52:10.885163Z",
     "shell.execute_reply": "2024-04-19T00:52:10.915424Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_preds = []\n\nfor i,row in test.iterrows():\n\n    tokenized_sample = preprocess(row, infer_mode=True, max_seq=2048, return_tensors=\"pt\")\n    generated_ids = model.generate(**tokenized_sample, \n                                    max_new_tokens=1,\n                                    pad_token_id=tokenizer.eos_token_id,\n                                    do_sample=False)\n    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    \n    try:\n        answer = decoded[0].rsplit(\"The score is: \", 1)[1]\n        test_preds.append( int(answer) )\n    except:\n        test_preds.append( 3 )\n        \n    if i==2: print(f'test_preds[:3]={test_preds}, ',end='')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:54:08.064916Z",
     "iopub.execute_input": "2024-04-19T00:54:08.06541Z",
     "iopub.status.idle": "2024-04-19T00:54:09.809071Z",
     "shell.execute_reply.started": "2024-04-19T00:54:08.065373Z",
     "shell.execute_reply": "2024-04-19T00:54:09.808056Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sub = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')\nprint('Sub shape',sub.shape)\nsub.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:54:36.110231Z",
     "iopub.execute_input": "2024-04-19T00:54:36.110616Z",
     "iopub.status.idle": "2024-04-19T00:54:36.129139Z",
     "shell.execute_reply.started": "2024-04-19T00:54:36.110586Z",
     "shell.execute_reply": "2024-04-19T00:54:36.128057Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sub.score = test_preds\nsub.score = sub.score.astype('int') # JUST TO BE SAFE\nsub.to_csv('submission.csv',index=False)\nsub.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T00:56:31.76529Z",
     "iopub.execute_input": "2024-04-19T00:56:31.76606Z",
     "iopub.status.idle": "2024-04-19T00:56:31.780289Z",
     "shell.execute_reply.started": "2024-04-19T00:56:31.766024Z",
     "shell.execute_reply": "2024-04-19T00:56:31.77935Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
