{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8029842,
     "sourceType": "datasetVersion",
     "datasetId": 4732809
    },
    {
     "sourceId": 8126207,
     "sourceType": "datasetVersion",
     "datasetId": 4791897
    },
    {
     "sourceId": 170434135,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170531930,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## ℹ️ Info\n\n* **2024/04/15 forked original great work kernels**\n    * https://www.kaggle.com/code/olyatsimboy/5-fold-deberta-lgbm\n    * https://www.kaggle.com/code/aikhmelnytskyy/quick-start-lgbm\n    * https://www.kaggle.com/code/hideyukizushi/aes2-5folddeberta-lgbm-countvectorizer-lb-810\n    \n\n* **My Contriduction: add Deberta predictions to LGBM as features**\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd \nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding\n)\nfrom datasets import Dataset\nfrom glob import glob\n\nMAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nMODEL_PATH = '/kaggle/input/aes-deberta-large/*/*'\nEVAL_BATCH_SIZE = 1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:27:46.728987Z",
     "iopub.execute_input": "2024-04-15T21:27:46.729652Z",
     "iopub.status.idle": "2024-04-15T21:28:08.576838Z",
     "shell.execute_reply.started": "2024-04-15T21:27:46.729615Z",
     "shell.execute_reply": "2024-04-15T21:28:08.575581Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import gc\nimport torch\n\nfrom scipy.special import softmax\n\n\nmodels = glob(MODEL_PATH)\n\ntokenizer = AutoTokenizer.from_pretrained(models[0])\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\ndf_test = pd.read_csv(TEST_DATA_PATH)\nds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n    report_to=\"none\"\n)\n\n\npredictions = []\n\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=DataCollatorWithPadding(tokenizer), \n        tokenizer=tokenizer\n    )\n    \n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    \n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:28:08.57881Z",
     "iopub.execute_input": "2024-04-15T21:28:08.579484Z",
     "iopub.status.idle": "2024-04-15T21:29:19.606048Z",
     "shell.execute_reply.started": "2024-04-15T21:28:08.579448Z",
     "shell.execute_reply": "2024-04-15T21:29:19.605105Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "predicted_score = 0.\n\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:19.607658Z",
     "iopub.execute_input": "2024-04-15T21:29:19.608043Z",
     "iopub.status.idle": "2024-04-15T21:29:19.613527Z",
     "shell.execute_reply.started": "2024-04-15T21:29:19.608006Z",
     "shell.execute_reply": "2024-04-15T21:29:19.612465Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:19.698551Z",
     "iopub.execute_input": "2024-04-15T21:29:19.699356Z",
     "iopub.status.idle": "2024-04-15T21:29:19.715904Z",
     "shell.execute_reply.started": "2024-04-15T21:29:19.699307Z",
     "shell.execute_reply": "2024-04-15T21:29:19.714876Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:19.717284Z",
     "iopub.execute_input": "2024-04-15T21:29:19.717807Z",
     "iopub.status.idle": "2024-04-15T21:29:19.733278Z",
     "shell.execute_reply.started": "2024-04-15T21:29:19.717766Z",
     "shell.execute_reply": "2024-04-15T21:29:19.732294Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importing necessary libraries\nimport gc\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport random\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import cohen_kappa_score\n#from autogluon.tabular.models import NNFastAiTabularModel\n#from autogluon.tabular import TabularDataset, TabularPredictor\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport polars as pl\n\nnltk.download('wordnet')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:19.741787Z",
     "iopub.execute_input": "2024-04-15T21:29:19.742121Z",
     "iopub.status.idle": "2024-04-15T21:29:43.730657Z",
     "shell.execute_reply.started": "2024-04-15T21:29:19.742092Z",
     "shell.execute_reply": "2024-04-15T21:29:43.729474Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n# 显示训练集中的第一个样本数据\n# Display the first sample data in the training set\ntrain.head(1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:43.731922Z",
     "iopub.execute_input": "2024-04-15T21:29:43.732782Z",
     "iopub.status.idle": "2024-04-15T21:29:44.447662Z",
     "shell.execute_reply.started": "2024-04-15T21:29:43.732748Z",
     "shell.execute_reply": "2024-04-15T21:29:44.445986Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importing the ProfileReport class from the ydata_profiling library\n#from ydata_profiling import ProfileReport\n\n# Generating a profile report for the train dataset\n# Setting the title of the report to \"Pandas Profiling Report\"\n#profile = ProfileReport(train, title=\"Pandas Profiling Report\")\n\n# Displaying the generated profile report\n#profile",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:44.449413Z",
     "iopub.execute_input": "2024-04-15T21:29:44.450223Z",
     "iopub.status.idle": "2024-04-15T21:29:44.455732Z",
     "shell.execute_reply.started": "2024-04-15T21:29:44.450182Z",
     "shell.execute_reply": "2024-04-15T21:29:44.454444Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**A quick analysis showed that the class distribution resembles a normal distribution, interestingly….**\n\n\n**Let's try to build a simple base model so we have something to go off of.\nI will use a bag-of-words model with the addition of feature selection, which can simplify the classification model.**\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# **Preprocessing**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Features engineering - https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # 将单词转化为小写\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    # 移除html\n    x = removeHTML(x)\n    # 删除以@作为首字母的字符串\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # 删除数字\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # 删除网址\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # 将连续空白符替换为一个空格字符\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # 替换连续的句号和逗号为一个\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # 去除开头结尾的空白符\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:44.461407Z",
     "iopub.execute_input": "2024-04-15T21:29:44.462044Z",
     "iopub.status.idle": "2024-04-15T21:29:44.473525Z",
     "shell.execute_reply.started": "2024-04-15T21:29:44.462004Z",
     "shell.execute_reply": "2024-04-15T21:29:44.472211Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.Paragraph Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# 获取特征名称\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:44.475414Z",
     "iopub.execute_input": "2024-04-15T21:29:44.475959Z",
     "iopub.status.idle": "2024-04-15T21:29:52.772497Z",
     "shell.execute_reply.started": "2024-04-15T21:29:44.475921Z",
     "shell.execute_reply": "2024-04-15T21:29:52.771442Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.Sentence Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:29:52.774205Z",
     "iopub.execute_input": "2024-04-15T21:29:52.774677Z",
     "iopub.status.idle": "2024-04-15T21:30:00.579365Z",
     "shell.execute_reply.started": "2024-04-15T21:29:52.774632Z",
     "shell.execute_reply": "2024-04-15T21:30:00.578354Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.Word Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:30:00.580708Z",
     "iopub.execute_input": "2024-04-15T21:30:00.581042Z",
     "iopub.status.idle": "2024-04-15T21:30:15.447153Z",
     "shell.execute_reply.started": "2024-04-15T21:30:00.581012Z",
     "shell.execute_reply": "2024-04-15T21:30:15.446065Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.Tf-idf features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,4),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# 将全部数据集都填充进TfidfVectorizer里，这可能会造成泄露和过于乐观的CV分数\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n# 转换为数组\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# 转换为dataframe\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# 重命名特征\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:30:15.448609Z",
     "iopub.execute_input": "2024-04-15T21:30:15.448946Z",
     "iopub.status.idle": "2024-04-15T21:32:50.851764Z",
     "shell.execute_reply.started": "2024-04-15T21:30:15.448917Z",
     "shell.execute_reply": "2024-04-15T21:32:50.850732Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### 5.CountVectorizer",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:32:50.853107Z",
     "iopub.execute_input": "2024-04-15T21:32:50.853476Z",
     "iopub.status.idle": "2024-04-15T21:32:50.858228Z",
     "shell.execute_reply.started": "2024-04-15T21:32:50.853446Z",
     "shell.execute_reply": "2024-04-15T21:32:50.857045Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.10,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:32:50.859635Z",
     "iopub.execute_input": "2024-04-15T21:32:50.859975Z",
     "iopub.status.idle": "2024-04-15T21:34:16.908326Z",
     "shell.execute_reply.started": "2024-04-15T21:32:50.859935Z",
     "shell.execute_reply": "2024-04-15T21:34:16.907436Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Model training**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import joblib\n\ndeberta_oof = joblib.load('/kaggle/input/aes-deberta-large/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))    \n\ntrain_feats.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:34:16.909583Z",
     "iopub.execute_input": "2024-04-15T21:34:16.909905Z",
     "iopub.status.idle": "2024-04-15T21:34:16.946071Z",
     "shell.execute_reply.started": "2024-04-15T21:34:16.909877Z",
     "shell.execute_reply": "2024-04-15T21:34:16.944972Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:34:16.947373Z",
     "iopub.execute_input": "2024-04-15T21:34:16.947709Z",
     "iopub.status.idle": "2024-04-15T21:34:16.956603Z",
     "shell.execute_reply.started": "2024-04-15T21:34:16.947681Z",
     "shell.execute_reply": "2024-04-15T21:34:16.955483Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Converting the 'text' column to string type and assigning to X\nX = train_feats[feature_names].astype(np.float32).values\n\n# Converting the 'score' column to integer type and assigning to y\ny_split = train_feats['score'].astype(int).values\ny = train_feats['score'].astype(np.float32).values-a\noof = train_feats['score'].astype(int).values",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:34:16.958194Z",
     "iopub.execute_input": "2024-04-15T21:34:16.958914Z",
     "iopub.status.idle": "2024-04-15T21:34:17.978074Z",
     "shell.execute_reply.started": "2024-04-15T21:34:16.958876Z",
     "shell.execute_reply": "2024-04-15T21:34:17.977135Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "len(feature_names)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:34:17.979227Z",
     "iopub.execute_input": "2024-04-15T21:34:17.979511Z",
     "iopub.status.idle": "2024-04-15T21:34:17.985956Z",
     "shell.execute_reply.started": "2024-04-15T21:34:17.979487Z",
     "shell.execute_reply": "2024-04-15T21:34:17.984951Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Let's use cross-validation**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define the number of splits for cross-validation\nn_splits = 15\n\n# Initialize StratifiedKFold with the specified number of splits\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\n# Lists to store scores\nf1_scores = []\nkappa_scores = []\nmodels = []\npredictions = []\ncallbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n# Loop through each fold of the cross-validation\ni=1\nfor train_index, test_index in skf.split(X, y_split):\n    # Split the data into training and testing sets for this fold\n    print('fold',i)\n    X_train_fold, X_test_fold = X[train_index], X[test_index]\n    \n   \n    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n\n    \n    model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.1,\n                max_depth = 5,\n                num_leaves = 10,\n                colsample_bytree=0.5,\n                reg_alpha = 0.1,\n                reg_lambda = 0.8,\n                n_estimators=1024,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n        \n                verbosity = - 1)\n\n    # Fit the model on the training data for this fold\n    \n    predictor = model.fit(X_train_fold,\n                                  y_train_fold,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks,)\n    models.append(predictor)\n    # Make predictions on the test data for this fold\n    predictions_fold = predictor.predict(X_test_fold)\n    predictions_fold = predictions_fold + a\n    oof[test_index]=predictions_fold\n    predictions_fold = predictions_fold.clip(1, 6).round()\n    predictions.append(predictions_fold)\n    # Calculate and store the F1 score for this fold\n    f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n    f1_scores.append(f1_fold)\n    \n    # Calculate and store the Cohen's kappa score for this fold\n    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n    kappa_scores.append(kappa_fold)\n    \n    # Calculating the confusion matrix\n    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n    # Displaying the confusion matrix\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=[x for x in range(1,7)])\n    disp.plot()\n    plt.show()\n    print(f'F1 score across fold: {f1_fold}')\n    print(f'Cohen kappa score across fold: {kappa_fold}')\n    i+=1\n# Calculate the mean scores across all folds\nmean_f1_score = np.mean(f1_scores)\nmean_kappa_score = np.mean(kappa_scores)\n\n# Print the mean scores\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T21:34:17.987249Z",
     "iopub.execute_input": "2024-04-15T21:34:17.987605Z",
     "iopub.status.idle": "2024-04-15T22:04:27.552233Z",
     "shell.execute_reply.started": "2024-04-15T21:34:17.987577Z",
     "shell.execute_reply": "2024-04-15T22:04:27.551128Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pickle\n\n\n# Зберегти список моделей у файл\nwith open('models.pkl', 'wb') as f:\n    pickle.dump(models)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:27.553861Z",
     "iopub.execute_input": "2024-04-15T22:04:27.554628Z",
     "iopub.status.idle": "2024-04-15T22:04:27.712545Z",
     "shell.execute_reply.started": "2024-04-15T22:04:27.554587Z",
     "shell.execute_reply": "2024-04-15T22:04:27.711506Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "with open('models.pkl', 'rb') as f:\n    models = pickle.load(f)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:27.714066Z",
     "iopub.execute_input": "2024-04-15T22:04:27.714569Z",
     "iopub.status.idle": "2024-04-15T22:04:27.83098Z",
     "shell.execute_reply.started": "2024-04-15T22:04:27.714517Z",
     "shell.execute_reply": "2024-04-15T22:04:27.829931Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.scatter(deberta_oof.argmax(-1), oof)\nplt.xlabel('deberta_oof')\nplt.ylabel('lgbm_oof')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:31:41.719605Z",
     "iopub.execute_input": "2024-04-15T22:31:41.720285Z",
     "iopub.status.idle": "2024-04-15T22:31:42.028797Z",
     "shell.execute_reply.started": "2024-04-15T22:31:41.720252Z",
     "shell.execute_reply": "2024-04-15T22:31:42.027465Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Inference**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# CountVectorizer\ntest_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:27.835452Z",
     "iopub.execute_input": "2024-04-15T22:04:27.835865Z",
     "iopub.status.idle": "2024-04-15T22:04:28.141949Z",
     "shell.execute_reply.started": "2024-04-15T22:04:27.835836Z",
     "shell.execute_reply": "2024-04-15T22:04:28.140825Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "probabilities = []\nfor model in models:\n    proba= model.predict(test_feats[feature_names])+ a\n    probabilities.append(proba)\n# Compute the average probabilities across all models\npredictions = np.mean(probabilities, axis=0)\n\npredictions = np.round(predictions.clip(1, 6))\n\n# Print the predictions\nprint(predictions)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:28.143251Z",
     "iopub.execute_input": "2024-04-15T22:04:28.143607Z",
     "iopub.status.idle": "2024-04-15T22:04:28.887557Z",
     "shell.execute_reply.started": "2024-04-15T22:04:28.143578Z",
     "shell.execute_reply": "2024-04-15T22:04:28.886477Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#predictions = model.predict(test['text'])\n#predictions\n# Initialize an empty list to store probabilities for each model\n\"\"\"\nprobabilities = []\n\n# Iterate over each model in the list of models\nfor model in models:\n    # Make predictions using the current model\n    print(model.predict(test['text']))\n    \n    # Compute decision scores for the current model\n    decision_scores = model.decision_function(test['text'])\n    \n    # Convert decision scores to probabilities using the sigmoid function\n    proba = 1 / (1 + np.exp(-decision_scores))\n    # Append the probabilities to the list\n    probabilities.append(proba)\n\n# Compute the average probabilities across all models\nprobabilities = np.mean(probabilities, axis=0)\n\n# Determine the predicted class by selecting the class with the highest probability\n# Add 1 to the index to match the class labels (assuming classes start from 1)\npredictions = np.argmax(probabilities, axis=1) + 1\n\n# Print the predictions\nprint(predictions)\n\"\"\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:28.88905Z",
     "iopub.execute_input": "2024-04-15T22:04:28.889403Z",
     "iopub.status.idle": "2024-04-15T22:04:28.898393Z",
     "shell.execute_reply.started": "2024-04-15T22:04:28.889373Z",
     "shell.execute_reply": "2024-04-15T22:04:28.897201Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsubmission['score']=predictions\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-15T22:04:28.899974Z",
     "iopub.execute_input": "2024-04-15T22:04:28.90032Z",
     "iopub.status.idle": "2024-04-15T22:04:28.930631Z",
     "shell.execute_reply.started": "2024-04-15T22:04:28.900287Z",
     "shell.execute_reply": "2024-04-15T22:04:28.9297Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
