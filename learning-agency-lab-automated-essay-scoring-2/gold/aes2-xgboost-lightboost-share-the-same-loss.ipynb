{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8166166,"sourceType":"datasetVersion","datasetId":4832208},{"sourceId":8217734,"sourceType":"datasetVersion","datasetId":4871040},{"sourceId":8260229,"sourceType":"datasetVersion","datasetId":4902588},{"sourceId":170434135,"sourceType":"kernelVersion"},{"sourceId":170531930,"sourceType":"kernelVersion"},{"sourceId":174935366,"sourceType":"kernelVersion"}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":610.563738,"end_time":"2024-05-03T06:26:19.553039","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-03T06:16:08.989301","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"19814212c28940d8b4ba79345b1537cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3070787da7bf41b1a741181015ab2c66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3811bec636a64bffa1fc1b8e65d9a13d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3070787da7bf41b1a741181015ab2c66","placeholder":"​","style":"IPY_MODEL_46a7bcf93a894175bd0b18823b5701df","value":"Map: 100%"}},"46a7bcf93a894175bd0b18823b5701df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5eaaa6660cbd40d3bef5c68dcbb32985":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61feb8219a0a46f8b0ffc20fa30ba50e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3811bec636a64bffa1fc1b8e65d9a13d","IPY_MODEL_c0f821bb8fa943b1a7b9997527b1afb3","IPY_MODEL_83ccd0ec46cd47e2881b8cfb74a6b9ac"],"layout":"IPY_MODEL_5eaaa6660cbd40d3bef5c68dcbb32985"}},"83ccd0ec46cd47e2881b8cfb74a6b9ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beae43fbe6f04c44867fcec05cc67742","placeholder":"​","style":"IPY_MODEL_c9be27dfb25c497ea767dfa4056ccc53","value":" 3/3 [00:00&lt;00:00, 64.23 examples/s]"}},"bc622db548574390a6f28dccddfce53d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beae43fbe6f04c44867fcec05cc67742":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0f821bb8fa943b1a7b9997527b1afb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc622db548574390a6f28dccddfce53d","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19814212c28940d8b4ba79345b1537cf","value":3}},"c9be27dfb25c497ea767dfa4056ccc53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport gc\nimport pickle\nimport torch\n\n\n_test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nENABLE_DONT_WASTE_YOUR_RUN_TIME = len(_test) < 10\n# ENABLE_DONT_WASTE_YOUR_RUN_TIME = False\nif ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n    import shutil\n\n#     shutil.copyfile(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\", \"submission.csv\")\n#     exit(0)\n    del _test\n    gc.collect()\n\nimport torch\n\nCUDA_AVAILABLE = torch.cuda.is_available()\nprint(f\"{CUDA_AVAILABLE = }\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-03T06:16:11.979523Z","iopub.status.busy":"2024-05-03T06:16:11.97917Z","iopub.status.idle":"2024-05-03T06:16:16.823215Z","shell.execute_reply":"2024-05-03T06:16:16.821985Z"},"papermill":{"duration":4.858557,"end_time":"2024-05-03T06:16:16.825265","exception":false,"start_time":"2024-05-03T06:16:11.966708","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.009953,"end_time":"2024-05-03T06:16:16.845385","exception":false,"start_time":"2024-05-03T06:16:16.835432","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Project Overview:\nAutomated Essay Scoring with Open-Source Solutions Background The first automated essay scoring competition, the Automated Student Assessment Prize (ASAP), was held twelve years ago. Since then, advancements in Automated Writing Evaluation (AWE) systems have aimed to reduce the time and cost associated with manual grading of student essays. However, many of these advancements are not widely accessible due to cost barriers, limiting their impact, especially in underserved communities.\n\nGoal The goal of this competition\nis to develop an open-source essay scoring algorithm that improves upon the original ASAP competition. By leveraging updated datasets and new ideas, the aim is to provide reliable and accessible automated grading solutions to overtaxed teachers, particularly in underserved communities. The competition seeks to reduce the high expense and time required for manual grading and enable the introduction of essays into testing, a key indicator of student learning.\n\nDataset\nThe competition utilizes the largest open-access writing dataset aligned with current standards for student-appropriate assessments. The dataset includes high-quality, realistic classroom writing samples, addressing the limitations of previous efforts by encompassing diverse economic and location populations to mitigate potential algorithmic bias. The dataset focuses on common essay formats used in classroom settings, providing a more expansive and representative sample for training and evaluation. Approach\n\nData Loading,\nWe begin by importing the necessary libraries for our task, including pandas, matplotlib.pyplot, seaborn, numpy, and torch. Data Loading We load the training data from a CSV file using pandas read_csv function. The df_train DataFrame now holds our training data","metadata":{"papermill":{"duration":0.009299,"end_time":"2024-05-03T06:16:16.864489","exception":false,"start_time":"2024-05-03T06:16:16.85519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport gc\nimport pickle\nimport torch\n\n\n_test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nENABLE_DONT_WASTE_YOUR_RUN_TIME = len(_test) < 10\n# ENABLE_DONT_WASTE_YOUR_RUN_TIME = False\nif ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n    import shutil\n\n#     shutil.copyfile(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\", \"submission.csv\")\n#     exit(0)\n    del _test\n    gc.collect()\n\nimport torch\n\nCUDA_AVAILABLE = torch.cuda.is_available()\nprint(f\"{CUDA_AVAILABLE = }\")","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:16:16.885737Z","iopub.status.busy":"2024-05-03T06:16:16.885359Z","iopub.status.idle":"2024-05-03T06:16:16.991472Z","shell.execute_reply":"2024-05-03T06:16:16.990115Z"},"papermill":{"duration":0.119185,"end_time":"2024-05-03T06:16:16.993748","exception":false,"start_time":"2024-05-03T06:16:16.874563","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport torch\nimport copy\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\nimport nltk\nfrom datasets import Dataset\nfrom glob import glob\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport re\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.special import softmax\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom lightgbm import log_evaluation, early_stopping\nimport lightgbm as lgb\nnltk.download('wordnet')\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:16:17.017991Z","iopub.status.busy":"2024-05-03T06:16:17.017646Z","iopub.status.idle":"2024-05-03T06:16:56.656332Z","shell.execute_reply":"2024-05-03T06:16:56.655323Z"},"papermill":{"duration":39.663982,"end_time":"2024-05-03T06:16:56.668558","exception":false,"start_time":"2024-05-03T06:16:17.004576","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n)\nfrom scipy.special import softmax\nimport torch\nimport gc\nimport glob\nMODEL_PATHS = [\n    '/kaggle/input/aes2-400-20240419134941/*/*',\n    '/kaggle/input/best-model-1/deberta-large-fold1/checkpoint-100/',\n    '/kaggle/input/train-best-model-3/deberta-large-fold1/checkpoint-200/'\n]\nEVAL_BATCH_SIZE = 1\n\nmodels = []\nfor path in MODEL_PATHS:\n    models.extend(glob.glob(path))\n\ntokenizer = AutoTokenizer.from_pretrained(models[0])\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\ndf_test = pd.read_csv(TEST_DATA_PATH)\nds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n    report_to=\"none\"\n)\n\npredictions = []\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=DataCollatorWithPadding(tokenizer), \n        tokenizer=tokenizer\n    )    \n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:16:56.690866Z","iopub.status.busy":"2024-05-03T06:16:56.690547Z","iopub.status.idle":"2024-05-03T06:18:02.658358Z","shell.execute_reply":"2024-05-03T06:18:02.657423Z"},"papermill":{"duration":65.982429,"end_time":"2024-05-03T06:18:02.660946","exception":false,"start_time":"2024-05-03T06:16:56.678517","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:02.685414Z","iopub.status.busy":"2024-05-03T06:18:02.685067Z","iopub.status.idle":"2024-05-03T06:18:02.689694Z","shell.execute_reply":"2024-05-03T06:18:02.688814Z"},"papermill":{"duration":0.01907,"end_time":"2024-05-03T06:18:02.691681","exception":false,"start_time":"2024-05-03T06:18:02.672611","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()\ndf_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:02.714605Z","iopub.status.busy":"2024-05-03T06:18:02.714292Z","iopub.status.idle":"2024-05-03T06:18:02.728049Z","shell.execute_reply":"2024-05-03T06:18:02.727184Z"},"papermill":{"duration":0.027444,"end_time":"2024-05-03T06:18:02.730057","exception":false,"start_time":"2024-05-03T06:18:02.702613","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n\ntrain.head(1)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:02.754414Z","iopub.status.busy":"2024-05-03T06:18:02.754103Z","iopub.status.idle":"2024-05-03T06:18:03.428517Z","shell.execute_reply":"2024-05-03T06:18:03.427663Z"},"papermill":{"duration":0.688992,"end_time":"2024-05-03T06:18:03.430627","exception":false,"start_time":"2024-05-03T06:18:02.741635","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n#     x = expandContractions(x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:03.456063Z","iopub.status.busy":"2024-05-03T06:18:03.455327Z","iopub.status.idle":"2024-05-03T06:18:03.496709Z","shell.execute_reply":"2024-05-03T06:18:03.495799Z"},"papermill":{"duration":0.056362,"end_time":"2024-05-03T06:18:03.4987","exception":false,"start_time":"2024-05-03T06:18:03.442338","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raise Exception(\"If you find this notebook, Please consider upvoting. This encourages me to share more useful codes\")\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport re\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nwith open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\n    \ndef count_spelling_errors(text):\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n    return spelling_errors","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:03.524531Z","iopub.status.busy":"2024-05-03T06:18:03.523958Z","iopub.status.idle":"2024-05-03T06:18:06.182846Z","shell.execute_reply":"2024-05-03T06:18:06.181835Z"},"papermill":{"duration":2.674779,"end_time":"2024-05-03T06:18:06.185698","exception":false,"start_time":"2024-05-03T06:18:03.510919","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:06.218068Z","iopub.status.busy":"2024-05-03T06:18:06.217644Z","iopub.status.idle":"2024-05-03T06:18:06.224434Z","shell.execute_reply":"2024-05-03T06:18:06.223398Z"},"papermill":{"duration":0.023231,"end_time":"2024-05-03T06:18:06.226585","exception":false,"start_time":"2024-05-03T06:18:06.203354","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Paragraph_Preprocess(tmp):\n\n    tmp = tmp.explode('paragraph')\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\nparagraph_fea2 = ['paragraph_error_num'] + paragraph_fea\ndef Paragraph_Eng(train_tmp):\n    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n    aggs = [\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],\n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\nif ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n    with open(\"/kaggle/input/aes2-cache/paragraph_preprocess_tmp.pickle\", \"rb\") as f:\n        tmp = pickle.load(f)\n    with open(\"/kaggle/input/aes2-cache/paragraph_preprocess_train_feats.pickle\", \"rb\") as f:\n        train_feats = pickle.load(f)\nelse:\n    tmp = Paragraph_Preprocess(train)\n    train_feats = Paragraph_Eng(tmp)\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:06.253064Z","iopub.status.busy":"2024-05-03T06:18:06.252649Z","iopub.status.idle":"2024-05-03T06:18:07.550285Z","shell.execute_reply":"2024-05-03T06:18:07.549225Z"},"papermill":{"duration":1.313463,"end_time":"2024-05-03T06:18:07.552389","exception":false,"start_time":"2024-05-03T06:18:06.238926","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Sentence_Preprocess(tmp):\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))    \n    return tmp\n\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],\n    \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:07.578222Z","iopub.status.busy":"2024-05-03T06:18:07.577892Z","iopub.status.idle":"2024-05-03T06:18:15.508876Z","shell.execute_reply":"2024-05-03T06:18:15.507826Z"},"papermill":{"duration":7.946483,"end_time":"2024-05-03T06:18:15.511334","exception":false,"start_time":"2024-05-03T06:18:07.564851","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    tmp = tmp.filter(pl.col('word_len')!=0)    \n    return tmp\n\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:15.540394Z","iopub.status.busy":"2024-05-03T06:18:15.540064Z","iopub.status.idle":"2024-05-03T06:18:29.121129Z","shell.execute_reply":"2024-05-03T06:18:29.120061Z"},"papermill":{"duration":13.597359,"end_time":"2024-05-03T06:18:29.12328","exception":false,"start_time":"2024-05-03T06:18:15.525921","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,6),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Number of Features: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:18:29.151459Z","iopub.status.busy":"2024-05-03T06:18:29.151132Z","iopub.status.idle":"2024-05-03T06:21:57.267952Z","shell.execute_reply":"2024-05-03T06:21:57.266995Z"},"papermill":{"duration":208.148033,"end_time":"2024-05-03T06:21:57.285078","exception":false,"start_time":"2024-05-03T06:18:29.137045","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.10,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:21:57.31448Z","iopub.status.busy":"2024-05-03T06:21:57.313584Z","iopub.status.idle":"2024-05-03T06:23:18.100331Z","shell.execute_reply":"2024-05-03T06:23:18.099143Z"},"papermill":{"duration":80.80543,"end_time":"2024-05-03T06:23:18.104176","exception":false,"start_time":"2024-05-03T06:21:57.298746","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\n\ndeberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))    \n\ntrain_feats.shape\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:23:18.140841Z","iopub.status.busy":"2024-05-03T06:23:18.139912Z","iopub.status.idle":"2024-05-03T06:23:18.250198Z","shell.execute_reply":"2024-05-03T06:23:18.249023Z"},"papermill":{"duration":0.128133,"end_time":"2024-05-03T06:23:18.252787","exception":false,"start_time":"2024-05-03T06:23:18.124654","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    if isinstance(y_pred, xgb.QuantileDMatrix):\n        # XGB\n        y_true, y_pred = y_pred, y_true\n\n        y_true = (y_true.get_label() + a).round()\n        y_pred = (y_pred + a).clip(1, 6).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk\n\n    else:\n        # For lgb\n        y_true = y_true + a\n        y_pred = (y_pred + a).clip(1, 6).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:23:18.285098Z","iopub.status.busy":"2024-05-03T06:23:18.284219Z","iopub.status.idle":"2024-05-03T06:23:18.297681Z","shell.execute_reply":"2024-05-03T06:23:18.296676Z"},"papermill":{"duration":0.03224,"end_time":"2024-05-03T06:23:18.299954","exception":false,"start_time":"2024-05-03T06:23:18.267714","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/aes2-400-fes-202404291649/usefe_list.pkl', mode='br') as fi:\n  feature_names = pickle.load(fi)\nfeature_select = feature_names\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:23:18.332136Z","iopub.status.busy":"2024-05-03T06:23:18.331077Z","iopub.status.idle":"2024-05-03T06:23:18.343454Z","shell.execute_reply":"2024-05-03T06:23:18.342561Z"},"papermill":{"duration":0.030671,"end_time":"2024-05-03T06:23:18.345529","exception":false,"start_time":"2024-05-03T06:23:18.314858","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_feats[feature_names].astype(np.float32).values\n\ny_split = train[\"score\"].to_pandas().astype(int).values\ny = train[\"score\"].to_pandas().astype(np.float32).values-a\noof = train[\"score\"].to_pandas().astype(int).values\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:23:18.374362Z","iopub.status.busy":"2024-05-03T06:23:18.373551Z","iopub.status.idle":"2024-05-03T06:23:19.535215Z","shell.execute_reply":"2024-05-03T06:23:19.534326Z"},"papermill":{"duration":1.17833,"end_time":"2024-05-03T06:23:19.537666","exception":false,"start_time":"2024-05-03T06:23:18.359336","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD = True # re-train\n# Define the number of splits for cross-validation\n# n_splits = 15\nn_splits = 16\nmodels = []\n\n\nimport xgboost as xgb\n\nclass Predictor:\n    def __init__(self, models: list):\n        self.models = models\n    def predict(self, X):\n        n_models = len(self.models)\n        predicted = None\n        for i, model in enumerate(self.models):\n            if i == 0:\n                predicted = 0.76*model.predict(X)\n            else:\n                predicted += 0.24*model.predict(X)\n        return predicted\n\n\nif not LOAD:\n    for i in range(n_splits):\n        models.append(lgb.Booster(model_file=f'/kaggle/input/aes-15fold/fold_{i+1}.txt'))\nelse:\n    # Initialize StratifiedKFold with the specified number of splits\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n    # Lists to store scores\n    f1_scores = []\n    kappa_scores = []\n    models = []\n    predictions = []\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n    # Loop through each fold of the cross-validation\n    i=1\n    for train_index, test_index in skf.split(X, y_split):\n        # Split the data into training and testing sets for this fold\n        print('fold',i)\n        X_train_fold, X_test_fold = X[train_index], X[test_index]\n        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n        light = lgb.LGBMRegressor(\n                    objective = qwk_obj,\n                    metrics = 'None',\n                    learning_rate = 0.05,\n                    max_depth = 8,\n                    num_leaves = 10,\n                    colsample_bytree=0.3,\n                    reg_alpha = 2.,\n                    reg_lambda = 0.1,\n                    n_estimators=700,\n                    random_state=42,\n                    extra_trees=True,\n                    class_weight='balanced',\n                    device='gpu' if CUDA_AVAILABLE else 'cpu',\n                    verbosity = - 1\n        )\n\n        # Fit the model on the training data for this fold  \n        light.fit(X_train_fold,\n                              y_train_fold,\n                              eval_names=['train', 'valid'],\n                              eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                              eval_metric=quadratic_weighted_kappa,\n                              callbacks=callbacks\n                             )\n        xgb_regressor = xgb.XGBRegressor(\n            objective = qwk_obj,\n            metrics = 'None',\n            learning_rate = 0.1,\n            max_depth = 8,\n            num_leaves = 10,\n            colsample_bytree=0.5,\n            reg_alpha = 1.0,\n            reg_lambda = 0.1,\n            n_estimators=1024,\n            random_state=42,\n            extra_trees=True,\n            class_weight='balanced',\n            tree_method=\"hist\",\n            device=\"gpu\" if CUDA_AVAILABLE else \"cpu\"\n        #             device='gpu',\n        #             verbosity = 1\n        )\n    \n        xgb_callbacks = [\n            xgb.callback.EvaluationMonitor(period=25),\n            xgb.callback.EarlyStopping(75, metric_name=\"QWK\", maximize=True, save_best=True)\n        ]\n        xgb_regressor.fit(\n            X_train_fold,\n            y_train_fold,\n            eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n            eval_metric=quadratic_weighted_kappa,\n            callbacks=xgb_callbacks\n        )\n        predictor = Predictor([light, xgb_regressor])\n        # predictor = lgb_regressor.fit(X_train_fold, y_train_fold)\n        #predictor = xgb_regressor.fit(X_train_fold, y_train_fold)\n        models.append(predictor)\n        # Make predictions on the test data for this fold\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        # Calculate and store the F1 score for this fold\n        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n        f1_scores.append(f1_fold)\n\n        # Calculate and store the Cohen's kappa score for this fold\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        kappa_scores.append(kappa_fold)\n#         predictor.booster_.save_model(f'fold_{i}.txt')\n\n        print(f'F1 score across fold: {f1_fold}')\n        print(f'Cohen kappa score across fold: {kappa_fold}')\n        i+=1\n        gc.collect()\n        if ENABLE_DONT_WASTE_YOUR_RUN_TIME:\n            break","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:23:19.566793Z","iopub.status.busy":"2024-05-03T06:23:19.566417Z","iopub.status.idle":"2024-05-03T06:26:12.75946Z","shell.execute_reply":"2024-05-03T06:26:12.75814Z"},"papermill":{"duration":173.210227,"end_time":"2024-05-03T06:26:12.761792","exception":false,"start_time":"2024-05-03T06:23:19.551565","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# CountVectorizer\ntest_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:26:12.822Z","iopub.status.busy":"2024-05-03T06:26:12.821204Z","iopub.status.idle":"2024-05-03T06:26:13.47681Z","shell.execute_reply":"2024-05-03T06:26:13.475663Z"},"papermill":{"duration":0.688699,"end_time":"2024-05-03T06:26:13.479212","exception":false,"start_time":"2024-05-03T06:26:12.790513","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = []\nfor model in models:\n    proba= model.predict(test_feats[feature_select])+ a\n    probabilities.append(proba)\n\npredictions = np.mean(probabilities, axis=0)\n\npredictions = np.round(predictions.clip(1, 6))\n\nprint(predictions)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:26:13.542004Z","iopub.status.busy":"2024-05-03T06:26:13.541597Z","iopub.status.idle":"2024-05-03T06:26:15.429014Z","shell.execute_reply":"2024-05-03T06:26:15.42744Z"},"papermill":{"duration":1.9205,"end_time":"2024-05-03T06:26:15.431405","exception":false,"start_time":"2024-05-03T06:26:13.510905","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsubmission['score']=predictions\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:26:15.487484Z","iopub.status.busy":"2024-05-03T06:26:15.486752Z","iopub.status.idle":"2024-05-03T06:26:15.506914Z","shell.execute_reply":"2024-05-03T06:26:15.505946Z"},"papermill":{"duration":0.049854,"end_time":"2024-05-03T06:26:15.508934","exception":false,"start_time":"2024-05-03T06:26:15.45908","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data\n#df1 = pd.read_csv('submission1.csv')\n#df2 = pd.read_csv('submission.csv')\n\n# # Merging the dataframes on 'essay_id'\n#df = pd.merge(left=df1, right=df2, on='essay_id', suffixes=('_1', '_2'))\n\n# # Calculating the average score directly without apply()\n#df['score'] = (df['score_1'] * 0.02 + df['score_2'] * 0.98).round().astype(int)\n\n# # Saving the desired columns to a new csv file\n#df[['essay_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.026927,"end_time":"2024-05-03T06:26:15.562608","exception":false,"start_time":"2024-05-03T06:26:15.535681","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.02684,"end_time":"2024-05-03T06:26:15.616239","exception":false,"start_time":"2024-05-03T06:26:15.589399","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.074402,"end_time":"2024-05-03T06:26:15.716712","exception":false,"start_time":"2024-05-03T06:26:15.64231","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.027147,"end_time":"2024-05-03T06:26:15.77105","exception":false,"start_time":"2024-05-03T06:26:15.743903","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.025199,"end_time":"2024-05-03T06:26:15.822721","exception":false,"start_time":"2024-05-03T06:26:15.797522","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.0259,"end_time":"2024-05-03T06:26:15.873727","exception":false,"start_time":"2024-05-03T06:26:15.847827","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026207,"end_time":"2024-05-03T06:26:15.925383","exception":false,"start_time":"2024-05-03T06:26:15.899176","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.028843,"end_time":"2024-05-03T06:26:15.982581","exception":false,"start_time":"2024-05-03T06:26:15.953738","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026445,"end_time":"2024-05-03T06:26:16.037164","exception":false,"start_time":"2024-05-03T06:26:16.010719","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026852,"end_time":"2024-05-03T06:26:16.091224","exception":false,"start_time":"2024-05-03T06:26:16.064372","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.029508,"end_time":"2024-05-03T06:26:16.147698","exception":false,"start_time":"2024-05-03T06:26:16.11819","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026181,"end_time":"2024-05-03T06:26:16.201418","exception":false,"start_time":"2024-05-03T06:26:16.175237","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}