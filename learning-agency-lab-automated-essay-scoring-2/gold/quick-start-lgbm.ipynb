{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8126207,"sourceType":"datasetVersion","datasetId":4791897},{"sourceId":8146752,"sourceType":"datasetVersion","datasetId":4817638},{"sourceId":8166166,"sourceType":"datasetVersion","datasetId":4832208},{"sourceId":8220031,"sourceType":"datasetVersion","datasetId":4873016},{"sourceId":8220517,"sourceType":"datasetVersion","datasetId":4796193},{"sourceId":170434135,"sourceType":"kernelVersion"},{"sourceId":170531930,"sourceType":"kernelVersion"}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Hi to all! This is my quick start notebook.**","metadata":{}},{"cell_type":"markdown","source":"# Hmm, the result of this notebook on the public dataset is interesting.... 0.540 is a lot, considering the failure of the model on the validation set, we will improve the model\n\n# version 2 and 3 - Added cross validation\n\n# version 4 - Added LGBM from this notebook https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n\n# versions 5-8 - I tried changing the ngram parameter, but ngram=(1,1) seems to work best\n\n# version 9 - Used autogluon to build the model\n\n# version 12 - Make custom scorer fot the autogluon - This only worsened the result\n\n# version 14-16 - Added cross validation \n\n# version 17 - Used new preprocessing from this great notebook https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments\n\n# version 21 - Used new model from this great notebook https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments, which works much better\n\n# version 23 - I will try to use GaussianNB to transform the results of the regression model into class labels\n\n# version 25 - Restored the previous version because the idea didn't work\n\n# version 27 - The number of folds has been increased\n\n# version 31 - add pseudo labeling: The idea did not work\n\n# version 35 - Added new post-processing and result analysisk\n\n# version 36 - Added CountVectorizer from https://www.kaggle.com/code/hideyukizushi/aes2-5folddeberta-lgbm-countvectorizer-lb-810\n\n# version 44 - Partially added pipeline from here https://www.kaggle.com/code/hideyukizushi/aes2-deberta-lgbm-countvectorizer-lb-814?scriptVersionId=173181256\n\n# version 51 - Added feature selection from here https://www.kaggle.com/code/yongsukprasertsuk/0-818-deberta-v3-large-lgbm-baseline","metadata":{}},{"cell_type":"markdown","source":"Choose the appropriate version of autogluon to train on GPU or CPU","metadata":{}},{"cell_type":"code","source":"#!pip install -q --no-index --find-links=/kaggle/input/autogluon-cpu autogluon\n#!pip install -q --no-index --find-links=/kaggle/input/autogluon-gpu autogluon","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:24:17.424648Z","iopub.execute_input":"2024-04-24T20:24:17.425126Z","iopub.status.idle":"2024-04-24T20:24:17.430766Z","shell.execute_reply.started":"2024-04-24T20:24:17.425092Z","shell.execute_reply":"2024-04-24T20:24:17.429573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport torch\nimport copy\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\nimport nltk\nfrom datasets import Dataset\nfrom glob import glob\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport re\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport random\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import cohen_kappa_score\n#from autogluon.tabular.models import NNFastAiTabularModel\n#from autogluon.tabular import TabularDataset, TabularPredictor\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport polars as pl\nimport pickle\nfrom scipy.special import softmax\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:24:17.433558Z","iopub.execute_input":"2024-04-24T20:24:17.433924Z","iopub.status.idle":"2024-04-24T20:25:03.235115Z","shell.execute_reply.started":"2024-04-24T20:24:17.433895Z","shell.execute_reply":"2024-04-24T20:25:03.233955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nMODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\nEVAL_BATCH_SIZE = 1\nmodels = glob(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(models[0])\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\ndf_test = pd.read_csv(TEST_DATA_PATH)\nds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n    report_to=\"none\"\n)\n\npredictions = []\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=DataCollatorWithPadding(tokenizer), \n        tokenizer=tokenizer\n    )    \n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \npredicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)\ndf_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()\ndf_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:25:03.237501Z","iopub.execute_input":"2024-04-24T20:25:03.238707Z","iopub.status.idle":"2024-04-24T20:26:13.479424Z","shell.execute_reply.started":"2024-04-24T20:25:03.238667Z","shell.execute_reply":"2024-04-24T20:26:13.478443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n# 显示训练集中的第一个样本数据\n# Display the first sample data in the training set\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:13.480988Z","iopub.execute_input":"2024-04-24T20:26:13.481763Z","iopub.status.idle":"2024-04-24T20:26:14.208445Z","shell.execute_reply.started":"2024-04-24T20:26:13.481704Z","shell.execute_reply":"2024-04-24T20:26:14.207475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the ProfileReport class from the ydata_profiling library\n#from ydata_profiling import ProfileReport\n\n# Generating a profile report for the train dataset\n# Setting the title of the report to \"Pandas Profiling Report\"\n#profile = ProfileReport(train, title=\"Pandas Profiling Report\")\n\n# Displaying the generated profile report\n#profile","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:14.211137Z","iopub.execute_input":"2024-04-24T20:26:14.211878Z","iopub.status.idle":"2024-04-24T20:26:14.216071Z","shell.execute_reply.started":"2024-04-24T20:26:14.211839Z","shell.execute_reply":"2024-04-24T20:26:14.215061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A quick analysis showed that the class distribution resembles a normal distribution, interestingly….**\n\n\n**Let's try to build a simple base model so we have something to go off of.\nI will use a bag-of-words model with the addition of feature selection, which can simplify the classification model.**\n","metadata":{}},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"## Features engineering - https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments","metadata":{}},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:14.217733Z","iopub.execute_input":"2024-04-24T20:26:14.218111Z","iopub.status.idle":"2024-04-24T20:26:14.242199Z","shell.execute_reply.started":"2024-04-24T20:26:14.218076Z","shell.execute_reply":"2024-04-24T20:26:14.241062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.Paragraph Features","metadata":{}},{"cell_type":"code","source":"# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# 获取特征名称\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:14.243887Z","iopub.execute_input":"2024-04-24T20:26:14.244481Z","iopub.status.idle":"2024-04-24T20:26:22.764268Z","shell.execute_reply.started":"2024-04-24T20:26:14.244442Z","shell.execute_reply":"2024-04-24T20:26:22.763183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.Sentence Features","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:22.765783Z","iopub.execute_input":"2024-04-24T20:26:22.766183Z","iopub.status.idle":"2024-04-24T20:26:30.706604Z","shell.execute_reply.started":"2024-04-24T20:26:22.766146Z","shell.execute_reply":"2024-04-24T20:26:30.705363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.Word Features","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:30.710084Z","iopub.execute_input":"2024-04-24T20:26:30.710396Z","iopub.status.idle":"2024-04-24T20:26:45.489381Z","shell.execute_reply.started":"2024-04-24T20:26:30.710369Z","shell.execute_reply":"2024-04-24T20:26:45.488226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.Tf-idf features","metadata":{}},{"cell_type":"code","source":"# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(4,8),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# 将全部数据集都填充进TfidfVectorizer里，这可能会造成泄露和过于乐观的CV分数\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n# 转换为数组\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# 转换为dataframe\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# 重命名特征\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:26:45.490937Z","iopub.execute_input":"2024-04-24T20:26:45.491282Z","iopub.status.idle":"2024-04-24T20:32:12.188934Z","shell.execute_reply.started":"2024-04-24T20:26:45.491252Z","shell.execute_reply":"2024-04-24T20:32:12.187797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.CountVectorizer","metadata":{}},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,5),\n            min_df=0.10,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:32:12.194163Z","iopub.execute_input":"2024-04-24T20:32:12.194465Z","iopub.status.idle":"2024-04-24T20:34:58.960891Z","shell.execute_reply.started":"2024-04-24T20:32:12.194439Z","shell.execute_reply":"2024-04-24T20:34:58.959729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Meta Features(deberta-v3-large)","metadata":{}},{"cell_type":"code","source":"import joblib\n\ndeberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))    \n\ntrain_feats.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:34:58.962184Z","iopub.execute_input":"2024-04-24T20:34:58.962508Z","iopub.status.idle":"2024-04-24T20:34:59.017468Z","shell.execute_reply.started":"2024-04-24T20:34:58.962479Z","shell.execute_reply":"2024-04-24T20:34:59.016525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:34:59.019187Z","iopub.execute_input":"2024-04-24T20:34:59.019641Z","iopub.status.idle":"2024-04-24T20:34:59.029858Z","shell.execute_reply.started":"2024-04-24T20:34:59.019601Z","shell.execute_reply":"2024-04-24T20:34:59.028466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the 'text' column to string type and assigning to X\nX = train_feats[feature_names].astype(np.float32).values\n\n# Converting the 'score' column to integer type and assigning to y\ny_split = train_feats['score'].astype(int).values\ny = train_feats['score'].astype(np.float32).values-a\noof = train_feats['score'].astype(np.float32).values\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:34:59.031283Z","iopub.execute_input":"2024-04-24T20:34:59.031695Z","iopub.status.idle":"2024-04-24T20:35:02.531169Z","shell.execute_reply.started":"2024-04-24T20:34:59.031665Z","shell.execute_reply":"2024-04-24T20:35:02.529909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection¶(https://www.kaggle.com/code/yongsukprasertsuk/0-818-deberta-v3-large-lgbm-baseline,  https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817)\n","metadata":{}},{"cell_type":"code","source":"if_train=True # You can train your model here, I'm currently using the models trained in version 51 of this notebook\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:35:02.532559Z","iopub.execute_input":"2024-04-24T20:35:02.532919Z","iopub.status.idle":"2024-04-24T20:35:02.538018Z","shell.execute_reply.started":"2024-04-24T20:35:02.53289Z","shell.execute_reply":"2024-04-24T20:35:02.53692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif if_train:\n    # Define the number of splits for cross-validation\n    n_splits = 15\n    fse=[]\n    # Initialize StratifiedKFold with the specified number of splits\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\n    # Lists to store scores\n    f1_scores = []\n    kappa_scores = []\n    models = []\n    predictions = []\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n    # Loop through each fold of the cross-validation\n    i=1\n    for train_index, test_index in skf.split(X, y_split):\n        # Split the data into training and testing sets for this fold\n        print('fold',i)\n        X_train_fold, X_test_fold = X[train_index], X[test_index]\n\n\n        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n\n\n        model = lgb.LGBMRegressor(\n                    objective = qwk_obj,\n                    metrics = 'None',\n                    learning_rate = 0.1,\n                    max_depth = 5,\n                    num_leaves = 10,\n                    colsample_bytree=0.5,\n                    reg_alpha = 0.1,\n                    reg_lambda = 0.8,\n                    n_estimators=1024,\n                    random_state=42,\n                    extra_trees=True,\n                    class_weight='balanced',\n\n                    verbosity = - 1)\n\n        # Fit the model on the training data for this fold\n\n        predictor = model.fit(X_train_fold,\n                                      y_train_fold,\n                                      eval_names=['train', 'valid'],\n                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                      eval_metric=quadratic_weighted_kappa,\n                                      callbacks=callbacks,)\n        models.append(predictor)\n        # Make predictions on the test data for this fold\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        # Calculate and store the F1 score for this fold\n        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n        f1_scores.append(f1_fold)\n\n        # Calculate and store the Cohen's kappa score for this fold\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        kappa_scores.append(kappa_fold)\n\n        # Calculating the confusion matrix\n        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n        # Displaying the confusion matrix\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                      display_labels=[x for x in range(1,7)])\n        disp.plot()\n        plt.show()\n        print(f'F1 score across fold: {f1_fold}')\n        print(f'Cohen kappa score across fold: {kappa_fold}')\n        i+=1\n        fse.append(pd.Series(predictor.feature_importances_, feature_names))\n    # Calculate the mean scores across all folds\n    mean_f1_score = np.mean(f1_scores)\n    mean_kappa_score = np.mean(kappa_scores)\n\n    # Print the mean scores\n    print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n    print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\n\n    # Зберегти список моделей у файл\n    with open('models_feature_select.pkl', 'wb') as f:\n        pickle.dump(models, f)\n    with open('fse.pkl', 'wb') as f:\n        pickle.dump(fse, f)\nelse:\n    with open('/kaggle/input/feature-select-learning-agency-lab/models_feature_select.pkl', 'rb') as f:\n        models = pickle.load(f)\n    with open('/kaggle/input/feature-select-learning-agency-lab/fse.pkl', 'rb') as f:\n        fse = pickle.load(f)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:35:02.539535Z","iopub.execute_input":"2024-04-24T20:35:02.539887Z","iopub.status.idle":"2024-04-24T20:35:03.009462Z","shell.execute_reply.started":"2024-04-24T20:35:02.539854Z","shell.execute_reply":"2024-04-24T20:35:03.008318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Let's use cross-validation**","metadata":{}},{"cell_type":"code","source":"if_train=True # You can train your model here, I'm currently using the models trained in version 51 of this notebook\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:43:35.321978Z","iopub.execute_input":"2024-04-24T20:43:35.323005Z","iopub.status.idle":"2024-04-24T20:43:35.328133Z","shell.execute_reply.started":"2024-04-24T20:43:35.322967Z","shell.execute_reply":"2024-04-24T20:43:35.326784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif if_train:\n    # Define the number of splits for cross-validation\n    n_splits = 15\n\n    # Initialize StratifiedKFold with the specified number of splits\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\n    # Lists to store scores\n    f1_scores = []\n    kappa_scores = []\n    models = []\n    predictions = []\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n    # Loop through each fold of the cross-validation\n    i=1\n    for train_index, test_index in skf.split(X, y_split):\n        # Split the data into training and testing sets for this fold\n        print('fold',i)\n        feature_select = fse[i-1].sort_values(ascending=False).index.tolist()[:13000]\n        X_train_fold, X_test_fold = train_feats[feature_select].astype(np.float32).values[train_index], train_feats[feature_select].astype(np.float32).values[test_index]\n\n        \n        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n\n\n        model = lgb.LGBMRegressor(\n                    objective = qwk_obj,\n                    metrics = 'None',\n                    learning_rate = 0.1,\n                    max_depth = 5,\n                    num_leaves = 10,\n                    colsample_bytree=0.5,\n                    reg_alpha = 0.1,\n                    reg_lambda = 0.8,\n                    n_estimators=1024,\n                    random_state=42,\n                    extra_trees=True,\n                    class_weight='balanced',\n\n                    verbosity = - 1)\n\n        # Fit the model on the training data for this fold\n\n        predictor = model.fit(X_train_fold,\n                                      y_train_fold,\n                                      eval_names=['train', 'valid'],\n                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                      eval_metric=quadratic_weighted_kappa,\n                                      callbacks=callbacks,)\n        models.append(predictor)\n        # Make predictions on the test data for this fold\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        # Calculate and store the F1 score for this fold\n        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n        f1_scores.append(f1_fold)\n\n        # Calculate and store the Cohen's kappa score for this fold\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        kappa_scores.append(kappa_fold)\n\n        # Calculating the confusion matrix\n        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n        # Displaying the confusion matrix\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                      display_labels=[x for x in range(1,7)])\n        disp.plot()\n        plt.show()\n        print(f'F1 score across fold: {f1_fold}')\n        print(f'Cohen kappa score across fold: {kappa_fold}')\n        i+=1\n    # Calculate the mean scores across all folds\n    mean_f1_score = np.mean(f1_scores)\n    mean_kappa_score = np.mean(kappa_scores)\n\n    # Print the mean scores\n    print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n    print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\n\n    # Зберегти список моделей у файл\n    with open('models.pkl', 'wb') as f:\n        pickle.dump(models, f)\nelse:\n    with open('/kaggle/input/quick-start-lgbm-version-30/models.pkl', 'rb') as f:\n        models = pickle.load(f)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:43:37.010181Z","iopub.execute_input":"2024-04-24T20:43:37.010549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the number of splits for cross-validation\nn_splits = 15\n\n# Initialize StratifiedKFold with the specified number of splits\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\noof=oof.astype('float32')\n\n# Loop through each fold of the cross-validation\ni=0\nfor train_index, test_index in skf.split(X, y_split):\n    # Split the data into training and testing sets for this fold\n    print('fold',i)\n    feature_select = fse[i].sort_values(ascending=False).index.tolist()[:13000]\n    X_train_fold, X_test_fold = train_feats[feature_select].astype(np.float32).values[train_index], train_feats[feature_select].astype(np.float32).values[test_index]\n\n\n    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n\n\n    predictor = models[i]\n    # Make predictions on the test data for this fold\n    predictions_fold = predictor.predict(X_test_fold)\n    predictions_fold = predictions_fold + a\n    oof[test_index]=predictions_fold\n    #predictions_fold = predictions_fold.clip(1, 6).round()\n    \n    i+=1\noof","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:41:16.684173Z","iopub.execute_input":"2024-04-24T20:41:16.685202Z","iopub.status.idle":"2024-04-24T20:42:18.387999Z","shell.execute_reply.started":"2024-04-24T20:41:16.685165Z","shell.execute_reply":"2024-04-24T20:42:18.386805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Displaying the results of model predictions","metadata":{}},{"cell_type":"code","source":" # Calculate and store the F1 score for this fold\nmean_f1_score = f1_score(y_split, oof.clip(1, 6).round(), average='weighted')\n\n# Calculate and store the Cohen's kappa score for this fold\nmean_kappa_score = cohen_kappa_score(y_split, oof.clip(1, 6).round(), weights='quadratic')\n\n# Calculating the confusion matrix\ncm = confusion_matrix(y_split, oof.clip(1, 6).round(), labels=[x for x in range(1,7)])\n\n# Displaying the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[x for x in range(1,7)])\ndisp.plot()\nplt.show()\n# Print the mean scores\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:42:18.389901Z","iopub.execute_input":"2024-04-24T20:42:18.390256Z","iopub.status.idle":"2024-04-24T20:42:18.892096Z","shell.execute_reply.started":"2024-04-24T20:42:18.390227Z","shell.execute_reply":"2024-04-24T20:42:18.891005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Припустимо, що oof та y_split - це ваші масиви\n\n# Створюємо кольорову палітру для різних класів\ncolors = ['r', 'g', 'b', 'c', 'm', 'y']\n\n# Отримуємо унікальні класи з y_split\nunique_classes = set(y_split)\n\n# Створюємо підграфік для візуалізації\nplt.figure()\n\n# Проходимося по кожному класу і відображаємо відповідні точки з oof\nfor cls in unique_classes:\n    # Відфільтруємо точки з oof, які відповідають поточному класу\n    points = oof[y_split == cls]\n    # Відображаємо точки для поточного класу з відповідним кольором\n    plt.scatter(points, [cls] * len(points), color=colors[cls-1], label=f'Class {cls}')\n\n# Налаштуємо заголовок та мітки осей\nplt.title('Visualization of oof with different classes from y_split')\nplt.xlabel('oof values')\nplt.ylabel('Class')\n\n# Додаємо легенду\nplt.legend()\n\n# Показуємо графік\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:36:13.616198Z","iopub.execute_input":"2024-04-24T20:36:13.616491Z","iopub.status.idle":"2024-04-24T20:36:14.435827Z","shell.execute_reply.started":"2024-04-24T20:36:13.616465Z","shell.execute_reply":"2024-04-24T20:36:14.434728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Припустимо, що oof та y_split - це ваші масиви\n\n# Отримуємо унікальні класи з y_split\nunique_classes = sorted(set(y_split))\n\n# Знайдемо максимальне значення для побудови однакової сітки\nmax_value = max([max(oof[y_split == cls]) for cls in unique_classes])\n\n# Створюємо новий графік для візуалізації\nplt.figure(figsize=(20, 12))\n\n# Проходимося по кожному класу і будуємо частотний графік для відповідних точок oof\nfor idx, cls in enumerate(unique_classes):\n    # Відфільтруємо точки з oof, які відповідають поточному класу\n    points = oof[y_split == cls]\n    # Побудова частотного графіка для поточного класу\n    plt.subplot(2, 3, idx+1)  # Побудова у вигляді 2x3 підграфіків\n    plt.hist(points, color='skyblue', edgecolor='black', bins=20)\n    plt.title(f'Class {cls}')\n    plt.xlabel('oof values')\n    plt.ylabel('Frequency')\n    plt.xlim(0, max_value)  # Встановлення однакового діапазону для осі x\n    plt.ylim(0, None)  # Встановлення діапазону для осі y (від 0 до максимума)\n\n# Налаштуємо заголовок та підзаголовки\nplt.suptitle('Frequency Distribution of oof for each y_split class')\nplt.tight_layout()\n\n# Показуємо графік\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:36:14.437336Z","iopub.execute_input":"2024-04-24T20:36:14.437779Z","iopub.status.idle":"2024-04-24T20:36:16.40382Z","shell.execute_reply.started":"2024-04-24T20:36:14.437709Z","shell.execute_reply":"2024-04-24T20:36:16.402668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's try to build a classification with thresholds","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Оптимальні пороги для переходу між класами\noptimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\noof_clip = oof.clip(1, 6)\ndef classify_with_thresholds(oof_clip, optimal_thresholds):\n\n    # Класифікуємо значення oof за допомогою оптимальних порогів\n    classified_oof = np.empty_like(oof_clip)\n    for i, threshold in enumerate(optimal_thresholds):\n        if i == 0:\n            classified_oof[oof_clip < threshold] = 1\n\n        else:\n            classified_oof[(oof_clip >= optimal_thresholds[i-1]) & (oof_clip < threshold)] = i + 1\n\n        if i == 4:\n            classified_oof[oof_clip >= threshold] = 6\n            \n    return classified_oof.astype(int)\n            \nclassified_oof =classify_with_thresholds(oof_clip, optimal_thresholds)\n    \nprint(\"Classified oof:\")\nprint(classified_oof)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:36:16.405202Z","iopub.execute_input":"2024-04-24T20:36:16.405498Z","iopub.status.idle":"2024-04-24T20:36:16.416248Z","shell.execute_reply.started":"2024-04-24T20:36:16.405473Z","shell.execute_reply":"2024-04-24T20:36:16.415187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Calculate and store the F1 score for this fold\nmean_f1_score = f1_score(y_split, classified_oof, average='weighted')\n\n# Calculate and store the Cohen's kappa score for this fold\nmean_kappa_score = cohen_kappa_score(y_split, classified_oof, weights='quadratic')\n\n# Calculating the confusion matrix\ncm = confusion_matrix(y_split, classified_oof, labels=[x for x in range(1,7)])\n\n# Displaying the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[x for x in range(1,7)])\ndisp.plot()\nplt.show()\n# Print the mean scores\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:36:16.418008Z","iopub.execute_input":"2024-04-24T20:36:16.418353Z","iopub.status.idle":"2024-04-24T20:36:16.87722Z","shell.execute_reply.started":"2024-04-24T20:36:16.418318Z","shell.execute_reply":"2024-04-24T20:36:16.876153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\nbest_kappa_score=mean_kappa_score\nfor i in range(5):\n    i_threshold=optimal_thresholds[i]-0.5\n    for j in range(1000):\n        thresholds=[x for x in optimal_thresholds]\n        thresholds[i]=i_threshold+j/1000\n        \n        classified_oof =classify_with_thresholds(oof_clip, thresholds)\n        threshold_kappa_score=cohen_kappa_score(y_split, classified_oof, weights='quadratic')\n        #print(i,thresholds,threshold_kappa_score)\n        if threshold_kappa_score>=best_kappa_score:\n            best_kappa_score=threshold_kappa_score\n            optimal_thresholds[i]=thresholds[i]\noptimal_thresholds","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:36:16.878516Z","iopub.execute_input":"2024-04-24T20:36:16.878854Z","iopub.status.idle":"2024-04-24T20:37:52.521601Z","shell.execute_reply.started":"2024-04-24T20:36:16.878826Z","shell.execute_reply":"2024-04-24T20:37:52.52035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classified_oof =classify_with_thresholds(oof_clip, optimal_thresholds)\n # Calculate and store the F1 score for this fold\nmean_f1_score = f1_score(y_split, classified_oof, average='weighted')\n\n# Calculate and store the Cohen's kappa score for this fold\nmean_kappa_score = cohen_kappa_score(y_split, classified_oof, weights='quadratic')\n\n# Calculating the confusion matrix\ncm = confusion_matrix(y_split, classified_oof, labels=[x for x in range(1,7)])\n\n# Displaying the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[x for x in range(1,7)])\ndisp.plot()\nplt.show()\n# Print the mean scores\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:52.523584Z","iopub.execute_input":"2024-04-24T20:37:52.523947Z","iopub.status.idle":"2024-04-24T20:37:52.985598Z","shell.execute_reply.started":"2024-04-24T20:37:52.523918Z","shell.execute_reply":"2024-04-24T20:37:52.984631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Inference**","metadata":{}},{"cell_type":"code","source":"# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# CountVectorizer\ntest_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# Meta Features(deberta-v3-large)\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:52.987361Z","iopub.execute_input":"2024-04-24T20:37:52.98779Z","iopub.status.idle":"2024-04-24T20:37:53.426557Z","shell.execute_reply.started":"2024-04-24T20:37:52.987751Z","shell.execute_reply":"2024-04-24T20:37:53.425399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = []\nfor i in range(len(models)):\n    feature_select = fse[i].sort_values(ascending=False).index.tolist()[:13000]\n    proba= models[i].predict(test_feats[feature_select])+ a\n    probabilities.append(proba)\n# Compute the average probabilities across all models\npredictions = np.mean(probabilities, axis=0)\n\n#predictions = predictions.clip(1, 6)\n\n\npredictions = np.round(predictions.clip(1, 6))\n#predictions =classify_with_thresholds(predictions, optimal_thresholds)\n# Print the predictions\nprint(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:53.428201Z","iopub.execute_input":"2024-04-24T20:37:53.429153Z","iopub.status.idle":"2024-04-24T20:37:54.424839Z","shell.execute_reply.started":"2024-04-24T20:37:53.429109Z","shell.execute_reply":"2024-04-24T20:37:54.423441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_vector=new_pipeline.transform(test['text'])\n#predictions=lgb_model.predict(test_vector)\n#predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:54.430794Z","iopub.execute_input":"2024-04-24T20:37:54.431191Z","iopub.status.idle":"2024-04-24T20:37:54.43539Z","shell.execute_reply.started":"2024-04-24T20:37:54.431156Z","shell.execute_reply":"2024-04-24T20:37:54.434265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions = model.predict(test['text'])\n#predictions\n# Initialize an empty list to store probabilities for each model\n\"\"\"\nprobabilities = []\n\n# Iterate over each model in the list of models\nfor model in models:\n    # Make predictions using the current model\n    print(model.predict(test['text']))\n    \n    # Compute decision scores for the current model\n    decision_scores = model.decision_function(test['text'])\n    \n    # Convert decision scores to probabilities using the sigmoid function\n    proba = 1 / (1 + np.exp(-decision_scores))\n    # Append the probabilities to the list\n    probabilities.append(proba)\n\n# Compute the average probabilities across all models\nprobabilities = np.mean(probabilities, axis=0)\n\n# Determine the predicted class by selecting the class with the highest probability\n# Add 1 to the index to match the class labels (assuming classes start from 1)\npredictions = np.argmax(probabilities, axis=1) + 1\n\n# Print the predictions\nprint(predictions)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:54.43662Z","iopub.execute_input":"2024-04-24T20:37:54.436965Z","iopub.status.idle":"2024-04-24T20:37:54.450277Z","shell.execute_reply.started":"2024-04-24T20:37:54.436935Z","shell.execute_reply":"2024-04-24T20:37:54.449015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsubmission['score']=predictions\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-24T20:37:54.451658Z","iopub.execute_input":"2024-04-24T20:37:54.452035Z","iopub.status.idle":"2024-04-24T20:37:54.477616Z","shell.execute_reply.started":"2024-04-24T20:37:54.452006Z","shell.execute_reply":"2024-04-24T20:37:54.476426Z"},"trusted":true},"execution_count":null,"outputs":[]}]}