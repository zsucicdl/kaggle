{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8029842,"sourceType":"datasetVersion","datasetId":4732809},{"sourceId":8042439,"sourceType":"datasetVersion","datasetId":4741731},{"sourceId":8046222,"sourceType":"datasetVersion","datasetId":4744550}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-08T13:59:19.810877Z","iopub.execute_input":"2024-04-08T13:59:19.811249Z","iopub.status.idle":"2024-04-08T13:59:19.831361Z","shell.execute_reply.started":"2024-04-08T13:59:19.811221Z","shell.execute_reply":"2024-04-08T13:59:19.830524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \n## Submission \nsub_df=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\ntrain_df=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\ntest_df=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:19.834458Z","iopub.execute_input":"2024-04-08T13:59:19.834977Z","iopub.status.idle":"2024-04-08T13:59:20.18552Z","shell.execute_reply.started":"2024-04-08T13:59:19.834947Z","shell.execute_reply":"2024-04-08T13:59:20.184677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.187058Z","iopub.execute_input":"2024-04-08T13:59:20.187383Z","iopub.status.idle":"2024-04-08T13:59:20.197317Z","shell.execute_reply.started":"2024-04-08T13:59:20.187357Z","shell.execute_reply":"2024-04-08T13:59:20.196326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.198537Z","iopub.execute_input":"2024-04-08T13:59:20.199085Z","iopub.status.idle":"2024-04-08T13:59:20.213252Z","shell.execute_reply.started":"2024-04-08T13:59:20.199056Z","shell.execute_reply":"2024-04-08T13:59:20.212374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.216112Z","iopub.execute_input":"2024-04-08T13:59:20.216362Z","iopub.status.idle":"2024-04-08T13:59:20.224093Z","shell.execute_reply.started":"2024-04-08T13:59:20.21634Z","shell.execute_reply":"2024-04-08T13:59:20.223177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.225103Z","iopub.execute_input":"2024-04-08T13:59:20.22542Z","iopub.status.idle":"2024-04-08T13:59:20.237533Z","shell.execute_reply.started":"2024-04-08T13:59:20.225389Z","shell.execute_reply":"2024-04-08T13:59:20.236691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.238605Z","iopub.execute_input":"2024-04-08T13:59:20.238875Z","iopub.status.idle":"2024-04-08T13:59:20.255067Z","shell.execute_reply.started":"2024-04-08T13:59:20.238852Z","shell.execute_reply":"2024-04-08T13:59:20.254114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(8, 6))\nsns.countplot(x='score', data=train_df)\nplt.title('Distribution of Essay Scores')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.256415Z","iopub.execute_input":"2024-04-08T13:59:20.256763Z","iopub.status.idle":"2024-04-08T13:59:20.510274Z","shell.execute_reply.started":"2024-04-08T13:59:20.256719Z","shell.execute_reply":"2024-04-08T13:59:20.509194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string\n# Explore the distribution of essay lengths in the training dataset\nPlot_train_df=train_df\n\nPlot_train_df['essay_length'] = Plot_train_df['full_text'].apply(lambda x: len(word_tokenize(x)))\nplt.figure(figsize=(8, 6))\nsns.histplot(Plot_train_df['essay_length'], bins=20, kde=True)\nplt.title('Distribution of Essay Lengths')\nplt.xlabel('Essay Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T13:59:20.511687Z","iopub.execute_input":"2024-04-08T13:59:20.512156Z","iopub.status.idle":"2024-04-08T14:00:39.064713Z","shell.execute_reply.started":"2024-04-08T13:59:20.512122Z","shell.execute_reply":"2024-04-08T14:00:39.063636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport gc\nimport torch\nimport re\nimport copy\nimport polars as pl\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm,trange\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.066294Z","iopub.execute_input":"2024-04-08T14:00:39.066602Z","iopub.status.idle":"2024-04-08T14:00:39.072696Z","shell.execute_reply.started":"2024-04-08T14:00:39.066576Z","shell.execute_reply":"2024-04-08T14:00:39.071586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Deberta Model","metadata":{}},{"cell_type":"code","source":"max_length = 1024\nmodel_path = '/kaggle/input/es-deberta-large-fold0'\neval_batch_size = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.076447Z","iopub.execute_input":"2024-04-08T14:00:39.076718Z","iopub.status.idle":"2024-04-08T14:00:39.085268Z","shell.execute_reply.started":"2024-04-08T14:00:39.076696Z","shell.execute_reply":"2024-04-08T14:00:39.084393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=max_length, truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.086278Z","iopub.execute_input":"2024-04-08T14:00:39.086878Z","iopub.status.idle":"2024-04-08T14:00:39.305511Z","shell.execute_reply.started":"2024-04-08T14:00:39.086853Z","shell.execute_reply":"2024-04-08T14:00:39.304661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_test_df = Dataset.from_pandas(test_df)\nupdated_test_df = updated_test_df.map(tokenize).remove_columns(['essay_id', 'full_text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.306659Z","iopub.execute_input":"2024-04-08T14:00:39.306982Z","iopub.status.idle":"2024-04-08T14:00:39.346689Z","shell.execute_reply.started":"2024-04-08T14:00:39.306956Z","shell.execute_reply":"2024-04-08T14:00:39.345844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_test_df","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.347924Z","iopub.execute_input":"2024-04-08T14:00:39.348287Z","iopub.status.idle":"2024-04-08T14:00:39.355575Z","shell.execute_reply.started":"2024-04-08T14:00:39.348255Z","shell.execute_reply":"2024-04-08T14:00:39.354586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_list = updated_test_df.column_names\nfeatures_list","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.357059Z","iopub.execute_input":"2024-04-08T14:00:39.357327Z","iopub.status.idle":"2024-04-08T14:00:39.366852Z","shell.execute_reply.started":"2024-04-08T14:00:39.357305Z","shell.execute_reply":"2024-04-08T14:00:39.365884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataCollator:\n    def __call__(self, features):\n        model_inputs = [{\n            'input_ids': feature['input_ids'],\n            'attention_mask': feature['attention_mask']\n        } for feature in features]\n        \n        batch = tokenizer.pad(\n            model_inputs,\n            padding=True,\n            max_length=max_length,\n            return_tensors='pt',\n            pad_to_multiple_of = 16\n        )\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.3681Z","iopub.execute_input":"2024-04-08T14:00:39.36842Z","iopub.status.idle":"2024-04-08T14:00:39.377196Z","shell.execute_reply.started":"2024-04-08T14:00:39.368391Z","shell.execute_reply":"2024-04-08T14:00:39.37648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ncollator=DataCollator()\n# Iterate over the dataset to get actual features\nmodel_inputs = [{\n    'input_ids': feature['input_ids'],\n    'attention_mask': feature['attention_mask']\n} for feature in updated_test_df]\n\n\nargs = TrainingArguments(\".\", per_device_eval_batch_size=eval_batch_size, report_to=\"none\")\ntrainer = Trainer(model=model, args=args, data_collator=collator, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:39.378232Z","iopub.execute_input":"2024-04-08T14:00:39.378552Z","iopub.status.idle":"2024-04-08T14:00:40.351359Z","shell.execute_reply.started":"2024-04-08T14:00:39.378523Z","shell.execute_reply":"2024-04-08T14:00:40.350556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = trainer.predict(updated_test_df).predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:40.353119Z","iopub.execute_input":"2024-04-08T14:00:40.353488Z","iopub.status.idle":"2024-04-08T14:00:41.035752Z","shell.execute_reply.started":"2024-04-08T14:00:40.353455Z","shell.execute_reply":"2024-04-08T14:00:41.03489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.036873Z","iopub.execute_input":"2024-04-08T14:00:41.037154Z","iopub.status.idle":"2024-04-08T14:00:41.043369Z","shell.execute_reply.started":"2024-04-08T14:00:41.03713Z","shell.execute_reply":"2024-04-08T14:00:41.042388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = predictions.argmax(-1) + 1\ntest_df['score'] = preds\ntest_df[['essay_id', 'score']].to_csv('submission_1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.044972Z","iopub.execute_input":"2024-04-08T14:00:41.045375Z","iopub.status.idle":"2024-04-08T14:00:41.056216Z","shell.execute_reply.started":"2024-04-08T14:00:41.045342Z","shell.execute_reply":"2024-04-08T14:00:41.05538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.0575Z","iopub.execute_input":"2024-04-08T14:00:41.058474Z","iopub.status.idle":"2024-04-08T14:00:41.072625Z","shell.execute_reply.started":"2024-04-08T14:00:41.058438Z","shell.execute_reply":"2024-04-08T14:00:41.071721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Submission 2 \n","metadata":{}},{"cell_type":"markdown","source":"For our second submission, we employed a baseline model using Term Frequency-Inverse Document Frequency (Tfidf) vectorization coupled with LightGBM (LGBM), a gradient boosting framework. Tfidf vectorization allows us to represent each essay in the dataset as a vector based on the frequency of words and their importance in distinguishing between essays. LGBM, known for its efficiency and effectiveness in handling large datasets, was employed to train a machine learning model on the Tfidf vectors for predicting essay scores.","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd \ntrain=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.073897Z","iopub.execute_input":"2024-04-08T14:00:41.074203Z","iopub.status.idle":"2024-04-08T14:00:41.410369Z","shell.execute_reply.started":"2024-04-08T14:00:41.074178Z","shell.execute_reply":"2024-04-08T14:00:41.409608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.41154Z","iopub.execute_input":"2024-04-08T14:00:41.411845Z","iopub.status.idle":"2024-04-08T14:00:41.421009Z","shell.execute_reply.started":"2024-04-08T14:00:41.411822Z","shell.execute_reply":"2024-04-08T14:00:41.420156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [  \n    # paragraph\n    (pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\ntrain = pl.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\").with_columns(columns)\ntest = pl.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\").with_columns(columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.422277Z","iopub.execute_input":"2024-04-08T14:00:41.422639Z","iopub.status.idle":"2024-04-08T14:00:41.588273Z","shell.execute_reply.started":"2024-04-08T14:00:41.422605Z","shell.execute_reply":"2024-04-08T14:00:41.587267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.589367Z","iopub.execute_input":"2024-04-08T14:00:41.589636Z","iopub.status.idle":"2024-04-08T14:00:41.596675Z","shell.execute_reply.started":"2024-04-08T14:00:41.589613Z","shell.execute_reply":"2024-04-08T14:00:41.595794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.597968Z","iopub.execute_input":"2024-04-08T14:00:41.598305Z","iopub.status.idle":"2024-04-08T14:00:41.607408Z","shell.execute_reply.started":"2024-04-08T14:00:41.598276Z","shell.execute_reply":"2024-04-08T14:00:41.606547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Paragraph Preprocess**\nIn this context, \"Paragraph Preprocess\" refers to the process of preparing each paragraph within the text data for further analysis or feature extraction.","metadata":{}},{"cell_type":"code","source":"def Paragraph_Preprocess(tmp):\n    \n    tmp = tmp.explode('paragraph')\n    # preprocess\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # paragraph_len\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # paragraph_sentence_cnt/paragraph_word_cnt\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # paragraph_len_cnt\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\nprint('feature_num: ',len(train_feats.columns)-2)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:41.608864Z","iopub.execute_input":"2024-04-08T14:00:41.609132Z","iopub.status.idle":"2024-04-08T14:00:48.80649Z","shell.execute_reply.started":"2024-04-08T14:00:41.60911Z","shell.execute_reply":"2024-04-08T14:00:48.805554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Sentence Preprocess**","metadata":{}},{"cell_type":"markdown","source":"It breaks text into tokens, applies text cleaning techniques like removing punctuation and stopwords, and calculates sentence statistics such as length and vocabulary richness. Finally, it extracts features for analysis, facilitating tasks like sentiment analysis and text classification","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    \n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # sentence_len\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # filter\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # sentence_word_cnt\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # sentence_cnt\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n# merge\ntmp = Sentence_Preprocess(train)\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\nprint('feature_num: ',len(train_feats.columns)-2)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:48.807947Z","iopub.execute_input":"2024-04-08T14:00:48.808314Z","iopub.status.idle":"2024-04-08T14:00:55.752109Z","shell.execute_reply.started":"2024-04-08T14:00:48.808281Z","shell.execute_reply":"2024-04-08T14:00:55.751188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Word Preprocessing:**\nIt tokenizes words, removes punctuation and stopwords, and performs lemmatization or stemming to normalize the text. Additionally, it calculates word frequency and other statistical measures to aid in feature extraction and analysis. This preprocessing step helps improve the performance of various NLP tasks such as text classification, sentiment analysis, and information retrieval.\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # word_len\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # filter\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # word_cnt\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n# merge\ntmp = Word_Preprocess(train)\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\nprint('feature_num: ',len(train_feats.columns)-2)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:00:55.758072Z","iopub.execute_input":"2024-04-08T14:00:55.758404Z","iopub.status.idle":"2024-04-08T14:01:09.04109Z","shell.execute_reply.started":"2024-04-08T14:00:55.758376Z","shell.execute_reply":"2024-04-08T14:01:09.04009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Tfidf feature:**\n TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. It calculates a weight for each word in the document based on its frequency (TF) and inverse document frequency (IDF), where rare words that appear in fewer documents receive higher weights. This representation is commonly used in text mining, information retrieval, and natural language processing tasks to capture the significance of words in a document corpus.\n ","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# merge\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\nprint('feature_num: ',len(train_feats.columns)-2)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:01:09.042186Z","iopub.execute_input":"2024-04-08T14:01:09.042489Z","iopub.status.idle":"2024-04-08T14:02:32.345364Z","shell.execute_reply.started":"2024-04-08T14:01:09.042462Z","shell.execute_reply":"2024-04-08T14:02:32.344367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = [col for col in train_feats.columns if col not in ['essay_id', 'score']]\nprint('Number of features:', len(feature_names))\ntrain_feats.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.346576Z","iopub.execute_input":"2024-04-08T14:02:32.346877Z","iopub.status.idle":"2024-04-08T14:02:32.376384Z","shell.execute_reply.started":"2024-04-08T14:02:32.346851Z","shell.execute_reply":"2024-04-08T14:02:32.375404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nfor i in range(5):\n    models.append(lgb.Booster(model_file=f'/kaggle/input/lal-lgb-baseline-2/fold_{i}.txt'))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.377921Z","iopub.execute_input":"2024-04-08T14:02:32.378231Z","iopub.status.idle":"2024-04-08T14:02:32.452081Z","shell.execute_reply.started":"2024-04-08T14:02:32.378204Z","shell.execute_reply":"2024-04-08T14:02:32.45122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.453432Z","iopub.execute_input":"2024-04-08T14:02:32.4538Z","iopub.status.idle":"2024-04-08T14:02:32.606975Z","shell.execute_reply.started":"2024-04-08T14:02:32.453767Z","shell.execute_reply":"2024-04-08T14:02:32.606152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.608142Z","iopub.execute_input":"2024-04-08T14:02:32.608436Z","iopub.status.idle":"2024-04-08T14:02:32.633303Z","shell.execute_reply.started":"2024-04-08T14:02:32.60841Z","shell.execute_reply":"2024-04-08T14:02:32.632154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = test_feats[['essay_id']].copy()\nprediction['score'] = 0\npred_test = models[0].predict(test_feats[feature_names])\nfor i in range(4):\n    pred_now = models[i+1].predict(test_feats[feature_names])\n    pred_test = np.add(pred_test,pred_now)\npred_test = pred_test/5\n\npred_test = pred_test.clip(1, 6).round()\nprediction['score'] = pred_test\nprediction.to_csv('submission_2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.63446Z","iopub.execute_input":"2024-04-08T14:02:32.634819Z","iopub.status.idle":"2024-04-08T14:02:32.7161Z","shell.execute_reply.started":"2024-04-08T14:02:32.634789Z","shell.execute_reply":"2024-04-08T14:02:32.715327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.717261Z","iopub.execute_input":"2024-04-08T14:02:32.717623Z","iopub.status.idle":"2024-04-08T14:02:32.727127Z","shell.execute_reply.started":"2024-04-08T14:02:32.71759Z","shell.execute_reply":"2024-04-08T14:02:32.726216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Submission:**","metadata":{}},{"cell_type":"code","source":"dataset1 = pd.read_csv('/kaggle/working/submission_1.csv')\ndataset2 = pd.read_csv('/kaggle/working/submission_2.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.728469Z","iopub.execute_input":"2024-04-08T14:02:32.729305Z","iopub.status.idle":"2024-04-08T14:02:32.739879Z","shell.execute_reply.started":"2024-04-08T14:02:32.72928Z","shell.execute_reply":"2024-04-08T14:02:32.738604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.741003Z","iopub.execute_input":"2024-04-08T14:02:32.741415Z","iopub.status.idle":"2024-04-08T14:02:32.75447Z","shell.execute_reply.started":"2024-04-08T14:02:32.741385Z","shell.execute_reply":"2024-04-08T14:02:32.753586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset2.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.755561Z","iopub.execute_input":"2024-04-08T14:02:32.757223Z","iopub.status.idle":"2024-04-08T14:02:32.770382Z","shell.execute_reply.started":"2024-04-08T14:02:32.757196Z","shell.execute_reply":"2024-04-08T14:02:32.769367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.merge(left=dataset1, right=dataset2, on='essay_id', suffixes=('_x', '_y'))\nmerged_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.772107Z","iopub.execute_input":"2024-04-08T14:02:32.772516Z","iopub.status.idle":"2024-04-08T14:02:32.787605Z","shell.execute_reply.started":"2024-04-08T14:02:32.772482Z","shell.execute_reply":"2024-04-08T14:02:32.78682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df['score'] = ((merged_df['score_x'] + merged_df['score_y']) / 2).round().astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.789002Z","iopub.execute_input":"2024-04-08T14:02:32.789302Z","iopub.status.idle":"2024-04-08T14:02:32.799244Z","shell.execute_reply.started":"2024-04-08T14:02:32.789278Z","shell.execute_reply":"2024-04-08T14:02:32.798298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.800269Z","iopub.execute_input":"2024-04-08T14:02:32.80065Z","iopub.status.idle":"2024-04-08T14:02:32.814527Z","shell.execute_reply.started":"2024-04-08T14:02:32.800618Z","shell.execute_reply":"2024-04-08T14:02:32.813603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the desired columns to a new csv file\nmerged_df[['essay_id', 'score']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.815715Z","iopub.execute_input":"2024-04-08T14:02:32.816347Z","iopub.status.idle":"2024-04-08T14:02:32.825164Z","shell.execute_reply.started":"2024-04-08T14:02:32.816314Z","shell.execute_reply":"2024-04-08T14:02:32.82424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_string = merged_df[['essay_id', 'score']].to_csv(index=False)\ncsv_lines = csv_string.split('\\n')\nfor line in csv_lines[:5]:\n    print(line)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.82624Z","iopub.execute_input":"2024-04-08T14:02:32.826486Z","iopub.status.idle":"2024-04-08T14:02:32.837458Z","shell.execute_reply.started":"2024-04-08T14:02:32.826465Z","shell.execute_reply":"2024-04-08T14:02:32.836504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:02:32.838666Z","iopub.execute_input":"2024-04-08T14:02:32.839014Z","iopub.status.idle":"2024-04-08T14:02:32.854395Z","shell.execute_reply.started":"2024-04-08T14:02:32.838989Z","shell.execute_reply":"2024-04-08T14:02:32.853379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}