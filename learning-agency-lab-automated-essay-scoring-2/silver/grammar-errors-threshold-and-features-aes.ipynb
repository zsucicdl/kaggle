{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8464773,
     "sourceType": "datasetVersion",
     "datasetId": 5046453
    }
   ],
   "dockerImageVersionId": 30698,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# About\n\n----------------\n\nIn my [last post](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/503453), I talked about how we shouldn't forget the Holistic Scoring Rubric document. An important part of my post (I find) is the following:\n\n> - **3**:\n    - In general: demonstrates developing mastery.\n    - Has **one or more of the following weaknesses** (this is different to scores 4, 5 and 6 - it is explicitly announced as \"one or more weaknesses\").\n        * Point of view exists, but it's hardly critical or without proper examples, reasons or evidence from the source text.\n        * Lapses in coherence and not properly organized.\n        * Weak coherence and unsmooth development of ideas.\n        * OK-ish use of language using sometimes weak vocab or inappropriate words, lacks variety.\n        * Substantial grammar errors and spelling errors.\n\nkeyphrase being: *\"Has **one or more of the following weaknesses**\"* \n\n## Experiment\n\n----------------\n\nHence I did some experimenting. Maybe I could find the threshold of grammar and spelling errors that determines when an essay musn't be scored more than a 3, as then it would have one of those weaknesses that doesn't allow it to be scored better. \n\nI used the [grammarly model](https://huggingface.co/grammarly/coedit-large) like so to detect those mistakes, sentence by sentence foreach essay, which resulted in about 350k sentences:\n\n```\ntokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\").to(device)\n\ninput_text = 'Fix grammatical errors in this sentence: When I grow up, I starti to understand what he said is quite right.'\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids, max_length=256)\nedited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(edited_text)\n# When I grow up, I start to understand what he said is quite right.\n```\n\nI then calculated the differences between the grammar corrected senteces and the original through the Levenshtein distance, normalized the value over the df and plotted it:\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F17421843%2F8841d7ffd7a6ce3a821b0463c11e3339%2Fgrammar_threshold.png?generation=1716191274537630&alt=media)\n\nit can be seen that **a) scores nicely depend on the grammar errors** and b) **there exists a threshold with the green line** (also observer how there are no outliers above score 3).\n\n---------------\n\n## This Notebook\n\nThis notebook is just for documentation sake so you can retrace what I did and maybe find errors in it. You can also play with the grammarly corrected texts yourself and maybe find features of your own.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport spacy\nimport re\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom difflib import SequenceMatcher\n\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:10.548675Z",
     "iopub.execute_input": "2024-05-22T06:49:10.549183Z",
     "iopub.status.idle": "2024-05-22T06:49:20.803315Z",
     "shell.execute_reply.started": "2024-05-22T06:49:10.549133Z",
     "shell.execute_reply": "2024-05-22T06:49:20.802186Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model\n\nCreate and test the model instance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-large\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-large\").to(device)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:20.805231Z",
     "iopub.execute_input": "2024-05-22T06:49:20.805789Z",
     "iopub.status.idle": "2024-05-22T06:49:46.650959Z",
     "shell.execute_reply.started": "2024-05-22T06:49:20.80576Z",
     "shell.execute_reply": "2024-05-22T06:49:46.649801Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "input_text = 'Fix grammatical errors in this sentence: When I grow up, I starti to understand what he said is quite right.'\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids, max_length=256)\nedited_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(edited_text)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:46.652605Z",
     "iopub.execute_input": "2024-05-22T06:49:46.653007Z",
     "iopub.status.idle": "2024-05-22T06:49:49.818997Z",
     "shell.execute_reply.started": "2024-05-22T06:49:46.652914Z",
     "shell.execute_reply": "2024-05-22T06:49:49.817824Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Work",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv').head(1)\ndisplay(train_df.head())\nprint(len(train_df))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:49.820675Z",
     "iopub.execute_input": "2024-05-22T06:49:49.821845Z",
     "iopub.status.idle": "2024-05-22T06:49:50.676428Z",
     "shell.execute_reply.started": "2024-05-22T06:49:49.821802Z",
     "shell.execute_reply": "2024-05-22T06:49:50.672779Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "nlp = spacy.load('en_core_web_sm')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:50.679248Z",
     "iopub.execute_input": "2024-05-22T06:49:50.679619Z",
     "iopub.status.idle": "2024-05-22T06:49:55.247655Z",
     "shell.execute_reply.started": "2024-05-22T06:49:50.67959Z",
     "shell.execute_reply": "2024-05-22T06:49:55.246416Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def remove_html(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef preprocess_text(x):\n    # x = x.lower()\n    x = remove_html(x)\n    x = re.sub(\"@\\w+\", '',x)\n    #x = re.sub(\"'\\d+\", '',x)\n    # x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    #x = re.sub(r\"\\s+\", \" \", x)\n    #x = re.sub(r\"\\.+\", \".\", x)\n    #x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x\n\ndef extract_sentences(text):\n    return [sent.text for sent in nlp(text).sents]\n\ndef extract_pos(text):\n    doc = nlp(text)\n    pos = []\n    for token in doc:\n        pos.append(token.pos_)\n    return ' '.join(pos)\n\ndef extract_grammar_correction(text):\n    input_text = f'Fix grammatical errors in this sentence: {text}'\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n    outputs = model.generate(input_ids, max_length=len(input_ids[0]) + 30)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef levenshtein_distance(token1, token2):\n    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n\n    for t1 in range(len(token1) + 1):\n        distances[t1][0] = t1\n\n    for t2 in range(len(token2) + 1):\n        distances[0][t2] = t2\n        \n    a = 0\n    b = 0\n    c = 0\n    \n    for t1 in range(1, len(token1) + 1):\n        for t2 in range(1, len(token2) + 1):\n            if (token1[t1-1] == token2[t2-1]):\n                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n            else:\n                a = distances[t1][t2 - 1]\n                b = distances[t1 - 1][t2]\n                c = distances[t1 - 1][t2 - 1]\n                \n                if (a <= b and a <= c):\n                    distances[t1][t2] = a + 1\n                elif (b <= a and b <= c):\n                    distances[t1][t2] = b + 1\n                else:\n                    distances[t1][t2] = c + 1\n    return distances[len(token1)][len(token2)]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:55.249592Z",
     "iopub.execute_input": "2024-05-22T06:49:55.250094Z",
     "iopub.status.idle": "2024-05-22T06:49:55.269812Z",
     "shell.execute_reply.started": "2024-05-22T06:49:55.250049Z",
     "shell.execute_reply": "2024-05-22T06:49:55.26863Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_df['sentences'] = train_df['full_text'].apply(lambda f: extract_sentences(preprocess_text(f)))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:55.27098Z",
     "iopub.execute_input": "2024-05-22T06:49:55.271898Z",
     "iopub.status.idle": "2024-05-22T06:49:57.254066Z",
     "shell.execute_reply.started": "2024-05-22T06:49:55.271865Z",
     "shell.execute_reply": "2024-05-22T06:49:57.252911Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "rows = []\ncounter = 0\n\nfor index, row in train_df.iterrows():\n    if(counter % 150 == 0):\n        print(\"Done with: \" + str(counter))\n\n    sentences = row['sentences']\n    for k in range(0, len(sentences)):\n        rows.append({\n            'essay_id': row['essay_id'],\n            'sentence': sentences[k],\n        })\n    counter += 1\n        \ntrain_df_sentences = pd.DataFrame(rows)\ndisplay(train_df_sentences.head())\nprint(len(train_df_sentences))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:57.255591Z",
     "iopub.execute_input": "2024-05-22T06:49:57.255973Z",
     "iopub.status.idle": "2024-05-22T06:49:57.272869Z",
     "shell.execute_reply.started": "2024-05-22T06:49:57.255943Z",
     "shell.execute_reply": "2024-05-22T06:49:57.271591Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_df_sentences['grammar'] = train_df_sentences['sentence'].apply(extract_grammar_correction)\ndisplay(train_df_sentences.head())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:49:57.274495Z",
     "iopub.execute_input": "2024-05-22T06:49:57.275161Z",
     "iopub.status.idle": "2024-05-22T06:51:28.162503Z",
     "shell.execute_reply.started": "2024-05-22T06:49:57.275116Z",
     "shell.execute_reply": "2024-05-22T06:51:28.159078Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**LV distance**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_df_sentences['lv_dist'] = np.vectorize(levenshtein_distance)(train_df_sentences['sentence'], train_df_sentences['grammar'])\ndisplay(train_df_sentences.head()) ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:28.168309Z",
     "iopub.execute_input": "2024-05-22T06:51:28.171421Z",
     "iopub.status.idle": "2024-05-22T06:51:30.215066Z",
     "shell.execute_reply.started": "2024-05-22T06:51:28.171318Z",
     "shell.execute_reply": "2024-05-22T06:51:30.21254Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# EDA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_df_grammarly = pd.read_json('/kaggle/input/train-grammarly-aes/train_grammarly.json')\ntrain_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:30.231387Z",
     "iopub.execute_input": "2024-05-22T06:51:30.232117Z",
     "iopub.status.idle": "2024-05-22T06:51:34.823745Z",
     "shell.execute_reply.started": "2024-05-22T06:51:30.232081Z",
     "shell.execute_reply": "2024-05-22T06:51:34.822642Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_df_grammarly.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:34.825832Z",
     "iopub.execute_input": "2024-05-22T06:51:34.826648Z",
     "iopub.status.idle": "2024-05-22T06:51:34.840834Z",
     "shell.execute_reply.started": "2024-05-22T06:51:34.82661Z",
     "shell.execute_reply": "2024-05-22T06:51:34.839489Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "agg_funcs = {\n    'sentence': lambda x: ' '.join(x),\n    'grammar': lambda x: ' '.join(x),\n    'lv_dist': 'sum'\n}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:34.846746Z",
     "iopub.execute_input": "2024-05-22T06:51:34.84721Z",
     "iopub.status.idle": "2024-05-22T06:51:34.854175Z",
     "shell.execute_reply.started": "2024-05-22T06:51:34.847176Z",
     "shell.execute_reply": "2024-05-22T06:51:34.852986Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_df_grammarly = train_df_grammarly.groupby('essay_id').agg(agg_funcs).reset_index()\ntrain_df_grammarly.rename(columns={'sentence': 'full_text'}, inplace=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:34.856322Z",
     "iopub.execute_input": "2024-05-22T06:51:34.857093Z",
     "iopub.status.idle": "2024-05-22T06:51:36.27448Z",
     "shell.execute_reply.started": "2024-05-22T06:51:34.857048Z",
     "shell.execute_reply": "2024-05-22T06:51:36.273181Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_df = pd.merge(train_df_grammarly, train_df[['essay_id', 'score']], on='essay_id', how='left')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:36.275999Z",
     "iopub.execute_input": "2024-05-22T06:51:36.276399Z",
     "iopub.status.idle": "2024-05-22T06:51:36.314377Z",
     "shell.execute_reply.started": "2024-05-22T06:51:36.276367Z",
     "shell.execute_reply": "2024-05-22T06:51:36.313144Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Store it**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "final_df.to_csv('train_grammarly.csv', index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:36.316128Z",
     "iopub.execute_input": "2024-05-22T06:51:36.316524Z",
     "iopub.status.idle": "2024-05-22T06:51:39.830595Z",
     "shell.execute_reply.started": "2024-05-22T06:51:36.316492Z",
     "shell.execute_reply": "2024-05-22T06:51:39.82939Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_df = pd.read_csv('/kaggle/working/train_grammarly.csv')\ndisplay(final_df.head())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:39.831808Z",
     "iopub.execute_input": "2024-05-22T06:51:39.833936Z",
     "iopub.status.idle": "2024-05-22T06:51:40.692208Z",
     "shell.execute_reply.started": "2024-05-22T06:51:39.833887Z",
     "shell.execute_reply": "2024-05-22T06:51:40.690108Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Normalize lv_dist**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "final_df['normalized_lv_dist'] = final_df['lv_dist'] / final_df['full_text'].apply(lambda f: len(f))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:40.693742Z",
     "iopub.execute_input": "2024-05-22T06:51:40.694132Z",
     "iopub.status.idle": "2024-05-22T06:51:40.718657Z",
     "shell.execute_reply.started": "2024-05-22T06:51:40.694102Z",
     "shell.execute_reply": "2024-05-22T06:51:40.717389Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Count grammar errors raw**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def count_corrections(full_text, grammar):\n    matcher = SequenceMatcher(None, full_text, grammar)\n    corrections = sum(triple[-1] for triple in matcher.get_opcodes() if triple[0] != 'equal')\n    return corrections",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:40.720632Z",
     "iopub.execute_input": "2024-05-22T06:51:40.72111Z",
     "iopub.status.idle": "2024-05-22T06:51:40.73308Z",
     "shell.execute_reply.started": "2024-05-22T06:51:40.721069Z",
     "shell.execute_reply": "2024-05-22T06:51:40.731832Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_df['corrections'] = final_df.apply(lambda row: count_corrections(row['full_text'], row['grammar']), axis=1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-22T06:51:40.735004Z",
     "iopub.execute_input": "2024-05-22T06:51:40.735416Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_df['corrections'] = final_df['full_text'].apply(lambda f: len(f)) / final_df['corrections']",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "display(final_df.head())",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Plotting**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(12, 6))\nsns.scatterplot(x='normalized_lv_dist', y='score', data=final_df)\nsns.regplot(x='normalized_lv_dist', y='score', data=final_df, scatter=False, color='red')\n\nplt.title('Levenshtein Distance vs Score')\nplt.xlabel('Levenshtein Distance (lv_dist)')\nplt.ylabel('Score')\nplt.ylim(0, 7)\nplt.show()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Where is the Grammar Treshhold?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "threshold = final_df[final_df['score'] > 3]['normalized_lv_dist'].max()\nprint(threshold)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(16, 6))\nsns.scatterplot(x='normalized_lv_dist', y='score', data=final_df)\n\n# Add vertical line at the threshold\nplt.axvline(x=threshold, color='green', linestyle='--', label=f'Threshold = {threshold:.2f}')\nplt.title('Normalized Error-Levenshtein Distance vs Score')\nplt.xlabel('Normalized Error-Levenshtein Distance')\nplt.ylabel('Score')\nplt.ylim(0, 7)\nplt.legend()\nplt.show()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
