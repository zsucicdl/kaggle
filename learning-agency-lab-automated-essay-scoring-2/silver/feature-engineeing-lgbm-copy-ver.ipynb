{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 2220985,
     "sourceType": "datasetVersion",
     "datasetId": 1332972
    },
    {
     "sourceId": 6258399,
     "sourceType": "datasetVersion",
     "datasetId": 3596984
    },
    {
     "sourceId": 8141507,
     "sourceType": "datasetVersion",
     "datasetId": 4813598
    },
    {
     "sourceId": 8166166,
     "sourceType": "datasetVersion",
     "datasetId": 4832208
    },
    {
     "sourceId": 8548775,
     "sourceType": "datasetVersion",
     "datasetId": 5108074
    },
    {
     "sourceId": 8614443,
     "sourceType": "datasetVersion",
     "datasetId": 5144702
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Summery: Featuring engineering + LGBM\nThis notebook is a modified from notebook provided by YE_AI [link](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=171264491). If you like my notebook remember to like YE_AI notebook also. \n\n***\n\n### Modification Done:\n1. Add sum, kurtosis, Quartile 1 and Quartile 3 to paragraph and sentence features.\n2. Add spelling mistake counting and Extra text processing.\n3. Add TFIDF vector.\n4. Add TFIDF word. \n\n***\n\n\n**Extra text processing**:\n> * Contraction Expension e.g. I'll --> i will, this is added as a text processing step.\n> * Punctuation removal is applied:\n    - When extra features are generation [**[Link](#extra-feature)**]\n> \n> **Note**: The <u>Extra text processing</u> is taken from the notebook [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817)\n\n***\n\n### References:\n* Paragraph, sentence & word based features [source](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799).\n* Spelling mistake counting [here](https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect).\n* Extra text processing [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817?scriptVersionId=173223907&cellId=11).\n* TFIDF vector [here](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=172203959&cellId=16).\n* TFIDF word [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Import modules",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-08T06:43:35.461217Z",
     "iopub.execute_input": "2024-06-08T06:43:35.461493Z",
     "iopub.status.idle": "2024-06-08T06:43:50.523582Z",
     "shell.execute_reply.started": "2024-06-08T06:43:35.461465Z",
     "shell.execute_reply": "2024-06-08T06:43:50.522302Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nimport torch\nfrom glob import glob\nfrom tqdm import tqdm\nfrom typing import List\nimport logging\nimport json, string\n\nfrom scipy.special import softmax\nimport gc\n\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom datasets import load_metric, disable_progress_bar\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom transformers import TrainingArguments, Trainer\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import log_evaluation, early_stopping\n\nimport joblib\n\n# logging setting \nimport warnings\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_distances\n\nwith open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\n\n# See stopwords at: http://mlg.ucd.ie/files/datasets/stopwords.txt?fbclid=IwAR2b3y3IJZ4DWdlbX5xhxlLOW5OW3UeBX8vkRTbBRSFiykQefzZttnkOrEA\nwith open('/kaggle/input/stop-words/stopwords.txt', 'r') as file:\n    stopwords_list = [word.strip().lower() for word in file]\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:43:50.526412Z",
     "iopub.execute_input": "2024-06-08T06:43:50.526733Z",
     "iopub.status.idle": "2024-06-08T06:43:52.152741Z",
     "shell.execute_reply.started": "2024-06-08T06:43:50.526695Z",
     "shell.execute_reply": "2024-06-08T06:43:52.151593Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(len(stopwords.words('english')))\nprint(len(stopwords_list))\nfinal_stopwords_list = list(set(stopwords.words('english')) | set(stopwords_list))\nprint(len(final_stopwords_list))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:43:52.154063Z",
     "iopub.execute_input": "2024-06-08T06:43:52.154299Z",
     "iopub.status.idle": "2024-06-08T06:43:52.162871Z",
     "shell.execute_reply.started": "2024-06-08T06:43:52.15427Z",
     "shell.execute_reply": "2024-06-08T06:43:52.161629Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:43:52.165318Z",
     "iopub.execute_input": "2024-06-08T06:43:52.165578Z",
     "iopub.status.idle": "2024-06-08T06:44:40.819161Z",
     "shell.execute_reply.started": "2024-06-08T06:43:52.165536Z",
     "shell.execute_reply": "2024-06-08T06:44:40.818018Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **Initial configuration:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\ntest_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\nsub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:40.821359Z",
     "iopub.execute_input": "2024-06-08T06:44:40.821762Z",
     "iopub.status.idle": "2024-06-08T06:44:40.828808Z",
     "shell.execute_reply.started": "2024-06-08T06:44:40.821707Z",
     "shell.execute_reply": "2024-06-08T06:44:40.827523Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "n_splits = 5\nseed = 42",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:40.830338Z",
     "iopub.execute_input": "2024-06-08T06:44:40.830575Z",
     "iopub.status.idle": "2024-06-08T06:44:40.848994Z",
     "shell.execute_reply.started": "2024-06-08T06:44:40.830549Z",
     "shell.execute_reply": "2024-06-08T06:44:40.847861Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **2. Load dataset:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv(train_path)\ntrain.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:40.850699Z",
     "iopub.execute_input": "2024-06-08T06:44:40.851024Z",
     "iopub.status.idle": "2024-06-08T06:44:41.333822Z",
     "shell.execute_reply.started": "2024-06-08T06:44:40.85098Z",
     "shell.execute_reply": "2024-06-08T06:44:41.332655Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv(test_path)\ntest.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:41.335383Z",
     "iopub.execute_input": "2024-06-08T06:44:41.335671Z",
     "iopub.status.idle": "2024-06-08T06:44:41.349905Z",
     "shell.execute_reply.started": "2024-06-08T06:44:41.335606Z",
     "shell.execute_reply": "2024-06-08T06:44:41.348552Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.1 Data preprocessing functions definations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef dataPreprocessing(x):\n    \n    x = x.lower() # Convert words to lowercase\n    x = removeHTML(x) # Remove HTML\n    x = re.sub(\"@\\w+\", '',x)     # Delete strings starting with @\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"'\\d+\", '',x) # Delete Numbers\n    x = re.sub(\"http\\w+\", '',x) # Delete URL\n    x = x.replace(u'\\xa0',' ') # Remove \\xa0\n    x = re.sub(r'_+', ' ', x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    \n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r\"\\s+\", \" \", x) # Replace consecutive empty spaces with a single space character\n    x = x.strip()\n    return x\n\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    text =  text.translate(translator)\n    text = re.sub(r\"\\s+\", ' ', text)\n    return text\n\ndef remove_stop_words (text):\n    tokens = nltk.word_tokenize(text)\n    stop_words = set(final_stopwords_list)\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\ndef lemmatization(text):\n    \n    words = nltk.word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if len(word) > 1]\n    \n    return ' '.join(lemmatized_words)\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    \n    x = x.lower() # Convert words to lowercase\n   \n    x = removeHTML(x)  # Remove HTML\n    \n    x = expandContractions(x)\n    \n    x = remove_stop_words(x) # Remove stopwords\n    \n    x = re.sub(\"@\\w+\", '',x) # Delete strings starting with @\n    x = x.replace(u'\\xa0',' ') # Remove \\xa0\n    x = re.sub(\"'\\d+\", '',x) # Delete Numbers\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(r'_+', ' ', x)\n    x = re.sub(\"http\\w+\", '',x)     # Delete URL\n    \n    x = remove_punctuation(x) # Remove punctuation\n    \n    x = re.sub(r\"\\s+\", \" \", x) # Replace consecutive empty spaces with a single space character\n    x = lemmatization(x) # Lemmatizing\n    x = x.strip()\n    return x\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:41.35228Z",
     "iopub.execute_input": "2024-06-08T06:44:41.352601Z",
     "iopub.status.idle": "2024-06-08T06:44:41.417056Z",
     "shell.execute_reply.started": "2024-06-08T06:44:41.352563Z",
     "shell.execute_reply": "2024-06-08T06:44:41.415706Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 Paragraph based feature\n\n<a id='paragraph-feature'></a>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def remove_duplicates(text):\n    sentences = text.split('. ')\n    \n    # Use an OrderedDict to remove duplicates while preserving order\n    from collections import OrderedDict\n    unique_sentences = list(OrderedDict.fromkeys(sentences))\n    \n    # Join the unique sentences back into a single string\n    result = '. '.join(unique_sentences)\n    \n    # Ensure the final sentence ends with a period if it originally did\n    if text.endswith('.'):\n        result += '.'\n    \n    return result\n\n\ndef extract_sentences(text):\n    # Use a regular expression to split the text into sentences\n    # This will handle periods, exclamation marks, and question marks as sentence terminators\n    sentences = re.split(r'[.!?]+\\s*', text)\n    # Remove any empty sentences\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return sentences\n\ndef extract_words(text):\n    words = []\n    for word in re.findall(r\"\\w+(?:[']\\w+)*\", text):\n        if word.count(\"'\") > 2:\n            split_words = word.split(\"'\")\n            words.extend(split_words)\n        else:\n            words.append(word)\n    return words\n\n\ndef preprocessing_for_paragraphs(text):\n    # If before /n/n is not a mark, this is not the end of a paragraph    \n    text = re.sub(r'(?<![\\.\\!\\?])\\n\\n', ' ', text)\n\n    #If after \\n\\n is an normal case, replace with space\n    text = re.sub(r'\\n\\n([a-z])', ' ', text)\n    \n    return text.strip()\n\ndef extract_paragraphs(text):\n    processed_text = preprocessing_for_paragraphs(text)\n    paragraphs = processed_text.split('\\n\\n')\n    \n    return paragraphs\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:41.422348Z",
     "iopub.execute_input": "2024-06-08T06:44:41.422725Z",
     "iopub.status.idle": "2024-06-08T06:44:41.436924Z",
     "shell.execute_reply.started": "2024-06-08T06:44:41.422683Z",
     "shell.execute_reply": "2024-06-08T06:44:41.435682Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# test = pl.from_pandas(test).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\n# test = test.with_columns(columns)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:41.438427Z",
     "iopub.execute_input": "2024-06-08T06:44:41.438745Z",
     "iopub.status.idle": "2024-06-08T06:44:41.454351Z",
     "shell.execute_reply.started": "2024-06-08T06:44:41.438704Z",
     "shell.execute_reply": "2024-06-08T06:44:41.453242Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "columns = [(pl.col(\"full_text\").apply(extract_paragraphs).alias(\"paragraph\"))]\ntrain = pl.from_pandas(train).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\ntest = pl.from_pandas(test).with_columns([pl.col(\"full_text\").apply(remove_duplicates)])\n\ntrain = train.with_columns(columns)\ntest = test.with_columns(columns)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:41.455881Z",
     "iopub.execute_input": "2024-06-08T06:44:41.456195Z",
     "iopub.status.idle": "2024-06-08T06:44:45.587685Z",
     "shell.execute_reply.started": "2024-06-08T06:44:41.456157Z",
     "shell.execute_reply": "2024-06-08T06:44:45.586148Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    \n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    \n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    \n    tmp = tmp.with_columns(\n        pl.col('paragraph').map_elements(lambda x: len(extract_sentences(x))).alias('paragraph_sentence_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(extract_words(x))).alias('paragraph_word_cnt'),\n        pl.col('paragraph').map_elements(lambda x: len(set(extract_words(x)))).alias('paragraph_unique_word_cnt')\n    )\n    return tmp\n\n\nlength_ranges = [(1, 100), (101, 200), (201, 300), (301, 400), (401, 500), (501, 600), (601, 800)]\n\n# feature_engineering\nparagraph_fea = ['paragraph_len', 'paragraph_sentence_cnt', 'paragraph_word_cnt', 'paragraph_unique_word_cnt']\n\ndef Paragraph_Eng(train_tmp):\n    \n    count_aggs = [\n        pl.col('paragraph').filter((pl.col('paragraph_len') >= start) & (pl.col('paragraph_len') <= end)).count().alias(f\"paragraph_len_between_{start}_{end}_cnt\")\n        for start, end in length_ranges\n    ]\n\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_len_geq_{i}_cnt\") for i in [100,150,200,300,350,400,500,600,700] ], \n\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:45.589728Z",
     "iopub.execute_input": "2024-06-08T06:44:45.590151Z",
     "iopub.status.idle": "2024-06-08T06:44:45.607501Z",
     "shell.execute_reply.started": "2024-06-08T06:44:45.590108Z",
     "shell.execute_reply": "2024-06-08T06:44:45.606262Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\n\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n\nprint('Features Number: ',len(feature_names))\nprint('Features:', train_feats.columns)\ntrain_feats.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:44:45.609053Z",
     "iopub.execute_input": "2024-06-08T06:44:45.609305Z",
     "iopub.status.idle": "2024-06-08T06:45:23.06989Z",
     "shell.execute_reply.started": "2024-06-08T06:44:45.609272Z",
     "shell.execute_reply": "2024-06-08T06:45:23.068662Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.3 Sentence based features\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns( pl.col('full_text').map_elements(dataPreprocessing).map_elements(extract_sentences).alias(\"sentences\"))\n    tmp = tmp.explode('sentences')\n    \n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentences').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    \n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentences').map_elements(lambda x: len(extract_words(x))).alias(\"sentence_word_cnt\"))\n    tmp = tmp.with_columns(pl.col('sentences').map_elements(lambda x: len(set(extract_words(x)))).alias(\"sentence_unique_word_cnt\"))\n    \n    return tmp\nsentence_length_ranges = [(1, 50), (51, 100), (101, 150), (151, 300)]\n\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt', 'sentence_unique_word_cnt']\ndef Sentence_Eng(train_tmp):\n    \n    count_aggs = [\n        pl.col('sentences').filter((pl.col('sentence_len') >= start) & (pl.col('sentence_len') <= end)).count().alias(f\"sentence_len_between_{start}_{end}_cnt\")\n        for start, end in sentence_length_ranges\n    ]\n    \n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentences').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_len_geq_{i}_cnt\") for i in [50,100,150,300] ], \n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n    ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    \n    df = df.to_pandas()\n    \n    return df\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:45:23.071354Z",
     "iopub.execute_input": "2024-06-08T06:45:23.071731Z",
     "iopub.status.idle": "2024-06-08T06:45:23.086288Z",
     "shell.execute_reply.started": "2024-06-08T06:45:23.071699Z",
     "shell.execute_reply": "2024-06-08T06:45:23.085131Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tmp = Sentence_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:45:23.088091Z",
     "iopub.execute_input": "2024-06-08T06:45:23.088373Z",
     "iopub.status.idle": "2024-06-08T06:46:00.81548Z",
     "shell.execute_reply.started": "2024-06-08T06:45:23.088323Z",
     "shell.execute_reply": "2024-06-08T06:46:00.814038Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.4 Word based feature\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).map_elements(extract_words).alias('word'))\n    tmp = tmp.explode('word')\n    \n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    \n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n\nword_length_ranges = [(1, 5), (6, 10), (11, 15)]\n# feature_eng\ndef Word_Eng(train_tmp):\n    \n    count_aggs = [\n        pl.col('word').filter((pl.col('word_len') >= start) & (pl.col('word_len') <= end)).count().alias(f\"word_len_between_{start}_{end}_cnt\")\n        for start, end in word_length_ranges\n    ]\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_len_geq_{i+1}_cnt\") for i in range(15) ], \n        # other\n        \n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').sum().alias(f\"word_len_sum\"), \n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(count_aggs + aggs).sort(\"essay_id\")\n    \n    df = df.to_pandas()\n    \n    return df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:46:00.817219Z",
     "iopub.execute_input": "2024-06-08T06:46:00.817488Z",
     "iopub.status.idle": "2024-06-08T06:46:00.831109Z",
     "shell.execute_reply.started": "2024-06-08T06:46:00.817456Z",
     "shell.execute_reply": "2024-06-08T06:46:00.829938Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tmp = Word_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:46:00.832712Z",
     "iopub.execute_input": "2024-06-08T06:46:00.832968Z",
     "iopub.status.idle": "2024-06-08T06:46:35.71265Z",
     "shell.execute_reply.started": "2024-06-08T06:46:00.832939Z",
     "shell.execute_reply": "2024-06-08T06:46:35.711567Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5 Character TFIDF feature:\n\n##### Note:\n* **tokenizer=lambda x: x**: \"`words are not tokenized from full-text? Tokenizer should only be overided by identity if text is already tokenized before. Perhaps vectorizer is receiving string (char sequence) instead of word sequence, so it behaves like a char ngram vectorizer`\" qouted from notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=11)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# character_tfidf_vectorizer = TfidfVectorizer(\n#             tokenizer = lambda x: x,\n#             preprocessor = lambda x: x,\n#             token_pattern = None,\n#             strip_accents = 'unicode',\n#             analyzer = 'word',\n#             ngram_range = (1,3),\n#             min_df = 0.1,\n#             max_df = 0.95,\n#             sublinear_tf = True,\n# )\n# # Processed text\n# processed_text = train.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n\n# # Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n# train_tfidf = character_tfidf_vectorizer.fit_transform([i for i in processed_text])\n\n# character_tfidf_feature_names = character_tfidf_vectorizer.get_feature_names_out()\n# print(character_tfidf_feature_names[0:110])\n\n# dense_matrix = train_tfidf.toarray()\n# df = pd.DataFrame(dense_matrix, columns=[f\"tfidf_{name}\" for name in character_tfidf_feature_names ])\n# df['essay_id'] = train_feats['essay_id']\n\n\n# # Merge the newly generated feature data with the previously generated feature data\n# train_feats = train_feats.merge(df, on='essay_id', how='left')\n\n# feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n# print('Features Number: ',len(feature_names))\n# train_feats.head(5)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:46:35.714339Z",
     "iopub.execute_input": "2024-06-08T06:46:35.715173Z",
     "iopub.status.idle": "2024-06-08T06:46:35.721794Z",
     "shell.execute_reply.started": "2024-06-08T06:46:35.715126Z",
     "shell.execute_reply": "2024-06-08T06:46:35.720514Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id='tfidf-word-feature'></a>\n## 3.6 Word TFIDF feature:\n\nSource notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17)\n\nhttps://www.kaggle.com/code/batprem/aes2-added-don-t-waste-your-run-time-feature/notebook",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\ntfidf_vectorizer = TfidfVectorizer(\n    preprocessor = lambda x: x,\n    strip_accents = 'unicode',\n    analyzer = 'word',\n    ngram_range = (1, 3),\n    min_df = 0.05,\n    max_df = 0.85,\n    sublinear_tf = True,\n    stop_words = final_stopwords_list,\n)\n\n# Processed text\nprocessed_text = train.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n\n# Fit all datasets into TfidfVectorizer\ntrain_tfidf = tfidf_vectorizer.fit_transform([i for i in processed_text])\nword_tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\nprint(word_tfidf_feature_names[0:100])\n\ndense_matrix = train_tfidf.toarray()\ndf = pd.DataFrame(dense_matrix, columns=[f\"tfidf_{name}\" for name in word_tfidf_feature_names])\ndf['essay_id'] = train_feats['essay_id']\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\n\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:46:35.723505Z",
     "iopub.execute_input": "2024-06-08T06:46:35.723854Z",
     "iopub.status.idle": "2024-06-08T06:49:43.254769Z",
     "shell.execute_reply.started": "2024-06-08T06:46:35.723794Z",
     "shell.execute_reply": "2024-06-08T06:49:43.253598Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## **3.7 CountVectorizer Features:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "count_vectorizer = CountVectorizer(\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.05, \n            max_df=0.85,\n)\n\ntrain_count = count_vectorizer.fit_transform([i for i in processed_text])\n\ndense_matrix = train_count.toarray()\nword_count_feature_names = count_vectorizer.get_feature_names_out()\nprint(word_count_feature_names[0:100])\n\ndf = pd.DataFrame(dense_matrix,  columns=[f\"count_{name}\" for name in word_count_feature_names])\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:49:43.256597Z",
     "iopub.execute_input": "2024-06-08T06:49:43.257457Z",
     "iopub.status.idle": "2024-06-08T06:49:56.846025Z",
     "shell.execute_reply.started": "2024-06-08T06:49:43.257406Z",
     "shell.execute_reply": "2024-06-08T06:49:56.844847Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## **Add centroid**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\ntfidf_w_columns = [ f'tfidf_{i}' for i in word_tfidf_feature_names]\nkmean_test = train_feats[tfidf_w_columns]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:49:56.847574Z",
     "iopub.execute_input": "2024-06-08T06:49:56.847911Z",
     "iopub.status.idle": "2024-06-08T06:49:56.877794Z",
     "shell.execute_reply.started": "2024-06-08T06:49:56.847876Z",
     "shell.execute_reply": "2024-06-08T06:49:56.876853Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Initialize KMeans with the number of clusters you want\nkmeans = KMeans(n_clusters=7, random_state=42)\n\n# Fit the model to the data\nkmeans.fit(train_feats[tfidf_w_columns])\n\n# Predict the clusters for the data points\nlabels = kmeans.labels_\n\n# Get the centroids\ncentroids = kmeans.cluster_centers_\n\n# Calculate the distance to the centroid\ndistances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\n\ncosine_distances_to_centroid = [\n    cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n    for i, label in enumerate(labels)\n]\n\n# Add the distances to the DataFrame\nkmean_test['DistanceToCentroid'] = distances\nkmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\ntrain_feats['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\ntrain_feats['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:49:56.879145Z",
     "iopub.execute_input": "2024-06-08T06:49:56.879406Z",
     "iopub.status.idle": "2024-06-08T06:50:09.708744Z",
     "shell.execute_reply.started": "2024-06-08T06:49:56.879371Z",
     "shell.execute_reply": "2024-06-08T06:50:09.707588Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:50:09.710195Z",
     "iopub.execute_input": "2024-06-08T06:50:09.710463Z",
     "iopub.status.idle": "2024-06-08T06:50:09.73494Z",
     "shell.execute_reply.started": "2024-06-08T06:50:09.71042Z",
     "shell.execute_reply": "2024-06-08T06:50:09.733878Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats['CosineDistanceToCentroid'].min()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:50:09.736629Z",
     "iopub.execute_input": "2024-06-08T06:50:09.736977Z",
     "iopub.status.idle": "2024-06-08T06:50:09.747879Z",
     "shell.execute_reply.started": "2024-06-08T06:50:09.736931Z",
     "shell.execute_reply": "2024-06-08T06:50:09.746735Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id='extra-feature'></a>\n## 3.7 Extra features:\nReference: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install /kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl >> none\n!pip install /kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl >> none",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:50:09.749409Z",
     "iopub.execute_input": "2024-06-08T06:50:09.749748Z",
     "iopub.status.idle": "2024-06-08T06:50:40.199661Z",
     "shell.execute_reply.started": "2024-06-08T06:50:09.749708Z",
     "shell.execute_reply": "2024-06-08T06:50:40.198054Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import textstat\n\nfrom textblob import TextBlob\n\nfrom collections import Counter\nfrom collections import defaultdict\nfrom nltk.metrics import BigramAssocMeasures\nfrom nltk.collocations import BigramCollocationFinder\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T06:50:40.20223Z",
     "iopub.execute_input": "2024-06-08T06:50:40.202557Z",
     "iopub.status.idle": "2024-06-08T06:50:41.87032Z",
     "shell.execute_reply.started": "2024-06-08T06:50:40.202516Z",
     "shell.execute_reply": "2024-06-08T06:50:41.869103Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class Preprocessor:\n    def __init__(self) -> None:\n        self.STOP_WORDS = set(final_stopwords_list)\n        self.spellchecker = SpellChecker()\n\n    def spelling(self, text):\n        text_2 = re.sub(r'[^\\w\\s]', ' ', text)\n        amount_miss = len(list(self.spellchecker.unknown(text_2.split())))\n        return amount_miss\n    \n    def find_wrong_punctuation(self, text):\n        punctuations = ['.', ',', ';', '?', '!', ':']\n        lowercase_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n        uppercase_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n\n        # find punctuation in the text\n        wrong_punctuations = 0\n        length = len(text)\n\n        for i in range(length):\n            if text[i] in punctuations and i < length - 1:\n                if text[i + 1] in lowercase_list or text[i + 1] in uppercase_list:\n                    wrong_punctuations += 1\n\n        return wrong_punctuations\n    \n    def noun_verb_adj_adv_adp_others(self, text):\n        doc = nlp(text)\n        pos_counts = doc.count_by(spacy.attrs.POS)\n        nouns = 0\n        verbs = 0\n        adj = 0\n        adv = 0\n        adp_conj = 0\n        others = 0\n\n        for pos_id, count in pos_counts.items():\n            pos_tag = doc.vocab.strings[pos_id]\n            if pos_tag in ['NOUN', 'PROPN', 'PRON']:\n                nouns += count\n            elif pos_tag in ['VERB', 'AUX']:\n                verbs += count\n            elif pos_tag == 'ADJ':\n                adj += count\n            elif pos_tag == 'ADV':\n                adv += count\n            elif pos_tag in ['ADP', 'CONJ']:\n                adp_conj += count\n            else:\n                others += count\n\n        return nouns, verbs, adj, adv, adp_conj, others\n    def count_sym(self, text, sym):\n        sym_count = 0\n        for l in text:\n            if l == sym:\n                sym_count += 1\n        return sym_count\n    \n    def lexical_diversity(self,text):\n        # Tokenize the text into words\n        words = nltk.word_tokenize(text)\n\n        # Calculate the number of unique words (types) and total number of words (tokens)\n        num_types = len(set(words))\n        num_tokens = len(words)\n\n        # Calculate the Type-Token Ratio (TTR)\n        ttr = num_types / num_tokens\n\n        return ttr\n    \n    def calculate_collocation_diversity(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        return len(finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(tokens))\n\n    def calculate_collocation_strength(self,text):\n        tokens = nltk.word_tokenize(text)\n        finder = BigramCollocationFinder.from_words(tokens)\n        collocations = finder.nbest(BigramAssocMeasures.mi_like, 10)  # Get top 10 collocations\n        return sum(score for bigram, score in finder.score_ngrams(BigramAssocMeasures.mi_like)) / float(len(collocations))\n\n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        \n        # preprocessing the text\n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n\n            \n        data[['nouns', 'verbs', 'adj', 'adv', 'adp_conj', 'others']] = data['processed_text'].apply(lambda x: pd.Series(self.noun_verb_adj_adv_adp_others(x)))\n        \n         # distinct word count\n        data['distinct_word_count'] = data['processed_text'].apply(lambda x: len(set(word_tokenize(x))))\n        \n        # coleman\n        data['coleman_liau'] = data['processed_text'].apply(lambda x: textstat.coleman_liau_index(x))\n        \n        # smog\n        data['smog'] = data['processed_text'].apply(lambda x: textstat.smog_index(x))\n\n        # sentiment\n        data['sentiment'] = data['processed_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n        \n        # Text tokenization\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        \n         # lexical diversity\n        data['lexical_diversity'] = data['full_text'].apply(lambda x: self.lexical_diversity(x))\n        \n        # collocation diversity\n        data['collocation_diversity'] = data['processed_text'].apply(lambda x: self.calculate_collocation_diversity(x))\n                \n        # collocation strength\n        data['collocation_strength'] = data['processed_text'].apply(lambda x: self.calculate_collocation_strength(x))\n        \n        # essay length\n        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        \n        # essay word count\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        \n        # essay unique word count\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        \n        # essay sentence count\n        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('.')))\n        \n        # essay paragraph count\n        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('\\n\\n')))\n        \n        # count misspelling\n        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        data[\"splling_err_ratio\"] = data[\"splling_err_num\"] / data[\"text_length\"]\n\n        print(\"Spelling mistake count done\")\n        \n        # ratio fullstop / text_length ** new\n        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n        \n        # ratio comma / text_length ** new\n        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n        \n        return data",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:41:15.207987Z",
     "iopub.execute_input": "2024-06-08T07:41:15.208258Z",
     "iopub.status.idle": "2024-06-08T07:41:15.24262Z",
     "shell.execute_reply.started": "2024-06-08T07:41:15.208231Z",
     "shell.execute_reply": "2024-06-08T07:41:15.241519Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preprocessor = Preprocessor()\ntmp = preprocessor.run(train.to_pandas(), mode=\"train\")\ntrain_feats = train_feats.merge(tmp, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:41:17.393154Z",
     "iopub.execute_input": "2024-06-08T07:41:17.39346Z",
     "iopub.status.idle": "2024-06-08T07:59:25.870961Z",
     "shell.execute_reply.started": "2024-06-08T07:41:17.393425Z",
     "shell.execute_reply": "2024-06-08T07:59:25.869702Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# print(list(train_feats.columns))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.285143Z",
     "iopub.status.idle": "2024-06-08T07:40:59.285534Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.285322Z",
     "shell.execute_reply": "2024-06-08T07:40:59.285349Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.8 Test dataset featurization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# # Character Tfidf\n# processed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n# test_tfidf = character_tfidf_vectorizer.transform([i for i in processed_text])\n# feature_names = character_tfidf_vectorizer.get_feature_names_out()\n# test_tfidf_df = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\n# test_tfidf_df['essay_id'] = test_feats['essay_id']\n# test_feats = test_feats.merge(test_tfidf_df, on='essay_id', how='left')\n\n\n# Word Tfidf\nprocessed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntest_tfidf = tfidf_vectorizer.transform([i for i in processed_text])\nfeature_names = tfidf_vectorizer.get_feature_names_out()\ntest_tfidf_df = pd.DataFrame(test_tfidf.toarray(), columns=[f'tfidf_{name}' for name in feature_names])\ntest_tfidf_df['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(test_tfidf_df, on='essay_id', how='left')\n\n# Word vectorize\nprocessed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntest_count = count_vectorizer.transform([i for i in processed_text])\nfeature_names = count_vectorizer.get_feature_names_out()\ntest_count_df = pd.DataFrame(test_count.toarray(), columns=[f'count_{name}' for name in feature_names])\ntest_count_df['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(test_count_df, on='essay_id', how='left')\n\n# Fit the model to the data\nlabels = kmeans.predict(test_feats[tfidf_w_columns])\nkmean_test = test_feats[tfidf_w_columns]\ncentroids = kmeans.cluster_centers_\nprint(len(centroids))\ndistances = np.sqrt(((kmean_test - centroids[labels]) ** 2).sum(axis=1))\ncosine_distances_to_centroid = [\n    cosine_distances([kmean_test.iloc[i]], [centroids[label]])[0][0]\n    for i, label in enumerate(labels)\n]\n# Add the distances to the DataFrame\nkmean_test['DistanceToCentroid'] = distances\nkmean_test['CosineDistanceToCentroid'] = cosine_distances_to_centroid\n\ntest_feats['DistanceToCentroid'] = kmean_test['DistanceToCentroid']\ntest_feats['CosineDistanceToCentroid'] = kmean_test['CosineDistanceToCentroid']\n\n    \n# Extra feature\ntmp = preprocessor.run(test.to_pandas(), mode=\"train\")\ntest_feats = test_feats.merge(tmp, on='essay_id', how='left')\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:39.770869Z",
     "iopub.execute_input": "2024-06-08T07:59:39.771178Z",
     "iopub.status.idle": "2024-06-08T07:59:40.377015Z",
     "shell.execute_reply.started": "2024-06-08T07:59:39.771137Z",
     "shell.execute_reply": "2024-06-08T07:59:40.375886Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\ntfidf_feature_names",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.289718Z",
     "iopub.status.idle": "2024-06-08T07:40:59.290084Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.289883Z",
     "shell.execute_reply": "2024-06-08T07:40:59.289904Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "count_feature_names = count_vectorizer.get_feature_names_out()\ncount_feature_names",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.291742Z",
     "iopub.status.idle": "2024-06-08T07:40:59.29212Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.291907Z",
     "shell.execute_reply": "2024-06-08T07:40:59.291931Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Data preparation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1 Add k-fold details",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfor i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"])):\n    train_feats.loc[val_index, \"fold\"] = i\n    \nprint(train_feats.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:44.951108Z",
     "iopub.execute_input": "2024-06-08T07:59:44.951415Z",
     "iopub.status.idle": "2024-06-08T07:59:44.971798Z",
     "shell.execute_reply.started": "2024-06-08T07:59:44.951382Z",
     "shell.execute_reply": "2024-06-08T07:59:44.970447Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Feature selection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "target = \"score\"\ntrain_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"] + [target]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:47.524999Z",
     "iopub.execute_input": "2024-06-08T07:59:47.525801Z",
     "iopub.status.idle": "2024-06-08T07:59:47.531273Z",
     "shell.execute_reply.started": "2024-06-08T07:59:47.525758Z",
     "shell.execute_reply": "2024-06-08T07:59:47.530151Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:49.354764Z",
     "iopub.execute_input": "2024-06-08T07:59:49.355649Z",
     "iopub.status.idle": "2024-06-08T07:59:49.360684Z",
     "shell.execute_reply.started": "2024-06-08T07:59:49.355587Z",
     "shell.execute_reply": "2024-06-08T07:59:49.359552Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.299262Z",
     "iopub.status.idle": "2024-06-08T07:40:59.299738Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.299492Z",
     "shell.execute_reply": "2024-06-08T07:40:59.299511Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.301647Z",
     "iopub.status.idle": "2024-06-08T07:40:59.302016Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.301818Z",
     "shell.execute_reply": "2024-06-08T07:40:59.301839Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def sanitize_feature_names(df):\n    sanitized_columns = {col: re.sub(r'[^\\w]', '_', col) for col in df.columns}\n    df.rename(columns=sanitized_columns, inplace=True)\n    return df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:51.215623Z",
     "iopub.execute_input": "2024-06-08T07:59:51.215959Z",
     "iopub.status.idle": "2024-06-08T07:59:51.222544Z",
     "shell.execute_reply.started": "2024-06-08T07:59:51.215915Z",
     "shell.execute_reply": "2024-06-08T07:59:51.221221Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats = sanitize_feature_names(train_feats)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:52.928477Z",
     "iopub.execute_input": "2024-06-08T07:59:52.928799Z",
     "iopub.status.idle": "2024-06-08T07:59:52.937095Z",
     "shell.execute_reply.started": "2024-06-08T07:59:52.928767Z",
     "shell.execute_reply": "2024-06-08T07:59:52.93591Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats = sanitize_feature_names(test_feats)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:54.489218Z",
     "iopub.execute_input": "2024-06-08T07:59:54.489526Z",
     "iopub.status.idle": "2024-06-08T07:59:54.497841Z",
     "shell.execute_reply.started": "2024-06-08T07:59:54.489493Z",
     "shell.execute_reply": "2024-06-08T07:59:54.496518Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1 Evaluation function and loss function defination ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = (y_true + a).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    \n    return 'QWK', qwk, True\n\ndef quadratic_weighted_kappa_2(y_true, y_pred):\n    y_true = (y_true + a).round()\n    y_pred = (y_pred.get_label() + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2 * np.sum((preds-labels)**2)\n    g = 1/2 * np.sum((preds-a)**2 + b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2) * len(labels)\n    hess = np.ones(len(labels))\n    \n    return grad, hess\n\ndef qwk_param_calc(y):\n    a = y.mean()\n    b = (y ** 2).mean() - a**2\n    \n    return np.round(a, 4), np.round(b, 4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:56.448582Z",
     "iopub.execute_input": "2024-06-08T07:59:56.448897Z",
     "iopub.status.idle": "2024-06-08T07:59:56.461219Z",
     "shell.execute_reply.started": "2024-06-08T07:59:56.448866Z",
     "shell.execute_reply": "2024-06-08T07:59:56.46013Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2 Training LGBMRegressor model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### **K-fold**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.utils.class_weight import compute_sample_weight\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport lightgbm as lgb\nimport xgboost as xgb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:59:58.75883Z",
     "iopub.execute_input": "2024-06-08T07:59:58.759112Z",
     "iopub.status.idle": "2024-06-08T07:59:58.982138Z",
     "shell.execute_reply.started": "2024-06-08T07:59:58.759081Z",
     "shell.execute_reply": "2024-06-08T07:59:58.980735Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "actual_score = train_feats['score'].astype(np.float32).values\nactual_score",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:00:11.931952Z",
     "iopub.execute_input": "2024-06-08T08:00:11.93227Z",
     "iopub.status.idle": "2024-06-08T08:00:11.942132Z",
     "shell.execute_reply.started": "2024-06-08T08:00:11.932232Z",
     "shell.execute_reply": "2024-06-08T08:00:11.940834Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75, first_metric_only=True)\n]\n\n\nfor fold in range(n_splits):\n\n    model_lgb = lgb.LGBMRegressor(\n                                    objective = qwk_obj,\n                                    metrics = 'None',\n                                    learning_rate = 0.05, \n                                    max_depth = 5,\n                                    num_leaves = 10, \n                                    colsample_bytree = 0.3,  \n                                    reg_alpha = 0.1, \n                                    reg_lambda = 0.8,     #0.8,\n                                    n_estimators = 1024, #1024, \n                                    random_state = 42, \n                                    extra_trees=True,\n                                    verbosity = - 1\n    )\n\n   \n    a, b = qwk_param_calc(train_feats[train_feats[\"fold\"] != fold][\"score\"])\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold].drop(columns=train_drop_columns)\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n    \n    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n\n    # Training model\n    lgb_model = model_lgb.fit( \n                    X_train, y_train,\n                    sample_weight = sample_weights,\n                    eval_names = ['train', 'valid'],\n                    eval_set = [(X_train, y_train), (X_eval, y_eval)],\n                    eval_metric = quadratic_weighted_kappa,\n                    callbacks = callbacks\n                )\n\n    models.append(lgb_model)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:00:15.90693Z",
     "iopub.execute_input": "2024-06-08T08:00:15.907246Z",
     "iopub.status.idle": "2024-06-08T08:02:36.349022Z",
     "shell.execute_reply.started": "2024-06-08T08:00:15.90721Z",
     "shell.execute_reply": "2024-06-08T08:02:36.34779Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.3 Validating LGBMRegressor model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\npreds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]    \n\n    pred = model.predict(X_eval_cv) + a\n    \n    pred[pred < 1] = 1\n    pred[pred > 6] = 6\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n#     preds.extend(pred)\n    \n    v_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n    print(f\"Validation score {fold} : {v_score}\")\n\nv_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\nprint(f\"Validation score : {v_score}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:05:03.456157Z",
     "iopub.execute_input": "2024-06-08T08:05:03.456482Z",
     "iopub.status.idle": "2024-06-08T08:05:04.207051Z",
     "shell.execute_reply.started": "2024-06-08T08:05:03.456447Z",
     "shell.execute_reply": "2024-06-08T08:05:04.205908Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "cm = confusion_matrix(trues, preds, labels=[x for x in range(1,7)])\n\n# Displaying the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[x for x in range(1,7)])\ndisp.plot()\nplt.show()\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:05:29.183821Z",
     "iopub.execute_input": "2024-06-08T08:05:29.184122Z",
     "iopub.status.idle": "2024-06-08T08:05:29.669925Z",
     "shell.execute_reply.started": "2024-06-08T08:05:29.184087Z",
     "shell.execute_reply": "2024-06-08T08:05:29.668738Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model_prec = pd.DataFrame([trues, preds]).T\n    \nmodel_prec.rename(columns = {0: 'trues', 1: 'preds'}, inplace=True)\nmodel_prec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:05:38.750599Z",
     "iopub.execute_input": "2024-06-08T08:05:38.750937Z",
     "iopub.status.idle": "2024-06-08T08:05:39.179858Z",
     "shell.execute_reply.started": "2024-06-08T08:05:38.750902Z",
     "shell.execute_reply": "2024-06-08T08:05:39.178737Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def analyze_preds(trues, preds):\n    # Create dataframe\n    model_prec = pd.DataFrame([trues, preds]).T\n    \n    model_prec.rename(columns = {0: 'trues', 1: 'preds'}, inplace=True)\n    model_prec['correct'] = model_prec['trues'] == model_prec['preds']\n    model_prec['count'] = model_prec.groupby('trues')['trues'].transform('count')\n    model_prec['correct_count'] = model_prec.groupby('trues')['correct'].transform('sum')\n    model_prec['correct_rate'] = model_prec['correct_count'] / model_prec['count']\n    \n    # Print binary correction rate\n    print(model_prec[['trues', 'correct_rate', 'correct_count', 'count']].drop_duplicates().sort_values(by='trues'))\n    \n    # Plot predictions by score    \n    def plot_model(ax, counts, true):\n        bars = ax.bar(counts.index, counts.values, color='skyblue')\n\n        # Find the index of the column with the specified label\n        highlight_index = counts.index.get_loc(true)\n\n        # Highlight the specified column\n        bars[highlight_index].set_color('orange')\n\n        ax.set_xlabel('Predicted Values')\n        ax.set_ylabel('Count')\n        ax.set_title(\"Score \" + str(true))\n    \n    score_list = [1,2,3,4,5,6]\n    test_pred_by_score = [model_prec[model_prec['trues'] ==  score]['preds'].value_counts() for score in score_list]\n\n    # Create a figure and six subplots arranged in a 2x3 grid\n    fig, axs = plt.subplots(2, 3, figsize=(15, 10))    \n    plot_model(axs[0, 0], test_pred_by_score[0], 1)\n    plot_model(axs[0, 1], test_pred_by_score[1], 2)\n    plot_model(axs[0, 2], test_pred_by_score[2], 3)\n    plot_model(axs[1, 0], test_pred_by_score[3], 4)\n    plot_model(axs[1, 1], test_pred_by_score[4], 5)\n    plot_model(axs[1, 2], test_pred_by_score[5], 6)\n    \nanalyze_preds(trues, preds)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:05:41.646824Z",
     "iopub.execute_input": "2024-06-08T08:05:41.647887Z",
     "iopub.status.idle": "2024-06-08T08:05:43.525714Z",
     "shell.execute_reply.started": "2024-06-08T08:05:41.647841Z",
     "shell.execute_reply": "2024-06-08T08:05:43.524566Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### **Classification with thresholds**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# import numpy as np\n\n\n# optimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\n\n# def classify_with_thresholds(predicted_value, optimal_thresholds):\n\n\n#     new_predicted_value = np.empty_like(predicted_value)\n    \n#     for i, threshold in enumerate(optimal_thresholds):\n#         if i == 0:\n#             new_predicted_value[predicted_value < threshold] = 1\n\n#         else:\n#             new_predicted_value[(predicted_value >= optimal_thresholds[i-1]) & (predicted_value < threshold)] = i + 1\n\n#         if i == 4:\n#             new_predicted_value[predicted_value >= threshold] = 6\n            \n#     return new_predicted_value.astype(int)\n            \n# new_predicted_value = classify_with_thresholds(preds, optimal_thresholds)\n    \n# print(\"New value\")\n# print(new_predicted_value)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.357672Z",
     "iopub.status.idle": "2024-06-08T07:40:59.358073Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.357849Z",
     "shell.execute_reply": "2024-06-08T07:40:59.357872Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# optimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\n# best_kappa_score = v_score\n\n# for i in range(5):\n#     i_threshold = optimal_thresholds[i] - 0.5\n    \n#     for j in range(1000):\n#         thresholds = [x for x in optimal_thresholds]\n#         thresholds[i] = i_threshold + j / 1000\n        \n#         classified_oof = classify_with_thresholds(preds, thresholds)\n#         threshold_kappa_score = cohen_kappa_score(trues, classified_oof, weights='quadratic')\n\n#         if threshold_kappa_score >= best_kappa_score:\n#             best_kappa_score = threshold_kappa_score\n#             optimal_thresholds[i] = thresholds[i]\n# optimal_thresholds",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.359736Z",
     "iopub.status.idle": "2024-06-08T07:40:59.360102Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.359904Z",
     "shell.execute_reply": "2024-06-08T07:40:59.359926Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# new_predicted_value = classify_with_thresholds(preds, optimal_thresholds)\n    \n# print(\"New value\")\n# print(new_predicted_value)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.361502Z",
     "iopub.status.idle": "2024-06-08T07:40:59.361884Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.361693Z",
     "shell.execute_reply": "2024-06-08T07:40:59.36171Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# analyze_preds(trues, new_predicted_value)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.364346Z",
     "iopub.status.idle": "2024-06-08T07:40:59.364735Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.364509Z",
     "shell.execute_reply": "2024-06-08T07:40:59.364524Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.4 Testing and collecting prediction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_drop_columns",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.366697Z",
     "iopub.status.idle": "2024-06-08T07:40:59.367082Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.366882Z",
     "shell.execute_reply": "2024-06-08T07:40:59.366904Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.369098Z",
     "iopub.status.idle": "2024-06-08T07:40:59.369604Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.369318Z",
     "shell.execute_reply": "2024-06-08T07:40:59.369341Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# predecting for 5 models\npreds = []\nfor fold, model in enumerate(models):\n    X_eval_cv = test_feats.drop(columns=test_drop_columns)\n    print(X_eval_cv.shape)\n    pred = model.predict(X_eval_cv) + a\n    \n    \n    pred[pred < 1] = 1\n    pred[pred > 6] = 6\n    \n    preds.append(pred)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:05:51.686716Z",
     "iopub.execute_input": "2024-06-08T08:05:51.68699Z",
     "iopub.status.idle": "2024-06-08T08:05:51.727051Z",
     "shell.execute_reply.started": "2024-06-08T08:05:51.686962Z",
     "shell.execute_reply": "2024-06-08T08:05:51.725751Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for i, pred in enumerate(preds):\n    test_feats[f\"score_pred_{i}\"] = pred\ntest_feats[\"score\"] = np.round(test_feats[[f\"score_pred_{fold}\" for fold in range(n_splits)]].mean(axis=1),0).astype('int32')\n\n# new_predicts = classify_with_thresholds(test_predicts, optimal_thresholds)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:07:24.149885Z",
     "iopub.execute_input": "2024-06-08T08:07:24.150158Z",
     "iopub.status.idle": "2024-06-08T08:07:24.160825Z",
     "shell.execute_reply.started": "2024-06-08T08:07:24.150131Z",
     "shell.execute_reply": "2024-06-08T08:07:24.159605Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# new_predicts",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.380111Z",
     "iopub.status.idle": "2024-06-08T07:40:59.380595Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.380317Z",
     "shell.execute_reply": "2024-06-08T07:40:59.380339Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# test_predicts",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:06:10.173569Z",
     "iopub.execute_input": "2024-06-08T08:06:10.17396Z",
     "iopub.status.idle": "2024-06-08T08:06:10.184128Z",
     "shell.execute_reply.started": "2024-06-08T08:06:10.173919Z",
     "shell.execute_reply": "2024-06-08T08:06:10.182706Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\n# test_feats[\"score\"] = test_predicts",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.383066Z",
     "iopub.status.idle": "2024-06-08T07:40:59.383448Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.383234Z",
     "shell.execute_reply": "2024-06-08T07:40:59.383256Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:07:26.276708Z",
     "iopub.execute_input": "2024-06-08T08:07:26.277011Z",
     "iopub.status.idle": "2024-06-08T08:07:26.308204Z",
     "shell.execute_reply.started": "2024-06-08T08:07:26.276978Z",
     "shell.execute_reply": "2024-06-08T08:07:26.307087Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Submission",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-24T07:55:55.840356Z",
     "iopub.status.idle": "2024-05-24T07:55:55.840716Z",
     "shell.execute_reply.started": "2024-05-24T07:55:55.840506Z",
     "shell.execute_reply": "2024-05-24T07:55:55.84053Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "test_feats[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T08:07:33.090937Z",
     "iopub.execute_input": "2024-06-08T08:07:33.091276Z",
     "iopub.status.idle": "2024-06-08T08:07:33.101743Z",
     "shell.execute_reply.started": "2024-06-08T08:07:33.091239Z",
     "shell.execute_reply": "2024-06-08T08:07:33.100653Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\n# Plot the tree with the feature names\nax = lgb.plot_tree(models[0], tree_index=0, figsize=(20, 8), dpi=300)\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.390156Z",
     "iopub.status.idle": "2024-06-08T07:40:59.390659Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.39036Z",
     "shell.execute_reply": "2024-06-08T07:40:59.390382Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\nfor model in models:  \n    ax = lgb.plot_tree(model, tree_index=0, figsize=(20, 8), dpi=300)\n    plt.show()\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.393097Z",
     "iopub.status.idle": "2024-06-08T07:40:59.3935Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.393257Z",
     "shell.execute_reply": "2024-06-08T07:40:59.393311Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "ax = lgb.plot_importance(models[0], figsize=(20, 300), importance_type=\"split\")\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.395451Z",
     "iopub.status.idle": "2024-06-08T07:40:59.395833Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.395605Z",
     "shell.execute_reply": "2024-06-08T07:40:59.395627Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ax = lgb.plot_split_value_histogram(models[0], figsize=(20, 8), feature='splling_err_num')\n# plt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.397379Z",
     "iopub.status.idle": "2024-06-08T07:40:59.397798Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.397562Z",
     "shell.execute_reply": "2024-06-08T07:40:59.397586Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "ax = lgb.plot_importance(models[0], figsize=(20, 100), importance_type=\"gain\")\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T07:40:59.400097Z",
     "iopub.status.idle": "2024-06-08T07:40:59.400557Z",
     "shell.execute_reply.started": "2024-06-08T07:40:59.400297Z",
     "shell.execute_reply": "2024-06-08T07:40:59.40032Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
