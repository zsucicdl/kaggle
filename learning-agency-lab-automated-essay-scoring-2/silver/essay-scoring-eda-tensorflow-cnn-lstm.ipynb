{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Learning Agency Lab - Automated Essay Scoring 2.0\n\nIn this notebook, I demonstrated a baseline modelling for auto essay scoring using Tensorflow.\nI used my previous work for this task which was for detecting if an essay written by AI or Human. [Notebook](https://www.kaggle.com/code/umar47/notebooked8b6a149c)","metadata":{}},{"cell_type":"markdown","source":"1- Importing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport re\nimport string\nimport tensorflow_text as tf_text\nfrom collections import Counter\nfrom sklearn.metrics import cohen_kappa_score\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nRANDOM_SEED = 3\ntf.random.set_seed(RANDOM_SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-30T08:03:23.739107Z","iopub.execute_input":"2024-04-30T08:03:23.739625Z","iopub.status.idle":"2024-04-30T08:04:00.525058Z","shell.execute_reply.started":"2024-04-30T08:03:23.739599Z","shell.execute_reply":"2024-04-30T08:04:00.523921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    train = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\"\n    test = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\n    # Model constants\n    max_features = 75000\n    embedding_dim = 128\n    sequence_length = 512\n    batch_size = 128\n    num_classes = 6\n    epochs = 15\n    ngram = (1,4)#(2,4)#-0.71","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:00.526469Z","iopub.execute_input":"2024-04-30T08:04:00.52732Z","iopub.status.idle":"2024-04-30T08:04:00.532597Z","shell.execute_reply.started":"2024-04-30T08:04:00.527294Z","shell.execute_reply":"2024-04-30T08:04:00.531582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Reading train and test data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(Config.train)\ntest = pd.read_csv(Config.test)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:00.533986Z","iopub.execute_input":"2024-04-30T08:04:00.534367Z","iopub.status.idle":"2024-04-30T08:04:01.316793Z","shell.execute_reply.started":"2024-04-30T08:04:00.534332Z","shell.execute_reply":"2024-04-30T08:04:01.315763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* first look at the data","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.319231Z","iopub.execute_input":"2024-04-30T08:04:01.319576Z","iopub.status.idle":"2024-04-30T08:04:01.342288Z","shell.execute_reply.started":"2024-04-30T08:04:01.319516Z","shell.execute_reply":"2024-04-30T08:04:01.341543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['score']=train['score'] - 1","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.343299Z","iopub.execute_input":"2024-04-30T08:04:01.34358Z","iopub.status.idle":"2024-04-30T08:04:01.349804Z","shell.execute_reply.started":"2024-04-30T08:04:01.343557Z","shell.execute_reply":"2024-04-30T08:04:01.348983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.350877Z","iopub.execute_input":"2024-04-30T08:04:01.351122Z","iopub.status.idle":"2024-04-30T08:04:01.381196Z","shell.execute_reply.started":"2024-04-30T08:04:01.351101Z","shell.execute_reply":"2024-04-30T08:04:01.380324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.382255Z","iopub.execute_input":"2024-04-30T08:04:01.382542Z","iopub.status.idle":"2024-04-30T08:04:01.393807Z","shell.execute_reply.started":"2024-04-30T08:04:01.382496Z","shell.execute_reply":"2024-04-30T08:04:01.392802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Simple EDA","metadata":{}},{"cell_type":"markdown","source":"* distribution of scores, seems like very few example for score 5 and 6","metadata":{}},{"cell_type":"code","source":"train['score'].value_counts().plot(kind = 'bar', color = ['steelblue', 'orange'])\nplt.ylabel('Percentage');","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.394913Z","iopub.execute_input":"2024-04-30T08:04:01.39518Z","iopub.status.idle":"2024-04-30T08:04:01.640398Z","shell.execute_reply.started":"2024-04-30T08:04:01.395158Z","shell.execute_reply":"2024-04-30T08:04:01.639514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Distribution of length of essays","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nsequence_lengths = train['full_text'].apply(lambda x: len(x.split()))\n\nplt.hist(sequence_lengths, bins=30, alpha=0.75, color='blue')\nplt.title('Distribution of Text Sequence Lengths for Essays')\nplt.xlabel('Sequence Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:01.641756Z","iopub.execute_input":"2024-04-30T08:04:01.642113Z","iopub.status.idle":"2024-04-30T08:04:02.390277Z","shell.execute_reply.started":"2024-04-30T08:04:01.642082Z","shell.execute_reply":"2024-04-30T08:04:02.389275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Distribution of texts for each score individually.\nIt changes a lot for some scores as seems which tells that length of essay is an important metric here","metadata":{}},{"cell_type":"code","source":"scores = train['score'].unique()\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\n\nfor i, score in enumerate(scores):\n    # Get sequence lengths for the current score\n    sequence_lengths = train.loc[train['score'] == score]['full_text'].apply(lambda x: len(x.split()))\n    \n    # Plot histogram\n    axes[i].hist(sequence_lengths, bins=30, alpha=0.75, color='blue')\n    axes[i].set_title(f'Distribution of Text Sequence Lengths for score {score}')\n    axes[i].set_xlabel('Sequence Length')\n    axes[i].set_ylabel('Frequency')\n\n# Hide any remaining empty subplots\nfor j in range(len(scores), len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:02.394682Z","iopub.execute_input":"2024-04-30T08:04:02.394949Z","iopub.status.idle":"2024-04-30T08:04:04.580263Z","shell.execute_reply.started":"2024-04-30T08:04:02.394927Z","shell.execute_reply":"2024-04-30T08:04:04.579368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"essays = train['full_text'].values \nscores = train['score'].values  # List of corresponding scores\ndef preprocess_text(text):\n    text = re.sub(\"[^a-zA-Z]\", ' ', text)\n\n    text = text.lower().split()\n    swords = set(stopwords.words(\"english\"))\n    text = [w for w in text if w not in swords]\n    text = \" \".join(text)\n    return text\n\n# Function to plot top 5 words for each score category\ndef plot_top_words_per_score(essays, scores):\n    # Create subplots\n    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n    fig.tight_layout(pad=5.0)\n    for i, score in enumerate(range(0, 6)):\n        essays_for_score = [essay for essay, s in zip(essays, scores) if s == score]\n        \n        tokenized_essays = [preprocess_text(essay).split() for essay in essays_for_score]\n\n        all_words = [word for essay_words in tokenized_essays for word in essay_words]\n\n        word_counts = Counter(all_words)\n\n        # Get the top 5 most common words\n        top_words = word_counts.most_common(5)\n\n        # Plot the top 5 words\n        ax = axs[i // 3, i % 3]  # Select subplot\n        ax.bar([word[0] for word in top_words], [word[1] for word in top_words])\n        ax.set_title(f\"Score {score}\")\n        ax.set_xlabel(\"Word\")\n        ax.set_ylabel(\"Frequency\")\n\n    plt.show()\nplot_top_words_per_score(essays, scores)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:04.581489Z","iopub.execute_input":"2024-04-30T08:04:04.581837Z","iopub.status.idle":"2024-04-30T08:04:14.257673Z","shell.execute_reply.started":"2024-04-30T08:04:04.581808Z","shell.execute_reply":"2024-04-30T08:04:14.256768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset Prep.","metadata":{}},{"cell_type":"markdown","source":"* Lets start prepraring dataset for training. ","metadata":{}},{"cell_type":"code","source":"# Split the data into training, validation, and test sets\ntrain_df, test_df = train_test_split(train, test_size=0.3, random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)\n\nbatch_size = Config.batch_size\n\nraw_train_ds = tf.data.Dataset.from_tensor_slices(\n    (train_df['full_text'].values, train_df['score'].values)\n).batch(batch_size)\n\nraw_val_ds = tf.data.Dataset.from_tensor_slices(\n    (val_df['full_text'].values, val_df['score'].values)\n).batch(batch_size)\n\nraw_test_ds = tf.data.Dataset.from_tensor_slices(\n    (test_df['full_text'].values, test_df['score'].values)\n).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:14.258851Z","iopub.execute_input":"2024-04-30T08:04:14.25918Z","iopub.status.idle":"2024-04-30T08:04:15.132825Z","shell.execute_reply.started":"2024-04-30T08:04:14.259153Z","shell.execute_reply":"2024-04-30T08:04:15.131815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_lower_and_split_punct(text):\n    # Convert words to lowercase\n    text = tf.strings.lower(text)\n\n    # Remove HTML tags\n    text = tf.strings.regex_replace(text, '<.*?>', '')\n\n    # Delete strings starting with @\n    text = tf.strings.regex_replace(text, '@\\w+', '')\n\n    # Delete Numbers\n    text = tf.strings.regex_replace(text, \"'\\d+\", '')\n    text = tf.strings.regex_replace(text, '\\d+', '')\n\n    # Delete URL\n    text = tf.strings.regex_replace(text, 'http\\w+', '')\n\n    # Replace consecutive empty spaces with a single space character\n    text = tf.strings.regex_replace(text, r'\\s+', ' ')\n\n    # Replace consecutive commas and periods with one comma and period character\n    text = tf.strings.regex_replace(text, r'\\.+', '.')\n    text = tf.strings.regex_replace(text, r'\\,+', ',')\n\n    # Strip whitespace.\n    text = tf.strings.strip(text)\n\n    # Join with '[START]' and '[END]'\n    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n    \n    return text\n\n\n# Text vectorization layer\nvectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=tf_lower_and_split_punct,\n    max_tokens=Config.max_features,\n    ngrams = Config.ngram,\n    output_mode=\"int\",\n    output_sequence_length=Config.sequence_length,\n    pad_to_max_tokens=True\n)\n\ntext_ds = raw_train_ds.map(lambda x, y: x)\nvectorize_layer.adapt(text_ds)\n\n# Vectorize the data\ndef vectorize_text(text, label):\n    text = tf.expand_dims(text, -1)\n    return vectorize_layer(text), label\n\ntrain_ds = raw_train_ds.map(vectorize_text)\nval_ds = raw_val_ds.map(vectorize_text)\ntest_ds = raw_test_ds.map(vectorize_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:15.134471Z","iopub.execute_input":"2024-04-30T08:04:15.134941Z","iopub.status.idle":"2024-04-30T08:04:28.119448Z","shell.execute_reply.started":"2024-04-30T08:04:15.134906Z","shell.execute_reply":"2024-04-30T08:04:28.118469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data, labels in train_ds.take(1):\n    print(\"Data shape:\", data.shape)\n    print(\"Labels:\", labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:28.120999Z","iopub.execute_input":"2024-04-30T08:04:28.121784Z","iopub.status.idle":"2024-04-30T08:04:28.242563Z","shell.execute_reply.started":"2024-04-30T08:04:28.121757Z","shell.execute_reply":"2024-04-30T08:04:28.241578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_batch, label_batch = next(iter(raw_train_ds))\nfirst_text, first_label = text_batch[5], label_batch[5]\nprint(\"text: \", first_text)\nprint(\"Vectorized text: \", vectorize_text(first_text, first_label))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:28.243843Z","iopub.execute_input":"2024-04-30T08:04:28.24412Z","iopub.status.idle":"2024-04-30T08:04:29.670789Z","shell.execute_reply.started":"2024-04-30T08:04:28.244097Z","shell.execute_reply":"2024-04-30T08:04:29.66982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_layer.get_vocabulary()[3000:3010]","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:29.671937Z","iopub.execute_input":"2024-04-30T08:04:29.672234Z","iopub.status.idle":"2024-04-30T08:04:30.003893Z","shell.execute_reply.started":"2024-04-30T08:04:29.672209Z","shell.execute_reply":"2024-04-30T08:04:30.002982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_context_strings, example_target_strings in raw_train_ds.take(2024):\n    #print(example_context_strings[10:12])\n    print()\n    #print(example_target_strings[10:12])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.004976Z","iopub.execute_input":"2024-04-30T08:04:30.005236Z","iopub.status.idle":"2024-04-30T08:04:30.017171Z","shell.execute_reply.started":"2024-04-30T08:04:30.005213Z","shell.execute_reply":"2024-04-30T08:04:30.016266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_tokens = vectorize_layer(example_context_strings)\nexample_tokens[:3, :]","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.019162Z","iopub.execute_input":"2024-04-30T08:04:30.019479Z","iopub.status.idle":"2024-04-30T08:04:30.078824Z","shell.execute_reply.started":"2024-04-30T08:04:30.019455Z","shell.execute_reply":"2024-04-30T08:04:30.07797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padded sequences viz\nplt.subplot(1, 2, 1)\nplt.pcolormesh(example_tokens)\nplt.title('Token IDs')\n\nplt.subplot(1, 2, 2)\nplt.pcolormesh(example_tokens != 0)\nplt.title('Mask')","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.079903Z","iopub.execute_input":"2024-04-30T08:04:30.080202Z","iopub.status.idle":"2024-04-30T08:04:30.701922Z","shell.execute_reply.started":"2024-04-30T08:04:30.080177Z","shell.execute_reply":"2024-04-30T08:04:30.700872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Modelling","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\nfrom tensorflow.keras import Model, Input","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.703231Z","iopub.execute_input":"2024-04-30T08:04:30.703597Z","iopub.status.idle":"2024-04-30T08:04:30.710277Z","shell.execute_reply.started":"2024-04-30T08:04:30.703557Z","shell.execute_reply":"2024-04-30T08:04:30.709426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* A simple transformer block","metadata":{}},{"cell_type":"code","source":"# great work: https://www.kaggle.com/code/ichigoe/acc0-921-bidirectionlstm-transformer-cnn-approach\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n            tf.keras.layers.Dense(embed_dim)  # Adjust output dimension to embed_dim\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # Use out1 instead of inputs\n        return out2","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.711514Z","iopub.execute_input":"2024-04-30T08:04:30.711896Z","iopub.status.idle":"2024-04-30T08:04:30.72124Z","shell.execute_reply.started":"2024-04-30T08:04:30.711872Z","shell.execute_reply":"2024-04-30T08:04:30.720163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* model will need many tuning ","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(Config.sequence_length,), dtype=\"int64\")\nx = tf.keras.layers.Embedding(Config.max_features, Config.embedding_dim)(inputs)\nx = tf.keras.layers.SpatialDropout1D(0.5)(x)\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\ntransformer_block = TransformerBlock(Config.embedding_dim, 6, 32)\nx = transformer_block(x, training=True)\nx = tf.keras.layers.Conv1D(128, 4,  activation=\"relu\", strides=3)(x)\nx = tf.keras.layers.Conv1D(128, 4,  activation=\"relu\", strides=3)(x)\nx = tf.keras.layers.Conv1D(128, 4,  activation=\"relu\", strides=3)(x)\nx = tf.keras.layers.Conv1D(128, 4,  activation=\"relu\", strides=3)(x)\nx = tf.keras.layers.Dense(128, activation=\"relu\")(x)\ntransformer_block = TransformerBlock(Config.embedding_dim, 6, 32)\nx = transformer_block(x, training=True)\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n#x = tf.keras.layers.SpatialDropout1D(0.5)(x)\nx = tf.keras.layers.Dropout(0.5)(x)\nx = tf.keras.layers.Dense(128, activation=\"relu\")(x)\nx = tf.keras.layers.GlobalMaxPooling1D()(x)\nx = tf.keras.layers.Dense(128, activation=\"relu\")(x)\nx = tf.keras.layers.Dropout(0.5)(x)\n\npredictions = tf.keras.layers.Dense(Config.num_classes, activation=\"softmax\", name=\"predictions\")(x)\nmodel = tf.keras.Model(inputs, predictions)\nmodel.summary() ","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:30.722475Z","iopub.execute_input":"2024-04-30T08:04:30.72278Z","iopub.status.idle":"2024-04-30T08:04:31.618491Z","shell.execute_reply.started":"2024-04-30T08:04:30.722757Z","shell.execute_reply":"2024-04-30T08:04:31.617365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* testing cohen kappa implementation","metadata":{}},{"cell_type":"code","source":"class CohenKappaWeightedMetric(tf.keras.metrics.Metric):\n    def __init__(self, num_classes, name='cohen_kappa_weighted', **kwargs):\n        super(CohenKappaWeightedMetric, self).__init__(name=name, **kwargs)\n        self.num_classes = num_classes\n        self.conf_mtx = self.add_weight(name='conf_mtx', shape=(num_classes, num_classes), initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.int32)\n        #tf.print(\"y_p\", y_pred)\n        #y_pred = tf.cast(y_pred, tf.int32)\n        y_pred = tf.argmax(y_pred, axis=1)\n        \n        #tf.print(\"y_t\", y_true)\n        #tf.print(\"y_p after\", y_pred)\n        conf_mtx = tf.math.confusion_matrix(labels=y_true, predictions=y_pred, num_classes=self.num_classes)\n        self.conf_mtx.assign_add(conf_mtx)\n\n    def result(self):\n        weights = np.arange(1, self.num_classes + 1)\n        true_counts = tf.reduce_sum(self.conf_mtx, axis=1)\n        pred_counts = tf.reduce_sum(self.conf_mtx, axis=0)\n        true_sum = tf.reduce_sum(true_counts * weights)\n        pred_sum = tf.reduce_sum(pred_counts * weights)\n        observed = tf.reduce_sum(tf.linalg.diag_part(self.conf_mtx) * weights)\n        expected = true_sum * pred_sum / tf.reduce_sum(self.conf_mtx)\n        kappa = 1 - (1 - observed / expected) / (1 - pred_sum / expected)\n        return kappa\n\n    def reset_states(self):\n        tf.keras.backend.set_value(self.conf_mtx, np.zeros((self.num_classes, self.num_classes)))\n\n#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[CohenKappaWeightedMetric(num_classes=6)])\n#history = model.fit(train_ds, validation_data=val_ds, epochs=Config.epochs, class_weight=dict(enumerate(class_weights)))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:31.61981Z","iopub.execute_input":"2024-04-30T08:04:31.620103Z","iopub.status.idle":"2024-04-30T08:04:31.631032Z","shell.execute_reply.started":"2024-04-30T08:04:31.620079Z","shell.execute_reply":"2024-04-30T08:04:31.63005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* train, evaluation and submission.(model for some reason is overfitting and i am investigating it but if you have any idea why feel free to comment!)","metadata":{}},{"cell_type":"code","source":"class_counts = np.bincount(train['score'])\ntotal_samples = np.sum(class_counts)\nnum_classes = len(class_counts)\nclass_weights = total_samples / (num_classes * class_counts)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:31.632198Z","iopub.execute_input":"2024-04-30T08:04:31.632499Z","iopub.status.idle":"2024-04-30T08:04:31.646565Z","shell.execute_reply.started":"2024-04-30T08:04:31.632474Z","shell.execute_reply":"2024-04-30T08:04:31.64544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_learning_rate = 0.0001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=1000, \n    decay_rate=0.96,\n    staircase=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:31.64798Z","iopub.execute_input":"2024-04-30T08:04:31.648286Z","iopub.status.idle":"2024-04-30T08:04:31.657706Z","shell.execute_reply.started":"2024-04-30T08:04:31.648261Z","shell.execute_reply":"2024-04-30T08:04:31.656652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])# Change loss function\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=Config.epochs, class_weight=dict(enumerate(class_weights)))\n\n#model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[CohenKappaWeightedMetric(num_classes=6)])\n#history = model.fit(train_ds, validation_data=val_ds, epochs=Config.epochs, class_weight=dict(enumerate(class_weights)))","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:04:31.658941Z","iopub.execute_input":"2024-04-30T08:04:31.659587Z","iopub.status.idle":"2024-04-30T08:10:53.014666Z","shell.execute_reply.started":"2024-04-30T08:04:31.659553Z","shell.execute_reply":"2024-04-30T08:10:53.013788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(test_ds)\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:10:53.017318Z","iopub.execute_input":"2024-04-30T08:10:53.017782Z","iopub.status.idle":"2024-04-30T08:10:57.739908Z","shell.execute_reply.started":"2024-04-30T08:10:53.017745Z","shell.execute_reply":"2024-04-30T08:10:57.739015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = test['full_text'].values\nvectorized_test_text = vectorize_layer(test_text)\npredictions = model.predict(vectorized_test_text)\ntest['score'] = np.argmax(predictions, axis=1) + 1\ntest","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:10:57.744192Z","iopub.execute_input":"2024-04-30T08:10:57.744492Z","iopub.status.idle":"2024-04-30T08:10:58.712697Z","shell.execute_reply.started":"2024-04-30T08:10:57.744467Z","shell.execute_reply":"2024-04-30T08:10:58.7116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['essay_id', 'score']].to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:10:58.713956Z","iopub.execute_input":"2024-04-30T08:10:58.714276Z","iopub.status.idle":"2024-04-30T08:10:58.726437Z","shell.execute_reply.started":"2024-04-30T08:10:58.714248Z","shell.execute_reply":"2024-04-30T08:10:58.725476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* To dos\n\n\n1 - Quadratic Weighted Kappa (QWK)  metric\n","metadata":{}}]}