{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":175600347,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi everyone! I took the idea of finetuning Funnel Transformer from discussion [here](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/498571). \n\nThis is a 5-fold small baseline for [U-net like transformer model](https://arxiv.org/pdf/2006.03236). Hit the upvote if you find it useful. You can also notice that I've changed scheduler type to cosine.\n\nHere is a notebook which helps to build a good CV strategy (running it rn so see how it works). Check it out https://www.kaggle.com/code/emiz6413/predict-the-prompts\n\nThere is a lot more to be done here but unfortunately I'm running out of GPU:\n* Including persuade dataset\n* Regression instead of classification (Done, looks way better than classification)\n* Hyperparameters tuning\n* Bigger model and **even more** folds\n* Experiments with sequence length (no good)","metadata":{}},{"cell_type":"code","source":"import random\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom tokenizers import AddedToken\nfrom transformers import (AutoTokenizer, FunnelForSequenceClassification, AutoConfig,\n                          DataCollatorWithPadding, Trainer, TrainingArguments)\nfrom datasets import Dataset\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T16:45:06.050878Z","iopub.execute_input":"2024-05-06T16:45:06.051685Z","iopub.status.idle":"2024-05-06T16:45:24.607427Z","shell.execute_reply.started":"2024-05-06T16:45:06.051653Z","shell.execute_reply":"2024-05-06T16:45:24.606451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"REGRESSION = True\nMODEL_SIZE = 'large'\nMODEL_NAME = f'funnel-transformer/{MODEL_SIZE}'\nVERSION = 8\nEPOCHS = 1\nLR = 5e-5\nBATCH_SIZE = 2 if MODEL_SIZE == 'small' else 1\nMAX_LENGTH = 3072\nnum_labels = 1 if REGRESSION else 6\nSEED = 0","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:24.609001Z","iopub.execute_input":"2024-05-06T16:45:24.609639Z","iopub.status.idle":"2024-05-06T16:45:24.615343Z","shell.execute_reply.started":"2024-05-06T16:45:24.60961Z","shell.execute_reply":"2024-05-06T16:45:24.614267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:24.616745Z","iopub.execute_input":"2024-05-06T16:45:24.617079Z","iopub.status.idle":"2024-05-06T16:45:24.653283Z","shell.execute_reply.started":"2024-05-06T16:45:24.617049Z","shell.execute_reply":"2024-05-06T16:45:24.652531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ntest = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\nsample = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')\nprompts = pd.read_csv('/kaggle/input/predict-the-prompts/predicted_prompt.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:24.655735Z","iopub.execute_input":"2024-05-06T16:45:24.656064Z","iopub.status.idle":"2024-05-06T16:45:25.539861Z","shell.execute_reply.started":"2024-05-06T16:45:24.656029Z","shell.execute_reply":"2024-05-06T16:45:25.538929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.merge(prompts, on='essay_id').drop('predicted', axis=1)\ndata['score_prompt'] = data['score'].astype(str) + '_' + data['prompt_name']","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.541092Z","iopub.execute_input":"2024-05-06T16:45:25.541432Z","iopub.status.idle":"2024-05-06T16:45:25.59374Z","shell.execute_reply.started":"2024-05-06T16:45:25.541406Z","shell.execute_reply":"2024-05-06T16:45:25.592858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['full_text'] = data['full_text'].apply(lambda x: x.strip())\ndata['full_text'].replace('', \"'\") \\\n                 .replace('', '“') \\\n                 .replace('', '”') \\\n                 .replace('', '')\ndata['labels'] = data['score'].map(lambda x: x-1)\nif REGRESSION:\n    data[\"labels\"] = data[\"labels\"].astype('float32')\nelse:\n    data[\"labels\"] = data[\"labels\"].astype('int32')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.594951Z","iopub.execute_input":"2024-05-06T16:45:25.595263Z","iopub.status.idle":"2024-05-06T16:45:25.650863Z","shell.execute_reply.started":"2024-05-06T16:45:25.595237Z","shell.execute_reply":"2024-05-06T16:45:25.649793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nfor i, (_, val_index) in enumerate(skf.split(data, data[\"score_prompt\"])):\n    data.loc[val_index, \"fold\"] = i\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.65201Z","iopub.execute_input":"2024-05-06T16:45:25.652277Z","iopub.status.idle":"2024-05-06T16:45:25.728326Z","shell.execute_reply.started":"2024-05-06T16:45:25.652254Z","shell.execute_reply":"2024-05-06T16:45:25.72741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenize:\n    def __init__(self, train, valid, tokenizer):\n        self.tokenizer = tokenizer\n        self.train = train\n        self.valid = valid\n        \n    def get_dataset(self, df):\n        ds = Dataset.from_dict({\n                'essay_id': [e for e in df['essay_id']],\n                'full_text': [ft for ft in df['full_text']],\n                'label': [s for s in df['labels']],\n            })\n        return ds\n        \n    def tokenize_function(self, sample):\n        tokenized_inputs = self.tokenizer(\n            sample['full_text'], truncation=True, max_length=MAX_LENGTH\n        )\n        return tokenized_inputs\n    \n    def __call__(self):\n        train_ds = self.get_dataset(train)\n        valid_ds = self.get_dataset(valid)\n        \n        tokenized_train = train_ds.map(\n            self.tokenize_function, batched=True\n        )\n        tokenized_valid = valid_ds.map(\n            self.tokenize_function, batched=True\n        )\n        \n        return tokenized_train, tokenized_valid","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.729631Z","iopub.execute_input":"2024-05-06T16:45:25.730034Z","iopub.status.idle":"2024-05-06T16:45:25.739681Z","shell.execute_reply.started":"2024-05-06T16:45:25.729983Z","shell.execute_reply":"2024-05-06T16:45:25.738776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if REGRESSION:\n    def compute_metrics(p):\n        predictions, labels = p\n        qwk = cohen_kappa_score(labels, predictions.clip(0, 5).round(0), weights='quadratic')\n        return {'qwk': qwk}\nelse:\n    def compute_metrics(p):\n        predictions, labels = p\n        qwk = cohen_kappa_score(labels, predictions.argmax(-1), weights='quadratic')\n        return {'qwk': qwk}","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.740892Z","iopub.execute_input":"2024-05-06T16:45:25.741258Z","iopub.status.idle":"2024-05-06T16:45:25.75177Z","shell.execute_reply.started":"2024-05-06T16:45:25.741224Z","shell.execute_reply":"2024-05-06T16:45:25.750741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'/kaggle/working/funnel-{MODEL_SIZE}-ft_ver{VERSION}', \n    fp16=True,\n    learning_rate=LR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=4,\n    report_to=\"none\",\n    evaluation_strategy=\"steps\",\n    do_eval=True,\n    eval_steps=100,\n    save_total_limit=1,\n    save_strategy=\"steps\",\n    save_steps=100,\n    logging_steps=100,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"qwk\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    save_safetensors=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.755637Z","iopub.execute_input":"2024-05-06T16:45:25.756001Z","iopub.status.idle":"2024-05-06T16:45:25.835705Z","shell.execute_reply.started":"2024-05-06T16:45:25.755975Z","shell.execute_reply":"2024-05-06T16:45:25.834595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(len(data['fold'].unique())):\n    train = data[data['fold'] != fold]\n    valid = data[data['fold'] == fold]\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenize = Tokenize(train, valid, tokenizer)\n    tokenized_train, tokenized_valid = tokenize()\n    \n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    if REGRESSION:\n        config.attention_dropout = 0.0 # attention_probs_dropout_prob\n        config.hidden_dropout = 0.0 # hidden_dropout_prob\n\n    config.num_labels = num_labels\n    config.truncate_seq = False\n    \n    model = FunnelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n    collator = DataCollatorWithPadding(tokenizer)\n    \n    trainer = Trainer(\n        args=training_args,\n        model=model,\n        data_collator=collator,\n        eval_dataset=tokenized_valid,\n        train_dataset=tokenized_train,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer\n    )\n    \n    trainer.train()\n    \n    y_true = valid['score'].values\n    predictions = trainer.predict(tokenized_valid).predictions\n    predictions = predictions.round(0) + 1\n    cm = confusion_matrix(y_true, predictions, labels=[x for x in range(1, 7)])\n    draw_cm = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                     display_labels=[x for x in range(1, 7)])\n    draw_cm.plot()\n    plt.show()\n    \n    trainer.save_model(f'f-tfm-{MODEL_SIZE}_AES2_fold_{fold}')\n    tokenizer.save_pretrained(f'f-tfm-{MODEL_SIZE}_AES2_fold_{fold}')\n    \n    valid.to_csv(f'valid_df_fold_{fold}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:45:25.837004Z","iopub.execute_input":"2024-05-06T16:45:25.837359Z"},"trusted":true},"execution_count":null,"outputs":[]}]}