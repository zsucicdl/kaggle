{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### ‚≠êÔ∏è What this notebook is about\nHuggingface `Trainer` by default optimizes CrossEntropy, which neglects the order of the labels.\nWhereas this competition uses quadratic weighted kappa as a evaluation metric which takes the order of labels into account. <br>\nWeighted kappa loss was introduced by [J de la Torre *et al.*, (2018)](https://www.sciencedirect.com/science/article/abs/pii/S0167865517301666). By using this loss, we can directly optimize a model with respect to quadratic weighted kappa score.\n\n### üìä Result\n#### 5-fold StratifiedKFold cv\nDeberta-v3-small was fine-tuned using the weighted kappa loss with quadratic weight. <br>\nAs for the hyperparameters and the training procedure, I used the ones proposed in [the notebook](https://www.kaggle.com/code/cdeotte/deberta-v3-small-starter-cv-0-820-lb-0-800) by @cdeotte and obtained the following result. (Please refer [version 5](https://www.kaggle.com/code/emiz6413/directly-optimize-quadratic-weighted-kappa-loss?scriptVersionId=174480998) of this notebook to see the full run.)\n\n| Fold | QWK |\n| - | - |\n| 0 | 0.775 |\n| 1 | 0.782 |\n| 2 | 0.786 |\n| 3 | 0.768 |\n| 4 | 0.781 |\n| average | 0.778 |\n\n| LB | QWK |\n| - | - |\n| 5-fold | 0.777 |\n\n### üö® Disclaimer\nThe result using the configuration below did NOT result in as good performance as the ordinary cross-entropy training. The kappa score on the LB was lower by 0.022 than the cross entropy models, but I think it's good to know someone tried it. <br>\nI don't have access to the full article of the aforementioned paper. I refered the [TensorFlow implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/WeightedKappaLoss) of the loss function and ported to PyTorch.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport copy\nfrom dataclasses import dataclass\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets.arrow_dataset import Dataset\nfrom tokenizers import AddedToken\nfrom transformers.trainer import Trainer\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.training_args import TrainingArguments\nfrom transformers.models.deberta_v2 import DebertaV2ForSequenceClassification, DebertaV2TokenizerFast\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, matthews_corrcoef, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-28T14:26:35.500015Z",
     "iopub.execute_input": "2024-04-28T14:26:35.500594Z",
     "iopub.status.idle": "2024-04-28T14:26:56.264866Z",
     "shell.execute_reply.started": "2024-04-28T14:26:35.500565Z",
     "shell.execute_reply": "2024-04-28T14:26:56.264011Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass Config:\n    checkpoint: str = \"microsoft/deberta-v3-small\"\n    per_device_train_batch_size: int = 4\n    per_device_eval_batch_size: int = 8\n    gradient_accumulation_steps: int = 8 // torch.cuda.device_count() // per_device_train_batch_size\n    num_train_epochs: float = 4\n    train_max_length: int = 1024\n    eval_max_length: int = 2048\n    lr: float = 1e-5\n    scheduler: str = \"linear\"\n    warmup_ratio: float = 0.0\n    weight_decay = 0.01\n    amp: bool = True\n    n_splits: int = 5\n    optim: str = \"adamw_torch\"\n    \nconfig = Config()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:20:53.499794Z",
     "iopub.execute_input": "2024-04-28T14:20:53.500932Z",
     "iopub.status.idle": "2024-04-28T14:20:53.507774Z",
     "shell.execute_reply.started": "2024-04-28T14:20:53.500872Z",
     "shell.execute_reply": "2024-04-28T14:20:53.50695Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "args = TrainingArguments(\n    output_dir=\"output\",\n    report_to=\"none\",\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    num_train_epochs=config.num_train_epochs,\n    weight_decay=config.weight_decay,\n    evaluation_strategy='epoch',\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    save_total_limit=1,\n    metric_for_best_model=\"qwk\",\n    greater_is_better=True,\n    load_best_model_at_end=True,\n    fp16=config.amp,\n    learning_rate=config.lr,\n    lr_scheduler_type=config.scheduler,\n    warmup_ratio=config.warmup_ratio,\n    optim=config.optim\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:20:53.508791Z",
     "iopub.execute_input": "2024-04-28T14:20:53.509069Z",
     "iopub.status.idle": "2024-04-28T14:20:53.642562Z",
     "shell.execute_reply.started": "2024-04-28T14:20:53.509046Z",
     "shell.execute_reply": "2024-04-28T14:20:53.641798Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Instantiate the model & tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ModelInit:\n    model_class = DebertaV2ForSequenceClassification\n    \n    def __init__(self, checkpoint: str, num_labels: int = 6) -> None:\n        self.model = self.model_class.from_pretrained(checkpoint, num_labels=num_labels)\n        self.state_dict = copy.deepcopy(self.model.state_dict())\n        \n    def __call__(self) -> model_class:\n        self.model.load_state_dict(self.state_dict)\n        return self.model\n\nmodel_init = ModelInit(config.checkpoint)\ntokenizer = DebertaV2TokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\ntokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:03.740109Z",
     "iopub.execute_input": "2024-04-28T14:21:03.74047Z",
     "iopub.status.idle": "2024-04-28T14:21:08.052162Z",
     "shell.execute_reply.started": "2024-04-28T14:21:03.740441Z",
     "shell.execute_reply": "2024-04-28T14:21:08.051251Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Instantiate the dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:10.951822Z",
     "iopub.execute_input": "2024-04-28T14:21:10.95269Z",
     "iopub.status.idle": "2024-04-28T14:21:11.645136Z",
     "shell.execute_reply.started": "2024-04-28T14:21:10.952658Z",
     "shell.execute_reply": "2024-04-28T14:21:11.644356Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### StratifiedKFold split according to the score",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=42)\nfolds = list(cv.split(np.zeros(len(df)), y=df[\"score\"].values))\nidx2fold = {idx: fold for fold, (_, val_idx) in enumerate(folds) for idx in val_idx}\ndf[\"fold\"] = [idx2fold[i] for i in df.index]\ndf.to_csv(\"train_split.csv\", index=False)\nds = Dataset.from_pandas(df)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:12.827694Z",
     "iopub.execute_input": "2024-04-28T14:21:12.828533Z",
     "iopub.status.idle": "2024-04-28T14:21:14.706475Z",
     "shell.execute_reply.started": "2024-04-28T14:21:12.828502Z",
     "shell.execute_reply": "2024-04-28T14:21:14.705683Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class Encoder:\n    def __init__(self, tokenizer, **encode_kwargs):\n        self.tokenizer = tokenizer\n        self.kwargs = encode_kwargs\n        \n    def __call__(self, batch: dict) -> dict:\n        encoded = self.tokenizer(batch[\"full_text\"], **self.kwargs)\n        encoded[\"labels\"] = [s-1 for s in batch[\"score\"]]  # score is 1~6\n        return encoded\n    \ntrain_encoder = Encoder(tokenizer, max_length=config.train_max_length, truncation=True)\neval_encoder = Encoder(tokenizer, max_length=config.eval_max_length, truncation=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:17.895613Z",
     "iopub.execute_input": "2024-04-28T14:21:17.896005Z",
     "iopub.status.idle": "2024-04-28T14:21:17.902593Z",
     "shell.execute_reply.started": "2024-04-28T14:21:17.895975Z",
     "shell.execute_reply": "2024-04-28T14:21:17.90166Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n    predictions = eval_pred.predictions\n    y_true = eval_pred.label_ids\n    y_pred = predictions.argmax(-1)\n    kappa = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    corr = matthews_corrcoef(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    return {\"qwk\": kappa, \"corr\": corr, \"acc\": acc}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:18.432425Z",
     "iopub.execute_input": "2024-04-28T14:21:18.432773Z",
     "iopub.status.idle": "2024-04-28T14:21:18.43843Z",
     "shell.execute_reply.started": "2024-04-28T14:21:18.432745Z",
     "shell.execute_reply": "2024-04-28T14:21:18.437339Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Custom Trainer with Weighted Kappa Loss",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class WeightedKappaLoss(torch.nn.Module):\n    def __init__(\n        self, num_classes: int, weights: str = \"quadratic\", epsilon: float = 1e-6,\n    ) -> None:\n        super().__init__()\n        label_vec = torch.arange(0, num_classes).float()\n        self.row_label_vec = label_vec.view(1, num_classes)\n        self.col_label_vec = label_vec.view(num_classes, 1)\n        row_mat = torch.tile(self.row_label_vec, (num_classes, 1))\n        col_mat = torch.tile(self.col_label_vec, (1, num_classes))\n        if weights == 'quadratic':\n            self.ops = torch.square\n        elif weights == 'linear':\n            self.ops = torch.abs\n        else:\n            raise ValueError()\n        self.num_classes = num_classes\n        self.weight_mat = self.ops(col_mat - row_mat)\n        self.epsilon = epsilon\n        \n    def forward(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n        bs = y_true.size(0)\n        y_true = torch.nn.functional.one_hot(y_true, self.num_classes)\n        y_true = y_true.to(device=y_pred.device, dtype=y_pred.dtype)\n        col_label_vec = self.col_label_vec.clone().to(y_pred.device)\n        row_label_vec = self.row_label_vec.clone().to(y_pred.device)\n        weight_mat = self.weight_mat.clone().to(y_pred.device)\n        cat_labels = torch.matmul(y_true, col_label_vec)\n        cat_label_mat = torch.tile(cat_labels, (1, self.num_classes))\n        row_label_mat = torch.tile(row_label_vec, (bs, 1))\n        weight = self.ops(cat_label_mat - row_label_mat)\n        numerator = torch.sum(weight * y_pred)\n        label_dist = torch.sum(y_true, dim=0, keepdim=True)\n        pred_dist = torch.sum(y_pred, dim=0, keepdim=True)\n        w_pred_dist = torch.matmul(weight_mat, pred_dist.T)\n        dominator = torch.sum(torch.matmul(label_dist, w_pred_dist)) / bs\n        loss = torch.log(numerator / dominator + self.epsilon)\n        return loss",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:20.500621Z",
     "iopub.execute_input": "2024-04-28T14:21:20.501541Z",
     "iopub.status.idle": "2024-04-28T14:21:20.514248Z",
     "shell.execute_reply.started": "2024-04-28T14:21:20.501496Z",
     "shell.execute_reply": "2024-04-28T14:21:20.512995Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class QWKLossTrainer(Trainer):\n    def compute_loss(self, model: PreTrainedModel, inputs: dict, return_outputs: bool = False) -> tuple:\n        ce_loss, outputs = super().compute_loss(model, inputs, True)\n        labels = inputs[\"labels\"]\n        logits = outputs[\"logits\"]\n        y_pred = logits.softmax(-1)\n        loss_fn = WeightedKappaLoss(num_classes=y_pred.size(-1))\n        loss = loss_fn(y_true=labels, y_pred=y_pred)\n        outputs[\"loss\"] = loss\n        return (loss, outputs) if return_outputs else loss",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T14:21:44.63815Z",
     "iopub.execute_input": "2024-04-28T14:21:44.638802Z",
     "iopub.status.idle": "2024-04-28T14:21:44.645637Z",
     "shell.execute_reply.started": "2024-04-28T14:21:44.638771Z",
     "shell.execute_reply": "2024-04-28T14:21:44.644624Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Train",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-26T09:44:29.178919Z",
     "iopub.execute_input": "2024-04-26T09:44:29.179293Z",
     "iopub.status.idle": "2024-04-26T09:44:29.210433Z",
     "shell.execute_reply.started": "2024-04-26T09:44:29.17926Z",
     "shell.execute_reply": "2024-04-26T09:44:29.209755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "cv_res = []\n\nfor fold_idx in sorted(df[\"fold\"].unique()):\n    args.output_dir = os.path.join(\"output\", f\"fold_{fold_idx}\")\n    args.run_name = f\"{config.checkpoint}_fold-{fold_idx}\"\n    train_ds = ds.select([i for i, d in enumerate(ds) if d[\"fold\"] != fold_idx])\n    eval_ds = ds.select([i for i, d in enumerate(ds) if d[\"fold\"] == fold_idx])\n    train_ds = train_ds.map(train_encoder, batched=True)\n    eval_ds = eval_ds.map(eval_encoder, batched=True)\n    trainer = QWKLossTrainer(\n        args=args, \n        train_dataset=train_ds, \n        eval_dataset=eval_ds,\n        tokenizer=tokenizer,\n        model_init=model_init,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train()\n    preds = trainer.predict(eval_ds).predictions\n    qwk = cohen_kappa_score(y1=np.array(eval_ds[\"labels\"]), y2=preds.argmax(-1), weights=\"quadratic\")\n    fig, ax = plt.subplots()\n    ConfusionMatrixDisplay.from_predictions(\n        y_true=np.array(eval_ds[\"labels\"]), \n        y_pred=preds.argmax(-1),\n        ax=ax\n    )\n    ax.set_title(f\"fold-{fold_idx} qwk: {qwk:.3f}\")\n    fig.show()\n    cv_res.append(qwk)\n    break  # delete this line for 5-fold cv",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pd.DataFrame(\n    {\n        \"fold\":[i for i in range(len(cv_res))] + [\"mean\"],\n        \"qwk\": cv_res + [np.mean(cv_res)]\n    }\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
