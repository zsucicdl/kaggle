{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 7369493,
     "sourceType": "datasetVersion",
     "datasetId": 4281572
    },
    {
     "sourceId": 164836055,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 4761
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Mixtral Baseline\n\nThis notebook offers insights into potential strategies for tackling the competition, along with code for inference using the pretrained Mixtral model, which could serve as a strong baseline for this task.\n\nFirst of all, does it make sense to use a pretrained model for this task? The answer is 'likely yes' and this notebook aims to validate that through leaderboard evaluation. <br>\nA pretrained generative LLM like Mixtral could be viewed as an approximation of a human evaluator. It's probable that, given precise assessment criteria in the prompt, it may highly correlate with human evaluation. Without appropriate assessment criteria, however, it might exhibit bias compared to human assessment. <br>\nSupervised training on the provided training set can be seen as calibrating the model to match human assessment criteria. At the same time, an LLM like Mixtral, having been exposed to similar texts in its training set, could in principle evaluate essays in a manner akin to humans, considering that human evaluation, even with defined criteria, still involves a degree of subjectivity. <br>\nEvaluating with Mixtral in a zero-shot fashion could provide an estimate of how closely a general LLM's evaluations align with human judgments, given specific assessment criteria.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms -qq\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/accelerate-0.27.2-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms -qq\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/transformers-4.38.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms -qq\n!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/optimum-1.17.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms -qq",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:37:46.934102Z",
     "iopub.execute_input": "2024-04-04T08:37:46.934891Z",
     "iopub.status.idle": "2024-04-04T08:39:04.926411Z",
     "shell.execute_reply.started": "2024-04-04T08:37:46.934854Z",
     "shell.execute_reply": "2024-04-04T08:39:04.925127Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import gc\nimport re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T09:04:14.752596Z",
     "iopub.execute_input": "2024-04-04T09:04:14.753492Z",
     "iopub.status.idle": "2024-04-04T09:04:14.758563Z",
     "shell.execute_reply.started": "2024-04-04T09:04:14.75346Z",
     "shell.execute_reply": "2024-04-04T09:04:14.757569Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Inference parameters estimation\n\nAs inference with an LLM can be slow and memory-intensive, it makes sense to roughly estimate the size of the input data and explore approaches for optimization. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n\ntrain[\"length\"] = train[\"full_text\"].map(lambda x: len(x.split(\" \")))\ntrain.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:22.11989Z",
     "iopub.execute_input": "2024-04-04T08:50:22.120999Z",
     "iopub.status.idle": "2024-04-04T08:50:23.263645Z",
     "shell.execute_reply.started": "2024-04-04T08:50:22.120963Z",
     "shell.execute_reply": "2024-04-04T08:50:23.262608Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sns.histplot(train, x='length')\nplt.xscale('log')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:23.382748Z",
     "iopub.execute_input": "2024-04-04T08:50:23.383542Z",
     "iopub.status.idle": "2024-04-04T08:50:26.126005Z",
     "shell.execute_reply.started": "2024-04-04T08:50:23.383515Z",
     "shell.execute_reply": "2024-04-04T08:50:26.125094Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "It's clear that most essays contain fewer than 1000 words.\n\nFurthermore, one could hypothesize that the quality of an essay should be approximately uniform. Therefore, if one were to evaluate the first and second parts of an essay independently, they should be scored similarly. Thus, we can limit the essay length in the input to 500 words.\n\nThis also leads to the idea of multiple assessments for a single essay; we can split it into several chunks, evaluate them independently, and then take the final score as the mean or mode of the chunk scores.\n\nSo for now lets prepare utils for such an evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_prompt(essay):\n    \n    return f\"\"\"\nYour task is to score a student essay. \\n\\n\nRead the essay below carefully and assign it a score from the following range: [1, 2, 3, 4, 5, 6]. \\n\\n\n1 - is a bad essay, 6 - very good essay.\nAvoid additional text description in your answer. \\n\\n\nYour answer should consist of only a single digit score, for example: Answer: 4 \\n\\n\n\nEssay to evaluate: \\n\\n\n{essay}\n\"\"\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T09:04:24.173131Z",
     "iopub.execute_input": "2024-04-04T09:04:24.174076Z",
     "iopub.status.idle": "2024-04-04T09:04:24.178541Z",
     "shell.execute_reply.started": "2024-04-04T09:04:24.174044Z",
     "shell.execute_reply": "2024-04-04T09:04:24.177504Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_prompts(essay, chunk_size=100, max_chunks=5):\n    \"\"\"Represent essay as several chunks.\"\"\"\n    l = len(essay.split(\" \"))\n    n_chunks = l // chunk_size\n    essay_words = essay.split(\" \")\n    prompts = [get_prompt(\" \".join(essay_words[k*chunk_size: (k+1)*chunk_size])) for k in range(n_chunks)]\n    return prompts[:max_chunks]\n    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T09:04:23.13323Z",
     "iopub.execute_input": "2024-04-04T09:04:23.134118Z",
     "iopub.status.idle": "2024-04-04T09:04:23.140124Z",
     "shell.execute_reply.started": "2024-04-04T09:04:23.134087Z",
     "shell.execute_reply": "2024-04-04T09:04:23.139101Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def parse_output(decoded_output):\n    \"\"\"Parse digit evaluation from model output.\"\"\"\n    single_digits = re.findall(r'\\b\\d\\b', decoded_output)\n    single_digits = [int(s) for s in single_digits]\n    return single_digits",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:26.9538Z",
     "iopub.execute_input": "2024-04-04T08:50:26.954194Z",
     "iopub.status.idle": "2024-04-04T08:50:26.959967Z",
     "shell.execute_reply.started": "2024-04-04T08:50:26.954164Z",
     "shell.execute_reply": "2024-04-04T08:50:26.958809Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def evaluate_essay(essay, model, chunk_size=100, max_chunks=5):\n    \"\"\"Evaluate single essay by chunks.\"\"\"\n    scores = []\n    query_prompts = get_prompts(essay, chunk_size=chunk_size, max_chunks=max_chunks)\n    \n    for query_prompt in query_prompts:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": query_prompt\n            }\n        ]\n        \n        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n        with torch.no_grad():\n            encoded_output = model.generate(\n                inputs,\n                max_new_tokens=20,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        decoded_output = tokenizer.decode(\n            encoded_output[0],\n            skip_special_tokens=True\n        ).replace(query_prompt, '').replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n        \n        score = parse_output(decoded_output)\n        scores.extend(score)\n    \n    try:\n        score = int(np.mean(scores))\n    except ValueError:\n        score = 3\n    \n    return score",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:27.4311Z",
     "iopub.execute_input": "2024-04-04T08:50:27.431471Z",
     "iopub.status.idle": "2024-04-04T08:50:27.439248Z",
     "shell.execute_reply.started": "2024-04-04T08:50:27.431443Z",
     "shell.execute_reply": "2024-04-04T08:50:27.438228Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nconfig = AutoConfig.from_pretrained(MODEL_PATH)\nconfig.gradient_checkpointing = True\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n    config=config\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:39:12.265557Z",
     "iopub.execute_input": "2024-04-04T08:39:12.26607Z",
     "iopub.status.idle": "2024-04-04T08:50:07.449504Z",
     "shell.execute_reply.started": "2024-04-04T08:39:12.266037Z",
     "shell.execute_reply": "2024-04-04T08:50:07.448675Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\ntest.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:30.594495Z",
     "iopub.execute_input": "2024-04-04T08:50:30.594885Z",
     "iopub.status.idle": "2024-04-04T08:50:30.610639Z",
     "shell.execute_reply.started": "2024-04-04T08:50:30.594853Z",
     "shell.execute_reply": "2024-04-04T08:50:30.609589Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_submission(test, model, chunk_size=100, max_chunks=5):\n    ids = []\n    scores = []\n    for i in tqdm(range(len(test))):\n        \n        essay_id = test[\"essay_id\"].loc[i]\n        essay = test[\"full_text\"].loc[i]\n        \n        score = evaluate_essay(essay, model, chunk_size=chunk_size, max_chunks=max_chunks)\n        \n        scores.append(score)\n        ids.append(essay_id)\n        \n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    submission = pd.DataFrame()\n    submission[\"essay_id\"] = ids\n    submission[\"score\"] = scores\n    \n    return submission",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:33.311499Z",
     "iopub.execute_input": "2024-04-04T08:50:33.311853Z",
     "iopub.status.idle": "2024-04-04T08:50:33.318281Z",
     "shell.execute_reply.started": "2024-04-04T08:50:33.311825Z",
     "shell.execute_reply": "2024-04-04T08:50:33.317341Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "submission = create_submission(test, model, chunk_size=100, max_chunks=3)\nsubmission.to_csv(\"submission.csv\", index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T08:50:34.094242Z",
     "iopub.execute_input": "2024-04-04T08:50:34.094616Z",
     "iopub.status.idle": "2024-04-04T08:51:44.32849Z",
     "shell.execute_reply.started": "2024-04-04T08:50:34.094586Z",
     "shell.execute_reply": "2024-04-04T08:51:44.327581Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "For inference on the full test set, it should be optimized (at least with batchified inference). In the current version, it leads to a timeout error upon submission.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "submission.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-04T09:23:08.158212Z",
     "iopub.execute_input": "2024-04-04T09:23:08.158935Z",
     "iopub.status.idle": "2024-04-04T09:23:08.168326Z",
     "shell.execute_reply.started": "2024-04-04T09:23:08.158904Z",
     "shell.execute_reply": "2024-04-04T09:23:08.167338Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
