{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 38321,
     "databundleVersionId": 4196674,
     "sourceType": "competition"
    },
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 4155787,
     "sourceType": "datasetVersion",
     "datasetId": 2453564,
     "isSourceIdPinned": true
    },
    {
     "sourceId": 4175339,
     "sourceType": "datasetVersion",
     "datasetId": 2463739
    },
    {
     "sourceId": 4175399,
     "sourceType": "datasetVersion",
     "datasetId": 2463766
    },
    {
     "sourceId": 4175481,
     "sourceType": "datasetVersion",
     "datasetId": 2463813
    },
    {
     "sourceId": 4188681,
     "sourceType": "datasetVersion",
     "datasetId": 2470700
    },
    {
     "sourceId": 4188851,
     "sourceType": "datasetVersion",
     "datasetId": 2470788
    },
    {
     "sourceId": 4188894,
     "sourceType": "datasetVersion",
     "datasetId": 2470813
    },
    {
     "sourceId": 4199525,
     "sourceType": "datasetVersion",
     "datasetId": 2476300
    },
    {
     "sourceId": 4205352,
     "sourceType": "datasetVersion",
     "datasetId": 2479214
    },
    {
     "sourceId": 4225626,
     "sourceType": "datasetVersion",
     "datasetId": 2490591
    },
    {
     "sourceId": 4225745,
     "sourceType": "datasetVersion",
     "datasetId": 2490638
    },
    {
     "sourceId": 4247871,
     "sourceType": "datasetVersion",
     "datasetId": 2503176
    },
    {
     "sourceId": 4256181,
     "sourceType": "datasetVersion",
     "datasetId": 2508009
    },
    {
     "sourceId": 4256323,
     "sourceType": "datasetVersion",
     "datasetId": 2508107
    },
    {
     "sourceId": 8166166,
     "sourceType": "datasetVersion",
     "datasetId": 4832208
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "If you have time, please check my other notebooks.\n\n* Train : https://www.kaggle.com/kojimar/fb3-single-pytorch-model-train\n* Inference : https://www.kaggle.com/kojimar/fb3-single-pytorch-model-inference",
   "metadata": {
    "_uuid": "cf8604e7-c1fe-4d2c-bbab-8e395537ef3b",
    "_cell_guid": "e903a5fc-aa7e-4475-baa8-c661d5a84125",
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": "References\n* https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train\n* https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-inference",
   "metadata": {
    "_uuid": "c3f5955f-4e99-44ba-b4ef-a5983a61a85d",
    "_cell_guid": "96a34ceb-010d-4095-9699-ac117e0f9813",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "metadata": {
    "_uuid": "0ef252fc-6f8c-45c5-a70c-d93ada235e18",
    "_cell_guid": "d4a434af-3532-49a4-8ada-83147918f7cc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:26.978843Z",
     "iopub.execute_input": "2024-05-05T20:59:26.979248Z",
     "iopub.status.idle": "2024-05-05T20:59:47.669529Z",
     "shell.execute_reply.started": "2024-05-05T20:59:26.979213Z",
     "shell.execute_reply": "2024-05-05T20:59:47.668288Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# CFG",
   "metadata": {
    "_uuid": "3803376f-9520-49b2-8e5c-f5ca570b2cba",
    "_cell_guid": "ad858fce-47a7-48bb-9304-f5266e5a03ae",
    "papermill": {
     "duration": 0.003425,
     "end_time": "2022-09-08T02:59:40.459399",
     "exception": false,
     "start_time": "2022-09-08T02:59:40.455974",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "markdown",
   "source": "Deberta Family ver. 13\n\nSep. 28, LB 21th\n\n* CFG1 : 10 fold deberta-v3-base CV/LB: 0.4595/0.44\n* CFG2 : 10 fold deberta-v3-large CV/LB: 0.4553/0.44\n* CFG3 : 10 fold deberta-v2-xlarge CV/LB: 0.4604/0.44\n* CFG4 : 10 fold deberta-v3-base FGM CV/LB: 0.4590/0.44\n* CFG5 : 10 fold deberta-v3-large FGM CV/LB: 0.4564/0.44\n* CFG6 : 10 fold deberta-v2-xlarge CV/LB: 0.4666/0.44\n* CFG7 : 10 fold deberta-v2-xlarge-mnli CV/LB: 0.4675/0.44\n* CFG8 : 10 fold deberta-v3-large unscale CV/LB: 0.4616/0.43\n* CFG9 : 10 fold deberta-v3-large unscale CV/LB: 0.4548/0.43\n* CFG10 : 10 fold deberta-v3-large unscale CV/LB: 0.4569/0.43",
   "metadata": {
    "_uuid": "762aff0d-7674-4b7f-9c51-a89553c8dd52",
    "_cell_guid": "7124d03a-d387-4100-8ff5-0fa0d31af91a",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "class CFG1:\n    model = \"microsoft/deberta-v3-base\"\n    path = \"../input/0911-deberta-v3-base/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-base/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=24\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG2:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0911-deberta-v3-large/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG3:\n    model = \"microsoft/deberta-v2-xlarge\"\n    path = \"../input/0911-deberta-v2-xlarge/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n\nclass CFG4:\n    model = \"microsoft/deberta-v3-base\"\n    path = \"../input/0913-deberta-v3-base-fgm/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-base/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=24\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG5:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0914-deberta-v3-large-fgm/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG6:\n    model = \"microsoft/deberta-v2-xlarge\"\n    path = \"../input/0919-deberta-v2-xlarge/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG7:\n    model = \"microsoft/deberta-v2-xlarge-mnli\"\n    path = \"../input/0919-deberta-v2-xlarge-mnli/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG8:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0925-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG9:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0926-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG10:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"/kaggle/input/0927-deberta-v3-large-unscale/\"\n    base = \"/kaggle/input/fb3models/microsoft-deberta-v3-large/\"\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=16\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nCFG_list = [CFG10]",
   "metadata": {
    "_uuid": "83447fc1-f8fb-455c-af61-41368a18066e",
    "_cell_guid": "b9773321-38ec-4f7d-b0f6-b99dce2e6ee4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:47.671514Z",
     "iopub.execute_input": "2024-05-05T20:59:47.672116Z",
     "iopub.status.idle": "2024-05-05T20:59:55.807971Z",
     "shell.execute_reply.started": "2024-05-05T20:59:47.672086Z",
     "shell.execute_reply": "2024-05-05T20:59:55.806737Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Utils",
   "metadata": {
    "_uuid": "13018fc3-b78f-448a-aa9c-ff3642f94114",
    "_cell_guid": "14f1f2a9-154f-446d-bc1b-d32265473448",
    "papermill": {
     "duration": 0.003431,
     "end_time": "2022-09-08T02:59:41.602809",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.599378",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)",
   "metadata": {
    "_uuid": "bbc6c0bb-5792-4bfb-bf29-0f31810fc4d5",
    "_cell_guid": "655bde6e-05fa-49c2-8d31-a03039759f42",
    "collapsed": false,
    "papermill": {
     "duration": 0.019843,
     "end_time": "2022-09-08T02:59:41.626234",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.606391",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:55.809184Z",
     "iopub.execute_input": "2024-05-05T20:59:55.809517Z",
     "iopub.status.idle": "2024-05-05T20:59:55.823565Z",
     "shell.execute_reply.started": "2024-05-05T20:59:55.809488Z",
     "shell.execute_reply": "2024-05-05T20:59:55.822483Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# OOF",
   "metadata": {
    "_uuid": "0e87d261-60d4-4863-9f6c-23a39251dfde",
    "_cell_guid": "bf028b58-23ab-43aa-bba7-5a1e85893f31",
    "papermill": {
     "duration": 0.003606,
     "end_time": "2022-09-08T02:59:41.633413",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.629807",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "# ====================================================\n# oof\n# ====================================================\nfor CFG in CFG_list:\n    oof_df = pd.read_pickle(CFG.path+'oof_df.pkl')\n    labels = oof_df[CFG.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n    score, scores = get_score(labels, preds)\n    LOGGER.info(f'Model: {CFG.model} Score: {score:<.4f}  Scores: {scores}')",
   "metadata": {
    "_uuid": "40a1e775-3bf3-4877-b2d9-51b3495879a5",
    "_cell_guid": "b1179f7b-c8f5-4178-9594-ba83562ec238",
    "collapsed": false,
    "papermill": {
     "duration": 0.245898,
     "end_time": "2022-09-08T02:59:41.883068",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.63717",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:55.825937Z",
     "iopub.execute_input": "2024-05-05T20:59:55.826283Z",
     "iopub.status.idle": "2024-05-05T20:59:56.079245Z",
     "shell.execute_reply.started": "2024-05-05T20:59:55.826256Z",
     "shell.execute_reply": "2024-05-05T20:59:56.078075Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset",
   "metadata": {
    "_uuid": "57eb1f3b-697c-476a-bab4-505e02d4ae3b",
    "_cell_guid": "29d0d4b5-14c4-4198-90a5-a0b29e4c190a",
    "papermill": {
     "duration": 0.003671,
     "end_time": "2022-09-08T02:59:41.891081",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.88741",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        #max_length=CFG.max_len,\n        #pad_to_max_length=True,\n        #truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs",
   "metadata": {
    "_uuid": "000f9740-d5e9-4f78-871f-855481dac1bd",
    "_cell_guid": "ed631122-9f41-4293-a3bd-ba671f49093f",
    "collapsed": false,
    "papermill": {
     "duration": 0.014446,
     "end_time": "2022-09-08T02:59:41.909458",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.895012",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:56.081167Z",
     "iopub.execute_input": "2024-05-05T20:59:56.081815Z",
     "iopub.status.idle": "2024-05-05T20:59:56.088447Z",
     "shell.execute_reply.started": "2024-05-05T20:59:56.081782Z",
     "shell.execute_reply": "2024-05-05T20:59:56.087597Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model",
   "metadata": {
    "_uuid": "7a7114ea-31fa-4292-acad-6486c748c17e",
    "_cell_guid": "2c86a963-32b9-4b55-8067-ce7778750295",
    "papermill": {
     "duration": 0.003578,
     "end_time": "2022-09-08T02:59:41.916767",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.913189",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n        \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            LOGGER.info(self.config)\n        else:\n            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output",
   "metadata": {
    "_uuid": "0ac540d9-2666-4982-a1ca-e83ae7efeec1",
    "_cell_guid": "35030564-2dbb-4291-ba6c-684a3d594e9a",
    "collapsed": false,
    "papermill": {
     "duration": 0.024494,
     "end_time": "2022-09-08T02:59:41.945014",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.92052",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:56.09005Z",
     "iopub.execute_input": "2024-05-05T20:59:56.090414Z",
     "iopub.status.idle": "2024-05-05T20:59:56.109322Z",
     "shell.execute_reply.started": "2024-05-05T20:59:56.090385Z",
     "shell.execute_reply": "2024-05-05T20:59:56.108315Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# inference",
   "metadata": {
    "_uuid": "009870b6-06c0-460d-9f8a-670ff1a97e54",
    "_cell_guid": "59a6bdd2-ac39-4e52-aa95-c18bd3130715",
    "papermill": {
     "duration": 0.003537,
     "end_time": "2022-09-08T02:59:41.952301",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.948764",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions",
   "metadata": {
    "_uuid": "31769e2d-136f-4677-9def-1823f0a4cde8",
    "_cell_guid": "1aab8f28-8f47-4b36-bc4f-039d6419c845",
    "collapsed": false,
    "papermill": {
     "duration": 0.012906,
     "end_time": "2022-09-08T02:59:41.96896",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.956054",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:56.111003Z",
     "iopub.execute_input": "2024-05-05T20:59:56.111388Z",
     "iopub.status.idle": "2024-05-05T20:59:56.126845Z",
     "shell.execute_reply.started": "2024-05-05T20:59:56.111359Z",
     "shell.execute_reply": "2024-05-05T20:59:56.125402Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\n\n\n\ndef predict_chunk(test: pd.DataFrame) -> pd.DataFrame:\n\n    for _idx, CFG in enumerate(CFG_list):\n    #     test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n        \n        submission = pd.DataFrame(np.ones((len(test), 7)))\n        submission.columns = ['text_id','cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar','conventions']\n        submission[\"text_id\"] = test[\"text_id\"]\n        print(test[\"text_id\"].head(5))\n        print(submission[\"text_id\"].head(5))\n        # sort by length to speed up inference\n        test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n        test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n\n        test_dataset = TestDataset(CFG, test)\n        test_loader = DataLoader(test_dataset,\n                                 batch_size=CFG.batch_size,\n                                 shuffle=False,\n                                 collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                                 num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n        predictions = []\n        for fold in CFG.trn_fold:\n            model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n            print(model)\n            print(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n            state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                               map_location=torch.device('cpu'))\n            model.load_state_dict(state['model'], strict=False)\n            prediction = inference_fn(test_loader, model, device)\n            predictions.append(prediction)\n            del model, state, prediction; gc.collect()\n            torch.cuda.empty_cache()\n            break\n        predictions = np.mean(predictions, axis=0)\n        test[CFG.target_cols] = predictions\n        submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n#         display(submission.head())\n        submission[['text_id'] + CFG.target_cols].to_csv(f'submission_{_idx + 1}.csv', index=False)\n        torch.cuda.empty_cache()\n        gc.collect()\n        return submission\n\n         \ndef predict(test: pd.DataFrame) -> pd.DataFrame:\n    result = pd.DataFrame()\n    for i, chunk in enumerate(np.array_split(test, 10)):\n        if len(chunk) == 0:\n            continue\n        result = pd.concat(\n            [result, predict_chunk(chunk.reset_index(drop=True))]\n        ).reset_index(drop=True)\n    return result\n\nif __name__ == \"__main__\":\n    test = pd.read_csv(\n            '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n    ).rename(columns={\"essay_id\": \"text_id\"})\n    submission = predict(test)\n    submission.to_csv(f'submission.csv', index=False)",
   "metadata": {
    "_uuid": "5d502c19-e6f4-4aa1-9c07-10bd97495c84",
    "_cell_guid": "51acd9be-77c2-4a69-92db-c8760f435256",
    "collapsed": false,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 470.042141,
     "end_time": "2022-09-08T03:07:32.01481",
     "exception": false,
     "start_time": "2022-09-08T02:59:41.972669",
     "status": "completed"
    },
    "tags": [],
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-05T20:59:56.128521Z",
     "iopub.execute_input": "2024-05-05T20:59:56.128963Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
