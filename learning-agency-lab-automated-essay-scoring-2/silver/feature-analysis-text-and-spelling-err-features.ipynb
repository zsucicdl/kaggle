{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 2977194,
     "sourceType": "datasetVersion",
     "datasetId": 1825054
    },
    {
     "sourceId": 6258399,
     "sourceType": "datasetVersion",
     "datasetId": 3596984
    }
   ],
   "dockerImageVersionId": 30698,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Feature Analysis data processing affect on score\n\n***\n\n### Features Used\n1. **Text length**: Length of the given essay.\n2. **Word count**: Number of word in essay (tokenized using nltk TreebankWordDetokenizer).\n3. **Unique word count**: Number of unique words.\n4. **Spelling mistake count**: Number of spelling mistake (identified using spellchecker liberary).\n\n***\n\n### Model used:\n- Model used: LGBM\n- Metric: quadratic weighted kappa\n- Loss function: quadratic weighted kappa (given in **[link](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799)**)\n\n***\n\n### Data prosessing setting:\n1. **Full text without text processing**: \n    * The essay is used for feature generation without any preprocessing.\n2. **Full text with text processing**: \n    * The essay is used for feature generation with preprocessing.\n3. **Full text with text processing + contraction expansion**: \n    * The essay is used for feature generation with preprocessing, where preprocessing include contraction expension.\n4. **Full text with text processing + punctuation removal**: \n    * The essay is used for feature generation with preprocessing, where preprocessing include punctuation removal.\n4. **Full text with text processing + contraction expansion + punctuation removal**: \n    * The essay is used for feature generation with preprocessing, where preprocessing include contraction expansion and punctuation removal.\n\n***\n### Observations:\n1. The **text length** decrease significantly after the text processing, this is happing because the non-textual content is removed i.e. cleaning of text data is done. But after removing punctuation the increase in text length is observed.  **[Link](#graph)**\n2. The **word count** increases after removing punctuation. (this happened because after removing the punctuation the token the are considered as one word are broken into more than one word.) **[Link](#graph)**\n3. The **spelling mistakes** decreases significently after removing the puntuation and that because the puntuation may be causing a valid word into a spelling miskake. **[Link](#graph)**\n***\n\n### Score:\n| Data prosessing setting | Validation score | \n| :--- | :--- |\n| Full text without text processing | 0.7100431 |\n| Full text with text processing | 0.7134137 |\n| Full text with text processing + contraction expansion | 0.7134168 |\n| Full text with text processing + punctuation removal | 0.7175229 |\n| Full text with text processing + contraction expansion + punctuation removal | 0.7197249 |\n\n***\n\n### Reference:\nI would like to give thanks to the authors of these public notebooks. I have learned a lot from you.\n* https://www.kaggle.com/code/yongsukprasertsuk/0-818-deberta-v3-large-lgbm-baseline\n* https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799\n* https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Import modules",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-05-08T00:56:44.895227Z",
     "iopub.execute_input": "2024-05-08T00:56:44.89559Z",
     "iopub.status.idle": "2024-05-08T00:57:19.074549Z",
     "shell.execute_reply.started": "2024-05-08T00:56:44.895562Z",
     "shell.execute_reply": "2024-05-08T00:57:19.073084Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json\nimport string\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\n# logging setting \n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:19.076709Z",
     "iopub.execute_input": "2024-05-08T00:57:19.077034Z",
     "iopub.status.idle": "2024-05-08T00:57:42.88216Z",
     "shell.execute_reply.started": "2024-05-08T00:57:19.077006Z",
     "shell.execute_reply": "2024-05-08T00:57:42.881045Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:42.883701Z",
     "iopub.execute_input": "2024-05-08T00:57:42.884004Z",
     "iopub.status.idle": "2024-05-08T00:57:42.988395Z",
     "shell.execute_reply.started": "2024-05-08T00:57:42.883968Z",
     "shell.execute_reply": "2024-05-08T00:57:42.987398Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Load data initial configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# set random seed\ndef seed_everything(seed: int):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=42)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:42.991114Z",
     "iopub.execute_input": "2024-05-08T00:57:42.991365Z",
     "iopub.status.idle": "2024-05-08T00:57:43.000061Z",
     "shell.execute_reply.started": "2024-05-08T00:57:42.991336Z",
     "shell.execute_reply": "2024-05-08T00:57:42.999208Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class PATHS:\n    train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n    test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\n    sub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:43.00172Z",
     "iopub.execute_input": "2024-05-08T00:57:43.002166Z",
     "iopub.status.idle": "2024-05-08T00:57:43.008703Z",
     "shell.execute_reply.started": "2024-05-08T00:57:43.002135Z",
     "shell.execute_reply": "2024-05-08T00:57:43.00754Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv(PATHS.train_path)\ntrain.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:43.012376Z",
     "iopub.execute_input": "2024-05-08T00:57:43.012612Z",
     "iopub.status.idle": "2024-05-08T00:57:43.840559Z",
     "shell.execute_reply.started": "2024-05-08T00:57:43.01258Z",
     "shell.execute_reply": "2024-05-08T00:57:43.838483Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Data preprocessing functions definations\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\n\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    # \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    # \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    # \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    # \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    # \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    # \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    # \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    # \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:43.842136Z",
     "iopub.execute_input": "2024-05-08T00:57:43.842804Z",
     "iopub.status.idle": "2024-05-08T00:57:43.871692Z",
     "shell.execute_reply.started": "2024-05-08T00:57:43.842766Z",
     "shell.execute_reply": "2024-05-08T00:57:43.870611Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Feature",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Other feature\n\nSource: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class Preprocessor:\n    def __init__(self) -> None:\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        self.spellchecker = SpellChecker() \n\n    def spelling(self, text):\n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n        return amount_miss\n    \n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        data[\"text_tokens\"] = data[\"full_text\"].apply(lambda x: word_tokenize(x))\n        data[\"text_length\"] = data[\"full_text\"].apply(lambda x: len(x))\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        data[\"splling_err_num\"] = data[\"full_text\"].progress_apply(self.spelling)\n    \n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing(x))\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        data[\"text_length_p\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"word_count_p\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        data[\"unique_word_count_p\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        data[\"splling_err_num_p\"] = data[\"processed_text\"].progress_apply(self.spelling)\n    \n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract(x))\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        data[\"text_length_pc\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"word_count_pc\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        data[\"unique_word_count_pc\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        data[\"splling_err_num_pc\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        \n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_punct_remove(x))\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        data[\"text_length_ppr\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"word_count_ppr\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        data[\"unique_word_count_ppr\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        data[\"splling_err_num_ppr\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        \n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        data[\"text_length_pcpr\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        data[\"word_count_pcpr\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        data[\"unique_word_count_pcpr\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        data[\"splling_err_num_pcpr\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        data.drop(columns=[\"processed_text\", \"text_tokens\"], inplace=True)\n        return data\n    \npreprocessor = Preprocessor()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:43.873477Z",
     "iopub.execute_input": "2024-05-08T00:57:43.873722Z",
     "iopub.status.idle": "2024-05-08T00:57:43.981836Z",
     "shell.execute_reply.started": "2024-05-08T00:57:43.873694Z",
     "shell.execute_reply": "2024-05-08T00:57:43.980565Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats = preprocessor.run(train, mode=\"train\")\n\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T00:57:43.983188Z",
     "iopub.execute_input": "2024-05-08T00:57:43.983996Z",
     "iopub.status.idle": "2024-05-08T01:02:56.039554Z",
     "shell.execute_reply.started": "2024-05-08T00:57:43.983959Z",
     "shell.execute_reply": "2024-05-08T01:02:56.038531Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# EDA: Engineered features",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Paragraph Length Analysis\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "feats = [\n    \"text_length\",\"text_length_p\",\"text_length_pc\",\"text_length_ppr\",\"text_length_pcpr\",\n    \"word_count\",\"word_count_p\",\"word_count_pc\",\"word_count_ppr\",\"word_count_pcpr\",\n    \"unique_word_count\",\"unique_word_count_p\",\"unique_word_count_pc\",\"unique_word_count_ppr\",\"unique_word_count_pcpr\",\n    \"splling_err_num\",\"splling_err_num_p\",\"splling_err_num_pc\",\"splling_err_num_ppr\",\"splling_err_num_pcpr\"\n]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:02:56.044604Z",
     "iopub.execute_input": "2024-05-08T01:02:56.044901Z",
     "iopub.status.idle": "2024-05-08T01:02:56.052115Z",
     "shell.execute_reply.started": "2024-05-08T01:02:56.044868Z",
     "shell.execute_reply": "2024-05-08T01:02:56.050058Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sns.set(rc={'figure.figsize': (15, 15)})\ntrain_feats[feats + [\"score\"]].hist(bins=50);",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:02:56.05423Z",
     "iopub.execute_input": "2024-05-08T01:02:56.054491Z",
     "iopub.status.idle": "2024-05-08T01:03:03.819342Z",
     "shell.execute_reply.started": "2024-05-08T01:02:56.054464Z",
     "shell.execute_reply": "2024-05-08T01:03:03.81844Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id='graph'></a>\n### Observation:\n**From above charts**\n1. The **text length** decrease significantly after the text processing, this is happing because the non-textual content is removed i.e. cleaning of text data is done. But after removing punctuation the text length increases.\n2. The **word count** increases after removing punctuation, this happened because after removing the punctuation this may break token into more then one after removal of puntuation.\n3. The **spelling mistakes** decreases significently after removing the puntuation and that because the puntuation may be causing a valid word into a spelling miskake.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "correlation_matrix = train_feats[[\"score\"]+feats].corr()\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.2)\nplt.title('Correlation Matrix')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:03.821282Z",
     "iopub.execute_input": "2024-05-08T01:03:03.821804Z",
     "iopub.status.idle": "2024-05-08T01:03:05.711885Z",
     "shell.execute_reply.started": "2024-05-08T01:03:03.821765Z",
     "shell.execute_reply": "2024-05-08T01:03:05.710537Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for col_idx in range(0, len(feats), 5):\n    fig, axes = plt.subplots(1, 5, figsize = (25, 6))\n    for i in range(0, 5):\n        if col_idx+i < len(feats):\n            sns.scatterplot(ax=axes[i], data=train_feats, x=feats[col_idx+i], y='score', color='steelblue');",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:05.714232Z",
     "iopub.execute_input": "2024-05-08T01:03:05.714882Z",
     "iopub.status.idle": "2024-05-08T01:03:12.976918Z",
     "shell.execute_reply.started": "2024-05-08T01:03:05.714838Z",
     "shell.execute_reply": "2024-05-08T01:03:12.975893Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(12, 2))\nsns.boxplot(data=train_feats[feats[:5]], orient=\"h\")\nplt.figure(figsize=(12, 2))\nsns.boxplot(data=train_feats[feats[5:10]], orient=\"h\")\nplt.figure(figsize=(12, 2))\nsns.boxplot(data=train_feats[feats[10:15]], orient=\"h\")\nplt.figure(figsize=(12, 2))\nsns.boxplot(data=train_feats[feats[15:]], orient=\"h\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:12.978521Z",
     "iopub.execute_input": "2024-05-08T01:03:12.979017Z",
     "iopub.status.idle": "2024-05-08T01:03:15.381193Z",
     "shell.execute_reply.started": "2024-05-08T01:03:12.978986Z",
     "shell.execute_reply": "2024-05-08T01:03:15.38012Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Observation:\n* After removing the punctuation the word-count spikes have been reduced.\n* After text processing, contraction expension and punctuation removal the number of spelling mistakes are also reduced.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Testing the result with featues",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class CFG:\n    n_splits = 5\n    seed = 42\n    num_labels = 6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:15.382749Z",
     "iopub.execute_input": "2024-05-08T01:03:15.383006Z",
     "iopub.status.idle": "2024-05-08T01:03:15.388621Z",
     "shell.execute_reply.started": "2024-05-08T01:03:15.382975Z",
     "shell.execute_reply": "2024-05-08T01:03:15.387511Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\nfor i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"])):\n    train_feats.loc[val_index, \"fold\"] = i\nprint(train_feats.shape)\ntrain_feats.head(2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:15.390272Z",
     "iopub.execute_input": "2024-05-08T01:03:15.390882Z",
     "iopub.status.idle": "2024-05-08T01:03:15.434827Z",
     "shell.execute_reply.started": "2024-05-08T01:03:15.39084Z",
     "shell.execute_reply": "2024-05-08T01:03:15.433832Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Metric and loss function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:15.43697Z",
     "iopub.execute_input": "2024-05-08T01:03:15.437644Z",
     "iopub.status.idle": "2024-05-08T01:03:15.446567Z",
     "shell.execute_reply.started": "2024-05-08T01:03:15.437599Z",
     "shell.execute_reply": "2024-05-08T01:03:15.445404Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Testing : Features without any text processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "selected_featues = [\"text_length\",\"word_count\",\"unique_word_count\",\"splling_err_num\"]\ntrain_feats[selected_featues].head(2)",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:15.4483Z",
     "iopub.execute_input": "2024-05-08T01:03:15.448936Z",
     "iopub.status.idle": "2024-05-08T01:03:15.469759Z",
     "shell.execute_reply.started": "2024-05-08T01:03:15.448893Z",
     "shell.execute_reply": "2024-05-08T01:03:15.468524Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=42, verbosity = - 1\n    )\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold][selected_featues]\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:15.471305Z",
     "iopub.execute_input": "2024-05-08T01:03:15.472207Z",
     "iopub.status.idle": "2024-05-08T01:03:38.987366Z",
     "shell.execute_reply.started": "2024-05-08T01:03:15.47216Z",
     "shell.execute_reply": "2024-05-08T01:03:38.986169Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nrmse = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {rmse}\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:38.988894Z",
     "iopub.execute_input": "2024-05-08T01:03:38.98956Z",
     "iopub.status.idle": "2024-05-08T01:03:39.129214Z",
     "shell.execute_reply.started": "2024-05-08T01:03:38.989524Z",
     "shell.execute_reply": "2024-05-08T01:03:39.128061Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Testing : Features with text processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "selected_featues = [\"text_length_p\",\"word_count_p\",\"unique_word_count_p\",\"splling_err_num_p\"]\ntrain_feats[selected_featues].head(2)",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:39.131145Z",
     "iopub.execute_input": "2024-05-08T01:03:39.13171Z",
     "iopub.status.idle": "2024-05-08T01:03:39.145645Z",
     "shell.execute_reply.started": "2024-05-08T01:03:39.131668Z",
     "shell.execute_reply": "2024-05-08T01:03:39.1442Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=42, verbosity = - 1\n    )\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold][selected_featues]\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:03:39.147642Z",
     "iopub.execute_input": "2024-05-08T01:03:39.147994Z",
     "iopub.status.idle": "2024-05-08T01:04:02.855202Z",
     "shell.execute_reply.started": "2024-05-08T01:03:39.147951Z",
     "shell.execute_reply": "2024-05-08T01:04:02.854041Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nrmse = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {rmse}\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:02.857169Z",
     "iopub.execute_input": "2024-05-08T01:04:02.857428Z",
     "iopub.status.idle": "2024-05-08T01:04:03.007754Z",
     "shell.execute_reply.started": "2024-05-08T01:04:02.857399Z",
     "shell.execute_reply": "2024-05-08T01:04:03.006523Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Testing : Features with text processing + contraction expension",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "selected_featues = [\"text_length_pc\",\"word_count_pc\",\"unique_word_count_pc\",\"splling_err_num_pc\"]\ntrain_feats[selected_featues].head(2)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:03.009368Z",
     "iopub.execute_input": "2024-05-08T01:04:03.009687Z",
     "iopub.status.idle": "2024-05-08T01:04:03.024909Z",
     "shell.execute_reply.started": "2024-05-08T01:04:03.009648Z",
     "shell.execute_reply": "2024-05-08T01:04:03.023669Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=42, verbosity = - 1\n    )\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold][selected_featues]\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:03.026501Z",
     "iopub.execute_input": "2024-05-08T01:04:03.027432Z",
     "iopub.status.idle": "2024-05-08T01:04:29.412139Z",
     "shell.execute_reply.started": "2024-05-08T01:04:03.027397Z",
     "shell.execute_reply": "2024-05-08T01:04:29.411262Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nrmse = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {rmse}\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:29.413547Z",
     "iopub.execute_input": "2024-05-08T01:04:29.414186Z",
     "iopub.status.idle": "2024-05-08T01:04:29.56777Z",
     "shell.execute_reply.started": "2024-05-08T01:04:29.41414Z",
     "shell.execute_reply": "2024-05-08T01:04:29.566433Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Testing : Features with text processing + puntuation removal",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "selected_featues = [\"text_length_ppr\",\"word_count_ppr\",\"unique_word_count_ppr\",\"splling_err_num_ppr\"]\ntrain_feats[selected_featues].head(2)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:29.569483Z",
     "iopub.execute_input": "2024-05-08T01:04:29.569825Z",
     "iopub.status.idle": "2024-05-08T01:04:29.583727Z",
     "shell.execute_reply.started": "2024-05-08T01:04:29.569785Z",
     "shell.execute_reply": "2024-05-08T01:04:29.582621Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=42, verbosity = - 1\n    )\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold][selected_featues]\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:29.589667Z",
     "iopub.execute_input": "2024-05-08T01:04:29.59Z",
     "iopub.status.idle": "2024-05-08T01:04:50.216441Z",
     "shell.execute_reply.started": "2024-05-08T01:04:29.589971Z",
     "shell.execute_reply": "2024-05-08T01:04:50.215193Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nrmse = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {rmse}\")",
   "metadata": {
    "_kg_hide-input": false,
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:50.218674Z",
     "iopub.execute_input": "2024-05-08T01:04:50.219721Z",
     "iopub.status.idle": "2024-05-08T01:04:50.35938Z",
     "shell.execute_reply.started": "2024-05-08T01:04:50.219657Z",
     "shell.execute_reply": "2024-05-08T01:04:50.358041Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Testing : Features with text processing + contraction expension + punctuation removal",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "selected_featues = [\"text_length_pcpr\",\"word_count_pcpr\",\"unique_word_count_pcpr\",\"splling_err_num_pcpr\"]\ntrain_feats[selected_featues].head(2)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:50.360857Z",
     "iopub.execute_input": "2024-05-08T01:04:50.361152Z",
     "iopub.status.idle": "2024-05-08T01:04:50.378557Z",
     "shell.execute_reply.started": "2024-05-08T01:04:50.361118Z",
     "shell.execute_reply": "2024-05-08T01:04:50.377154Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=42, verbosity = - 1\n    )\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold][selected_featues]\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:04:50.379965Z",
     "iopub.execute_input": "2024-05-08T01:04:50.380244Z",
     "iopub.status.idle": "2024-05-08T01:05:12.428484Z",
     "shell.execute_reply.started": "2024-05-08T01:04:50.380211Z",
     "shell.execute_reply": "2024-05-08T01:05:12.425246Z"
    },
    "collapsed": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold][selected_featues]\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nrmse = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {rmse}\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T01:05:12.430773Z",
     "iopub.execute_input": "2024-05-08T01:05:12.431565Z",
     "iopub.status.idle": "2024-05-08T01:05:12.567517Z",
     "shell.execute_reply.started": "2024-05-08T01:05:12.431516Z",
     "shell.execute_reply": "2024-05-08T01:05:12.566356Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
