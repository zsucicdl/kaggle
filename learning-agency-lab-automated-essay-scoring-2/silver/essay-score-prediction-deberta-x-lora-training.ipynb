{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 4620664,
     "sourceType": "datasetVersion",
     "datasetId": 2663421
    },
    {
     "sourceId": 8575516,
     "sourceType": "datasetVersion",
     "datasetId": 5127900
    }
   ],
   "dockerImageVersionId": 30683,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Thank you for visiting my notebook!<br>\n\nThis notebook is a DeBERTa starter and a simple demonstration of the finetuning for those new to HuggingFace LLM. If you have any questions, please comment!<br>\n\n#### Summary:\n* MODEL: [deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base)\n* PROBLEM: 6-class classification\n* TRAINING: Hold-out method, \n\n#### Notes:\nThe following code is probably not the best way to go because it's just a simple baseline. As described in the last section (*What You Can Do Next*), there is plenty of room to improve the score.<br>\n\n\nEnjoy, kagglers!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare for Offline Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# !pip install /kaggle/input/lal-scoring-wheels/peft-0.10.0-py3-none-any.whl\n!pip install -q /kaggle/input/bnb-to-load-transformers-models/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n!pip install -q /kaggle/input/bnb-to-load-transformers-models/accelerate-0.30.1-py3-none-any.whl\n!pip install /kaggle/input/bnb-to-load-transformers-models/peft-0.11.1-py3-none-any.whl\n#!pip install -q /kaggle/input/bnb-to-load-transformers-models/transformers-4.41.2-py3-none-any.whl",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T11:28:02.60972Z",
     "iopub.execute_input": "2024-06-08T11:28:02.610303Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport random\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport datasets\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n\n# When using PEFT, comment out the below line.\nfrom peft import LoftQConfig, LoraConfig, TaskType, get_peft_model, PeftModel, PeftConfig",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class CFG:\n    n_labels = 6\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    seed = 1\n    \n    # ----- Model checkpoint -----\n    #model_ckpt = '/kaggle/input/deberta-v3-for-offline/base'\n    model_ckpt = '/kaggle/input/huggingfacedebertav3variants/deberta-v3-base'\n    # model_ckpt = 'microsoft/deberta-v3-base' # When 'INTERNET ON'\n    \n    # ----- Training params -----\n    max_input_length = 2000\n    use_peft = False\n    n_freeze = None\n    n_folds = 4 \n    learning_rate = 5.0e-5\n    warmup_ratio = 0.1\n    n_epochs = 2\n    train_batch_size = 4\n    eval_batch_size = 1\n    grad_accum_steps = 4\n    steps = 200\n    fp16 = True\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "DATA_DIR = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'\ndf = pd.read_csv(DATA_DIR + 'train.csv')\n\n# score: [1,2,3,4,5,6] -> label: [0,1,2,3,4,5]\ndf['label'] = df['score'].apply(lambda x: int(x - 1)).astype('uint8')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df['label'].value_counts()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Train Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(CFG.model_ckpt)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\ndef tokenize(batch):\n    tokenized_inputs = tokenizer(\n        batch['full_text'],\n        padding=False,\n        truncation=True,\n        max_length=CFG.max_input_length,\n    )\n    return tokenized_inputs\n\n\ndef model_init():\n    ### Load model from checkpoint\n    model = AutoModelForSequenceClassification.from_pretrained(\n        CFG.model_ckpt,\n        num_labels=CFG.n_labels,\n    ).to(CFG.device)\n    ### Freeze layers\n    if CFG.n_freeze is not None:\n        # embedding layer\n        for param in model.base_model.embeddings.parameters():\n            param.requires_grad = False\n        # eack encoder layer\n        for i in range(CFG.n_freeze):\n            for param in model.base_model.encoder.layer[i].parameters():\n                param.requires_grad = False\n    ### Create PEFT (LoRA) model\n    if CFG.use_peft:\n        loftq_config = LoftQConfig(loftq_bits=4)\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            inference_mode=False,\n            init_lora_weights='loftq',\n            loftq_config=loftq_config,\n            use_rslora=True,\n            #target_modules='all-linear',\n            r=16,\n            lora_alpha=8,\n            lora_dropout=0,\n        )\n        model = get_peft_model(model, peft_config)\n    return model\n\n\ndef compute_metrics(outputs):\n    predictions, labels = outputs\n    preds = np.argmax(predictions, axis=-1)\n#     print(f\"Predictions: {preds[:10]}\")\n#     print(f\"Labels: {labels[:10]}\")\n    qwk = cohen_kappa_score(\n        y1=labels, y2=preds,\n        labels=range(CFG.n_labels),\n        weights='quadratic'\n    )\n    return {'qwk': qwk}\n\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n",
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "\ndef print_trainable_params(model):\n    trainable_params = 0\n    all_params = 0\n    for _, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad == True:\n            trainable_params += param.numel()\n    \n    print(f\"trainable parameters: {trainable_params}, all parameters: {all_params}, ratio: {100 * trainable_params / all_params}%\")\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model = model_init()\nprint_trainable_params(model)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#print(model)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Set seed\nseed_everything(CFG.seed)\n\n### Cross Validation\nskf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(df, df['label'])):\n    # Split train/valid\n    df_train = df.loc[tr_idx, ['full_text', 'label']].copy()\n    df_valid = df.loc[va_idx, ['full_text', 'label']].copy()\n    print('#'*25, f\"Fold {fold}\", '#'*25)\n    # Prepare PyArrow dataset\n    ds_train = datasets.Dataset.from_pandas(df_train)\n    ds_valid = datasets.Dataset.from_pandas(df_valid)\n    # Tokenize\n    tokenized_ds_train = ds_train.map(tokenize, batched=True, batch_size=None)\n    tokenized_ds_valid = ds_valid.map(tokenize, batched=True, batch_size=None)\n    # Convert dataset's format: List -> Torch\n    tokenized_ds_train.set_format('torch')\n    tokenized_ds_valid.set_format('torch')\n    # Train\n    training_args = TrainingArguments(\n        output_dir='/kaggle/temp/',\n        overwrite_output_dir=True,\n        learning_rate=CFG.learning_rate,\n        warmup_ratio=CFG.warmup_ratio,\n        num_train_epochs=CFG.n_epochs,\n        per_device_train_batch_size=CFG.train_batch_size,\n        per_device_eval_batch_size=CFG.eval_batch_size,\n        gradient_accumulation_steps=CFG.grad_accum_steps,\n        gradient_checkpointing=True,\n        fp16=CFG.fp16,\n        logging_strategy='steps',\n        logging_steps=CFG.steps,\n        evaluation_strategy='steps',\n        eval_steps=CFG.steps,\n        save_strategy='steps',\n        save_steps=CFG.steps,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        report_to='none',\n        seed=CFG.seed,\n        )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_ds_train,\n        eval_dataset=tokenized_ds_valid,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    #model.config.use_cache = False \n    trainer.train()\n    \n    # Only 1 fold to save time.\n    # Of course, you can comment out 'break' to perform cross-validation.\n    #break",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#trainer.save_model(\"peft_model\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Infer Test Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare test data\ndf_test = pd.read_csv(DATA_DIR + 'test.csv')\nds_test = datasets.Dataset.from_pandas(df_test[['full_text']])\ntokenized_ds_test = ds_test.map(tokenize, batched=True, batch_size=None)\ntokenized_ds_test.set_format('torch')\n\n",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Predict\noutputs = trainer.predict(tokenized_ds_test)\npredictions = torch.softmax(torch.from_numpy(outputs.predictions), dim=-1).numpy()\npreds = np.argmax(predictions, axis=-1)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Submit",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Don't forget to add 1 to preds\n# label: [0,1,2,3,4,5] -> score[1,2,3,4,5,6]\ndf_test['score'] = preds + 1\ndf_test.drop(columns=['full_text'],inplace=True)\ndf_test[['essay_id', 'score']].to_csv('submission.csv', index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_test.dtypes",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_test",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# What You Can Do Next",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Tips for improving score:\n* Cross-validate and average those predictions\n* Tune hyperparameters (epoch, batch size, learning rate, etc.)\n* Freeze layers\n* Try *PEFT* for fast/cost-efficient training\n* Try a larger model such as deberta-v3-*large*\n* Try a *regression* model instead of a classification model\n\netc.",
   "metadata": {}
  }
 ]
}
