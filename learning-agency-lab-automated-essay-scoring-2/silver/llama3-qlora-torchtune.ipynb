{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8246533,"sourceType":"datasetVersion","datasetId":4891771},{"sourceId":33144,"sourceType":"modelInstanceVersion","modelInstanceId":27730},{"sourceId":33547,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28079}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customizable pytorch torchtune QLora LLM training pipeline\n\n## Why this notebook ?\n\nThis notebook is a continuation of my LLM pipeline sharing here : https://www.kaggle.com/code/optimo/llm-simple-lora-customizable-pipeline-train-infer\n\nIn this notebook I explore the use of torchtune library: https://github.com/pytorch/torchtune\nTorchtune is an amazing initiative from the pytorch developpers to create a solid LLM tuning library, which implements only the proven methods that will stay overtime to train LLMs. You won't find the latest methods but all the workings ones, with simple implementations of LORA, QLORA and quantization techniques. The library is improving fast so new things are coming every week!\n\nAs my objective is still to be in controle of what is going on, I won't show how to use their single line recipes which allow to finetune a model. In this notebook, I'll show an end to end pipeline which finetunes a LLAMA3 8B model with QLora, the quantized version of LORA.\n\n## Technical overview\n\n- I am here use torchtune to define the qlora llama3 network, but I still use the tokenizers from hugging face for convenience.\n- All the training is done in bfloat16 which saves a lot of GPU memory\n- The full training is going to be prohibitively long with Kaggle kernels. It takes 2:30 for one epoch with an A6000. This is just an example on how to use QLora. You can can simply set \"quantize\" to False and things will be much faster.\n\n## Additional tips\n\n- If you with to do this with your local setup you can download the models by running:\n`tune download company/model --output-dir your/path --hf-token \"yourHFtoken\" --ignore-patterns \"\"`\n- I am here showing how to use QLora, quantization allows you to train with larger backbones, but it comes at the cost of slower training because you need to decompress the weights each time to compute the gradients in bfloat16. Simply using LORA and bfloat16 should be enough to infer in a kaggle notebook with a 8B parameters model.\n\n## further improvements\n- saving strategy: in this notebook I save all the weights i.e. LLM bacbone + the LORA weights. This is not ideal since it takes more space the the original LLM and it takes longer to infer because you are doing the inference on the original architecture and the LORA adapter (one addition per linear adapter). You can refer to torchtune to see how to merge the adapter to the original weights to improve that.\n- loading gemma or phi3: in this notebook I use `FullModelMetaCheckpointer`, if you wish to load weights in the hugging face format you'll need to use `FullModelHFCheckpointer` with minor changes.\n\nHappy Kaggling!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# torchtune dependencies\n!pip install /kaggle/input/torchtune-wheels/torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/omegaconf-2.3.0-py3-none-any.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/torchao-0.1-py3-none-any.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/antlr4_python3_runtime-4.9.3-py3-none-any.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/torchtune-0.1.1-py3-none-any.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n!pip install /kaggle/input/torchtune-wheels/scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport time\nimport datetime\nimport os\nimport copy","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:04.239441Z","iopub.execute_input":"2024-05-05T13:39:04.24013Z","iopub.status.idle":"2024-05-05T13:39:06.140371Z","shell.execute_reply.started":"2024-05-05T13:39:04.240092Z","shell.execute_reply":"2024-05-05T13:39:06.13951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment configuration\nOnce your set up is finalized you only need to play with hyperparameters to find the best model","metadata":{}},{"cell_type":"code","source":"class Config():\n    def __init__(self):\n        # Parameters related to the problem\n        self.num_classes = 1 # 1 class for regression\n        \n        # Parameters related to network\n        self.original_checkpoint_path = \"/kaggle/input/llama-3/pytorch/8b/1\"\n        self.original_checkpoint_file_path = \"/kaggle/input/llama-3/pytorch/8b/1/consolidated.00.pth\"\n        self.network_type = \"llama3\"\n        self.architecture = {\"backbone\": \"/kaggle/input/llama-3/transformers/8b-hf/1\", # this is only used for tokenizers (from HF)\n                             \"params\": {}}\n\n        self.remove_layers = 16 # number of layer to remove to make the model smaller\n        self.freeze_layers = None # number of layers to freeze to reduce number of training parameters\n        self.lora_config = {\"r\": 8, # rank of the decomposed matrix (higher means less memory saving)\n                            \"lora_alpha\": 16, # scaling factor, should be 2xr according to https://www.entrypointai.com/blog/lora-fine-tuning/\n                            \"lora_dropout\": 0.05,\n                            # make sure that you name correctly your modules according to your backbone\n                            # you should spot the linear layers in the attention blocks\n                            \"target_modules\": ['q_proj', 'v_proj', 'k_proj', 'output_proj'],\n                            \"quantize\" : True, # True for QLora, False for LORA\n                           }\n        self.attn_dropout = 0.05\n        self.computation_type = \"bfloat16\" \n        self.token_info = {\"padding\" :\"longest\", # batch are going to be the length of longest sequence\n                           \"max_length\" : 1024, # max training sample length\n                           \"truncation\": True,\n                           \"pad_to_multiple_of\" : 512 # I heard that modern GPUs are fastest with multiple of 512? is that True?\n                          }\n\n        # Parameters related to training\n        self.max_epochs = 2 # number of epochs\n\n        self.initial_lr =3e-4\n        self.optimizer_name = \"AdamW\" #\"AdamW\" # try 8 bit adam AdamW8bit     \n        self.optimizer_params = {\"lr\": self.initial_lr, \n                                 \"weight_decay\":1e-2\n                                }\n        self.loss_config = {\"loss_name\" : \"MSELoss\",\n                            \"reduction\":\"mean\",\n                           }\n        \n        self.scheduler_name = \"OneCycleLR\"\n        self.steps_per_epochs = -1 # this is automatically overwritten\n        self.scheduler_params={\n                              \"max_lr\":self.optimizer_params[\"lr\"] if type(self.optimizer_params)==dict else self.optimizer_params[-1][\"lr\"],\n                               \"div_factor\":10,\n                              \"steps_per_epoch\": self.steps_per_epochs,\n                              \"final_div_factor\":1e2, #1e2\n                               \"anneal_strategy\":\"cos\", #\"cos\"\n                               \"three_phase\" : False,\n                              \"pct_start\":0.1, #0.3\n                              \"epochs\": self.max_epochs}\n        \n        \n        self.eval_on_train = False # You might want to compute the exact metric on training set to monitor overfitting\n        self.batch_size = 1 #2 # Let's start small\n        self.gradient_accumulation = 16 // self.batch_size # this allows you to train with low batch size but compute gradients on more that a few samples\n        self.mixed_precision = True\n        self.num_workers = 2 # I think num_workers for kaggle environment should be kept low\n        self.pin_memory = True\n        self.clip_value = 10.0\n\n        # parameters related to logs\n        self.verbose = 1 # how often do you want to compute the competition metric?\n        self.save_path = \"./torch-tune-logs\"\n\nPATH_TO_DATA = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2\"\nexp_config = Config()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:06.141806Z","iopub.execute_input":"2024-05-05T13:39:06.142181Z","iopub.status.idle":"2024-05-05T13:39:06.154944Z","shell.execute_reply.started":"2024-05-05T13:39:06.142155Z","shell.execute_reply":"2024-05-05T13:39:06.154081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define datasets\n\nNothing different from my previous notebook here, you simply need to make sure two things:\n- your tokenizer has a pad_token and and eos_token\n- your tokenizer uses `padding_side='right` : otherwise the pooling method I use won't work correctly when using batch sizes > 1","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom torch.utils.data import DataLoader, Dataset\nfrom typing import Optional, Union, Any\nfrom transformers import DataCollatorWithPadding\n\n\nfrom transformers import AutoTokenizer\n\ndef define_tokenizer(cfg):\n    \"\"\"\n    Let's use basic AutoTokenizer\n    \"\"\"\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.architecture[\"backbone\"], trust_remote_code=True)\n\n    # Make sure that we have a pad token and and eos token that will be used for pooling\n    if tokenizer.pad_token is None:\n        print(\"Setting new pad token\")\n        # pad token is missig        \n        tokenizer.pad_token=\"<|reserved_special_token_0|>\"\n        \n    if tokenizer.eos_token is None:\n        print(\"Setting new eos_token token\")\n        # eos_token token is missig\n        tokenizer.eos_token=\"<|reserved_special_token_1|>\"\n    \n    # Make sure that padding is always \"right\"\n    if tokenizer.padding_side != \"right\":\n        print(f\"Changing padding side from {tokenizer.padding_side} to 'right'\")\n        tokenizer.padding_side = \"right\"\n    return tokenizer\n    \nclass LALDataset(Dataset):\n    \"\"\"\n    There are simpler ways of creating a dataset nowadays (using datasets library for example).\n    But I prefer to define it that way as I feel more in control of what is actually happening.\n    Here the dataset is very simple, but more customization could be done.\n    \n    If there is a good reason not to do that and use more recent methods please let me know!\n    \"\"\"\n    def __init__(self, df, config, inference, remove=True):\n        \"\"\"\n        df: pandas dataframe\n        config: experiment config\n        inference (bool): are we in inference mode ?\n        remove (bool): should we remove unecessary columns that might not colate correctly?\n        \"\"\"\n        self.df = df\n        # tokenizer needs to be defined as it's used by datacollator\n        self.tokenizer = define_tokenizer(config)\n        self.inference = inference\n        self.config = config\n        self.remove = remove\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, row_idx):\n        \n        full_text = self.df.loc[row_idx, \"full_text\"]\n        \n        tokenized_text = self.tokenizer(full_text,\n                                        return_offsets_mapping=False, # mostly needed for entity recognition\n                                        truncation=self.config.token_info[\"truncation\"],\n                                        max_length=self.config.token_info[\"max_length\"])\n        \n        labels = self.df.loc[row_idx, \"score\"]\n            \n        # here we append eos token at the end which will work as a CLS token\n        # note that this must be the last token as GPT like models have a causal attention\n        tokenized_text.input_ids.append(self.tokenizer.eos_token_id)\n        tokenized_text.attention_mask.append(1)\n       \n        out_dict = {\n                \"input_ids\": tokenized_text.input_ids,\n                \"attention_mask\": tokenized_text.attention_mask,\n                \"labels\": torch.Tensor([labels])\n            }\n        return out_dict\n\n\ndef define_loader(dataset,\n                  config,\n                  inference,\n                 ):\n    \"\"\"\n    Use config and inference mode to create dataloader for train and test.\n    \"\"\"\n    num_workers = config.num_workers\n    pin_memory = config.pin_memory\n    \n    # collate_fn = None\n    # we use here a basic data collator\n    collate_fn = DataCollatorWithPadding(tokenizer=dataset.tokenizer,\n                                         padding=config.token_info[\"padding\"],\n                                         max_length=config.token_info[\"max_length\"],\n                                         pad_to_multiple_of=config.token_info[\"pad_to_multiple_of\"]\n                                    )\n\n\n    loader = DataLoader(\n                dataset,\n                batch_size=config.batch_size,\n                shuffle=not inference,\n                drop_last=not inference,\n                num_workers=num_workers,\n                pin_memory=pin_memory, \n                collate_fn=collate_fn,\n                # worker_init_fn=worker_init_fn,\n            )\n    return loader\n\n\ndef get_dataset_and_loader(df, config, inference, remove=True):\n    \"\"\"\n    Returns both dataset and dataloader\n    \"\"\"\n    dataset = LALDataset(df, config, inference, remove=remove)\n    loader = define_loader(dataset, config, inference)\n    return dataset, loader\n\ndef create_loaders(df, train_idx, valid_idx, config, eval_on_train):\n    \n    # You can set larger max length for inference\n    valid_config = copy.deepcopy(config)\n    valid_config.token_info['max_length'] = config.token_info['max_length']\n    \n    _, train_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n                                        config=config,\n                                        inference=False,\n                                        )\n\n    _, valid_dl = get_dataset_and_loader(df=df.iloc[valid_idx].reset_index(drop=True),\n                                        config=valid_config,\n                                        inference=True)\n\n    if eval_on_train:\n        _, train_aux_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n                                                config=valid_config,\n                                                inference=True)\n\n        eval_loaders = [train_aux_dl, valid_dl]\n        eval_names = [\"train\", \"valid\"]\n    else:\n        eval_loaders = [valid_dl]\n        eval_names = [\"valid\"]\n    return train_dl, valid_dl, eval_loaders, eval_names","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:06.533965Z","iopub.execute_input":"2024-05-05T13:39:06.534341Z","iopub.status.idle":"2024-05-05T13:39:10.594401Z","shell.execute_reply.started":"2024-05-05T13:39:06.534309Z","shell.execute_reply":"2024-05-05T13:39:10.593569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define network's architecture\n\nHere we use a simple architecture composed of an LLM backbone finetuned with QLORA and a linear head.\nWe only use the last eos_token to predict the final score.\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\nfrom transformers import AutoConfig\nimport gc\nfrom torchtune.modules.peft.peft_utils import (\n    get_adapter_params,\n    get_merged_lora_ckpt,\n    set_trainable_params,\n    validate_missing_and_unexpected_for_lora,\n)\nfrom torchtune.models.convert_weights import hf_to_tune, meta_to_tune\nfrom torchtune.models.llama3._component_builders import lora_llama3\nfrom torchtune.utils import FullModelMetaCheckpointer, FullModelHFCheckpointer\n\n\nimport torch\nfrom torchtune import config, modules, utils\n\nclass CustomtorchtuneLLM(torch.nn.Module):\n    \"\"\"\n    Here is where you can customize your architecture\n    \"\"\"\n    def __init__(self, cfg, eos_token_id, device=\"cuda\"):\n        super().__init__()\n\n        self.computation_type = getattr(torch, cfg.computation_type)\n        self.num_classes = cfg.num_classes\n        self.eos_token_id = eos_token_id\n        self.model_config = AutoConfig.from_pretrained(\n                cfg.architecture[\"backbone\"],\n            )\n\n        self.activation = torch.nn.Identity() # Activation could be differnt\n        # get a backbone for our network\n        # Here let's go with AutoModelForCausalLM, it does not matter as we take remove the final layers\n        _device = utils.get_device(device=device)\n        with utils.set_default_dtype(self.computation_type), _device :\n            NUM_LAYERS = 32 if cfg.remove_layers is None else 32 - cfg.remove_layers\n            # Here we remove layers by simply creating a smaller model with less layers\n            print(f\"{NUM_LAYERS=}/32\")\n            self.backbone = lora_llama3(\n                                    lora_attn_modules=cfg.lora_config[\"target_modules\"],\n                                    apply_lora_to_mlp=True,\n                                    apply_lora_to_output=False,\n                                    vocab_size=128_256,\n                                    num_layers=NUM_LAYERS,\n                                    num_heads=32,\n                                    num_kv_heads=8,\n                                    embed_dim=4096,\n                                    max_seq_len=8192,\n                                    intermediate_dim=14336,\n                                    attn_dropout=cfg.attn_dropout,\n                                    norm_eps=1e-5,\n                                    rope_base=500000.0,\n                                    lora_rank=cfg.lora_config[\"r\"],\n                                    lora_alpha=cfg.lora_config[\"lora_alpha\"],\n                                    lora_dropout=cfg.lora_config[\"lora_dropout\"],\n                                    quantize_base=cfg.lora_config[\"quantize\"],\n                        )\n\n        adapter_params = get_adapter_params(self.backbone)\n        \n        # this will only set the adpater params to require_grad=True -> should be careful of having those extra layers in requires grad;\n        set_trainable_params(self.backbone, adapter_params)\n        \n        checkpointer = FullModelMetaCheckpointer(cfg.original_checkpoint_path,\n                                               checkpoint_files=[cfg.original_checkpoint_file_path,\n                                                                ],\n                                               model_type=cfg.network_type,\n                                               output_dir=\"../../logs/\")\n        \n        # As stated here I will have a problem of state dicts namings because I am using hf format\n        # https://pytorch.org/torchtune/stable/deep_dives/checkpointer.html#understand-checkpointer\n        \n        base_model_state_dict = checkpointer.load_checkpoint()\n        \n        # this will update all the pretrained weights from the original LLM\n        base_missing, base_unexpected = self.backbone.load_state_dict(\n                    base_model_state_dict[\"model\"], strict=False\n                )\n        # This is the case when the lora model has already been pretrained -> we load the corresponding weights\n        lora_weights_state_dict = None\n        if lora_weights_state_dict:\n            lora_missing, lora_unexpected = self.backbone.load_state_dict(\n                lora_weights_state_dict, strict=False\n            )\n        else:\n            lora_missing, lora_unexpected = None, None\n        \n        utils.validate_expected_param_dtype(\n                    adapter_params.items(), dtype=self.computation_type\n                )\n        del base_model_state_dict\n        # remove the head as we are going to use a custom head\n        self.backbone.output = torch.nn.Identity()\n\n        torch.cuda.empty_cache()      \n        \n        if hasattr(cfg, \"num_layers_to_freeze\"):\n            print(f\"freezing {cfg.num_layers_to_freez} layers.\")\n            if cfg.num_layers_to_freeze > 0:\n                if cfg.freeze_embeddings:\n                    # should you train embeddings?\n                    for param in self.backbone.tok_embeddings.parameters():\n                        param.requires_grad = False\n                # Here the first layers are frozen: only remaining last layers will be trained\n                for layer in self.backbone.layers[:cfg.num_layers_to_freeze]:\n                    for param in layer.parameters():\n                        param.requires_grad = False\n                \n        gc.collect()\n        torch.cuda.empty_cache()\n        # this is for gradient checkpoint, left for later\n        # self.transformers_model.gradient_checkpointing_enable()\n                    \n        self.final_linear = torch.nn.Linear(self.model_config.hidden_size, cfg.num_classes)\n\n    def forward(self, batch):\n        x = batch[\"input_ids\"] # (bs, num_tokens)\n\n        # this assumes that you only have one eos_token per example\n        eos_positions = torch.argwhere(x == self.eos_token_id)[:, 1]\n\n        x = self.backbone(\n            tokens=x,\n            # attention_mask=batch[\"attention_mask\"],\n        )# (bs, num_tokens, hidden_size)\n        # we are only interested in the eos_token\n        x = x[torch.arange(x.shape[0]), eos_positions] # (bs, hidden_size)\n        logits = self.final_linear(x) # (bs, num_classes)\n\n        return {\"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:12.662451Z","iopub.execute_input":"2024-05-05T13:39:12.663289Z","iopub.status.idle":"2024-05-05T13:39:14.515721Z","shell.execute_reply.started":"2024-05-05T13:39:12.663253Z","shell.execute_reply":"2024-05-05T13:39:14.514731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training recipe\n\nYou may need to change this if you make significant changes in your modelling apporach","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List, Any, Dict\nfrom torch.nn.utils import clip_grad_norm_\nfrom abc import abstractmethod\nfrom sklearn.base import BaseEstimator\nimport json\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport copy\n\n# Layers to which we do not want to apply weight decay with AdamW\nALL_LAYERNORM_LAYERS = [torch.nn.LayerNorm, torch.nn.Embedding]\n\n\ndef get_parameter_names(network, forbidden_layer_types):\n        \"\"\"\n        Returns the names of the model parameters that are not inside a forbidden layer.\n        \"\"\"\n        result = []\n        for name, child in network.named_children():\n            result += [\n                f\"{name}.{n}\"\n                for n in get_parameter_names(child, forbidden_layer_types)\n                if not isinstance(child, tuple(forbidden_layer_types))\n            ]\n        # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n        result += list(network._parameters.keys())\n        return result\n    \ndef define_loss_function(loss_config):\n    \"\"\"\n    Basic torch loss functions or locally defined loss\n    \"\"\"\n    copy_config = copy.copy(loss_config)\n    loss_name = copy_config.pop('loss_name')\n    try:\n        loss_fn = getattr(torch.nn, loss_name)(**copy_config)\n    except AttributeError:\n        try:\n            loss_fn = globals().get(loss_name)(copy_config)\n        except:\n            raise NotImplementedError(\"Unkown loss function :\", loss_name)\n    return loss_fn\n\ndef prepare_log_folder(log_path):\n    \"\"\"\n    Utility function to create experiment folder\n    Creates the directory for logging.\n    Logs will be saved at log_path/date_of_day/exp_id\n\n    Args:\n        log_path (str): Directory\n\n    Returns:\n        str: Path to the created log folder\n    \"\"\"\n    today = str(datetime.date.today())\n    log_today = os.path.join(log_path, today)\n\n    if not os.path.exists(log_today):\n        Path(log_today).mkdir(parents=True)\n\n    exp_id = (\n        np.max([int(f) if str(f).isdigit() else -1 for f in os.listdir(log_today)]) + 1\n        if len(os.listdir(log_today))\n        else 0\n    )\n    log_folder = os.path.join(log_today, f\"{exp_id}\")\n\n    assert not os.path.exists(log_folder), \"Experiment already exists\"\n    os.mkdir(log_folder)\n    print(\"Saving logs at :\", log_folder)\n    return log_folder\n\ndef save_config(config, folder):\n    \"\"\"\n    Saves a config as a json, copies data and model configs.\n\n    Args:\n        config (Config): Config.\n        folder (str): Folder to save at.\n    \"\"\"\n    with open(os.path.join(folder, \"config.json\"), \"w\") as f:\n        json.dump(config.__dict__.copy(), f)\n\n@dataclass\nclass AbstractBaseModel(BaseEstimator):\n    \"\"\" Abstract class for scikit-like model.\n        Allows to build upon to train, infer, save, load etc..\n    \"\"\"\n\n    network: torch.nn.Module = None\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    seed: int = 42\n    mixed_precision: bool = False\n\n    def __post_init__(self):\n        torch.manual_seed(self.seed)\n        self.network = self.network.to(self.device)\n\n    def fit(\n        self,\n        train_dataloader,\n        eval_loaders=None,\n        eval_names=None,\n        eval_metric=None,\n        loss_config=None,\n        max_epochs=100,\n        callbacks=None,\n        optimizer_name=\"Adam\",\n        optimizer_params={\"lr\": 1e-3},\n        gradient_accumulation=None,\n        scheduler_name=None,\n        scheduler_params=None,\n        mixed_precision=False,\n        clip_value=None,\n        log_path=None,\n        verbose=1,\n    ):\n        \"\"\"\n        Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        eval_loaders for validation.\n        Parameters\n        ----------\n        train_dataloader : Dataloader\n            Train set\n        eval_loader : list of dataloaders\n            The last one is used for early stopping\n        eval_name : list of str\n            List of eval set names.\n        eval_metric : list of str\n            List of evaluation metrics.\n            The last metric is used for early stopping.\n        loss_name : Name\n            a PyTorch loss function name\n        weights : bool or dictionnary\n            0 for no balancing\n            1 for automated balancing\n            dict for custom weights per class\n        max_epochs : int\n            Maximum number of epochs during training\n        batch_size : int\n            Training batch size\n        num_workers : int\n            Number of workers used in torch.utils.data.DataLoader\n        drop_last : bool\n            Whether to drop last batch during training\n        callbacks : list of callback function\n            List of custom callbacks\n        pin_memory: bool\n            Whether to set pin_memory to True or False during training\n        from_unsupervised: unsupervised trained model\n            Use a previously self supervised model as starting weights\n        clip_value: float (default to None)\n            Gradient clipping\n        \"\"\"\n        # update model name\n\n        self.max_epochs = max_epochs\n        self._stop_training = False\n        self.optimizer_name = optimizer_name\n        self.optimizer_params = optimizer_params\n        eval_loaders = eval_loaders if eval_loaders else []       \n        self.mixed_precision = mixed_precision\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.gradient_accumulation = gradient_accumulation\n        self.metrics = eval_metric\n        \n        if loss_config is None:\n            raise(NotImplementedError, \"Please specifify a loss\")\n        else:\n            self.loss_fn = define_loss_function(loss_config)\n        \n        self._set_optimizer()\n        \n        # scheduler\n        self.scheduler_fn = getattr(torch.optim.lr_scheduler, scheduler_name) # this will only accept torch schedulers\n        self.scheduler_params = copy.copy(scheduler_params)\n        self.scheduler = self.scheduler_fn(self._optimizer, **self.scheduler_params)\n        \n        \n        # Training loop over epochs\n        start_time = time.time()\n        for epoch_idx in range(self.max_epochs):\n            self.epoch_idx = epoch_idx\n            epoch_loss, epoch_lr = self._train_epoch(train_dataloader)\n            msg = f\"epoch {epoch_idx:<3} | lr: {epoch_lr:.2e} | loss: {epoch_loss:.4f} \"\n            # Apply predict epoch to all eval sets\n            if ((self.verbose != 0) and (epoch_idx % self.verbose == 0)) or (epoch_idx==self.max_epochs-1):\n                for eval_name, valid_dataloader in zip(eval_names, eval_loaders):\n                    with torch.no_grad():\n                        prob_pred, prob_true, scores = self._predict_epoch(eval_name, valid_dataloader)\n                    for metric_name, metric_score in scores:\n                        msg += f\"| {metric_name:<3} ({eval_name}): {metric_score:.4f} \"\n            total_time = int(time.time() - start_time)\n            msg += f\"|  {str(datetime.timedelta(seconds=total_time)) + 's':<6}\"\n            print(msg)\n        print(\"End of training!\")\n        self.network.eval()\n        return prob_pred, prob_true\n        \n    def predict_proba(self, dataloader, return_target=False):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n        X : a :tensor: `torch.Tensor`\n            Input data\n        Returns\n        -------\n        predictions : np.array\n            Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        results_prob = []\n        results_targets = []\n        pbar = tqdm(dataloader,\n                     leave=False,\n                     total=len(dataloader),\n                     desc=f'Inference')\n        \n        with torch.no_grad():\n            for batch in pbar:\n                out_probs = self._predict_batch(batch).cpu()\n                results_prob.append(out_probs)\n\n                if return_target:\n                    targets = batch[\"labels\"]\n                    targets = targets.to(\"cpu\").detach()\n                    results_targets.append(targets)\n\n        res_prob = self.stack_preds(results_prob)                \n        if return_target:\n            res_target = self.stack_targets(results_targets)\n            return res_prob, res_target\n        else:\n            return res_prob\n\n    def save_model(self, path, model_name):\n        \"\"\"\n        Save the model somewhere\n\n        Users can specify both the path and model_name\n        If no model_name given an automatic one will be creted\n        \"\"\"\n        Path(path).mkdir(parents=True, exist_ok=True)\n        # Save state_dict with half precision for less gpu usage during inference\n        torch.save(self.network.state_dict(), Path(path).joinpath(f\"{model_name}.pt\"))\n        return\n        \n    def _train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n        train_loader : a :class: `torch.utils.data.Dataloader`\n            DataLoader with train set\n        \"\"\"\n        self.network.train()\n        num_iter_epoch = len(train_loader)\n        pbar = tqdm(enumerate(train_loader),\n                                     leave=False,\n                                     total=len(train_loader),\n                                     desc=f'train epoch {self.epoch_idx}')\n        \n        epoch_loss = 0\n        for batch_idx, batch in pbar:\n            batch_loss = self._train_batch(batch, batch_idx, num_iter_epoch)\n            epoch_loss = (train_loader.batch_size*batch_idx*epoch_loss + train_loader.batch_size*batch_loss) / (train_loader.batch_size*(batch_idx+1))            \n            pbar.set_description(f'train epoch {self.epoch_idx}: loss {epoch_loss:.3f}', refresh=True)\n            # update scheduler\n            self.scheduler.step()\n\n        epoch_lr = self._optimizer.param_groups[-1][\"lr\"]\n        return epoch_loss, epoch_lr\n\n    def _train_batch(self, batch, batch_idx, num_iter_epoch):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n        batch_logs : dict\n            Dictionnary with \"batch_size\" and \"loss\".\n        \"\"\"\n        self._send_batch_to_device(batch)\n                                   \n        # with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n        # use mixed precision for float16 training\n        y = batch[\"labels\"]\n        batch_logs = {\"batch_size\": y.shape[0]}\n        \n        out_probs = self.network(batch)\n        # computing loss with division by gradient accumulation\n        loss = self.loss_fn(out_probs[\"logits\"], y.unsqueeze(-1)) / self.gradient_accumulation\n        # self.scaler.scale(loss).backward()\n        loss.backward()\n            \n        if ((batch_idx + 1) % self.gradient_accumulation == 0) or ((batch_idx + 1)==num_iter_epoch):\n            # Perform backward pass and optimization\n            # if self.clip_value is not None:\n            #     self.scaler.unscale_(self._optimizer)\n            #     clip_grad_norm_(self.network.parameters(), max_norm=self.clip_value)\n\n            self._optimizer.step()\n            self._optimizer.zero_grad(set_to_none=True)\n        return loss.detach().item()\n\n    def _predict_epoch(self, name, loader):\n        \"\"\"\n        Predict an epoch and update metrics.\n        Parameters\n        ----------\n        name : str\n            Name of the validation set\n        loader : torch.utils.data.Dataloader\n                DataLoader with validation set\n        \"\"\"\n        prob_pred, prob_true = self.predict_proba(loader, return_target=True)\n        \n        scores = []\n        for metric_fn in self.metrics:\n            metric_score = metric_fn(prob_true, prob_pred)\n            scores.append((metric_fn._name, metric_score))\n        # need to compute metrics here\n        return prob_pred, prob_true, scores\n\n    def stack_preds(self, list_prob):\n        return torch.vstack(list_prob)\n\n    def stack_targets(self, list_prob):\n        return torch.hstack(list_prob)\n\n    def _send_batch_to_device(self, batch):\n        for key, value in batch.items():\n            batch[key] = value.to(self.device)\n            \n    def _predict_batch(self, batch):\n        \"\"\"\n        Predict one batch of data.\n        \"\"\"\n        with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n            self._send_batch_to_device(batch)\n            # compute model output\n            out_probs = self.network(batch)[\"logits\"]\n            # apply activation\n            if isinstance(self.network, torch.nn.DataParallel):\n                # deal with data parrallel\n                out_probs = self.network.module.activation(out_probs)\n            else:\n                out_probs = self.network.activation(out_probs)\n            \n        return out_probs.detach()\n\n    def _set_optimizer(self):\n        \"\"\"Setup optimizer.\"\"\"\n        \n        name = self.optimizer_name\n\n        # disable decay for layer norm\n        decay_parameters = get_parameter_names(self.network, ALL_LAYERNORM_LAYERS)\n        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.network.named_parameters() if (n in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": self.optimizer_params[\"weight_decay\"],\n            },\n            {\n                \"params\": [\n                    p for n, p in self.network.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        other_params = self.optimizer_params.copy()\n        _ = other_params.pop(\"weight_decay\")\n                \n        self._optimizer = getattr(torch.optim, name)(optimizer_grouped_parameters, **other_params)        \n        # self.scaler = torch.cuda.amp.GradScaler(enabled=self.mixed_precision)\n        return\n\n    def load_pretrained_weights(self, path):\n        # the pretrained weight must have been saved through the simple save\n        saved_state_dict = torch.load(path,\n                                      map_location=\"cpu\",\n                                      mmap=True,\n                                      weights_only=True,)\n        self.network.load_state_dict(saved_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:15.492098Z","iopub.execute_input":"2024-05-05T13:39:15.4928Z","iopub.status.idle":"2024-05-05T13:39:15.57863Z","shell.execute_reply.started":"2024-05-05T13:39:15.492768Z","shell.execute_reply":"2024-05-05T13:39:15.577886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics to track\n\nHere you can define metrics you want to track during model training (every epoch)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import (\n    root_mean_squared_error\n)\nclass RMSE:\n    \"\"\"\n    Root Mean Squared Error.\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"rmse\"\n\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        Compute MSE (Mean Squared Error) of predictions.\n\n        Parameters\n        ----------\n        y_true : np.ndarray\n            Target matrix or vector\n        y_score : np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n        float\n            MSE of predictions vs targets.\n        \"\"\"\n        return root_mean_squared_error(y_true.numpy(), y_score.numpy())\n\n\nimport numpy as np\nfrom numba import jit \n\n# @jit\ndef qwk6(a1, a2, max_rat=6):\n    assert(len(a1) == len(a2))\n    \n    a1 = a1.astype(np.int64).reshape(-1)\n    # take closest integer for continuous predictions\n    a2 = np.clip(np.rint(a2), 1, max_rat).astype(np.int64).reshape(-1)\n    # a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n    return (1 - o / e)\n\nclass QWK:\n    \"\"\"\n    Comp metric taken from CPMP: https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/145105\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"qwk\"\n    def __call__(self, y_true, y_pred, max_rat=6):\n        return qwk6(y_true.numpy(), y_pred.numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:17.325232Z","iopub.execute_input":"2024-05-05T13:39:17.325608Z","iopub.status.idle":"2024-05-05T13:39:17.503072Z","shell.execute_reply.started":"2024-05-05T13:39:17.325577Z","shell.execute_reply":"2024-05-05T13:39:17.501832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Puting everything together for training one fold\n\nThis is just a simple function that will allow you to train one fold and save the corresponding configs and model checkpoint.","metadata":{}},{"cell_type":"code","source":"def update_sched_params(config, train_loader):\n    \"\"\"\n    This helper function allows to define steps per epoch dynamically\n\n    Parameters\n    ----------\n    - config : experiment config\n    - train_loader : train data loader used for this fold\n    \"\"\"\n    nb_epochs = config.max_epochs\n    is_per_epoch = config.scheduler_params.get(\"steps_per_epoch\", None)\n\n    if is_per_epoch is not None:\n        if is_per_epoch <= 0:\n            # this means automatic number of steps\n            config.scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n        # else use the defined value\n    \n    \n    # for get_cosine_schedule_with_warmup\n    warmup_ratio = config.scheduler_params.pop(\"warmup_ratio\", None)\n\n    if warmup_ratio is not None:\n        num_train_steps = int(len(train_loader) * nb_epochs)\n        num_warmup_steps = int(num_train_steps * warmup_ratio)\n        config.scheduler_params[\"num_warmup_steps\"] = num_warmup_steps\n        config.scheduler_params[\"num_training_steps\"] = num_train_steps\n        # else use the defined value\n    return config\n\ndef train_fold(df,\n               train_idx,\n               valid_idx,\n               config,\n               fold_nb):\n\n    print(\"Num train and valid samples:\", train_idx.shape[0], valid_idx.shape[0])\n    config = copy.deepcopy(config)\n    train_dl, valid_dl, eval_loaders, eval_names =  create_loaders(df,\n                                                                   train_idx,\n                                                                   valid_idx,\n                                                                   config,\n                                                                   eval_on_train=config.eval_on_train\n                                                                   )\n    log_folder = prepare_log_folder(config.save_path)\n    # add the eos_token_id to config\n    config.eos_token_id = train_dl.dataset.tokenizer.eos_token_id\n    save_config(config, log_folder)\n\n    \n    network = CustomtorchtuneLLM(config, train_dl.dataset.tokenizer.eos_token_id)\n    model = AbstractBaseModel(network=network)\n        \n    # update scheduler\n    config = update_sched_params(config, train_dl)\n\n    prob_pred, prob_true = model.fit(train_dl,\n                                      eval_loaders= eval_loaders,\n                                      eval_names=eval_names,\n                                      eval_metric=[RMSE(), QWK()], #  , ScikitQWK()\n                                      loss_config=config.loss_config, \n                                      max_epochs=config.max_epochs,\n                                      callbacks=None,\n                                      optimizer_name=config.optimizer_name,\n                                      optimizer_params=config.optimizer_params,\n                                      scheduler_name=config.scheduler_name,\n                                      scheduler_params=config.scheduler_params,\n                                      gradient_accumulation=config.gradient_accumulation,\n                                      mixed_precision=config.mixed_precision,\n                                      clip_value=config.clip_value,\n                                      verbose=config.verbose,\n             )\n\n    # prob_pred, prob_true = model.predict_proba(loader, return_target=True)\n    \n    model.save_model(path=log_folder, model_name=f\"fold_{fold_nb}\")\n    torch.cuda.empty_cache()\n        \n    return prob_pred, prob_true","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:19.574427Z","iopub.execute_input":"2024-05-05T13:39:19.574815Z","iopub.status.idle":"2024-05-05T13:39:19.587748Z","shell.execute_reply.started":"2024-05-05T13:39:19.574783Z","shell.execute_reply":"2024-05-05T13:39:19.586787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use stratified kfold\nfrom sklearn.model_selection import StratifiedKFold\n\n# download training data\ndf_train = pd.read_csv(os.path.join(PATH_TO_DATA, \"train.csv\"))\n\nTRAIN = True\nINFERENCE = not TRAIN\nDEBUG = True # Switch that to False if you wish to perform full training\n\nif TRAIN:\n    if DEBUG:\n        df_train = df_train[:500]\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    for fold_nb, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train.score)):\n        prob_pred, prob_true = train_fold(df_train,\n                                           train_idx,\n                                           valid_idx,\n                                           exp_config,\n                                           fold_nb=fold_nb,\n                                           )\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-05T13:39:20.262297Z","iopub.execute_input":"2024-05-05T13:39:20.263148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What you should see:\n# NUM_LAYERS=28/32\n# epoch 0   | lr: 1.76e-04 | loss: 0.0620 | rmse (valid): 0.5182 | qwk (valid): 0.8328 |  2:48:10s\n# epoch 1   | lr: 3.00e-07 | loss: 0.0301 | rmse (valid): 0.5101 | qwk (valid): 0.8332 |  5:36:15s\n# End of training!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\nFor inference, please refer to my previous notebook (link on top of this notebook) as everything works exactly the same.\n\nI've reached LB 0.802, please let me know how far you can go with this pipeline!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}