{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8029842,"sourceType":"datasetVersion","datasetId":4732809},{"sourceId":8126207,"sourceType":"datasetVersion","datasetId":4791897},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8166166,"sourceType":"datasetVersion","datasetId":4832208,"isSourceIdPinned":true}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport torch\nimport copy\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\nimport nltk\nfrom datasets import Dataset\nfrom glob import glob\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport re\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.special import softmax\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom lightgbm import log_evaluation, early_stopping\nimport lightgbm as lgb\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:31:31.506969Z","iopub.execute_input":"2024-04-20T05:31:31.507335Z","iopub.status.idle":"2024-04-20T05:32:14.383308Z","shell.execute_reply.started":"2024-04-20T05:31:31.507303Z","shell.execute_reply":"2024-04-20T05:32:14.382343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nMODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\nEVAL_BATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:32:14.384802Z","iopub.execute_input":"2024-04-20T05:32:14.385066Z","iopub.status.idle":"2024-04-20T05:32:14.389462Z","shell.execute_reply.started":"2024-04-20T05:32:14.385043Z","shell.execute_reply":"2024-04-20T05:32:14.388415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = glob(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(models[0])\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\ndf_test = pd.read_csv(TEST_DATA_PATH)\nds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n    report_to=\"none\"\n)\n\npredictions = []\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(\n        model=model, \n        args=args, \n        data_collator=DataCollatorWithPadding(tokenizer), \n        tokenizer=tokenizer\n    )    \n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:32:14.390526Z","iopub.execute_input":"2024-04-20T05:32:14.39083Z","iopub.status.idle":"2024-04-20T05:33:20.199915Z","shell.execute_reply.started":"2024-04-20T05:32:14.390807Z","shell.execute_reply":"2024-04-20T05:33:20.199017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:20.202089Z","iopub.execute_input":"2024-04-20T05:33:20.202397Z","iopub.status.idle":"2024-04-20T05:33:20.207252Z","shell.execute_reply.started":"2024-04-20T05:33:20.202372Z","shell.execute_reply":"2024-04-20T05:33:20.206279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:20.208505Z","iopub.execute_input":"2024-04-20T05:33:20.208905Z","iopub.status.idle":"2024-04-20T05:33:20.230085Z","shell.execute_reply.started":"2024-04-20T05:33:20.208842Z","shell.execute_reply":"2024-04-20T05:33:20.228967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:20.231385Z","iopub.execute_input":"2024-04-20T05:33:20.23199Z","iopub.status.idle":"2024-04-20T05:33:20.244969Z","shell.execute_reply.started":"2024-04-20T05:33:20.231956Z","shell.execute_reply":"2024-04-20T05:33:20.244162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading\nLoad training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data","metadata":{}},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:20.246268Z","iopub.execute_input":"2024-04-20T05:33:20.246613Z","iopub.status.idle":"2024-04-20T05:33:20.894923Z","shell.execute_reply.started":"2024-04-20T05:33:20.246583Z","shell.execute_reply":"2024-04-20T05:33:20.893875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport re\n\n# 加载英语模型\nnlp = spacy.load(\"en_core_web_sm\")\n# 加载英语词汇表\nwith open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\ndef count_spelling_errors(text):\n#     # 移除标点符号\n#     text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # 分词并词形还原\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n\n    # 统计拼写错误数量\n    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n\n#     for token in lemmatized_tokens:\n#         if token not in english_vocab:\n#             print(token)\n\n    return spelling_errors\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:20.896363Z","iopub.execute_input":"2024-04-20T05:33:20.896648Z","iopub.status.idle":"2024-04-20T05:33:23.30489Z","shell.execute_reply.started":"2024-04-20T05:33:20.896625Z","shell.execute_reply":"2024-04-20T05:33:23.304025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n#     x = expandContractions(x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:23.30606Z","iopub.execute_input":"2024-04-20T05:33:23.306361Z","iopub.status.idle":"2024-04-20T05:33:23.35188Z","shell.execute_reply.started":"2024-04-20T05:33:23.306332Z","shell.execute_reply":"2024-04-20T05:33:23.350873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Paragraph Features","metadata":{}},{"cell_type":"code","source":"import string\n\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n    # 使用 string.punctuation 中定义的标点符号集合来去除文本中的标点符号\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n# 测试函数\ntext = \"Hello, world! This is a test.\"\nprint(remove_punctuation(text))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:23.356111Z","iopub.execute_input":"2024-04-20T05:33:23.356399Z","iopub.status.idle":"2024-04-20T05:33:23.367384Z","shell.execute_reply.started":"2024-04-20T05:33:23.356375Z","shell.execute_reply":"2024-04-20T05:33:23.366468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n#     tmp = tmp.with_columns(\n#         (pl.col('paragraph_len') / pl.col('paragraph_sentence_cnt')).alias(\"paragraph_len/sentence_cnt\")\n#     )\n#     tmp = tmp.with_columns(\n#         (pl.col('paragraph_len') / pl.col('paragraph_word_cnt')).alias(\"paragraph_len/word_cnt\")\n#     )\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\nparagraph_fea2 = ['paragraph_error_num'] + paragraph_fea\ndef Paragraph_Eng(train_tmp):\n    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n#         *[pl.col('paragraph').filter((pl.col('paragraph_len') >= num) & (pl.col('paragraph_len') < num_list2[i + 1])).count().alias(f\"paragraph_>{num}<{num_list2[i + 1]}_cnt\") for i,num in enumerate(num_list) ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],  # 求四分之一值\n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],  # 求四分之三值\n    \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# 获取特征名称\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:33:23.368347Z","iopub.execute_input":"2024-04-20T05:33:23.3686Z","iopub.status.idle":"2024-04-20T05:55:22.04157Z","shell.execute_reply.started":"2024-04-20T05:33:23.368568Z","shell.execute_reply":"2024-04-20T05:55:22.040549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sentence Features","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n#     tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],  # 求四分之一值\n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],  # 求四分之三值\n    \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:55:22.043445Z","iopub.execute_input":"2024-04-20T05:55:22.04379Z","iopub.status.idle":"2024-04-20T05:55:29.197651Z","shell.execute_reply.started":"2024-04-20T05:55:22.043763Z","shell.execute_reply":"2024-04-20T05:55:29.196681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Features","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:55:29.199629Z","iopub.execute_input":"2024-04-20T05:55:29.200573Z","iopub.status.idle":"2024-04-20T05:55:42.40879Z","shell.execute_reply.started":"2024-04-20T05:55:29.200535Z","shell.execute_reply":"2024-04-20T05:55:42.407759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.Tf-idf features","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,6),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Number of Features: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:55:42.410381Z","iopub.execute_input":"2024-04-20T05:55:42.411072Z","iopub.status.idle":"2024-04-20T05:59:01.050003Z","shell.execute_reply.started":"2024-04-20T05:55:42.411034Z","shell.execute_reply":"2024-04-20T05:59:01.048932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CountVectorizer Features","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:20:02.359229Z","iopub.execute_input":"2024-04-15T13:20:02.359567Z","iopub.status.idle":"2024-04-15T13:20:02.363863Z","shell.execute_reply.started":"2024-04-15T13:20:02.359539Z","shell.execute_reply":"2024-04-15T13:20:02.362844Z"}}},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.10,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T05:59:01.051444Z","iopub.execute_input":"2024-04-20T05:59:01.05184Z","iopub.status.idle":"2024-04-20T06:00:18.030542Z","shell.execute_reply.started":"2024-04-20T05:59:01.051804Z","shell.execute_reply":"2024-04-20T06:00:18.029734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"import joblib\n\ndeberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))    \n\ntrain_feats.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:00:18.031696Z","iopub.execute_input":"2024-04-20T06:00:18.032006Z","iopub.status.idle":"2024-04-20T06:00:18.103761Z","shell.execute_reply.started":"2024-04-20T06:00:18.031982Z","shell.execute_reply":"2024-04-20T06:00:18.102755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:00:18.104817Z","iopub.execute_input":"2024-04-20T06:00:18.105093Z","iopub.status.idle":"2024-04-20T06:00:18.11284Z","shell.execute_reply.started":"2024-04-20T06:00:18.105069Z","shell.execute_reply":"2024-04-20T06:00:18.111763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_feats[feature_names].astype(np.float32).values\n\ny_split = train_feats['score'].astype(int).values\ny = train_feats['score'].astype(np.float32).values-a\noof = train_feats['score'].astype(int).values","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:00:18.114126Z","iopub.execute_input":"2024-04-20T06:00:18.114473Z","iopub.status.idle":"2024-04-20T06:00:20.029959Z","shell.execute_reply.started":"2024-04-20T06:00:18.114441Z","shell.execute_reply":"2024-04-20T06:00:20.029148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(feature_names)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:00:20.03121Z","iopub.execute_input":"2024-04-20T06:00:20.03156Z","iopub.status.idle":"2024-04-20T06:00:20.039682Z","shell.execute_reply.started":"2024-04-20T06:00:20.031534Z","shell.execute_reply":"2024-04-20T06:00:20.037512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_select_wrapper():\n    \"\"\"\n    lgm特征重要性筛选函数\n    :param train:训练数据集\n    :param test:测试数据集\n    :return:特征筛选后的训练集和测试集\n    \"\"\"\n    \n    # Part 1.划分特征名称，删除ID列和标签列\n    print('feature_select_wrapper...')\n    features = feature_names\n\n    \n    # 实例化评估器\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    # 创建空容器\n    fse = pd.Series(0, index=features)\n    \n        \n        \n    for train_index, test_index in skf.split(X, y_split):\n\n        X_train_fold, X_test_fold = X[train_index], X[test_index]\n\n\n        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n\n        model = lgb.LGBMRegressor(\n                    objective = qwk_obj,\n                    metrics = 'None',\n                    learning_rate = 0.05,\n                    max_depth = 5,\n                    num_leaves = 10,\n                    colsample_bytree=0.3,\n                    reg_alpha = 0.7,\n                    reg_lambda = 0.1,\n                    n_estimators=700,\n                    random_state=412,\n                    extra_trees=True,\n                    class_weight='balanced',\n                    verbosity = - 1)\n\n        predictor = model.fit(X_train_fold,\n                                      y_train_fold,\n                                      eval_names=['train', 'valid'],\n                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                      eval_metric=quadratic_weighted_kappa,\n                                      callbacks=callbacks,)\n        models.append(predictor)\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        oof[test_index]=predictions_fold\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n        f1_scores.append(f1_fold)\n\n\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        kappa_scores.append(kappa_fold)\n\n        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                      display_labels=[x for x in range(1,7)])\n        disp.plot()\n        plt.show()\n        print(f'F1 score across fold: {f1_fold}')\n        print(f'Cohen kappa score across fold: {kappa_fold}')\n\n        fse += pd.Series(predictor.feature_importances_, features)\n    \n    \n    \n    # Part 4.选择最重要的300个特征\n    feature_select = fse.sort_values(ascending=False).index.tolist()[:12800]\n    print('done')\n    return feature_select","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:32:28.78778Z","iopub.execute_input":"2024-04-20T06:32:28.788151Z","iopub.status.idle":"2024-04-20T06:32:28.802719Z","shell.execute_reply.started":"2024-04-20T06:32:28.788125Z","shell.execute_reply":"2024-04-20T06:32:28.801617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores = []\nkappa_scores = []\nmodels = []\npredictions = []\ncallbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\nfeature_select = feature_select_wrapper()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:32:29.030628Z","iopub.execute_input":"2024-04-20T06:32:29.031257Z","iopub.status.idle":"2024-04-20T06:52:27.351057Z","shell.execute_reply.started":"2024-04-20T06:32:29.031218Z","shell.execute_reply":"2024-04-20T06:52:27.350011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_feats[feature_select].astype(np.float32).values","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:58:57.921963Z","iopub.execute_input":"2024-04-20T06:58:57.922772Z","iopub.status.idle":"2024-04-20T06:58:59.100032Z","shell.execute_reply.started":"2024-04-20T06:58:57.922736Z","shell.execute_reply":"2024-04-20T06:58:59.099117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Let's use cross-validation**","metadata":{}},{"cell_type":"code","source":"n_splits = 15\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\nf1_scores = []\nkappa_scores = []\nmodels = []\npredictions = []\ncallbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n\ni=1\nfor train_index, test_index in skf.split(X, y_split):\n   \n    print('fold',i)\n    X_train_fold, X_test_fold = X[train_index], X[test_index]\n    \n   \n    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n    \n    model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.05,\n                max_depth = 5,\n                num_leaves = 10,\n                colsample_bytree=0.3,\n                reg_alpha = 0.7,\n                reg_lambda = 0.1,\n                n_estimators=700,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n                verbosity = - 1)\n\n    predictor = model.fit(X_train_fold,\n                                  y_train_fold,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks,)\n    models.append(predictor)\n    predictions_fold = predictor.predict(X_test_fold)\n    predictions_fold = predictions_fold + a\n    oof[test_index]=predictions_fold\n    predictions_fold = predictions_fold.clip(1, 6).round()\n    predictions.append(predictions_fold)\n    f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n    f1_scores.append(f1_fold)\n    \n    \n    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n    kappa_scores.append(kappa_fold)\n    \n    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=[x for x in range(1,7)])\n    disp.plot()\n    plt.show()\n    print(f'F1 score across fold: {f1_fold}')\n    print(f'Cohen kappa score across fold: {kappa_fold}')\n    i+=1\n\nmean_f1_score = np.mean(f1_scores)\nmean_kappa_score = np.mean(kappa_scores)\n\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T06:59:04.536672Z","iopub.execute_input":"2024-04-20T06:59:04.537542Z","iopub.status.idle":"2024-04-20T07:34:44.866951Z","shell.execute_reply.started":"2024-04-20T06:59:04.537511Z","shell.execute_reply":"2024-04-20T07:34:44.865978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('models.pkl', 'wb') as f:\n    pickle.dump(models, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:44:42.531617Z","iopub.execute_input":"2024-04-15T14:44:42.531904Z","iopub.status.idle":"2024-04-15T14:44:42.67115Z","shell.execute_reply.started":"2024-04-15T14:44:42.531879Z","shell.execute_reply":"2024-04-15T14:44:42.670051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('models.pkl', 'rb') as f:\n    models = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:44:42.672535Z","iopub.execute_input":"2024-04-15T14:44:42.672918Z","iopub.status.idle":"2024-04-15T14:44:42.775819Z","shell.execute_reply.started":"2024-04-15T14:44:42.672883Z","shell.execute_reply":"2024-04-15T14:44:42.774828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Inference**","metadata":{}},{"cell_type":"code","source":"# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# CountVectorizer\ntest_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:45:22.873794Z","iopub.execute_input":"2024-04-15T14:45:22.874742Z","iopub.status.idle":"2024-04-15T14:45:23.02144Z","shell.execute_reply.started":"2024-04-15T14:45:22.874704Z","shell.execute_reply":"2024-04-15T14:45:23.020591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = []\nfor model in models:\n    proba= model.predict(test_feats[feature_select])+ a\n    probabilities.append(proba)\n\npredictions = np.mean(probabilities, axis=0)\n\npredictions = np.round(predictions.clip(1, 6))\n\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:47:42.519792Z","iopub.execute_input":"2024-04-15T14:47:42.52018Z","iopub.status.idle":"2024-04-15T14:47:43.148836Z","shell.execute_reply.started":"2024-04-15T14:47:42.520151Z","shell.execute_reply":"2024-04-15T14:47:43.147703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsubmission['score']=predictions\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:48:33.165747Z","iopub.execute_input":"2024-04-15T14:48:33.166113Z","iopub.status.idle":"2024-04-15T14:48:33.180375Z","shell.execute_reply.started":"2024-04-15T14:48:33.166084Z","shell.execute_reply":"2024-04-15T14:48:33.179489Z"},"trusted":true},"execution_count":null,"outputs":[]}]}