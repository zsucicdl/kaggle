{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 7369493,
     "sourceType": "datasetVersion",
     "datasetId": 4281572
    },
    {
     "sourceId": 7923821,
     "sourceType": "datasetVersion",
     "datasetId": 4656352
    },
    {
     "sourceId": 26154,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 22015
    },
    {
     "sourceId": 33436,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 27992
    }
   ],
   "dockerImageVersionId": 30683,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "\n<center><h1>Meet your new teacher ... Gemma-Lora Pefting</h1><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjopwHAmKL3wi6SrOSr1-UDGnxWM1-Qu0kFkEoVK_Wa8570PZBOu4r8e5M6wJfOrjDLUn95VcuqujoIciiGIZk6G0VqNLSDjM_-_7I9Jcj9IwvC1E2FxquO-JKzYGd8gRfgnEzEJVBma3Bd/s1600/Bad+Teacher.jpg\" ></center>This code snippet orchestrates a fascinating fusion of technologies for automated essay scoring. <br><br>\n\n**The commented out code is the LoRa training. So, basically I have trained the model with the given dataset, saved it and load the saved model for submission (since the internet must be off).**<br><br>\nLet's break it down:\n\n## Imports: \nThe code begins by importing necessary libraries. peft and accelerate are used for performance optimization, transformers for working with pre-trained models, and torch for tensor computation. Additionally, it imports modules like bitsandbytes, pandas, datasets, and trl.\n\n## Configuration Setup: \nIt initializes configuration settings, including quantization configuration, model path, tokenizer, and model for auto-regressive language modeling (LM). The model is equipped with LoraConfig for Low-Rank Adaptive Attention.\n\n## Model Parameter Analysis: \nThe function print_number_of_trainable_model_parameters computes and prints the number of trainable model parameters, which is essential for understanding model complexity and training feasibility.\n\n## Data Preparation: \nIt loads a dataset from a CSV file, preprocesses it by tokenizing text and scores, and converts it into a Dataset object.\n\n## Training Setup: \nThe SFTTrainer is initialized with the pre-trained model, training dataset, training arguments, and specific configurations such as peft_config and formatting_func. It prepares the model for training with settings like batch size, learning rate, and optimization algorithm.\n\n## Model Training: \nThe trainer.train() function initiates the training process.\n\n## Model Saving: \nAfter training, the model is saved to a specified directory.\n\n## Text Processing Functions: \nTwo utility functions, truncate_txt and gen_prompt, are defined for text truncation and prompt generation, respectively.\n\n## Inference: \nIn the inference section, it reads another dataset for testing, iterates over each essay, generates prompts, and utilizes the model to predict scores. The scores are parsed from the generated output. There's also a timeout mechanism to prevent lengthy computations.\n\n## Output Generation: \nThe predicted scores are collected and formatted into a DataFrame, which is then saved as a CSV file for further analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip install accelerate",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip install /kaggle/input/llm-peft-pkg/peft-0.10.0-py3-none-any.whl",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from accelerate import Accelerator\nimport transformers\nimport bitsandbytes\nimport torch\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\nimport pandas as pd\nfrom peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForCausalLM",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:35:54.103614Z",
     "iopub.execute_input": "2024-04-17T14:35:54.10431Z",
     "iopub.status.idle": "2024-04-17T14:36:07.419556Z",
     "shell.execute_reply.started": "2024-04-17T14:35:54.104274Z",
     "shell.execute_reply": "2024-04-17T14:36:07.4187Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "accelerator = Accelerator()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:36:15.442639Z",
     "iopub.execute_input": "2024-04-17T14:36:15.443722Z",
     "iopub.status.idle": "2024-04-17T14:36:15.454433Z",
     "shell.execute_reply.started": "2024-04-17T14:36:15.443686Z",
     "shell.execute_reply": "2024-04-17T14:36:15.453509Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "quantization_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:36:17.116335Z",
     "iopub.execute_input": "2024-04-17T14:36:17.117092Z",
     "iopub.status.idle": "2024-04-17T14:36:17.12319Z",
     "shell.execute_reply.started": "2024-04-17T14:36:17.117058Z",
     "shell.execute_reply": "2024-04-17T14:36:17.122307Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MODEL_PATH = \"/kaggle/input/gemma/transformers/1.1-7b-it/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n# model = AutoModelForCausalLM.from_pretrained(\n#     MODEL_PATH,\n#     device_map = \"auto\",\n#     trust_remote_code = True,\n#     quantization_config=quantization_config,\n# )\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/peft_gemma/transformers/1.0/1\", quantization_config=quantization_config, device_map = \"auto\", trust_remote_code = True,)\n\nmodel = accelerator.prepare(model)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:36:53.338016Z",
     "iopub.execute_input": "2024-04-17T14:36:53.338736Z",
     "iopub.status.idle": "2024-04-17T14:39:33.385981Z",
     "shell.execute_reply.started": "2024-04-17T14:36:53.338701Z",
     "shell.execute_reply": "2024-04-17T14:39:33.385122Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:40:09.599542Z",
     "iopub.execute_input": "2024-04-17T14:40:09.600559Z",
     "iopub.status.idle": "2024-04-17T14:40:09.624158Z",
     "shell.execute_reply.started": "2024-04-17T14:40:09.600512Z",
     "shell.execute_reply": "2024-04-17T14:40:09.623096Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# lora_config = LoraConfig(\n#     r=32, # Rank\n#     lora_alpha=32,\n#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=TaskType.CAUSAL_LM \n# )",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# def print_number_of_trainable_model_parameters(model):\n#     trainable_model_params = 0\n#     all_model_params = 0\n#     for _, param in model.named_parameters():\n#         all_model_params += param.numel()\n#         if param.requires_grad:\n#             trainable_model_params += param.numel()\n#     return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\n# print(print_number_of_trainable_model_parameters(model))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# peft_model = get_peft_model(model, \n#                             lora_config)\n# print(print_number_of_trainable_model_parameters(peft_model))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TRAIN_DF_FILE = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\"\n# df = pd.read_csv(TRAIN_DF_FILE)\n# df[\"score\"] = df[\"score\"].astype(str)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# from datasets import load_dataset,Dataset\n\n# data = Dataset.from_pandas(df)\n# data = data.map(lambda samples: tokenizer(samples[\"full_text\"]), batched=True)\n# data = data.map(lambda samples: tokenizer(samples[\"score\"]), batched=True)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# !pip install /kaggle/input/llm-peft-pkg/shtab-1.7.1-py3-none-any.whl\n# !pip install /kaggle/input/llm-peft-pkg/tyro-0.7.3-py3-none-any.whl\n# !pip install /kaggle/input/llm-peft-pkg/trl-0.8.1-py3-none-any.whl",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# from trl import SFTTrainer",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# def formatting_func(example):\n#     text = f\"Essay:\\n{example['full_text'][0]}\\n\\nInstruction:\\n You are a teacher. Score the given essay written by your student, the score must be an integer between 1 and 6. Your score should reflect the overall quality of the essay, considering factors such as clarity, coherence, organization, argumentation, language proficiency, and adherence to standard essay conventions. \\n\\nResponse: \\n{example['score'][0]}\"\n#     return [text]\n\n# trainer = SFTTrainer(\n#     model=peft_model,\n#     train_dataset=data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=2,\n#         max_steps=10,\n#         learning_rate=1e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs\",\n#         optim=\"paged_adamw_8bit\"\n#     ),\n#     peft_config=lora_config,\n#     formatting_func=formatting_func,\n# )\n# trainer.train()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#peft_model.save_pretrained(\"/kaggle/working/model\")",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def truncate_txt(text, length):\n    text_list = text.split()\n    \n    if len(text_list) <= length:\n        return text\n    \n    return \" \".join(text_list[:length])\n\n\ndef gen_prompt(essay):\n    \n    # Truncate the texts to first 200 words for now\n    #og_text = truncate_txt(essay, 150)\n    return f\"\"\"   \n    Essay:\n    \\\"\"\"{essay}\\\"\"\"\n    \n    Instruction:\n    You are a teacher. Fairly score the given essay written by your student, the score must be an integer between 1 and 6. Your score should reflect the overall quality of the essay, considering factors such as clarity, coherence, organization, argumentation, language proficiency, and adherence to standard essay conventions. Pay attention to misspellings and slang words.\n    \n    Response: \n    \\\"\"\"\\\"\"\"\n    \"\"\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:40:22.070491Z",
     "iopub.execute_input": "2024-04-17T14:40:22.070888Z",
     "iopub.status.idle": "2024-04-17T14:40:22.077771Z",
     "shell.execute_reply.started": "2024-04-17T14:40:22.070856Z",
     "shell.execute_reply": "2024-04-17T14:40:22.076779Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import datetime\nfrom tqdm import tqdm\nimport gc\nimport re\n\nstart_time = datetime.datetime.now()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:40:24.329191Z",
     "iopub.execute_input": "2024-04-17T14:40:24.329584Z",
     "iopub.status.idle": "2024-04-17T14:40:24.334531Z",
     "shell.execute_reply.started": "2024-04-17T14:40:24.329551Z",
     "shell.execute_reply": "2024-04-17T14:40:24.333633Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def parse(text):\n    answer = []\n    start = False\n    end = False\n    for l in reversed(list(text)):\n        if l in '0123456789' and not end:\n            start = True\n            answer.append(l)\n        else:\n            if start:\n                end = True\n        \n    answer = reversed(answer)\n    return ''.join(answer)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:40:26.546614Z",
     "iopub.execute_input": "2024-04-17T14:40:26.547286Z",
     "iopub.status.idle": "2024-04-17T14:40:26.553151Z",
     "shell.execute_reply.started": "2024-04-17T14:40:26.54725Z",
     "shell.execute_reply": "2024-04-17T14:40:26.552007Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "device = accelerator.device\n\ntdf = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\npbar = tqdm(total=tdf.shape[0])\nit = iter(tdf.iterrows())\nidx, row = next(it, (None, None))\n\nres = []\n\nwhile idx is not None: \n    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=24, minutes=30):\n        res.append([row[\"essay_id\"], 0])\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n        continue\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n    try:  \n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": gen_prompt(row[\"full_text\"])\n            }\n        ]\n        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n\n        with torch.no_grad():\n            encoded_output = model.generate(encoded_input, temperature = 0.1, max_new_tokens=200, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n\n        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n        mark = parse(decoded_output)\n        #decoded_output = result = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n        print(decoded_output)        \n        res.append([row[\"essay_id\"], int(mark)])\n                            \n    except Exception as e:\n        print(f\"ERROR: {e}\")\n        res.append([row[\"essay_id\"], 0])\n        \n    finally:\n        idx, row = next(it, (None, None))\n        pbar.update(1)\n\n        \npbar.close()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:40:52.263813Z",
     "iopub.execute_input": "2024-04-17T14:40:52.264756Z",
     "iopub.status.idle": "2024-04-17T14:42:10.215469Z",
     "shell.execute_reply.started": "2024-04-17T14:40:52.264713Z",
     "shell.execute_reply": "2024-04-17T14:42:10.214466Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "res",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:42:17.703671Z",
     "iopub.execute_input": "2024-04-17T14:42:17.704387Z",
     "iopub.status.idle": "2024-04-17T14:42:17.710825Z",
     "shell.execute_reply.started": "2024-04-17T14:42:17.704351Z",
     "shell.execute_reply": "2024-04-17T14:42:17.70984Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sub = pd.DataFrame(res, columns=['essay_id', 'score'])\n\nsub.to_csv(\"submission.csv\", index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-17T14:42:26.056323Z",
     "iopub.execute_input": "2024-04-17T14:42:26.057109Z",
     "iopub.status.idle": "2024-04-17T14:42:26.068263Z",
     "shell.execute_reply.started": "2024-04-17T14:42:26.057065Z",
     "shell.execute_reply": "2024-04-17T14:42:26.067424Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
