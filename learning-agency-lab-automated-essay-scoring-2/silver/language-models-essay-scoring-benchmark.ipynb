{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 172458161,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 173092759,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174216901,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174300709,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174438342,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3900
    },
    {
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 4761
    },
    {
     "sourceId": 28808,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 8332
    },
    {
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 28083
    },
    {
     "sourceId": 26140,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 22003
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Language Models Essay Scoring Benchmark\n\n#### Motivation\n\nThis notebook aims to achieve three main goals:\n\n1. **Model Selection:** Evaluate the performance of various language models, both local and remote, to determine which models work best for the competition and provide general insights.\n\n2. **Inference Speed:** Measure the time taken by each model for inference, considering whether a specific model is feasible for use within the competition's constraints.\n\n3. **Prompt Engineering:** Experiment with different prompting techniques to compare results and explore how prompting can influence model performance.\n\nThis notebook utilizes [ai-microcore](https://github.com/Nayjest/ai-microcore) for querying various locally deployed and remote language models.\n\n**While raw scores generated by LLMs may not be ideal for direct submissions, they could form a crucial part of a high-performing solution**, especially if you're not focused on the Efficiency Prize.\n\nFor further analysis, raw essay scores generated by different LLMs are stored in the following file: `/kaggle/working/preds.csv` (see notebook output).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### General Notebook Settings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "VERBOSE = True\nQUANTIZE = True\nLIMIT_ROWS = 50",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:09:01.421434Z",
     "iopub.execute_input": "2024-04-29T22:09:01.421798Z",
     "iopub.status.idle": "2024-04-29T22:09:01.426293Z",
     "shell.execute_reply.started": "2024-04-29T22:09:01.42177Z",
     "shell.execute_reply": "2024-04-29T22:09:01.425256Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### LLM packages offline insallation, logging setup, show system info, prepare torch & transformers, [see details here](https://www.kaggle.com/code/nayjest/kaggle-llm-notebook-bootstrap)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "__import__('kaggle_llm_notebook_bootstrap').bootstrap()",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-29T22:09:02.904751Z",
     "iopub.execute_input": "2024-04-29T22:09:02.905095Z",
     "iopub.status.idle": "2024-04-29T22:10:28.730405Z",
     "shell.execute_reply.started": "2024-04-29T22:09:02.905067Z",
     "shell.execute_reply": "2024-04-29T22:10:28.729445Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd, timers, sklearn.metrics, os, kaggle_secrets, re, json\nimport microcore as mc\nfrom microcore import ui\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:10:39.781082Z",
     "iopub.execute_input": "2024-04-29T22:10:39.781427Z",
     "iopub.status.idle": "2024-04-29T22:10:39.786772Z",
     "shell.execute_reply.started": "2024-04-29T22:10:39.781401Z",
     "shell.execute_reply": "2024-04-29T22:10:39.785623Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Read Essays Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "INPUT = f\"{mc.utils.is_kaggle() and '/kaggle/input/' or ''}learning-agency-lab-automated-essay-scoring-2/{'train.csv'}\"\ntdf = pd.read_csv(INPUT)\ntdf.tail(2)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:10:40.8752Z",
     "iopub.execute_input": "2024-04-29T22:10:40.875843Z",
     "iopub.status.idle": "2024-04-29T22:10:41.242999Z",
     "shell.execute_reply.started": "2024-04-29T22:10:40.875812Z",
     "shell.execute_reply": "2024-04-29T22:10:41.24196Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "defaults = dict(\n    api_type=mc.ApiType.TRANSFORMERS,\n    chat_mode=True,\n    default_args=dict(\n        do_sample=True,\n    ),\n    init_params=dict(\n        quantize_4bit=QUANTIZE if mc.utils.is_kaggle() else True,\n        always_clear_mem=True,\n        gradient_checkpointing=True,\n    ),\n)\n\nconfigs = {\n    'gemma-1.1-2b': dict(\n        model='/kaggle/input/gemma/transformers/1.1-2b-it/1',\n        init_params=dict(\n            quantize_4bit=False,\n            always_clear_mem=True,\n        ),\n    ),\n    'llama3-8b': dict(\n        model='/kaggle/input/llama-3/transformers/8b-chat-hf/1/',\n        default_args=dict(\n            do_sample=True,\n            eos_token_id=128009,\n        ),\n    ),\n    'mistral-7b': dict(\n        model='/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1',\n    ),\n    'mixtral-8x-7b(Kaggle T4x2)': dict(\n        model='/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1',\n    ),\n    'gemma-7b': dict(\n        model='/kaggle/input/gemma/transformers/7b-it/3',\n    ),\n    'gpt-3.5-turbo': dict(\n        model='gpt-3.5-turbo',\n        api_type=mc.ApiType.OPEN_AI,\n        api_key = kaggle_secrets.UserSecretsClient().get_secret('OPENAI_API_KEY'),\n    ),\n    'gpt-4-turbo': dict(\n        model='gpt-4-turbo',\n        api_type=mc.ApiType.OPEN_AI,\n        api_key = kaggle_secrets.UserSecretsClient().get_secret('OPENAI_API_KEY'),\n    ),\n    'mistral-large': dict(\n        model='mistral-large-latest',\n        api_type=mc.ApiType.OPEN_AI,\n        api_base='https://api.mistral.ai/v1/',\n        api_key = kaggle_secrets.UserSecretsClient().get_secret('MISTRAL_API_KEY'),\n    ),\n    'mixtral-8x22b': dict(\n        model='open-mixtral-8x22b',\n        api_type=mc.ApiType.OPEN_AI,\n        api_base='https://api.mistral.ai/v1/',\n        api_key = kaggle_secrets.UserSecretsClient().get_secret('MISTRAL_API_KEY'),\n    ),\n    'mixtral-8x7b': dict(\n        model='open-mixtral-8x7b',\n        api_type=mc.ApiType.OPEN_AI,\n        api_base='https://api.mistral.ai/v1/',\n        api_key = kaggle_secrets.UserSecretsClient().get_secret('MISTRAL_API_KEY'),\n    ),\n    'claude-3-opus': dict(\n        model='claude-3-opus-20240229',\n        llm_default_args=dict(max_tokens=2500),\n        api_type=mc.ApiType.ANTHROPIC,\n        api_key =kaggle_secrets.UserSecretsClient().get_secret('ANTHROPIC_API_KEY'),\n    ),\n}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:11:18.052081Z",
     "iopub.execute_input": "2024-04-29T22:11:18.052831Z",
     "iopub.status.idle": "2024-04-29T22:11:18.831017Z",
     "shell.execute_reply.started": "2024-04-29T22:11:18.052798Z",
     "shell.execute_reply": "2024-04-29T22:11:18.830032Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Check if model works correctly",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def configure(config_name: str):\n    cfg =  configs[config_name] if configs[config_name].get('api_type') else {**defaults, **configs[config_name]}\n    cfg['use_logging'] = VERBOSE\n    mc.configure(**cfg).describe()\n    if mc.config().uses_local_model():\n        mc.utils.show_vram_usage()\n# configure('mixtral-8x22b')\n\n# timers.timer()\n# out = mc.llm(\"count from 1 to 5\")\n# timers.speed(len(out), 'characters')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:11:22.158019Z",
     "iopub.execute_input": "2024-04-29T22:11:22.158844Z",
     "iopub.status.idle": "2024-04-29T22:11:22.999232Z",
     "shell.execute_reply.started": "2024-04-29T22:11:22.158808Z",
     "shell.execute_reply": "2024-04-29T22:11:22.998232Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "how_to_score=\"\"\"# Holistic Rating for Source-Based Writing\n\nAfter reading the essay and completing the analytical rating form, assign a holistic score based on the rubric\nbelow. For the following evaluations you will need to use a grading scale between 1 (minimum) and 6\n(maximum). As with the analytical rating form, the distance between each grade (e.g., 1-2, 3-4, 4-5) should be\nconsidered equal.\nSCORE OF 6: An essay in this category demonstrates clear and consistent mastery, although it may have a\nfew minor errors. A typical essay effectively and insightfully develops a point of view on the issue and\ndemonstrates outstanding critical thinking; the essay uses clearly appropriate examples, reasons, and other\nevidence taken from the source text(s) to support its position; the essay is well organized and clearly focused,\ndemonstrating clear coherence and smooth progression of ideas; the essay exhibits skillful use of language,\nusing a varied, accurate, and apt vocabulary and demonstrates meaningful variety in sentence structure; the\nessay is free of most errors in grammar, usage, and mechanics.\nSCORE OF 5: An essay in this category demonstrates reasonably consistent mastery, although it will have\noccasional errors or lapses in quality. A typical essay effectively develops a point of view on the issue and\ndemonstrates strong critical thinking; the essay generally using appropriate examples, reasons, and other\nevidence taken from the source text(s) to support its position; the essay is well organized and focused,\ndemonstrating coherence and progression of ideas; the essay exhibits facility in the use of language, using\nappropriate vocabulary demonstrates variety in sentence structure; the essay is generally free of most errors in\ngrammar, usage, and mechanics.\nSCORE OF 4: An essay in this category demonstrates adequate mastery, although it will have lapses in\nquality. A typical essay develops a point of view on the issue and demonstrates competent critical thinking; the\nessay using adequate examples, reasons, and other evidence taken from the source text(s) to support its\nposition; the essay is generally organized and focused, demonstrating some coherence and progression of ideas\nexhibits adequate; the essay may demonstrate inconsistent facility in the use of language, using generally\nappropriate vocabulary demonstrates some variety in sentence structure; the essay may have some errors in\ngrammar, usage, and mechanics.\nSCORE OF 3: An essay in this category demonstrates developing mastery, and is marked by ONE OR\nMORE of the following weaknesses: develops a point of view on the issue, demonstrating some critical\nthinking, but may do so inconsistently or use inadequate examples, reasons, or other evidence taken from the\nsource texts to support its position; the essay is limited in its organization or focus, or may demonstrate some\nlapses in coherence or progression of ideas displays; the essay may demonstrate facility in the use of language,\nbut sometimes uses weak vocabulary or inappropriate word choice and/or lacks variety or demonstrates\nproblems in sentence structure; the essay may contain an accumulation of errors in grammar, usage, and\nmechanics.\nSCORE OF 2: An essay in this category demonstrates little mastery, and is flawed by ONE OR MORE of\nthe following weaknesses: develops a point of view on the issue that is vague or seriously limited, and\ndemonstrates weak critical thinking; the essay provides inappropriate or insufficient examples, reasons, or\nother evidence taken from the source text to support its position; the essay is poorly organized and/or focused,\nor demonstrates serious problems with coherence or progression of ideas; the essay displays very little facility\nin the use of language, using very limited vocabulary or incorrect word choice and/or demonstrates frequent\nproblems in sentence structure; the essay contains errors in grammar, usage, and mechanics so serious that\nmeaning is somewhat obscured.\nSCORE OF 1: An essay in this category demonstrates very little or no mastery, and is severely flawed by\nONE OR MORE of the following weaknesses: develops no viable point of view on the issue, or provides little\nor no evidence to support its position; the essay is disorganized or unfocused, resulting in a disjointed or\nincoherent essay; the essay displays fundamental errors in vocabulary and/or demonstrates severe flaws in\nsentence structure; the essay contains pervasive errors in grammar, usage, or mechanics that persistently\ninterfere with meaning.\n\"\"\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:11:45.597814Z",
     "iopub.execute_input": "2024-04-29T22:11:45.598466Z",
     "iopub.status.idle": "2024-04-29T22:11:45.606982Z",
     "shell.execute_reply.started": "2024-04-29T22:11:45.598422Z",
     "shell.execute_reply": "2024-04-29T22:11:45.605984Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "prompts = dict(\n    default=[\n        \"\"\"\n        {{how_to_score}}\n\n        # Target essay\n        {{essay}}\n\n        # Task\n        Please read the essay and assign a score of 1,2,3,4,5,6 where 6 is the best.\n        Output only a single number with no explanation.\n        \"\"\",\n    ],\n    simple=[\n        \"\"\"\n        Please read the following essay and assign a score of 1,2,3,4,5,6 where 6 is the best.\n\n        {{essay}}\n\n        Output only a single number with no explanation.\n        \"\"\",\n    ],\n)\n\n@timers.with_timer('Prediction')\ndef predict(essay, **kwargs):\n    kwargs = {\n        \"prompt\":prompts['default'],\n        \"how_to_score\": how_to_score,\n        \"essay\": essay,\n        **kwargs\n    }\n    prompt = mc.fmt(kwargs.pop('prompt'), **kwargs)\n    return mc.llm(prompt).parse_number(default=0, rounding=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:13:31.607353Z",
     "iopub.execute_input": "2024-04-29T22:13:31.608291Z",
     "iopub.status.idle": "2024-04-29T22:13:31.614315Z",
     "shell.execute_reply.started": "2024-04-29T22:13:31.608255Z",
     "shell.execute_reply": "2024-04-29T22:13:31.613429Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "RESET = False\nBENCH_FILE = 'bench.csv'\nPREDS_FILE = 'preds.csv'\nbench = pd.read_csv(BENCH_FILE) if os.path.exists(BENCH_FILE) and not RESET else pd.DataFrame({\n    \"model\":[],\n    \"prompt\":[],\n    \"acc\":[],\"acc%\":[],\n    \"kappa\":[],\n    \"dur\":[], \n    \n})#.set_index(['model', 'prompt'])\n\nif os.path.exists(PREDS_FILE) and not RESET:\n    preds = pd.read_csv(PREDS_FILE)[:LIMIT_ROWS]\nelse:\n    preds = tdf[['essay_id', 'score']][:LIMIT_ROWS]\n    preds.columns = ['id', 'gt']\n\ndisplay(bench)\ndisplay(preds)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-30T00:15:15.972884Z",
     "iopub.execute_input": "2024-04-30T00:15:15.973645Z",
     "iopub.status.idle": "2024-04-30T00:15:16.040245Z",
     "shell.execute_reply.started": "2024-04-30T00:15:15.973611Z",
     "shell.execute_reply": "2024-04-30T00:15:16.039325Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Benchmarking & Execution Loop Functions ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def benchmark(model, prompt_id, gt_scores, pred_scores, durations) -> dict:    \n    kappa = round(sklearn.metrics.cohen_kappa_score(gt_scores, pred_scores, weights='quadratic'),4)\n    acc = sum(gt_scores == pred_scores)\n    tot = len(gt_scores)\n    pacc = round(100*acc/tot)\n    dur = round(sum(durations)/len(durations),2)\n    return {\n        'model': model,\n        'prompt': prompt_id,\n        'acc': acc,\n        'acc%': pacc,\n        'kappa': kappa,\n        'dur':dur,\n    }  \n    \ndef predict_all(df, prompt_id) -> pd.DataFrame:\n    sub = df[['essay_id']].copy()\n    sub['score'] = 0\n    sub['dur'] = 0\n    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Scoring {len(df)} essays, prompt: {prompt_id}...\"):\n        s = datetime.now()\n        predicted = predict(row.full_text, prompt=prompts[prompt_id][0])\n        sub.loc[i, 'score'] = predicted\n        sub.loc[i, 'dur'] = (datetime.now() - s).total_seconds()\n        \n        if predicted == row.score:\n            print(ui.green(f\"{i+1}: score {predicted} (correct!)\"))\n        else:\n            print(ui.red(f\"{i+1}: score {predicted} (gt={row.score})\"))\n        \n    return sub\n    \n    \ndef not_benchmarked_prompts(model):\n    return [prompt for prompt in prompts if f\"{model}:{prompt}\" not in preds.columns]\n\nnot_benchmarked_prompts('gpt-4-turbo')\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:13:38.190132Z",
     "iopub.execute_input": "2024-04-29T22:13:38.190495Z",
     "iopub.status.idle": "2024-04-29T22:13:38.204965Z",
     "shell.execute_reply.started": "2024-04-29T22:13:38.190466Z",
     "shell.execute_reply": "2024-04-29T22:13:38.204077Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df = tdf[:LIMIT_ROWS].copy() if LIMIT_ROWS else tdf.copy()\n\n\ntarget_models = [  # Leave empty to run it for all models\n    'mixtral-8x-7b(Kaggle T4x2)'\n]\n\nfor model in configs:\n    if target_models and model not in target_models: continue\n    target_prompts = not_benchmarked_prompts(model)\n    if not target_prompts: continue\n        \n    print(ui.magenta(f\"Starting inference using {model}\"))\n    configure(model)\n    \n    for prompt_id in target_prompts:\n        \n        print(ui.magenta(f\"{model} x {prompt_id}\"))\n        \n        sub = predict_all(df, prompt_id)\n        \n        preds[f\"{model}:{prompt_id}\"] = sub.score\n        preds.to_csv(PREDS_FILE, index=False)\n        \n        bench_res = benchmark(model, prompt_id, df.score, sub.score, sub.dur)\n        bench.loc[bench[(bench.model == model) & (bench.prompt == prompt_id)].index.to_list() or len(bench)] = bench_res\n        bench = bench.sort_values(by='kappa', ascending=False)\n        bench.to_csv(BENCH_FILE, index=False)\n        display(bench)\n        display(preds)\n        \nprint(\"Done\")    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:13:41.819078Z",
     "iopub.execute_input": "2024-04-29T22:13:41.819445Z",
     "iopub.status.idle": "2024-04-29T23:57:08.136653Z",
     "shell.execute_reply.started": "2024-04-29T22:13:41.819416Z",
     "shell.execute_reply": "2024-04-29T23:57:08.135524Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Display the Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "display(bench)\ndisplay(preds)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T23:57:08.151058Z",
     "iopub.execute_input": "2024-04-29T23:57:08.151335Z",
     "iopub.status.idle": "2024-04-29T23:57:08.206386Z",
     "shell.execute_reply.started": "2024-04-29T23:57:08.151311Z",
     "shell.execute_reply": "2024-04-29T23:57:08.205572Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### If something goes wrong...",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def drop_data(model):\n    global bench, preds\n    bench = bench[bench['model'] != 'gpt-4-turbo']\n    bench.to_csv(BENCH_FILE, index=False)\n    for prompt_id in prompts:\n        if f'{model}:{prompt_id}' in preds.columns:\n            preds = preds.drop(columns=[f'{model}:{prompt_id}'])\n    preds.to_csv(PREDS_FILE, index=False)\n    \n# drop_data('gpt-4-turbo')\n# display(bench)\n# display(preds.head(3))\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T23:57:08.2079Z",
     "iopub.execute_input": "2024-04-29T23:57:08.208244Z",
     "iopub.status.idle": "2024-04-29T23:57:08.213975Z",
     "shell.execute_reply.started": "2024-04-29T23:57:08.208213Z",
     "shell.execute_reply": "2024-04-29T23:57:08.212895Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "bench.to_csv(BENCH_FILE, index=False) \npreds.to_csv(PREDS_FILE, index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-29T22:13:18.391027Z",
     "iopub.execute_input": "2024-04-29T22:13:18.391708Z",
     "iopub.status.idle": "2024-04-29T22:13:18.398609Z",
     "shell.execute_reply.started": "2024-04-29T22:13:18.391648Z",
     "shell.execute_reply": "2024-04-29T22:13:18.397749Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
