{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import re\nimport copy\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm,trange\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:45:22.663006Z","iopub.execute_input":"2024-05-18T13:45:22.663376Z","iopub.status.idle":"2024-05-18T13:45:24.83685Z","shell.execute_reply.started":"2024-05-18T13:45:22.663349Z","shell.execute_reply":"2024-05-18T13:45:24.835624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T13:45:25.023185Z","iopub.execute_input":"2024-05-18T13:45:25.024133Z","iopub.status.idle":"2024-05-18T13:45:25.032227Z","shell.execute_reply.started":"2024-05-18T13:45:25.024097Z","shell.execute_reply":"2024-05-18T13:45:25.031205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n\n# Display the first sample data in the training set\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:45:28.714168Z","iopub.execute_input":"2024-05-18T13:45:28.71457Z","iopub.status.idle":"2024-05-18T13:45:29.395198Z","shell.execute_reply.started":"2024-05-18T13:45:28.714538Z","shell.execute_reply":"2024-05-18T13:45:29.394409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features engineering","metadata":{}},{"cell_type":"markdown","source":"## 1.Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**ENG**\n* This code is used to clean and normalize text strings. Specifically, it removes HTML tags, converts text to lowercase, deletes strings starting with @, numbers, and URLs, normalizes consecutive spaces and punctuation, and removes leading and trailing whitespace.\n\n**JPN**\n* このコードは、テキストをクレンジングし、正規化するために使用されます。具体的には、HTMLタグの削除、テキストの小文字化、@で始まる文字列、数字、URLの削除、連続する空白や句読点の正規化、先頭と末尾の空白の削除などを行います。","metadata":{}},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:46:20.58885Z","iopub.execute_input":"2024-05-18T13:46:20.589419Z","iopub.status.idle":"2024-05-18T13:46:20.597252Z","shell.execute_reply.started":"2024-05-18T13:46:20.589392Z","shell.execute_reply":"2024-05-18T13:46:20.596029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Paragraph Features","metadata":{}},{"cell_type":"markdown","source":" **ENG**\n* This code performs preprocessing and feature creation from paragraphs in the dataset. The features include paragraph length, the number of sentences and words, and aggregates such as count, maximum, average, minimum, etc., of these features. The final result is a DataFrame with the calculated features, used as input for machine learning models or data analysis.\n\n**JPN**\n* このコードは、データセット内の段落を前処理し、特徴量を作成するものです。特徴量には、段落の長さ、文や単語の数、およびこれらの特徴量に対する集計（カウント、最大値、平均、最小値など）が含まれます。最終的な結果は、計算された特徴量を持つDataFrameであり、機械学習モデルやデータ分析の入力として使用されます。","metadata":{}},{"cell_type":"code","source":"# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:49:36.310881Z","iopub.execute_input":"2024-05-18T13:49:36.311294Z","iopub.status.idle":"2024-05-18T13:49:44.669524Z","shell.execute_reply.started":"2024-05-18T13:49:36.311263Z","shell.execute_reply":"2024-05-18T13:49:44.668497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.Sentence Features","metadata":{}},{"cell_type":"markdown","source":"**ENG**\n* This code preprocesses text and generates features related to sentences in the full text within a dataset. Features include sentence length, word count per sentence, and aggregations such as counts, maximum values, averages, minimum values, etc., of these features. The final result is a DataFrame with the computed features, which can be used as input for machine learning models or data analysis.\n\n**JPN**\n* このコードは、データセット内のフルテキストから文に関する特徴量を前処理し、生成します。特徴量には、文の長さ、単語数、およびそれらの特徴量のカウント、最大値、平均値、最小値などの集計が含まれます。最終的な結果は、計算された特徴量を含むDataFrameであり、機械学習モデルやデータ分析の入力として使用されます。","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:51:26.8461Z","iopub.execute_input":"2024-05-18T13:51:26.846489Z","iopub.status.idle":"2024-05-18T13:51:35.354817Z","shell.execute_reply.started":"2024-05-18T13:51:26.846462Z","shell.execute_reply":"2024-05-18T13:51:35.353759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.Word Features","metadata":{}},{"cell_type":"markdown","source":"**ENG**\n* This code preprocesses text and generates features related to words in the full text within a dataset. Features include word length and aggregations such as word counts, maximum values, averages, standard deviations, and quantiles of word lengths. The final result is a DataFrame with the computed features, which can be used as input for machine learning models or data analysis.\n\n**Japanese**\n* このコードは、データセット内のフルテキストから単語に関する特徴量を前処理し、生成します。特徴量には、単語の長さ、およびそれらの特徴量のカウント、最大値、平均値、標準偏差、四分位数などの集計が含まれます。最終的な結果は、計算された特徴量を含むDataFrameであり、機械学習モデルやデータ分析の入力として使用されます。","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:51:59.305247Z","iopub.execute_input":"2024-05-18T13:51:59.306274Z","iopub.status.idle":"2024-05-18T13:52:13.907729Z","shell.execute_reply.started":"2024-05-18T13:51:59.306233Z","shell.execute_reply":"2024-05-18T13:52:13.906849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.Tf-idf features","metadata":{}},{"cell_type":"markdown","source":"**ENG**\n* This code block is used to generate TF-IDF (Term Frequency-Inverse Document Frequency) features from the text data in the 'full_text' column of a dataset. TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents.\n\n**JPN**\n* データセット内の 'full_text' 列のテキストデータから TF-IDF (Term Frequency-Inverse Document Frequency) 特徴量を生成するために使用されます。 TF-IDF は、文書集合に対する単語の重要性を反映する数値統計です。","metadata":{}},{"cell_type":"code","source":"# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:52:29.46582Z","iopub.execute_input":"2024-05-18T13:52:29.466471Z","iopub.status.idle":"2024-05-18T13:54:07.37773Z","shell.execute_reply.started":"2024-05-18T13:52:29.466441Z","shell.execute_reply":"2024-05-18T13:54:07.376992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:54:08.205112Z","iopub.execute_input":"2024-05-18T13:54:08.205787Z","iopub.status.idle":"2024-05-18T13:54:08.214195Z","shell.execute_reply.started":"2024-05-18T13:54:08.205733Z","shell.execute_reply":"2024-05-18T13:54:08.21319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD = False \nmodels = []\nif LOAD:\n    for i in range(5):\n        models.append(lgb.Booster(model_file=f'../input/lal-lgb-baseline-4/fold_{i}.txt'))\nelse:\n    # OOF is used to store the prediction results of each model on the validation set\n    oof = []\n    x= train_feats\n    y= train_feats['score'].values\n    # 5 fold\n    kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n    for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n            # create model\n            model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.1,\n                max_depth = 5,\n                num_leaves = 10,\n                colsample_bytree=0.5,\n                reg_alpha = 0.1,\n                reg_lambda = 0.8,\n                n_estimators=1024,\n                random_state=42,\n                verbosity = - 1)\n            # Take out the training and validation sets for 5 kfold segmentation separately\n            X_train = train_feats.iloc[trn_idx][feature_names]\n            Y_train = train_feats.iloc[trn_idx]['score'] - a\n\n            X_val = train_feats.iloc[val_idx][feature_names]\n            Y_val = train_feats.iloc[val_idx]['score'] - a\n            print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n            # Training model\n            lgb_model = model.fit(X_train,\n                                  Y_train,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train, Y_train), (X_val, Y_val)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks,)\n            # Use the trained model to predict the validation set\n            pred_val = lgb_model.predict(\n                X_val, num_iteration=lgb_model.best_iteration_)\n            df_tmp = train_feats.iloc[val_idx][['essay_id', 'score']].copy()\n            df_tmp['pred'] = pred_val + a\n            oof.append(df_tmp)\n            # Save model parameters\n            models.append(model.booster_)\n            lgb_model.booster_.save_model(f'fold_{fold_id}.txt')\n    df_oof = pd.concat(oof)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:56:53.845937Z","iopub.execute_input":"2024-05-18T13:56:53.846373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV","metadata":{}},{"cell_type":"code","source":"if LOAD:\n    print('acc: ',0.6275495464263015)\n    print('kappa: ',0.7990509565910948)\nelse:\n    acc = accuracy_score(df_oof['score'], df_oof['pred'].clip(1, 6).round())\n    kappa = cohen_kappa_score(df_oof['score'], df_oof['pred'].clip(1, 6).round(), weights=\"quadratic\")\n    print('acc: ',acc)\n    print('kappa: ',kappa)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = test_feats[['essay_id']].copy()\nprediction['score'] = 0\npred_test = models[0].predict(test_feats[feature_names]) + a\nfor i in range(4):\n    pred_now = models[i+1].predict(test_feats[feature_names]) + a\n    pred_test = np.add(pred_test,pred_now)\n# The final prediction result needs to be divided by 5 because the prediction results of 5 models were added together\npred_test = pred_test/5\nprint(pred_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Round the prediction result to an integer and limit it to a range of 1-6 (score range)\npred_test = pred_test.clip(1, 6).round()\nprediction['score'] = pred_test\nprediction.to_csv('submission.csv', index=False)\nprediction.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference Notebook","metadata":{}},{"cell_type":"markdown","source":"I would like to give thanks to the authors of these public notebooks. I have learned a lot from you.¶\n* https://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n* https://www.kaggle.com/code/yunsuxiaozi/aes2-0-baseline-naivebayesclassifier\n* https://www.kaggle.com/code/finlay/llm-detect-0-to-1\n* https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features\n* https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features\n* https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective","metadata":{}},{"cell_type":"markdown","source":"# END","metadata":{}}]}