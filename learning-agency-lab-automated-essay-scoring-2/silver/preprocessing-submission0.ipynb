{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":818994,"sourceType":"datasetVersion","datasetId":428389}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports & Data Frame\nSetting up imports and functions to process dataframes and features","metadata":{"id":"f88UmaELKmIq"}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport subprocess\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist\n\n# Ensure necessary NLTK downloads (w/o internet)\nnltk.data.path.append('/kaggle/input/nltk-data/nltk_data')\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\n\n# Load data\ntrain_file = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\ndf = pd.read_csv(train_file)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T21:29:16.520176Z","iopub.execute_input":"2024-05-13T21:29:16.520591Z","iopub.status.idle":"2024-05-13T21:29:17.77541Z","shell.execute_reply.started":"2024-05-13T21:29:16.520561Z","shell.execute_reply":"2024-05-13T21:29:17.774197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Generation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport nltk\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('maxent_ne_chunker')\n# nltk.download('words')\n\n# Text processing algorithm, explained below\ndef comprehensive_text_preprocessing(text):\n    # Lowercase and remove HTML tags\n    text = text.lower()\n    text = re.sub(r'<.*?>', '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Tokenization\n    words = word_tokenize(text)\n\n    # Remove stop words\n    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n\n    # Stemming and Lemmatization\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n\n    return \" \".join(words)  # Return the processed text\n\n# Applying the comprehensive_text_preprocessing to the 'full_text' column\ndf['processed_text'] = df['full_text'].apply(comprehensive_text_preprocessing)\n\n# Get length of words as a feature\ndef calculate_word_lengths(text):\n    words = word_tokenize(text)\n    return [len(word) for word in words]\n\ndf['word_lengths'] = df['processed_text'].apply(calculate_word_lengths)\n\n# Get the average word lengths \ndef average_word_length(text):\n    words = text.split()  # Split the text into words\n    if not words:\n        return 0  # Avoid division by zero if there are no words\n    total_length = sum(len(word) for word in words)\n    return total_length / len(words)\n\n# Calculating average word length\ndf['avg_word_length'] = df['processed_text'].apply(average_word_length)\n\n# Get the length of all the essays\ndf['essay_length'] = df['processed_text'].apply(len)\n\n# Group by 'score' and calculate the mean of 'avg_word_length' for each score\naverage_lengths_by_score = df.groupby('score')['avg_word_length'].mean().reset_index()\n\n# Function to calculate sentence count\ndef calculate_sentence_count(text):\n    sentences = sent_tokenize(text)\n    return len(sentences)\n\n# Calculate sentence count for each essay\ndf['sentence_count'] = df['full_text'].apply(calculate_sentence_count)\n\n# Plot all our data for each feature to visualize trends related to score\n\nplt.figure(figsize=(10, 6))\nplt.scatter(df['essay_length'], df['score'], color='blue')\nplt.title('Score vs Essay Length')\nplt.xlabel('Essay Length')\nplt.ylabel('Score')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(df['sentence_count'], df['score'], color='blue')\nplt.title('Score vs Sentence Count')\nplt.xlabel('Sentence Count')\nplt.ylabel('Score')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.bar(average_lengths_by_score['score'], average_lengths_by_score['avg_word_length'], color='skyblue')\nplt.xlabel('Score')\nplt.ylabel('Average Word Length')\nplt.title('Average Word Length by Essay Score')\nplt.xticks(average_lengths_by_score['score'])  # Set x-ticks to be the scores\nplt.show()","metadata":{"id":"OvE6uG2d5zWZ","outputId":"f81efac0-7bca-47fe-98e3-7f4cb023f9a9","execution":{"iopub.status.busy":"2024-05-13T22:39:28.684227Z","iopub.execute_input":"2024-05-13T22:39:28.684663Z","iopub.status.idle":"2024-05-13T22:43:30.461034Z","shell.execute_reply.started":"2024-05-13T22:39:28.684635Z","shell.execute_reply":"2024-05-13T22:43:30.458992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PreProcessing allows us to clean up our text input by:\n* Removing punctuation\n* Tokenizing the words\n* Removing stop words\n* Running Stemmer and Lemmatization functions\n\nThen we generate some potential features such as:\n* Essay Length\n* Average Word Length\n* Word Length\n* Number of sentences\n\nWe then save this to a row called processed_text and also save a row giving us the lengths of each essay to analyze later.","metadata":{}},{"cell_type":"markdown","source":"# Word2Vec Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\n\nword2vec_model = Word2Vec(sentences=df['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n\nX = df['processed_text'] # Feature\ny = df['score'] # Target\n\ndef get_average_embedding(text):\n    words = comprehensive_text_preprocessing(text)\n    # Filter out words that are not in the vocabulary of the Word2Vec model\n    words_in_vocab = [word for word in words if word in word2vec_model.wv]\n    if len(words_in_vocab) > 0:\n        return np.mean([word2vec_model.wv[word] for word in words_in_vocab], axis=0)\n    else:\n        return np.zeros(word2vec_model.vector_size)  # Return zero vector if no words in vocabulary\n\nX_features = X.apply(get_average_embedding)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_features.tolist(), y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:02:39.359262Z","iopub.execute_input":"2024-05-13T22:02:39.360663Z","iopub.status.idle":"2024-05-13T22:06:57.206085Z","shell.execute_reply.started":"2024-05-13T22:02:39.360611Z","shell.execute_reply":"2024-05-13T22:06:57.204947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After processing all our words using our custom preprocessing code we clear all the words out that are not within our word2vec model based on processed text.\n\nThen, we calculate average word embedding per essay this tells us what words/features represent using dense vectors of real numbers. Each dimension of the vector represents a feature of the word.","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression Model\nCreate and run the model on **training** data","metadata":{"id":"CTtG6ICTJlpG"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Train linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict scores on test set\ny_pred = model.predict(X_test)\n\n# Evaluate model\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)","metadata":{"id":"Idn7fdlALrHu","outputId":"b70e0b16-de4d-4786-c5d0-cec299d6e681","execution":{"iopub.status.busy":"2024-05-13T22:06:57.208523Z","iopub.execute_input":"2024-05-13T22:06:57.208918Z","iopub.status.idle":"2024-05-13T22:06:57.308057Z","shell.execute_reply.started":"2024-05-13T22:06:57.208888Z","shell.execute_reply":"2024-05-13T22:06:57.306522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Generator\nRunning the model on **test** data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ntest_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n\n# ---- PreProcessing Our Test Data ----\n\n# We only need this feature to process our data and apply the model.\ntest_df['processed_text'] = test_df['full_text'].apply(comprehensive_text_preprocessing)\n\n# Using our prior Word2Vec model process our data and make predictions using the model\n\nX = test_df['processed_text']\n\n# Calculate the embeddings as done before\ndef get_average_embedding(text):\n    words = comprehensive_text_preprocessing(text)\n    # Filter out words that are not in the vocabulary of the Word2Vec model\n    words_in_vocab = [word for word in words if word in word2vec_model.wv]\n    if len(words_in_vocab) > 0:\n        return np.mean([word2vec_model.wv[word] for word in words_in_vocab], axis=0)\n    else:\n        return np.zeros(word2vec_model.vector_size)  # Return zero vector if no words in vocabulary\n\nX_features = X.apply(get_average_embedding)\n\n# Make predictions on the test set\ny_pred_lin = model.predict(X_features.tolist())\n\n# Create a DataFrame to store essay_id and predicted scores\npredictions_df = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': [int(np.round(x)) for x in y_pred_lin]})\n\n# Output predictions to a CSV file\npredictions_df.to_csv(\"/kaggle/working/submission.csv\", index=None)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:06:57.309838Z","iopub.execute_input":"2024-05-13T22:06:57.311383Z","iopub.status.idle":"2024-05-13T22:06:57.484448Z","shell.execute_reply.started":"2024-05-13T22:06:57.311333Z","shell.execute_reply":"2024-05-13T22:06:57.483309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data = pd.read_csv(\"/kaggle/working/submission.csv\")\n\nsubmission_data","metadata":{"execution":{"iopub.status.busy":"2024-05-13T22:06:57.486254Z","iopub.execute_input":"2024-05-13T22:06:57.486627Z","iopub.status.idle":"2024-05-13T22:06:57.506451Z","shell.execute_reply.started":"2024-05-13T22:06:57.486594Z","shell.execute_reply":"2024-05-13T22:06:57.505608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing our results we see scores within region from 1-6 and these values make sense based on ","metadata":{}}]}