{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### I would like to give thank to @ye11725 and @davidjlochner for strong baseline and idea, I have reconstucted the code and added a ensemble of 4 model of LGBM.  \n#### Here is some of my references: \nhttps://www.kaggle.com/code/davidjlochner/base-tfidf-lgbm\n\nhttps://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments","metadata":{}},{"cell_type":"code","source":"import re\nimport polars as pl\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn.ensemble import VotingClassifier,VotingRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom tqdm.auto import tqdm,trange\nfrom lightgbm import log_evaluation, early_stopping\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T17:12:51.645899Z","iopub.execute_input":"2024-04-13T17:12:51.646595Z","iopub.status.idle":"2024-04-13T17:12:51.653313Z","shell.execute_reply.started":"2024-04-13T17:12:51.646566Z","shell.execute_reply":"2024-04-13T17:12:51.652277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureEngineering():\n    def __init__(self):\n        self.columns = [\n            (pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))\n        ]\n        self.train_dataset = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv').with_columns(self.columns)\n        self.test_dataset = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv').with_columns(self.columns)\n        # feature_eng\n        self.sentence_fea = ['sentence_len','sentence_word_cnt']\n        # feature_eng\n        self.paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n        self.vectorizer = TfidfVectorizer(tokenizer=lambda x: x,\n                                          preprocessor=lambda x: x,\n                                          token_pattern=None,\n                                          strip_accents='unicode',\n                                          analyzer = 'word',\n                                          ngram_range=(1,3),\n                                          min_df=0.05,\n                                          max_df=0.9,\n                                          sublinear_tf=True  \n        )\n    def removeHTML(self,x):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',x)\n    def dataPreprocessing(self,x):\n        x = x.lower()             # covert all letter to lower form\n        x = self.removeHTML(x)\n        x = re.sub(\"@\\w+\", '',x)\n        x = re.sub(\"'\\d+\", '',x)\n        x = re.sub(\"\\d+\", '',x)\n        x = re.sub(\"http\\w+\", '',x)\n        x = re.sub(r\"\\s+\", \" \",x) # replace any sequence of whitespace characters with a sigle whitespace\n        x = re.sub(r\"\\.+\", \".\",x) # replace any sequence of periods with a sigle periods\n        x = re.sub(r\"\\,+\", \",\",x) # replace any sequence of commas with a sigle comma\n        x = x.strip()\n        return x \n    def Paragraph_Preprocess(self,tmp):\n        tmp = tmp.explode('paragraph')\n        # preprocess\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(self.dataPreprocessing))\n        # paragraph_len\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x:len(x)).alias(\"paragraph_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('paragraph_len')>=20)\n        # paragraph_sentence_count/paragraph_word_count\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split(\".\"))).alias(\"paragraph_sentence_cnt\"),\n                               pl.col('paragraph').map_elements(lambda x: len(x.split(\" \"))).alias(\"paragraph_word_cnt\")\n                              )\n        return tmp\n    def Paragraph_Eng(self,train_tmp):\n        aggs = [\n            # paragraph_len_cnt\n            *[pl.col('paragraph').filter(pl.col('paragraph_len')>=i)\n            .count().alias(f'paragraph_{i}_cnt') for i in [20,100,200,300,400,500,600,700]],\n            # other\n            *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in self.paragraph_fea],\n            *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in self.paragraph_fea],\n            *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in self.paragraph_fea],\n            *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in self.paragraph_fea],\n            *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in self.paragraph_fea],\n        ]\n        df = train_tmp.group_by([\"essay_id\"], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Paragraph_Eng +\",len(df.columns),\"features\")\n        return df\n    def Sentence_Preprocess(self,tmp):\n        tmp = tmp.with_columns(pl.col('full_text').map_elements(self.dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n        tmp = tmp.explode('sentence')\n        # sentence_len\n        tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('sentence_len')>=15)\n        # sentence_word_cnt\n        tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n\n        return tmp\n    def Sentence_Eng(self,train_tmp):\n        aggs = [\n            # sentence_cnt\n            *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n            # other\n            *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in self.sentence_fea],\n            *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in self.sentence_fea],\n            *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in self.sentence_fea],\n            *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in self.sentence_fea],\n            *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in self.sentence_fea],\n            ]\n        df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Sentence_Eng +\",len(df.columns),\"features\")\n        return df\n    # word feature\n    def Word_Preprocess(self,tmp):\n        tmp = tmp.with_columns(pl.col('full_text').map_elements(self.dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n        tmp = tmp.explode('word')\n        # word_len\n        tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('word_len')!=0)\n\n        return tmp\n    # feature_eng\n    def Word_Eng(self,train_tmp):\n        aggs = [\n            # word_cnt\n            *[pl.col('word').filter(pl.col('word_len') >= i+1)\n              .count().alias(f\"word_{i+1}_cnt\") for i in range(15)], \n            # other\n            pl.col('word_len').max().alias(f\"word_len_max\"),\n            pl.col('word_len').mean().alias(f\"word_len_mean\"),\n            pl.col('word_len').std().alias(f\"word_len_std\"),\n            pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n            pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n            pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n        df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Word_Eng +\",len(df.columns),\"features\")\n        return df\n    def process(self):\n        tmp = self.Paragraph_Preprocess(self.train_dataset)\n        train_feats = self.Paragraph_Eng(tmp)\n        train_feats['score'] = self.train_dataset['score']\n        \n        tmp = self.Sentence_Preprocess(self.train_dataset)\n        train_feats = train_feats.merge(self.Sentence_Eng(tmp), on='essay_id', how='left')\n        \n        tmp = self.Word_Preprocess(self.train_dataset)\n        train_feats = train_feats.merge(self.Word_Eng(tmp), on='essay_id', how='left')\n        \n        train_tfid = self.vectorizer.fit_transform([i for i in self.train_dataset['full_text']])\n        dense_matrix = train_tfid.toarray()\n        df = pd.DataFrame(dense_matrix)\n        tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n        df.columns = tfid_columns\n#         print(df)\n#         print(\"----------------------------------------------------------\")\n        df['essay_id'] = train_feats['essay_id']\n        # merge\n        train_feats = train_feats.merge(df, on='essay_id', how='left')\n        print('feature_num: ',len(train_feats.columns)-2)\n        return train_feats\n    def process_test(self):\n        temp = self.Paragraph_Preprocess(self.test_dataset)\n        test_feats = self.Paragraph_Eng(temp)\n        \n        temp = self.Sentence_Preprocess(self.test_dataset)\n        test_feats = test_feats.merge(self.Sentence_Eng(temp), on='essay_id', how='left')\n        \n        temp = self.Word_Preprocess(self.test_dataset)\n        test_feats = test_feats.merge(self.Word_Eng(temp), on='essay_id', how='left')\n        \n        test_tfid = self.vectorizer.transform([i for i in self.test_dataset['full_text']])\n        dense_matrix = test_tfid.toarray()\n        df = pd.DataFrame(dense_matrix)\n        tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n        df.columns = tfid_columns\n#         print(df)\n        df['essay_id'] = test_feats['essay_id']\n        # merge\n        test_feats = test_feats.merge(df, on='essay_id', how='left')\n        print('feature_num: ',len(test_feats.columns)-2)\n        \n        return test_feats","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:12:51.658921Z","iopub.execute_input":"2024-04-13T17:12:51.659204Z","iopub.status.idle":"2024-04-13T17:12:51.700615Z","shell.execute_reply.started":"2024-04-13T17:12:51.659181Z","shell.execute_reply":"2024-04-13T17:12:51.699943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LGBM():\n    def __init__(self):\n        self.data_train = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n        self.data_test = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n        self.num_models = 3\n        self.acc_metrics = []\n        self.cohen_metrics = []\n        \n        # coef for cohen kappa score\n        self.a = 2.948\n        self.b = 1.092\n        \n        self.lgb_parameters = {  \n                                 'metrics': 'None',\n                                 'objective': self.qwk_obj,\n                                 'learning_rate': 0.1,\n                                 'max_depth': 5,\n                                 'num_leaves': 15, # should be a number smaller than \"max_depth\"^2\n                                 'colsample_bytree': 0.5,\n                                 'min_data_in_leaf': 100,\n                                 'reg_alpha': 0.8,\n                                 'n_estimators': 256,\n                                 'verbosity': -1,\n#                                  'device' : \"gpu\"\n        }\n#         self.catboost_parameters = {\n#                                 'iterations': 1000,\n#                                 'task_type' : 'GPU',\n#                                 'learning_rate': 0.1,\n#                                 'depth': 6,\n#                                 'loss_function': 'MultiClass',\n#                                 'verbose': 0\n#         }\n\n        self.model = VotingRegressor(\n            estimators = [(f\"lgb_{i}\",lgb.LGBMRegressor(**self.lgb_parameters, random_state=i+40),)for i in range(self.num_models)\n#                           (f\"cb_{i}\",CatBoostClassifier(**self.catboost_parameters, random_state=i+40),)for i in range(self.num_models)\n                         ],n_jobs=-1\n#                             voting = \"soft\"\n        )\n        \n    def quadratic_weighted_kappa(self,y_true,y_pred):\n        y_true = y_true + self.a\n        y_pred = (y_pred + self.a).clip(1,6).round()\n#         print(y_true)\n#         print(y_pred)\n        qwk = cohen_kappa_score(y_true,y_pred,weights='quadratic')\n        \n        return \"QWK\",qwk,True\n    def qwk_obj(self,y_true,y_pred):\n        labels = y_true + self.a\n        preds = y_pred + self.a\n        preds = preds.clip(1,6)\n        f = 1/2 * np.sum((preds-labels)**2)\n        g = 1/2 * np.sum((preds-self.a)**2+self.b)\n        df = preds - labels\n        dg = preds - self.a\n        grad = (df/g - f*dg/g**2)*len(labels)\n        hess = np.ones(len(labels))\n        \n        return grad,hess\n    def fit(self,df,fold):\n        feature_names = list(filter(lambda x: x not in ['essay_id','score'], df.columns))\n        x= df\n        y= df['score'].values\n        kfold = KFold(n_splits=5, random_state=44, shuffle=True)\n        \n        for fold_id, (trn_idx, val_idx) in tqdm(enumerate(kfold.split(x.copy(), y.copy().astype(str)))):\n#             if fold_id != fold:\n#                 break\n            X_train = df.iloc[trn_idx][feature_names]\n            Y_train = df.iloc[trn_idx]['score'] - self.a\n\n            X_val = df.iloc[val_idx][feature_names]\n            Y_val = df.iloc[val_idx]['score'] - self.a\n            print('\\nFold_{} Training ================================\\n'.format(fold_id))\n            \n            self.model.fit(X_train,\n                           Y_train,\n                           )\n            pred_val = self.model.predict(X_val)\n#             print(pred_val)\n            df_tmp = df.iloc[val_idx][['essay_id', 'score']].copy()\n            df_tmp['pred'] = pred_val\n            cohen_score = self.quadratic_weighted_kappa(Y_val.values, df_tmp['pred'])\n#             accuracy = accuracy_score(Y_val.values,  df_tmp['pred'].clip(1, 6).round())\n#             self.acc_metrics.append(accuracy)\n            self.cohen_metrics.append(cohen_score[1])\n#             print(f\"Accuracy fold {fold_id}: {accuracy:.4f}\")\n#             print(cohen_score)\n#             print(f\"Cohen score fold {fold_id}: {cohen_score[1]:.4f}\")\n#         average_accuracy = np.mean(self.acc_metrics)\n        average_cohen = np.mean(self.cohen_metrics)\n        \n#         print(f'Average Accuracy all fold: {average_accuracy:.4f}')\n        print(f'Average Cohen all fold: {average_cohen:.4f}')\n\n    def predict(self,df):\n        feature_names = list(filter(lambda x: x not in ['essay_id'], df.columns))\n        predictions = self.model.predict(df[feature_names]+self.a).clip(1, 6).round()\n        return predictions\n    def submit(self,df):\n        feature_names = list(filter(lambda x: x not in ['essay_id'], df.columns))\n        return self.data_test.select('essay_id').with_columns(score = (self.model.predict(df[feature_names])+self.a).clip(1, 6).round())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:23:31.694817Z","iopub.execute_input":"2024-04-13T17:23:31.695284Z","iopub.status.idle":"2024-04-13T17:23:31.715595Z","shell.execute_reply.started":"2024-04-13T17:23:31.695256Z","shell.execute_reply":"2024-04-13T17:23:31.714697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FE = FeatureEngineering()\ntrain_feature = FE.process()\ntest_feature = FE.process_test()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:12:58.261683Z","iopub.execute_input":"2024-04-13T17:12:58.262554Z","iopub.status.idle":"2024-04-13T17:14:47.438262Z","shell.execute_reply.started":"2024-04-13T17:12:58.262512Z","shell.execute_reply":"2024-04-13T17:14:47.437168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LGBM()\nmodel.fit(df=train_feature,fold=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:23:34.851561Z","iopub.execute_input":"2024-04-13T17:23:34.852486Z","iopub.status.idle":"2024-04-13T17:24:04.466004Z","shell.execute_reply.started":"2024-04-13T17:23:34.852451Z","shell.execute_reply":"2024-04-13T17:24:04.464415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = model.submit(test_feature)\ndisplay(submission)\nsubmission.write_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:21:44.432853Z","iopub.execute_input":"2024-04-13T17:21:44.433792Z","iopub.status.idle":"2024-04-13T17:21:44.737917Z","shell.execute_reply.started":"2024-04-13T17:21:44.433758Z","shell.execute_reply":"2024-04-13T17:21:44.736952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}