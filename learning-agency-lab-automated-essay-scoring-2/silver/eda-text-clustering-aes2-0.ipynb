{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Hi everyone!  \nI am very happy that if my notebook is helpful to someone.  \nIf you like please read my EDA and text clustering notebook.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Import library and Load data  \nFirst, I import library to load and analysis dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from collections import Counter\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nimport nltk\nfrom nltk.corpus import stopwords\nimport warnings\n\nwarnings.filterwarnings('ignore')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:21.987258Z",
     "iopub.execute_input": "2024-05-08T11:36:21.987753Z",
     "iopub.status.idle": "2024-05-08T11:36:25.324514Z",
     "shell.execute_reply.started": "2024-05-08T11:36:21.987699Z",
     "shell.execute_reply": "2024-05-08T11:36:25.323556Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Create a class to collect paths.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PATHS:\n    train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n    test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\n    sample_sub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:26.93908Z",
     "iopub.execute_input": "2024-05-08T11:36:26.93957Z",
     "iopub.status.idle": "2024-05-08T11:36:26.944352Z",
     "shell.execute_reply.started": "2024-05-08T11:36:26.939539Z",
     "shell.execute_reply": "2024-05-08T11:36:26.943251Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load training dataset and check it simply.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv(PATHS.train_path)\ndisplay(train.head())\nprint('')\ndisplay(train.describe().T)\nprint('')\nprint(f'Shape of training dataset: {train.shape}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:27.821413Z",
     "iopub.execute_input": "2024-05-08T11:36:27.821834Z",
     "iopub.status.idle": "2024-05-08T11:36:28.653921Z",
     "shell.execute_reply.started": "2024-05-08T11:36:27.821801Z",
     "shell.execute_reply": "2024-05-08T11:36:28.6528Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load sample submission file and check it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "sample_submission = pd.read_csv(PATHS.sample_sub_path)\nsample_submission",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:29.936318Z",
     "iopub.execute_input": "2024-05-08T11:36:29.936704Z",
     "iopub.status.idle": "2024-05-08T11:36:29.950459Z",
     "shell.execute_reply.started": "2024-05-08T11:36:29.936671Z",
     "shell.execute_reply": "2024-05-08T11:36:29.949303Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load test data and check it.  \nThe test data is replaced during inference, so it has the same content as the first three rows of the training data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv(PATHS.test_path)\ntest",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:33.20434Z",
     "iopub.execute_input": "2024-05-08T11:36:33.204709Z",
     "iopub.status.idle": "2024-05-08T11:36:33.218932Z",
     "shell.execute_reply.started": "2024-05-08T11:36:33.204681Z",
     "shell.execute_reply": "2024-05-08T11:36:33.217798Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Create basic features  \nSecond, I create basic features from text.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def count_stopwords(text, stopwords):\n    '''Function that count a number of words which is not stopwords of nltk. '''\n    text = text.split()\n    stopwords_length = len([t for t in text if t in stopwords])\n    return stopwords_length\n\ndef create_features(df, stopwords):\n    '''Funcition that create features.'''\n    # Count letters\n    df['letters'] = df['full_text'].apply(lambda x: len(x))\n    # Count words\n    df['words'] = df['full_text'].apply(lambda x: len(x.split()))\n    # Count words that is unique in text\n    df['unique_words'] = df['full_text'].apply(lambda x: len(set(x.split())))\n    # Count sentence\n    df['sentences'] = df['full_text'].apply(lambda x: len(x.split('.')))\n    # Count paragraph\n    df['paragraph'] = df['full_text'].apply(lambda x: len(x.split('\\n\\n')))\n    # Count stopwords \n    df['stopwords'] = df['full_text'].apply(count_stopwords, args=(stopwords,))\n    # Count not stopwords\n    df['not_stopwords'] = df['words'] - df['stopwords']\n    return df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:34.398416Z",
     "iopub.execute_input": "2024-05-08T11:36:34.399336Z",
     "iopub.status.idle": "2024-05-08T11:36:34.407935Z",
     "shell.execute_reply.started": "2024-05-08T11:36:34.3993Z",
     "shell.execute_reply": "2024-05-08T11:36:34.406791Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Download stopwords and apply function that defined above to training dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "nltk.download('stopwords')\nstop_words = stopwords.words('english')\n\ndata = create_features(train, stop_words)\ndata",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:38.568524Z",
     "iopub.execute_input": "2024-05-08T11:36:38.56943Z",
     "iopub.status.idle": "2024-05-08T11:36:52.242187Z",
     "shell.execute_reply.started": "2024-05-08T11:36:38.569395Z",
     "shell.execute_reply": "2024-05-08T11:36:52.241137Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Check distribution and Plot data \nFirst, I check the distribution of score.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data['score'].value_counts().sort_index()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:52.244095Z",
     "iopub.execute_input": "2024-05-08T11:36:52.244409Z",
     "iopub.status.idle": "2024-05-08T11:36:52.258138Z",
     "shell.execute_reply.started": "2024-05-08T11:36:52.244381Z",
     "shell.execute_reply": "2024-05-08T11:36:52.257065Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(5, 3))\nplt.title('Destribution of score')\nsns.histplot(data=data, x='score')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:52.259647Z",
     "iopub.execute_input": "2024-05-08T11:36:52.260542Z",
     "iopub.status.idle": "2024-05-08T11:36:52.633858Z",
     "shell.execute_reply.started": "2024-05-08T11:36:52.260505Z",
     "shell.execute_reply": "2024-05-08T11:36:52.632698Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Second, check the distribusion of each features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "num_rows = 4\nnum_cols = 2\nplt.figure(figsize=(20, 20))\nfor i, col in enumerate(data.columns[3:], start=1):\n    plt.subplot(num_rows, num_cols, i)\n    sns.kdeplot(data=data, x=col, hue='score', palette='bright', fill=False)\n    plt.title(col, fontsize=20)\n    \nplt.tight_layout()\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:36:52.636324Z",
     "iopub.execute_input": "2024-05-08T11:36:52.636635Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Next is that plot scatter each features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(15, 15))\nsns.pairplot(data=data.drop(columns=['essay_id', 'full_text']), hue='score', palette='bright')",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-05-08T11:38:18.584615Z",
     "shell.execute_reply.started": "2024-05-08T11:36:56.427129Z",
     "shell.execute_reply": "2024-05-08T11:38:18.583242Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Lastly, check the corelation of each features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data.drop(columns=['essay_id', 'full_text']).corr()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:38:18.586035Z",
     "iopub.execute_input": "2024-05-08T11:38:18.586388Z",
     "iopub.status.idle": "2024-05-08T11:38:18.609711Z",
     "shell.execute_reply.started": "2024-05-08T11:38:18.586357Z",
     "shell.execute_reply": "2024-05-08T11:38:18.608655Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(5, 4))\nsns.heatmap(data=data.drop(columns=['essay_id', 'full_text']).corr(), annot=True, fmt=\".2f\", linewidth=.5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:38:18.611263Z",
     "iopub.execute_input": "2024-05-08T11:38:18.611632Z",
     "iopub.status.idle": "2024-05-08T11:38:19.143517Z",
     "shell.execute_reply.started": "2024-05-08T11:38:18.6116Z",
     "shell.execute_reply": "2024-05-08T11:38:19.142469Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Text Clustering to get prompt_name\nTraining data does not have `prompt_name`, so I try to text clustering using KMeans.  \nText preprocessing and creating features part is thanks to great notebook.  \n[https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef remove_stopwords(df, stopwords):\n    all_text = []\n    for text in df['cleaned_text']:\n        text = text.split()\n        each_text = []\n        for t in text:\n            if t not in stopwords:\n                each_text.append(t)\n        all_text.append(' '.join(each_text))\n    df['cleaned_text'] = all_text\n    return df               ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:38:27.354645Z",
     "iopub.execute_input": "2024-05-08T11:38:27.355407Z",
     "iopub.status.idle": "2024-05-08T11:38:27.363486Z",
     "shell.execute_reply.started": "2024-05-08T11:38:27.355373Z",
     "shell.execute_reply": "2024-05-08T11:38:27.36242Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data['cleaned_text'] = data['full_text'].apply(dataPreprocessing)\ndata = remove_stopwords(data, stop_words)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:38:27.91025Z",
     "iopub.execute_input": "2024-05-08T11:38:27.910636Z",
     "iopub.status.idle": "2024-05-08T11:38:44.442928Z",
     "shell.execute_reply.started": "2024-05-08T11:38:27.910605Z",
     "shell.execute_reply": "2024-05-08T11:38:44.441793Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\ntrain_tfid = vectorizer.fit_transform([i for i in data['cleaned_text']])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:38:44.44498Z",
     "iopub.execute_input": "2024-05-08T11:38:44.445303Z",
     "iopub.status.idle": "2024-05-08T11:39:27.637778Z",
     "shell.execute_reply.started": "2024-05-08T11:38:44.445275Z",
     "shell.execute_reply": "2024-05-08T11:39:27.63666Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "num_k = range(1, 11)\ninertia_values = []\nfor k in num_k:\n    kmeans = KMeans(n_clusters=k).fit(train_tfid.toarray())\n    inertia_values.append(kmeans.inertia_)\n\nplt.figure(figsize=(6, 4))\nx = [i for i in range(1, 11)]\nplt.plot(x, inertia_values)\nplt.title('Distance from cluster: Elbow curve')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:39:27.639213Z",
     "iopub.execute_input": "2024-05-08T11:39:27.639518Z",
     "iopub.status.idle": "2024-05-08T11:41:23.457873Z",
     "shell.execute_reply.started": "2024-05-08T11:39:27.639492Z",
     "shell.execute_reply": "2024-05-08T11:41:23.456882Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "n_clusters = 7\nkmeans = KMeans(n_clusters=n_clusters, random_state=1278).fit(train_tfid.toarray())\nlabels = kmeans.predict(train_tfid.toarray())\ndata['group'] = labels",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:41:23.460638Z",
     "iopub.execute_input": "2024-05-08T11:41:23.461163Z",
     "iopub.status.idle": "2024-05-08T11:41:36.543412Z",
     "shell.execute_reply.started": "2024-05-08T11:41:23.461124Z",
     "shell.execute_reply": "2024-05-08T11:41:36.542245Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plot_data = data['group'].value_counts().sort_index().reset_index()\nplt.figure(figsize=(6, 4))\nsns.barplot(data=plot_data, x='group', y='count')\nplt.title('A number of data of each groups')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:41:36.544606Z",
     "iopub.execute_input": "2024-05-08T11:41:36.544969Z",
     "iopub.status.idle": "2024-05-08T11:41:36.849257Z",
     "shell.execute_reply.started": "2024-05-08T11:41:36.544933Z",
     "shell.execute_reply": "2024-05-08T11:41:36.848098Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "I think the title of each groups is like below.  \n  \nGroup0: Car-free cities  \nGroup1: Exploring Venus  \nGroup2: Does the electoral college work?  \nGroup3: The Face on Mars  \nGroup4: \"A Cowboy Who Rode the Waves\"    \nGroup5: Driverless cars    \nGroup6: Facial action coding system  ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "for i in range(7):\n    print(f'================================== group_{i} ==================================')\n    print('')\n    for j in range(2):\n        print(data[data['group'] == i]['full_text'].iloc[j])\n        print('')\n        print('----------------------------------------------------------------------------------')\n    print('')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:41:36.85053Z",
     "iopub.execute_input": "2024-05-08T11:41:36.850903Z",
     "iopub.status.idle": "2024-05-08T11:41:36.883212Z",
     "shell.execute_reply.started": "2024-05-08T11:41:36.850873Z",
     "shell.execute_reply": "2024-05-08T11:41:36.882081Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "prompt_mapping = {\n    0: 'Exploring Venus',\n    1: 'Exploring Venus',\n    2: 'Does the electoral college work?',\n    3: 'The Face on Mars',\n    4: '\"A Cowboy Who Rode the Waves\"',\n    5: 'Driverless cars',\n    6: 'Facial action coding system'\n}\n\ndata['prompt_name'] = data['group'].map(prompt_mapping)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:42:04.459325Z",
     "iopub.execute_input": "2024-05-08T11:42:04.459737Z",
     "iopub.status.idle": "2024-05-08T11:42:04.47816Z",
     "shell.execute_reply.started": "2024-05-08T11:42:04.45969Z",
     "shell.execute_reply": "2024-05-08T11:42:04.47702Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "data = data[['essay_id', 'full_text', 'score', 'prompt_name', 'group']]\ndata.to_csv('train_containing_groups.csv', index=False)\ndata.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T11:47:20.859342Z",
     "iopub.execute_input": "2024-05-08T11:47:20.859697Z",
     "iopub.status.idle": "2024-05-08T11:47:21.879708Z",
     "shell.execute_reply.started": "2024-05-08T11:47:20.85967Z",
     "shell.execute_reply": "2024-05-08T11:47:21.878712Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "I really appreciate that you read my notebook till the end.    \nLet's enjoy Kaggle!!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
