{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8358085,"sourceType":"datasetVersion","datasetId":4966859},{"sourceId":8404441,"sourceType":"datasetVersion","datasetId":4956698},{"sourceId":8404458,"sourceType":"datasetVersion","datasetId":5001044}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I experimented with creating some new features without fine-tuning bag of words, tf-idf, or sentence embeddings directly on the training data. I came up with a bunch of basic features using some libraries, different datasets, and just common sense. Here's a look at some of the features I used:\n- Textstat features to measure readability, complexity, and grade level.\n- NER, POS, and TAG features using spacy.\n- Sentiment analysis and other length/ratio features using NLTK and basic functions.\n- Features derived from feedback data to assess cohesion, syntax, vocabulary, phraseology, grammar, and conventions.\n    - I used a basic Ridge regression model to get a quick sense of how well these features could predict scores. Then, I used a simple GBDT model with 5-fold cross-validation to generate the final predictions.\n    \nI'm sharing this to hopefully spark some new ideas or help you improve your current pipelines. I'm getting decent CV results, but I haven't reached the level I'm aiming for just yet.","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/huggingface_hub-0.23.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/sentence_transformers-2.8.0.dev0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/transformers-4.40.2-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/textstat-0.7.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/pyphen-0.15.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/einops-0.8.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/pyspellchecker-0.8.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:07:47.212958Z","iopub.execute_input":"2024-05-14T22:07:47.213378Z","iopub.status.idle":"2024-05-14T22:08:15.846363Z","shell.execute_reply.started":"2024-05-14T22:07:47.213349Z","shell.execute_reply":"2024-05-14T22:08:15.845095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tokenizers import (\n    decoders,\n    models,\n    pre_tokenizers,\n    normalizers,\n    processors,\n    trainers,\n    Tokenizer\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nimport gc\n\nimport spacy\nfrom collections import Counter\n\nimport nltk \n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport textstat\nfrom spellchecker import SpellChecker\n\nfrom sentence_transformers import SentenceTransformer, models\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nimport torch\n\ntqdm.pandas()\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T22:08:15.848808Z","iopub.execute_input":"2024-05-14T22:08:15.849873Z","iopub.status.idle":"2024-05-14T22:08:26.132499Z","shell.execute_reply.started":"2024-05-14T22:08:15.849832Z","shell.execute_reply":"2024-05-14T22:08:26.13144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ntest = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:26.133614Z","iopub.execute_input":"2024-05-14T22:08:26.133917Z","iopub.status.idle":"2024-05-14T22:08:26.852294Z","shell.execute_reply.started":"2024-05-14T22:08:26.13389Z","shell.execute_reply":"2024-05-14T22:08:26.85115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Textstat Features","metadata":{}},{"cell_type":"code","source":"def textstat_features(text):\n    features = {}\n    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n    features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n    features['smog_index'] = textstat.smog_index(text)\n    features['coleman_liau_index'] = textstat.coleman_liau_index(text)\n    features['automated_readability_index'] = textstat.automated_readability_index(text)\n    features['dale_chall_readability_score'] = textstat.dale_chall_readability_score(text)\n    features['difficult_words'] = textstat.difficult_words(text)\n    features['linsear_write_formula'] = textstat.linsear_write_formula(text)\n    features['gunning_fog'] = textstat.gunning_fog(text)\n    features['text_standard'] = textstat.text_standard(text, float_output=True)\n    features['spache_readability'] = textstat.spache_readability(text)\n    features['mcalpine_eflaw'] = textstat.mcalpine_eflaw(text)\n    features['reading_time'] = textstat.reading_time(text)\n    features['syllable_count'] = textstat.syllable_count(text)\n    features['lexicon_count'] = textstat.lexicon_count(text)\n    features['monosyllabcount'] = textstat.monosyllabcount(text)\n\n    return features\n\ntrain['textstat_features'] = train['full_text'].apply(textstat_features)\ntrain_textstat = pd.DataFrame(train['textstat_features'].tolist())\n\ntest['textstat_features'] = test['full_text'].apply(textstat_features)\ntest_textstat = pd.DataFrame(test['textstat_features'].tolist())\n\ntrain_textstat.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:26.853453Z","iopub.execute_input":"2024-05-14T22:08:26.853716Z","iopub.status.idle":"2024-05-14T22:08:27.687543Z","shell.execute_reply.started":"2024-05-14T22:08:26.853693Z","shell.execute_reply":"2024-05-14T22:08:27.68661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linguistic Features","metadata":{}},{"cell_type":"code","source":"def extract_linguistic_features(text):\n\n    doc = nlp(text)\n    features = {}\n\n    # NER Features\n    entity_counts = {\"GPE\": 0, \"PERCENT\": 0, \"NORP\": 0, \"ORG\": 0, \"CARDINAL\": 0, \"MONEY\": 0, \"DATE\": 0, \n                    \"LOC\": 0, \"PERSON\": 0, \"QUANTITY\": 0, \"EVENT\": 0, \"ORDINAL\": 0, \"WORK_OF_ART\": 0, \n                    \"LAW\": 0, \"PRODUCT\": 0, \"TIME\": 0, \"FAC\": 0, \"LANGUAGE\": 0}\n    for entity in doc.ents:\n        if entity.label_ in entity_counts:\n            entity_counts[entity.label_] += 1\n    features['NER_Features'] = entity_counts\n\n    # POS Features\n    pos_counts = {\"ADJ\": 0, \"NOUN\": 0, \"VERB\": 0, \"SCONJ\": 0, \"PRON\": 0, \"PUNCT\": 0, \"DET\": 0, \"AUX\": 0, \n                \"PART\": 0, \"ADP\": 0, \"SPACE\": 0, \"CCONJ\": 0, \"PROPN\": 0, \"NUM\": 0, \"ADV\": 0, \n                \"SYM\": 0, \"INTJ\": 0, \"X\": 0}\n    for token in doc:\n        if token.pos_ in pos_counts:\n            pos_counts[token.pos_] += 1\n    features['POS_Features'] = pos_counts\n\n    # tag Features\n    tags = {\"RB\": 0, \"-RRB-\": 0, \"PRP$\": 0, \"JJ\": 0, \"TO\": 0, \"VBP\": 0, \"JJS\": 0, \"DT\": 0, \"''\": 0, \"UH\": 0, \"RBS\": 0, \"WRB\": 0, \".\": 0, \n        \"HYPH\": 0, \"XX\": 0, \"``\": 0, \"SYM\": 0, \"VB\": 0, \"VBN\": 0, \"WP\": 0, \"CC\": 0, \"LS\": 0, \"POS\": 0, \"NN\": 0, \",\": 0, \"NNPS\": 0,\n          \"RP\": 0, \":\": 0, \"$\": 0, \"PDT\": 0, \"VBZ\": 0, \"VBD\": 0, \"JJR\": 0, \"-LRB-\": 0, \"IN\": 0, \"RBR\": 0, \"WDT\": 0, \"EX\": 0, \"MD\": 0,\n            \"_SP\": 0, \"NNP\": 0, \"CD\": 0, \"VBG\": 0, \"NNS\": 0, \"PRP\": 0}\n    \n    for token in doc:\n        if token.tag_ in tags:\n            tags[token.tag_] += 1\n    features['tag_Features'] = tags\n\n    # tense features\n    tenses = [i.morph.get(\"Tense\") for i in doc]\n    tenses = [i[0] for i in tenses if i]\n    tense_counts = Counter(tenses)\n    features['past_tense_ratio'] = tense_counts.get(\"Past\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n    features['present_tense_ratio'] = tense_counts.get(\"Pres\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n    \n    \n    # len features\n\n    features['word_count'] = len(doc)\n    features['sentence_count'] = len([sentence for sentence in doc.sents])\n    features['words_per_sentence'] = features['word_count'] / features['sentence_count']\n    features['std_words_per_sentence'] = np.std([len(sentence) for sentence in doc.sents])\n\n    features['unique_words'] = len(set([token.text for token in doc]))\n    features['lexical_diversity'] = features['unique_words'] / features['word_count']\n\n    paragraph = text.split('\\n\\n')\n\n    features['paragraph_count'] = len(paragraph)\n\n    features['avg_chars_by_paragraph'] = np.mean([len(paragraph) for paragraph in paragraph])\n    features['avg_words_by_paragraph'] = np.mean([len(nltk.word_tokenize(paragraph)) for paragraph in paragraph])\n    features['avg_sentences_by_paragraph'] = np.mean([len(nltk.sent_tokenize(paragraph)) for paragraph in paragraph]) \n\n    # sentiment features\n    analyzer = SentimentIntensityAnalyzer()\n    sentences = nltk.sent_tokenize(text)\n\n    compound_scores, negative_scores, positive_scores, neutral_scores = [], [], [], []\n    for sentence in sentences:\n        scores = analyzer.polarity_scores(sentence)\n        compound_scores.append(scores['compound'])\n        negative_scores.append(scores['neg'])\n        positive_scores.append(scores['pos'])\n        neutral_scores.append(scores['neu'])\n\n    features[\"mean_compound\"] = np.mean(compound_scores)\n    features[\"mean_negative\"] = np.mean(negative_scores)\n    features[\"mean_positive\"] = np.mean(positive_scores)\n    features[\"mean_neutral\"] = np.mean(neutral_scores)\n\n    features[\"std_compound\"] = np.std(compound_scores)\n    features[\"std_negative\"] = np.std(negative_scores)\n    features[\"std_positive\"] = np.std(positive_scores)\n    features[\"std_neutral\"] = np.std(neutral_scores)\n\n    return features\n\ntrain['linguistic_features'] = train['full_text'].progress_apply(extract_linguistic_features)\n\ntrain_linguistic = pd.json_normalize(train['linguistic_features'])\n\n\n\ntest['linguistic_features'] = test['full_text'].progress_apply(extract_linguistic_features)\n\ntest_linguistic = pd.json_normalize(test['linguistic_features'])\n\ntrain_linguistic.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:27.690381Z","iopub.execute_input":"2024-05-14T22:08:27.690671Z","iopub.status.idle":"2024-05-14T22:08:37.9054Z","shell.execute_reply.started":"2024-05-14T22:08:27.690645Z","shell.execute_reply":"2024-05-14T22:08:37.904396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_cols = [col for col in train_linguistic.columns if col.startswith('tag')]\ncol_cols = [col for col in train_linguistic.columns if col.startswith('col')]\npos_cols = [col for col in train_linguistic.columns if col.startswith('pos')]\n\nfor col in tag_cols:\n    train_linguistic[f\"{col}_ratio\"] = train_linguistic[col] / train_linguistic['word_count']\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\nfor col in col_cols:\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\nfor col in pos_cols:\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\ntrain_linguistic.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:37.906518Z","iopub.execute_input":"2024-05-14T22:08:37.906799Z","iopub.status.idle":"2024-05-14T22:08:37.982162Z","shell.execute_reply.started":"2024-05-14T22:08:37.906768Z","shell.execute_reply":"2024-05-14T22:08:37.98119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.concat([train_textstat, train_linguistic], axis=1)\n\nmerged_df_test = pd.concat([test_textstat, test_linguistic], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:37.983131Z","iopub.execute_input":"2024-05-14T22:08:37.983397Z","iopub.status.idle":"2024-05-14T22:08:37.992555Z","shell.execute_reply.started":"2024-05-14T22:08:37.983374Z","shell.execute_reply":"2024-05-14T22:08:37.991531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Counts","metadata":{}},{"cell_type":"code","source":"spell = SpellChecker()\n\ndef spell_check(text):\n    words = nltk.word_tokenize(text)\n    misspelled = spell.unknown(words)\n\n    mispelled_count = len(misspelled)\n    misspelled_ratio = mispelled_count / len(words)\n\n    return mispelled_count, misspelled_ratio\n\ntrain['spell_check_features'] = train['full_text'].progress_apply(spell_check)\n\nspell_check_df = pd.DataFrame(train['spell_check_features'].tolist(), columns=['misspelled_count', 'misspelled_ratio'])\n\ntest['spell_check_features'] = test['full_text'].progress_apply(spell_check)\n\ntest_check_df = pd.DataFrame(test['spell_check_features'].tolist(), columns=['misspelled_count', 'misspelled_ratio'])\n\nspell_check_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:37.99379Z","iopub.execute_input":"2024-05-14T22:08:37.994131Z","iopub.status.idle":"2024-05-14T22:08:38.818758Z","shell.execute_reply.started":"2024-05-14T22:08:37.994099Z","shell.execute_reply":"2024-05-14T22:08:38.817722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.concat((merged_df, spell_check_df), axis=1)\n\nmerged_df_test = pd.concat((merged_df_test, test_check_df), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:08:38.820139Z","iopub.execute_input":"2024-05-14T22:08:38.820515Z","iopub.status.idle":"2024-05-14T22:08:38.828122Z","shell.execute_reply.started":"2024-05-14T22:08:38.820486Z","shell.execute_reply":"2024-05-14T22:08:38.826981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feedback Features","metadata":{}},{"cell_type":"code","source":"feedback_df = pd.read_csv('/kaggle/input/feedback-data/feedback_data.csv')\n\nfeed_embeds = []\n\nmerged_embeds = []\n\ntest_embeds = []\n\nfor i in range(5):\n    model_path = f'/kaggle/input/sent-debsmall/deberta_small_trained/temp_fold{i}_checkpoints'\n    word_embedding_model = models.Transformer(model_path, max_seq_length=1024)\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n    model.half()\n    model = model.to('cuda')\n    \n    feed_custom_embeddings_train = model.encode(feedback_df.loc[:, 'full_text'].values, device='cuda',\n                                                show_progress_bar=True, normalize_embeddings=True)\n    \n    feed_embeds.append(feed_custom_embeddings_train)\n    \n    merged_custom_embeddings = model.encode(train.loc[:, 'full_text'].values, device='cuda',\n                                            show_progress_bar=True, normalize_embeddings=True)\n\n    merged_embeds.append(merged_custom_embeddings)\n    \n    \n    test_custom_embeddings = model.encode(test.loc[:, 'full_text'].values, device='cuda',\n                                            show_progress_bar=True, normalize_embeddings=True)\n    \n    test_embeds.append(test_custom_embeddings)\n    \nfeed_embeds = np.mean(feed_embeds, axis=0)\nmerged_embeds = np.mean(merged_embeds, axis=0)\ntest_embeds = np.mean(test_embeds, axis=0)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:00.994114Z","iopub.execute_input":"2024-05-14T22:10:00.994456Z","iopub.status.idle":"2024-05-14T22:10:21.860602Z","shell.execute_reply.started":"2024-05-14T22:10:00.994431Z","shell.execute_reply":"2024-05-14T22:10:21.859548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n\n\nridge = Ridge(alpha=1.0)\n\nmultioutputregressor = MultiOutputRegressor(ridge)\n\n\n\nmultioutputregressor.fit(feed_embeds, feedback_df.loc[:, targets])","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:21.862615Z","iopub.execute_input":"2024-05-14T22:10:21.863004Z","iopub.status.idle":"2024-05-14T22:10:22.023704Z","shell.execute_reply.started":"2024-05-14T22:10:21.862972Z","shell.execute_reply":"2024-05-14T22:10:22.022366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feedback_predictions = multioutputregressor.predict(merged_embeds)\n\nfeedback_predictions_df = pd.DataFrame(feedback_predictions, columns=targets)\n\ntest_feedback_predictions = multioutputregressor.predict(test_embeds)\n\ntest_feedback_predictions_df = pd.DataFrame(test_feedback_predictions, columns=targets)\n\nfeedback_predictions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:22.025624Z","iopub.execute_input":"2024-05-14T22:10:22.027572Z","iopub.status.idle":"2024-05-14T22:10:22.152004Z","shell.execute_reply.started":"2024-05-14T22:10:22.02752Z","shell.execute_reply":"2024-05-14T22:10:22.150631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.concat((merged_df, feedback_predictions_df), axis=1)\n\nmerged_df_test = pd.concat((merged_df_test, test_feedback_predictions_df), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:22.155662Z","iopub.execute_input":"2024-05-14T22:10:22.157284Z","iopub.status.idle":"2024-05-14T22:10:22.177683Z","shell.execute_reply.started":"2024-05-14T22:10:22.157237Z","shell.execute_reply":"2024-05-14T22:10:22.176504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:22.184517Z","iopub.execute_input":"2024-05-14T22:10:22.185565Z","iopub.status.idle":"2024-05-14T22:10:22.202369Z","shell.execute_reply.started":"2024-05-14T22:10:22.185512Z","shell.execute_reply":"2024-05-14T22:10:22.200968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:22.204555Z","iopub.execute_input":"2024-05-14T22:10:22.205517Z","iopub.status.idle":"2024-05-14T22:10:22.21888Z","shell.execute_reply.started":"2024-05-14T22:10:22.205468Z","shell.execute_reply":"2024-05-14T22:10:22.217483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\n\n# metric and objective based on public notebooks\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092\n\n\n\nskf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n\nscores = []\n\ntrain['oof'] = 0\n\ntest_preds = []\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train['full_text'], train['score'])):\n    print(f\"Fold: {fold}\")\n    print(f\"Train size: {len(train_idx)}\")\n    print(f\"Valid size: {len(valid_idx)}\")\n    print()\n\n\n    X_train = merged_df.iloc[train_idx].values\n    X_valid = merged_df.iloc[valid_idx].values\n\n\n    y_train = train['score'].astype('float32').values[train_idx]\n    y_valid = train['score'].astype('float32').values[valid_idx]\n\n\n    y_train = y_train -a\n    y_valid = y_valid -a\n\n \n\n    model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.01,\n                n_estimators=10000,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n                verbosity = - 1)\n    \n    callbacks = [lgb.early_stopping(500, verbose=True, first_metric_only=True), lgb.log_evaluation(period=500)]\n\n    \n    predictor = model.fit(X_train,\n                                  y_train,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks,)\n\n    valid_preds = predictor.predict(X_valid)\n\n    train.loc[valid_idx, 'oof'] = valid_preds + a\n\n    score = quadratic_weighted_kappa(y_valid, valid_preds)\n    scores.append(score[1])\n    \n    test_preds.append(predictor.predict(merged_df_test) + a)\n\n    print(f\"Train QWK: {score}\")\n\nprint(f\"Mean QWK: {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:22.2213Z","iopub.execute_input":"2024-05-14T22:10:22.222283Z","iopub.status.idle":"2024-05-14T22:10:45.697235Z","shell.execute_reply.started":"2024-05-14T22:10:22.222235Z","shell.execute_reply":"2024-05-14T22:10:45.695974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = (np.mean(test_preds, axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:45.698549Z","iopub.execute_input":"2024-05-14T22:10:45.698943Z","iopub.status.idle":"2024-05-14T22:10:45.703961Z","shell.execute_reply.started":"2024-05-14T22:10:45.698909Z","shell.execute_reply":"2024-05-14T22:10:45.702963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"light_gbm_preds = np.round(np.clip(final_preds, 1, 6))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:45.721101Z","iopub.execute_input":"2024-05-14T22:10:45.721468Z","iopub.status.idle":"2024-05-14T22:10:45.727426Z","shell.execute_reply.started":"2024-05-14T22:10:45.721434Z","shell.execute_reply":"2024-05-14T22:10:45.726365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['score'] = light_gbm_preds\n\nsample_submission['score']","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:45.728629Z","iopub.execute_input":"2024-05-14T22:10:45.728988Z","iopub.status.idle":"2024-05-14T22:10:45.742531Z","shell.execute_reply.started":"2024-05-14T22:10:45.728953Z","shell.execute_reply":"2024-05-14T22:10:45.741476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T22:10:45.743663Z","iopub.execute_input":"2024-05-14T22:10:45.743949Z","iopub.status.idle":"2024-05-14T22:10:45.756327Z","shell.execute_reply.started":"2024-05-14T22:10:45.743925Z","shell.execute_reply":"2024-05-14T22:10:45.755389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}