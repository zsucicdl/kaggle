{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>1. Initial Setting</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \n    ðŸ’¡<b>What's New</b><br>\n     Adding Puncutal Features & Ratio Features & Mid,SuperLong Sentence, Word, Parargraph Features</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc \nimport ctypes\nimport random\nimport time\nimport string\nimport re\nfrom tqdm import tqdm\nimport pickle\n\nimport pandas as pd, numpy as np\nimport polars as pl # For Feature Engineering\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import VotingRegressor\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:01:57.904341Z","iopub.execute_input":"2024-04-20T01:01:57.905049Z","iopub.status.idle":"2024-04-20T01:02:01.484982Z","shell.execute_reply.started":"2024-04-20T01:01:57.905006Z","shell.execute_reply":"2024-04-20T01:02:01.483563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Configure</b>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    SEED = 2024\n    VER = 3\n    LOAD_MODELS_FROM = None\n    LOAD_FEATURES_FROM = None\n    BASE_PATH = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:01.4872Z","iopub.execute_input":"2024-04-20T01:02:01.48772Z","iopub.status.idle":"2024-04-20T01:02:01.492875Z","shell.execute_reply.started":"2024-04-20T01:02:01.487688Z","shell.execute_reply":"2024-04-20T01:02:01.491902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Clean Memory</b>","metadata":{}},{"cell_type":"code","source":"Clean = True\n\ndef clean_memory():\n    if Clean:\n        ctypes.CDLL('libc.so.6').malloc_trim(0)\n        gc.collect()\n        \nclean_memory()        ","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:01.49426Z","iopub.execute_input":"2024-04-20T01:02:01.494582Z","iopub.status.idle":"2024-04-20T01:02:01.594243Z","shell.execute_reply.started":"2024-04-20T01:02:01.494555Z","shell.execute_reply":"2024-04-20T01:02:01.593128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Seed Everything</b>","metadata":{}},{"cell_type":"code","source":"def seed_everything(): # To proudce simliar result in each run \n    random.seed(CFG.SEED)\n    np.random.seed(CFG.SEED)\n    os.environ['PYTHONHASHSEED'] = str(CFG.SEED)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:01.597695Z","iopub.execute_input":"2024-04-20T01:02:01.598179Z","iopub.status.idle":"2024-04-20T01:02:01.60621Z","shell.execute_reply.started":"2024-04-20T01:02:01.598138Z","shell.execute_reply":"2024-04-20T01:02:01.605172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>2. Road and Read Data</b></div>","metadata":{}},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Meta Data</b>\n\n\n- **`[train/test].csv`**\n  - `essay_id`: The unique ID of the essay\n  - `full_text`: The full essay reponse\n  - `score`: Holistic score of the essay on a 1-6 scale\n  \n  \n- **`sample_submission.csv`**\n  - `essay_id`: The unique ID of the essay\n  - `score`: The predicted holistic score of the essay on a 1-6 scale\n  \n  \n> [Link to the Holistic Scoring Rubric](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf)","metadata":{}},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Train Data</b>","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(CFG.BASE_PATH + 'train.csv')\n\nprint('Shape of Train: ', df_train.shape)\nprint(display(df_train.head()))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:01.607718Z","iopub.execute_input":"2024-04-20T01:02:01.608063Z","iopub.status.idle":"2024-04-20T01:02:02.472276Z","shell.execute_reply.started":"2024-04-20T01:02:01.608004Z","shell.execute_reply":"2024-04-20T01:02:02.471096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x=df_train['score'])\nplt.title('Distribution of Score')\nplt.xlabel('Score of Essay')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:02.473715Z","iopub.execute_input":"2024-04-20T01:02:02.474066Z","iopub.status.idle":"2024-04-20T01:02:02.815041Z","shell.execute_reply.started":"2024-04-20T01:02:02.474037Z","shell.execute_reply":"2024-04-20T01:02:02.813926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Test Data</b>","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(CFG.BASE_PATH + 'test.csv')\n\nprint('Shape of Test: ', df_test.shape)\nprint(display(df_test.head()))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:02.816977Z","iopub.execute_input":"2024-04-20T01:02:02.817415Z","iopub.status.idle":"2024-04-20T01:02:02.835451Z","shell.execute_reply.started":"2024-04-20T01:02:02.817377Z","shell.execute_reply":"2024-04-20T01:02:02.834486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.path.join(CFG.BASE_PATH,'train.csv')\n# CFG.BASE_PATH + 'train.csv'\n\ntrain = pl.read_csv(os.path.join(CFG.BASE_PATH,'train.csv')).with_columns(\n    pl.col('full_text').str.split(by='\\n\\n').alias('paragraph'))\ntest = pl.read_csv(os.path.join(CFG.BASE_PATH,'test.csv')).with_columns(\n    pl.col('full_text').str.split(by='\\n\\n').alias('paragraph')\n)\n\nschema_train = train.schema # MetaData\nschema_test = test.schema # MetaData","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:02.83673Z","iopub.execute_input":"2024-04-20T01:02:02.837066Z","iopub.status.idle":"2024-04-20T01:02:03.251227Z","shell.execute_reply.started":"2024-04-20T01:02:02.837039Z","shell.execute_reply":"2024-04-20T01:02:03.250235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>3. FeatureGenerator</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>Features & TFIDF    \n   \n    \n    Thanks for Sharing! @yell725</b>\n\nhttps://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments\n</div>","metadata":{}},{"cell_type":"code","source":"def removeHTML(x):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',x) # html -> ''\n\ndef dataPreprocessing(x):\n    \n    x = x.lower()\n    # remove html\n    x = removeHTML(x)\n    \n    #remove string with starting @\n    x = re.sub(\"@\\w+\",'',x)\n    \n    # remove number\n    x = re.sub(\"'\\d+\",'',x)\n    x = re.sub(\"\\d+\",'',x)\n    \n    # remove url\n    x = re.sub(\"http\\w+\",'',x)\n    \n    # Replace consecutive empty spaces with a single empty spaces\n    x = re.sub(r'\\s+', \" \", x)\n    \n    # Replace consecutive commas and periods with a single comma and period\n    x = re.sub(r'\\.+', \".\",x)\n    x = re.sub(r'\\,+', \".\",x)\n    # Remove empty character at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:03.252609Z","iopub.execute_input":"2024-04-20T01:02:03.252919Z","iopub.status.idle":"2024-04-20T01:02:03.260894Z","shell.execute_reply.started":"2024-04-20T01:02:03.252894Z","shell.execute_reply":"2024-04-20T01:02:03.259589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Paragraph</b>","metadata":{}},{"cell_type":"code","source":"paragraph_features = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n\ndef Paragraph_Features(x):\n    # Expand the paragraph list to several lines of data\n    x = x.explode('paragraph') \n    \n    print('Paragraph Preprocessing')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(dataPreprocessing)\n    )\n    \n    print('Caculate the length of each paragraph')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\")\n    )\n    print('Caculate the number of sentences and words in each paragraph')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias('paragraph_sentence_cnt'),\n         pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias('paragraph_word_cnt')\n    )\n    return x\n\ndef Paragraph_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f'paragraph_{i}_cnt') for i in [100,150, 200, 250, 300, 350, 400, 450, 500, 600, 800]],\n    *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f'paragraph_{i}_cnt_v2') for i in [100,200]],\n    *[pl.col('paragraph').filter(pl.col('paragraph_sentence_cnt') >= i).count().alias(f'paragraph_sentence_{i}_cnt') for i in [2,4,6,8,10]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 300) & (pl.col('paragraph_len') > 100)).count().alias(f'short_paragraph_cnt')],    \n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 500) & (pl.col('paragraph_len') > 300)).count().alias(f'mid_paragraph_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 700) & (pl.col('paragraph_len') > 500)).count().alias(f'long_paragraph_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 4) & (pl.col('paragraph_sentence_cnt') > 2)).count().alias(f'short_paragraph_sentence_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 8) & (pl.col('paragraph_sentence_cnt') > 4)).count().alias(f'mid_paragraph_sentence_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 10) & (pl.col('paragraph_sentence_cnt') > 8)).count().alias(f'long_paragraph_sentence_cnt')],    \n    *[pl.col('paragraph').filter(pl.col('paragraph_word_cnt') >= i).count().alias(f'paragraph_word_{i}_cnt') for i in [30,60,90,120]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 60) & (pl.col('paragraph_word_cnt') > 30)).count().alias(f'short_paragraph_word_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 90) & (pl.col('paragraph_word_cnt') > 60)).count().alias(f'mid_paragraph_word_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 120) & (pl.col('paragraph_word_cnt') > 90)).count().alias(f'long_paragraph_word_cnt')],  \n    *[pl.col('paragraph').count().alias('paragraph_cnt')],   \n    ]\n      \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Sentence</b>","metadata":{}},{"cell_type":"code","source":"sentence_features = ['sentence_len','sentence_word_cnt','sentence_len_space_ratio','sentence_word_space_ratio']\n\ndef Sentence_Features(x):\n    print('Preprocess full_text and use periods to segment sentences in the text')\n    x = x.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x)).str.split(\".\").alias('sentence')\n    )\n    x = x.explode('sentence')\n    \n    print('Caculate the length of a sentence') \n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: x.count(' ')).alias('sentence_space_cnt'))\n    x = x.filter(pl.col('sentence_space_cnt')>0)\n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: len(x)).alias('sentence_len'))\n    x = x.filter(pl.col('sentence_len') > 3)\n    x = x.with_columns(\n        (pl.col('sentence_len')/pl.col('sentence_space_cnt')).alias('sentence_len_space_ratio'))\n        \n    print('Count the number of words in each sentence')\n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: len(x.split(\" \"))).alias(\"sentence_word_cnt\"))\n    x = x.with_columns(\n        (pl.col('sentence_word_cnt')/pl.col('sentence_space_cnt')).alias('sentence_word_space_ratio'))\n     \n    return x\n\ndef Sentence_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f'sentence_{i}_cnt') for i in [30,40,50,60,70,80,100,150]],    \n    *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f'sentence_{i}_cnt_v2') for i in [10,20,30]],\n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 50) & (pl.col('sentence_len') > 30)).count().alias(f'short_sentence_cnt')],    \n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 70) & (pl.col('sentence_len') > 50)).count().alias(f'mid_sentence_cnt')], \n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 100) & (pl.col('sentence_len') > 70)).count().alias(f'long_sentence_cnt')],\n    *[pl.col('sentence').filter(pl.col('sentence_word_cnt') >= i).count().alias(f'sentence_word_{i}_cnt') for i in [5,10,15,20]], \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 10) & (pl.col('sentence_word_cnt') > 5)).count().alias(f'short_sentence_word_cnt')],    \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 15) & (pl.col('sentence_word_cnt') > 10)).count().alias(f'mid_sentence_word_cnt')], \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 20) & (pl.col('sentence_word_cnt') > 15)).count().alias(f'long_sentence_word_cnt')],    \n    *[pl.col('sentence').count().alias('sentence_cnt')],\n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in sentence_features],\n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in sentence_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in sentence_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in sentence_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in sentence_features], \n    ]\n    \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    \n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Word</b>","metadata":{}},{"cell_type":"code","source":"word_features = ['word_len',]\n\ndef Word_Features(x):\n    print('Preprocess full_text and use spaces to seperate words from the text')\n    x = x.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x)).str.split(\".\").alias('sentence')\n    )\n    x = x.explode('sentence')\n    \n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: dataPreprocessing(x)).str.split(\" \").alias('word')\n    )\n    x = x.explode('word')\n    \n    print('Caculate the length of a word') \n    x = x.with_columns(\n        pl.col('word').map_elements(lambda x: len(x)).alias('word_len'))\n    x = x.filter(pl.col('word_len')>0)\n\n    return x\n\ndef Word_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('word').filter(pl.col('word_len') >= i).count().alias(f'word_{i}_cnt') for i in [3,4,5,6,8,10,15]],\n    *[pl.col('word').filter(pl.col('word_len') <= i).count().alias(f'word_{i}_cnt_v2') for i in [2,3]],\n    *[pl.col('word').filter((pl.col('word_len') <= 4) & (pl.col('word_len') > 2)).count().alias(f'short_word_cnt')],\n    *[pl.col('word').filter((pl.col('word_len') <= 7) & (pl.col('word_len') > 4)).count().alias(f'mid_word_cnt')], \n    *[pl.col('word').filter((pl.col('word_len') <= 10) & (pl.col('word_len') > 7)).count().alias(f'long_word_cnt')],     \n    *[pl.col('word').count().alias('word_cnt')],   \n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in word_features],    \n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in word_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in word_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in word_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in word_features],\n    *[pl.col(feat).quantile(0.25).alias(f'{feat}_q1') for feat in word_features],\n    *[pl.col(feat).quantile(0.75).alias(f'{feat}_q3') for feat in word_features],   \n    ]\n    \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.with_columns(\n    *[(pl.col(f'word_{i}_cnt')/pl.col(f'word_2_cnt_v2')).alias(f'word_2_{i}_cnt_ratio') for i in [3,4,5,6,8,10,15]], \n    *[(pl.col(f'word_{i}_cnt')/pl.col(f'word_3_cnt_v2')).alias(f'word_3_{i}_cnt_ratio') for i in [3,4,5,6,8,10,15]], \n    *[(pl.col(f'short_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'short_word_ratio_{i}') for i in [2,3]], \n    *[(pl.col(f'mid_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'mid_word_ratio_{i}') for i in [2,3]], \n    *[(pl.col(f'long_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'long_word_ratio_{i}') for i in [2,3]],        \n    \n    ).sort(\"essay_id\")\n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Punctual_Features(x):\n\n    \n    print(\"Caculate number of punctuation\")\n    x = x.explode('paragraph') \n    x = x.with_columns(\n         pl.col('paragraph').map_elements(dataPreprocessing)\n    )\n    x = x.with_columns(\n        pl.col('paragraph').map_elements(lambda x: x.count('?') + x.count('!')).alias('paragraph_punct_excl'),\n        pl.col('paragraph').map_elements(lambda x: x.count('\"') + x.count(\"'\") + x.count('`')).alias('paragraph_punct_quotes'),\n        pl.col('paragraph').map_elements(lambda x: x.count(',')).alias('paragraph_punct_para'),\n    )\n    \n    return x\n\npunctual_features = ['paragraph_punct_excl','paragraph_punct_quotes','paragraph_punct_para']\n\ndef Punctual_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col(feat).count().alias(f'{feat}_cnt') for feat in punctual_features],   \n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in punctual_features],\n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in punctual_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in punctual_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in punctual_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in punctual_features],      \n    ]\n    \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:03.349753Z","iopub.execute_input":"2024-04-20T01:02:03.350087Z","iopub.status.idle":"2024-04-20T01:02:03.364102Z","shell.execute_reply.started":"2024-04-20T01:02:03.35006Z","shell.execute_reply":"2024-04-20T01:02:03.362855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> TfidfVectorizer</b>","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    tokenizer = lambda x: x,\n    preprocessor = lambda x: x,\n    token_pattern=None,\n    strip_accents='unicode',\n    analyzer= 'word',\n    ngram_range = (1,4),\n    min_df =0.05,\n    max_df=0.95,\n    sublinear_tf = True # Term Frequency Log Scaling \n    )\n\n# TfidfVectorizer parameter\n\n# Fit all datasets into TfidfVectorizer\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n\n              \n# Convert to array\ndense_matrix = train_tfid.toarray()\n\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\ndf.columns = [f'tfidf_{i}' for i in range(len(df.columns))]\n\n\ndf['essay_id'] = df_train['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:02:03.365516Z","iopub.execute_input":"2024-04-20T01:02:03.365854Z","iopub.status.idle":"2024-04-20T01:04:38.440764Z","shell.execute_reply.started":"2024-04-20T01:02:03.365826Z","shell.execute_reply":"2024-04-20T01:04:38.439401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> CountVectorizer</b>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>CountVectorizer\n\nThanks for Sharing! *@yukiZ*\n\nhttps://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments\n</div>","metadata":{}},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n      tokenizer=lambda x: x,\n      preprocessor=lambda x: x,\n      token_pattern=None,\n      strip_accents='unicode',\n      analyzer = 'word',\n      ngram_range=(1,3),\n      min_df=0.10,\n      max_df=0.90,)\n    \n# TfidfVectorizer parameter\n\n# Fit all datasets into TfidfVectorizer\ntrain_cnt = vectorizer_cnt.fit_transform([i for i in train['full_text']])\n\n              \n# Convert to array\ndense_matrix2 = train_cnt.toarray()\n\n# Convert to dataframe\ndf2 = pd.DataFrame(dense_matrix2)\ndf2.columns = [f'cnt_{i}' for i in range(len(df2.columns))]\n\n\ndf2['essay_id'] = df_train['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:04:38.442751Z","iopub.execute_input":"2024-04-20T01:04:38.443197Z","iopub.status.idle":"2024-04-20T01:06:04.199496Z","shell.execute_reply.started":"2024-04-20T01:04:38.44315Z","shell.execute_reply":"2024-04-20T01:06:04.198192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>WordVec\n   </div>","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\n\ndef W2v_train(x):\n    print('training Word2Vec Model')\n    w2v = Word2Vec(sentences=x['full_text'], vector_size=50, window=4, min_count=1, workers=4)\n    return x, w2v\n\ndef W2v_Features(sentence, model):\n    words = sentence.split()\n    word_embeddings = [model.wv[word] for word in words if word in model.wv]\n\n    if len(word_embeddings) > 0:\n        return np.mean(word_embeddings, axis=0)\n    else:\n        return np.zeros(model.vector_size)\n\n\ndef W2v_aggregation(x,model):    \n    print('Aggregation Embedding Features')\n    x = x.with_columns(\n        pl.col('full_text').apply(lambda sentence: W2v_Features(sentence, model)).alias('text_embedding')\n    )\n    feature_columns = [f'text_embedding_{i}' for i in range(model.vector_size)]\n\n    for i, col_name in enumerate(feature_columns):\n        x = x.with_columns(\n            pl.col('text_embedding').apply(lambda embeddings: embeddings[i]).alias(col_name)\n        )\n\n    \n    \n    df = x.drop(['full_text','score','text_embedding','paragraph'])\n    df = df.to_pandas()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:06:04.202014Z","iopub.execute_input":"2024-04-20T01:06:04.20252Z","iopub.status.idle":"2024-04-20T01:06:16.827964Z","shell.execute_reply.started":"2024-04-20T01:06:04.202477Z","shell.execute_reply":"2024-04-20T01:06:16.826799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>Merging datasets\n   </div>","metadata":{}},{"cell_type":"code","source":"if CFG.LOAD_FEATURES_FROM is None: \n    \n    train_feats1 = Paragraph_Features(train)\n    train_feats1 = Paragraph_aggregation(train_feats1)\n    train_feats2 = Sentence_Features(train)\n    train_feats2 = Sentence_aggregation(train_feats2)\n    train_feats3 = Word_Features(train)\n    train_feats3 = Word_aggregation(train_feats3)\n    #train_feats4 = Punctual_Features(train)\n    #train_feats4 = Punctual_aggregation(train_feats4)\n    \n    train_feats = train_feats1.merge(train_feats2, on='essay_id', how='left')\n    train_feats = train_feats.merge(train_feats3, on='essay_id', how='left')\n    #train_feats = train_feats.merge(train_feats4, on='essay_id', how='left')\n    train_feats = train_feats.merge(df, on='essay_id', how='left')\n    train_feats = train_feats.merge(df2, on='essay_id', how='left')\n    train_feats['score'] = df_train['score'].values\nelse:\n    None","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:16:27.384629Z","iopub.execute_input":"2024-04-20T01:16:27.385012Z","iopub.status.idle":"2024-04-20T01:17:21.774206Z","shell.execute_reply.started":"2024-04-20T01:16:27.384984Z","shell.execute_reply":"2024-04-20T01:17:21.772819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_FEATURES_FROM is None: \n    print('Save train_feats.csv')\n    train_feats.to_csv(f'train_feats_{CFG.VER}.csv', index=False)\nelse: \n    print('Load train_feats.csv')\n    train_feats = pd.read_csv(CFG.LOAD_FEATURES_FROM)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:16:26.33553Z","iopub.status.idle":"2024-04-20T01:16:26.33592Z","shell.execute_reply.started":"2024-04-20T01:16:26.335733Z","shell.execute_reply":"2024-04-20T01:16:26.335749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_feats.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.776362Z","iopub.execute_input":"2024-04-20T01:17:21.776718Z","iopub.status.idle":"2024-04-20T01:17:21.802763Z","shell.execute_reply.started":"2024-04-20T01:17:21.776689Z","shell.execute_reply":"2024-04-20T01:17:21.801646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>4. LightGBM Model</b></div>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nimport lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation\nprint('LightGBM Version: ', lgb.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.804169Z","iopub.execute_input":"2024-04-20T01:17:21.80453Z","iopub.status.idle":"2024-04-20T01:17:21.811061Z","shell.execute_reply.started":"2024-04-20T01:17:21.804502Z","shell.execute_reply":"2024-04-20T01:17:21.809804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Quadatric Weighted Kappa</b>","metadata":{}},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.814778Z","iopub.execute_input":"2024-04-20T01:17:21.815199Z","iopub.status.idle":"2024-04-20T01:17:21.826317Z","shell.execute_reply.started":"2024-04-20T01:17:21.81516Z","shell.execute_reply":"2024-04-20T01:17:21.82548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> LightGBM Pipeline</b>","metadata":{}},{"cell_type":"code","source":"categorical_columns = train_feats.select_dtypes(include=['object','category']).columns.tolist()\nFEATURES = [col for col in train_feats.columns if col not in categorical_columns + ['score']]\nTARGET = 'score'","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.827835Z","iopub.execute_input":"2024-04-20T01:17:21.82821Z","iopub.status.idle":"2024-04-20T01:17:21.849785Z","shell.execute_reply.started":"2024-04-20T01:17:21.828182Z","shell.execute_reply":"2024-04-20T01:17:21.848571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b> Optuna(Cross Validation: train_test_split)\n    </div>","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# import optuna","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.851Z","iopub.execute_input":"2024-04-20T01:17:21.851315Z","iopub.status.idle":"2024-04-20T01:17:21.86Z","shell.execute_reply.started":"2024-04-20T01:17:21.851288Z","shell.execute_reply":"2024-04-20T01:17:21.85913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndef lgb_objective(trial):\n\n\n    params = {    \n              'objective': qwk_obj,\n              'learning_rate': trial.suggest_float('learning_rate', 0.001,0.5),\n              'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1,1.0),\n              'max_depth': trial.suggest_int('max_depth', 4,32),\n              'num_leaves': trial.suggest_int('num_leaves', 10,50),\n              'reg_alpha': trial.suggest_float('reg_alpha', 0.1,1.0),\n              'reg_lambda': trial.suggest_float('reg_lambda', 0.1,1.0),\n              'n_estimators': trial.suggest_int('n_estimators', 200,1500),\n    }\n    \n    train_x, valid_x, train_y, valid_y = train_test_split(train_feats[FEATURES], train_feats[TARGET], test_size=0.2, random_state=CFG.SEED)\n    model = lgb.LGBMRegressor(**params)\n    \n    train_y = train_y - a\n         \n    valid_y = valid_y - a\n    \n    model.fit(train_x, train_y,\n                  eval_set = [(valid_x, valid_y)],\n                  eval_metric = quadratic_weighted_kappa,\n                  callbacks = [early_stopping(stopping_rounds=100)]\n               )\n    oof = model.predict(valid_x, num_iteration=model.best_iteration_)    \n    cv = cohen_kappa_score(valid_y + a, (oof+a).clip(1,6).round(), weights=\"quadratic\")\n\n    \n    return -cv\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.861206Z","iopub.execute_input":"2024-04-20T01:17:21.861542Z","iopub.status.idle":"2024-04-20T01:17:21.875207Z","shell.execute_reply.started":"2024-04-20T01:17:21.861514Z","shell.execute_reply":"2024-04-20T01:17:21.873956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study = optuna.create_study(direction='minimize', study_name='Regression') \n#study.optimize(lgb_objective, n_trials=20, show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.876537Z","iopub.execute_input":"2024-04-20T01:17:21.876886Z","iopub.status.idle":"2024-04-20T01:17:21.888715Z","shell.execute_reply.started":"2024-04-20T01:17:21.876858Z","shell.execute_reply":"2024-04-20T01:17:21.887829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'Best Trial: score{study.best_value}, param{study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.889943Z","iopub.execute_input":"2024-04-20T01:17:21.890239Z","iopub.status.idle":"2024-04-20T01:17:21.900293Z","shell.execute_reply.started":"2024-04-20T01:17:21.890214Z","shell.execute_reply":"2024-04-20T01:17:21.899317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.904133Z","iopub.execute_input":"2024-04-20T01:17:21.904448Z","iopub.status.idle":"2024-04-20T01:17:21.911658Z","shell.execute_reply.started":"2024-04-20T01:17:21.904409Z","shell.execute_reply":"2024-04-20T01:17:21.910641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b> Optuna Plot\n    </div>","metadata":{}},{"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F16438831%2F11ea073c666ee5e4a1e08c75805c4f90%2Fplot.JPG?generation=1713440057876069&alt=media)","metadata":{}},{"cell_type":"code","source":"def lightgbm():\n    all_oof = []\n    all_true = []\n    \n    skf = StratifiedKFold(n_splits=5, random_state=CFG.SEED, shuffle=True)\n    for i, (train_index, valid_index) in enumerate(skf.split(train_feats, train_feats[TARGET])):\n      \n        print('#'*25)\n        print(f'### Fold {i+1}')\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n                              \n        model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.10,\n                colsample_bytree = 0.8, \n                max_depth = 25, \n                num_leaves = 20, \n                reg_alpha = 0.9,\n                reg_lambda = 0.2,\n                n_estimators = 1024,\n                class_weight='balanced',\n                random_state=CFG.SEED,\n                verbosity = - 1)\n                                               \n        train_x = np.clip(train_feats.loc[train_index, FEATURES].fillna(0),\n                          0, 10000)\n        train_y = train_feats.loc[train_index, TARGET] - a\n                                               \n        valid_x = np.clip(train_feats.loc[valid_index, FEATURES].fillna(0),\n                          0,10000)\n        valid_y = train_feats.loc[valid_index, TARGET] - a\n    \n        model.fit(train_x, train_y,\n                  eval_set = [(valid_x, valid_y)],\n                  eval_metric = quadratic_weighted_kappa,\n                  callbacks = [early_stopping(stopping_rounds=100)]\n               )\n    \n        # Save Model\n        pickle.dump(model, open(f'LGB_v{CFG.VER}_f{i}.pkl', 'wb'))\n      \n        oof = model.predict(valid_x, num_iteration=model.best_iteration_)\n        all_oof.append(oof + a)\n        all_true.append(valid_y.values + a)\n      \n        del train_x,train_y,valid_x,valid_y, oof, model\n        clean_memory()\n                                           \n    all_oof = np.concatenate(all_oof)\n    all_true = np.concatenate(all_true)\n\n    oof = pd.DataFrame(all_oof.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = cohen_kappa_score(true[0], oof[0].clip(1,6).round(), weights=\"quadratic\")\n    print('CV Score for LightGBM = ',cv)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.913533Z","iopub.execute_input":"2024-04-20T01:17:21.914078Z","iopub.status.idle":"2024-04-20T01:17:21.93219Z","shell.execute_reply.started":"2024-04-20T01:17:21.914026Z","shell.execute_reply":"2024-04-20T01:17:21.931007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_MODELS_FROM is None:\n    print('Training LightGBM')\n    lightgbm()\nelse: \n    None","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:17:21.933825Z","iopub.execute_input":"2024-04-20T01:17:21.934309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>Feature Importance\n    </div>","metadata":{}},{"cell_type":"code","source":"if CFG.LOAD_MODELS_FROM:\n    model = pickle.load(open(f'{CFG.LOAD_MODELS_FROM}LGB_v{CFG.VER}_f0.pkl', 'rb'))\nelse: \n    model = pickle.load(open(f'LGB_v{CFG.VER}_f0.pkl', 'rb'))\n\ndf_importance = pd.DataFrame({\n        'features_name': FEATURES,\n        'importance': model.feature_importances_,\n    })\ndf_importance = df_importance.sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(12,6))\nplt.bar(data=df_importance.head(30), x='features_name', height='importance', color='pink', edgecolor='black')\nplt.title('Distribution of Feature Importance of LightGBM')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>5. Inference</b></div>","metadata":{}},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Submit to Kaggle</b>","metadata":{}},{"cell_type":"code","source":"# del model, train; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:16:16.70777Z","iopub.status.idle":"2024-04-20T01:16:16.708355Z","shell.execute_reply.started":"2024-04-20T01:16:16.70804Z","shell.execute_reply":"2024-04-20T01:16:16.70806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf3 = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfidf_{i}' for i in range(len(df3.columns))]\ndf3.columns = tfid_columns\ndf3['essay_id'] = df_test['essay_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cnt = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_cnt.toarray()\ndf4 = pd.DataFrame(dense_matrix)\ncnt_columns = [ f'cnt_{i}' for i in range(len(df4.columns))]\ndf4.columns = cnt_columns\ndf4['essay_id'] = df_test['essay_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nðŸ“Œ<b>Merging test datasets\n    </div>","metadata":{}},{"cell_type":"code","source":"%%time \ntest_feats1 = Paragraph_Features(test)\ntest_feats1 = Paragraph_aggregation(test_feats1)\ntest_feats2 = Sentence_Features(test)\ntest_feats2 = Sentence_aggregation(test_feats2)\ntest_feats3 = Word_Features(test)\ntest_feats3 = Word_aggregation(test_feats3)\n#test_feats4 = Punctual_Features(test)\n#test_feats4 = Punctual_aggregation(test_feats4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats = test_feats1.merge(test_feats2, on='essay_id', how='left')\ntest_feats = test_feats.merge(test_feats3, on='essay_id', how='left')\n#test_feats = test_feats.merge(test_feats4, on='essay_id', how='left')\ntest_feats = test_feats.merge(df3, on='essay_id', how='left')\ntest_feats = test_feats.merge(df4, on='essay_id', how='left')\nprint('Shape of test_feats:', test_feats.shape)\ndisplay(test_feats.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\ncategorical_columns = test_feats.select_dtypes(include=['object','category']).columns.tolist()\nFEATURES = [col for col in test_feats.columns if col not in categorical_columns]\n\n\nfor i in range(5):\n    print(f'Fold {i+1}')\n    if CFG.LOAD_MODELS_FROM:\n        model = pickle.load(open(f'{CFG.LOAD_MODELS_FROM}LGB_v{CFG.VER}_f{i}.pkl', 'rb'))\n    else: \n        model = pickle.load(open(f'LGB_v{CFG.VER}_f{i}.pkl', 'rb'))\n        \n    pred = model.predict(test_feats[FEATURES]) + a\n    preds.append(pred)\npred1 = np.mean(preds,axis=0)          \n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'essay_id': df_test.essay_id.values})\nsub['score'] = pred1.clip(1,6).round()\nsub.to_csv('submission.csv',index=False)\nprint('Submission shape', sub.shape)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}