{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Motivations Behind this Notebook\nThe metric of this competition is \"quadratic weighted kappa\". \n\nTo solve the problem, it is necessary to understand how to improve the metric. \n\n\nTo undertand how to improve the metric, I need to understand the metric.\n\n\nSo, I look into the metric in this notebook. ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nMAX_RATING = 6 # the max score in this comp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T12:08:31.374475Z","iopub.execute_input":"2024-04-25T12:08:31.374947Z","iopub.status.idle":"2024-04-25T12:08:31.780164Z","shell.execute_reply.started":"2024-04-25T12:08:31.374907Z","shell.execute_reply":"2024-04-25T12:08:31.779013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Understand \"kappa\"\nThe quadratic weighted kappa consists of two parts: kappa and weights. I start with the kappa. \nKappa is a metric for quantifying the agreement between two nominal categorical variables. \nExamples of nominal categorical variables are gender, color, and so on. Importanly, norminal variables have no order in their values.\nThe kappa is not suitable to this comp, because scores of essays in this comp are not norminal variables. Therefore, \"quadratic weighting\" is necessary. I will explain it later!\n\nThe kappa is computed as the below:\n$$\\kappa = 1- \\frac{\\sum_{I, J} O{i,j}}{\\sum_{I, J} E{i,j}} $$\n\nTo improve the kappa, we decrease $\\frac{\\sum_{I, J} O{i,j}}{\\sum_{I, J} E{i,j}}$. The term quantifies the magnitude of the agreement between predicted scores and ground-truth scores.\n\n- $O_{i,j}$ is the element at (i, j) of the observed agreement matrix.\n- $E_{i,j}$ is the element at (i, j) of the expected agreement matrix.\n","metadata":{}},{"cell_type":"markdown","source":"## Observed Agreement Matrix O\nThe observed agreement matrix O is a confusion matrix where each cell contains the count of observations classified into predicted score i and ground-truth score j, if I say the case in this comp.\n","metadata":{}},{"cell_type":"code","source":"\ndef compute_observed_agreement_matrix(y_true, y_pred, max_rating):\n    \"\"\"\n    Calculate the observed agreement matrix.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n               Actual ratings.\n    - y_pred : array-like of shape (n_samples,)\n               Predicted ratings.\n    - max_rating: int\n               Maximum possible rating.\n    Returns:\n    - confusion : array-like of shape (max_rating, max_rating)\n               Observed agreement matrix.\n    \"\"\"\n    confusion = np.zeros((max_rating, max_rating))\n    for i, j in zip(y_true, y_pred):\n        confusion[i - 1, j - 1] += 1\n    return confusion","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:08:31.782434Z","iopub.execute_input":"2024-04-25T12:08:31.783Z","iopub.status.idle":"2024-04-25T12:08:31.789954Z","shell.execute_reply.started":"2024-04-25T12:08:31.782961Z","shell.execute_reply":"2024-04-25T12:08:31.788708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_y_true = np.array([1, 1, 1, 2, 2, 2])\nexample_y_pred = np.array([1, 1, 1, 2, 2, 2])\nO = compute_observed_agreement_matrix(example_y_true, example_y_pred, max_rating=MAX_RATING)\nprint(O)\n\n# Interpretation\n# i is a predicted score. j is a ground-truth score.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:08:31.79164Z","iopub.execute_input":"2024-04-25T12:08:31.792295Z","iopub.status.idle":"2024-04-25T12:08:31.805069Z","shell.execute_reply.started":"2024-04-25T12:08:31.792255Z","shell.execute_reply":"2024-04-25T12:08:31.803888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Expected Agreement Matrix E\nExpected agreement matrix E represents what the agreement would be by chance, given the marginal probabilities of the scores.","metadata":{}},{"cell_type":"code","source":"def compute_expected_agreement_matrix(y_true, y_pred, max_rating):\n    \"\"\"\n    Calculate the expected agreement matrix.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n               Actual ratings.\n    - y_pred : array-like of shape (n_samples,)\n               Predicted ratings.\n    - max_rating: int\n               Maximum possible rating.\n    Returns:\n    - expected : array-like of shape (max_rating, max_rating)\n               Expected agreement matrix.\n    \"\"\"\n   \n    confusion = compute_observed_agreement_matrix(y_true, y_pred, max_rating)\n    # Calculate the total number of samples\n    num_ratings = np.sum(confusion)\n\n    # Marginal sums for each rating\n    # Marginalized over true values (predicted histogram vector of outcomes)\n    marginal_pred = np.sum(confusion, axis=1) / num_ratings\n    # Marginalized over predicted values (actual histogram vector of outcomes)\n    marginal_true = np.sum(confusion, axis=0) / num_ratings\n\n    # Expected Agreement matrix E\n    expected = np.outer(marginal_pred, marginal_true) * num_ratings\n    return expected","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-24T10:50:23.583916600Z","start_time":"2024-04-24T10:50:23.581365300Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.807729Z","iopub.execute_input":"2024-04-25T12:08:31.808083Z","iopub.status.idle":"2024-04-25T12:08:31.819916Z","shell.execute_reply.started":"2024-04-25T12:08:31.808054Z","shell.execute_reply":"2024-04-25T12:08:31.818647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_y_true = np.array([1, 1, 1, 2, 2, 2])\nexample_y_pred = np.array([1, 1, 1, 2, 2, 2])\nE = compute_expected_agreement_matrix(example_y_true, example_y_pred, max_rating=MAX_RATING)\nprint(E)\n# The core component of kappa is the ratio of the total of O to the total E. \n# The matrix O represents the extent of the agreement between the predicted categorical variable and the ground-truth one. \n# On the other hand, E represents the extent of the agreement between the two variables \"by chance.\"","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-24T10:50:23.869045300Z","start_time":"2024-04-24T10:50:23.863297500Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.821169Z","iopub.execute_input":"2024-04-25T12:08:31.821962Z","iopub.status.idle":"2024-04-25T12:08:31.838888Z","shell.execute_reply.started":"2024-04-25T12:08:31.821925Z","shell.execute_reply":"2024-04-25T12:08:31.83779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n## Why does E represent the extent of agreement by chance? \n# By marginalizing the confusion matrix, we can get the two categroical probabilty distributions.\n# P_a(A) represents how likely each score (1~6) occurs when predicted?\n# P_b(B) represents how likely each score (1~6) occurs in the ground truth?\n# E assumes that the two distributions are statistically independent of each other. \"By chance\" refers to the independence assumption.\n# Under this assumption, the joint probability of P(A = 1 and B = 6) is represented as P_a(1) * P_b(6).\n# The computation is applied to the all combinations of the score. And, this is what the outer product do.\n\n\n# Appendix: native implementation of outer product\ndef outer_product(scores1, scores2):\n    # scores1: (batch_size, 1)\n    # scores2: (batch_size, 1)\n    result_matrix = np.zeros((len(scores1), len(scores2)))\n    for i, score1 in enumerate(scores1):\n        for j, score2 in enumerate(scores2):\n            result_matrix[i][j] += score1 * score2\n\n    return result_matrix\n\nscores1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nscores2 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# comparison with the result of numpy's outer product\nnp.all(outer_product(scores1, scores2) == np.outer(scores1, scores2))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:12:27.862752Z","iopub.execute_input":"2024-04-25T12:12:27.86313Z","iopub.status.idle":"2024-04-25T12:12:27.872752Z","shell.execute_reply.started":"2024-04-25T12:12:27.863101Z","shell.execute_reply":"2024-04-25T12:12:27.871605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute Kappa\nPutting it all together, the kappa score is calculated as follows:","metadata":{}},{"cell_type":"code","source":"\ndef compute_kappa(y_true, y_pred, max_rating):\n    \"\"\"\n    Calculate the kappa score.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n               Actual ratings.\n    - y_pred : array-like of shape (n_samples,)\n               Predicted ratings.\n    - max_rating: int\n               Maximum possible rating.\n    Returns:\n    - kappa : float\n            kappa score.\n    \"\"\"\n    # =========== 1. Creating the Observed Agreement Matrix O ======================\n    # i indicate predicted score, j indicate actual score.\n    confusion = np.zeros((max_rating, max_rating))\n    for i, j in zip(y_true, y_pred):\n        confusion[i - 1, j - 1] += 1\n        \n    # =========== 2. Calculating the Expected Agreement Matrix E ==============\n    # Calculate the total number of samples\n    num_ratings = np.sum(confusion)\n\n    # Marginal sums for each rating\n    # Marginalized over true values (predicted histogram vector of outcomes)\n    marginal_pred = np.sum(confusion, axis=1) / num_ratings\n    # Marginalized over predicted values (actual histogram vector of outcomes)\n    marginal_true = np.sum(confusion, axis=0) / num_ratings\n\n    # Expected ratings matrix E\n    # to match the scale with confusion matrix, we multiply num_ratings\n    expected = np.outer(marginal_pred, marginal_true) * num_ratings\n\n    # ================ 3. Score calculation ===============================\n    # zero out diagonal values\n    w_mat = np.ones([max_rating, max_rating], dtype=int)\n    w_mat.flat[:: max_rating + 1] = 0\n    observed_weighted_sum = np.sum(w_mat * confusion)\n    expected_weighted_sum = np.sum(w_mat * expected)\n\n    kappa = 1 - (observed_weighted_sum / expected_weighted_sum)\n\n    return {\n        \"kappa\": kappa,\n        \"observed_weighted_sum\": observed_weighted_sum,\n        \"expected_weighted_sum\": expected_weighted_sum,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:08:31.909727Z","iopub.status.idle":"2024-04-25T12:08:31.910158Z","shell.execute_reply.started":"2024-04-25T12:08:31.909952Z","shell.execute_reply":"2024-04-25T12:08:31.909971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The below example reveal the limitation of this metric to this comp.\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 2 # 2 is added to each element\nprint(compute_kappa(y_true, y_pred, max_rating=MAX_RATING))\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 1 # 1 is added to each element\nprint(compute_kappa(y_true, y_pred, max_rating=MAX_RATING))","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.912063Z","iopub.status.idle":"2024-04-25T12:08:31.912459Z","shell.execute_reply.started":"2024-04-25T12:08:31.912258Z","shell.execute_reply":"2024-04-25T12:08:31.912274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.913781Z","iopub.status.idle":"2024-04-25T12:08:31.914178Z","shell.execute_reply.started":"2024-04-25T12:08:31.913983Z","shell.execute_reply":"2024-04-25T12:08:31.914004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. \"Quadratic Weighted\" Kappa","metadata":{}},{"cell_type":"markdown","source":"Unlike the kappa, quadratic weighted kappa can take into account the magnitude of the disagreement between the predicted score and the actual score.\nLet k be the max score (In this comp, k is 6). The weights are calculated by the following formula.\n$$w_{ij}=(i−j)^2 / (k-1)^2$$\nThe larger the quadratic weighted kappa is, the better the model is at ordering categorical scores. This comp adopts this metric. In another words, this comp does not require the model to predict categorical scores of essays correctly. Precisely speaking, it requires the model to predict as close as possible to the actual score.\n$$\\kappa = 1- \\frac{\\sum_{I, J} w_{i,j}O{i,j}}{\\sum_{I, J} w_{i,j}E{i,j}} $$\n","metadata":{}},{"cell_type":"markdown","source":"## Creating the weights matrix W","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def compute_weights_matrix(max_rating):\n    \"\"\"\n    Calculate the weights matrix.\n\n    Parameters:\n    - max_rating: int\n               Maximum possible rating.\n    Returns:\n    - weights : array-like of shape (max_rating, max_rating)\n               Weights matrix.\n    \"\"\"\n    w_mat = np.zeros([max_rating, max_rating], dtype=int)\n    w_mat += np.arange(max_rating)\n    weights = (w_mat - w_mat.T) ** 2 / (max_rating - 1) ** 2\n    return weights\n\n\nW = compute_weights_matrix(max_rating=MAX_RATING)\nprint(W)\n# W[i, j] represents the quadratic difference in the position between the predicted score i and the ground-truth score j.\n# For example, if i = 1 and j = 2, its weight is (1 - 2)**2/(6-1)**2 = 1/25 = 0.04. The computation is done for all combinations of i and j.\n# The mechanism enables the kappa to reflect the ordinal difference.","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-24T10:51:10.684729800Z","start_time":"2024-04-24T10:51:10.682172700Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.915468Z","iopub.status.idle":"2024-04-25T12:08:31.915838Z","shell.execute_reply.started":"2024-04-25T12:08:31.915652Z","shell.execute_reply":"2024-04-25T12:08:31.915668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute quadratic weighted kappa\nIncorporating the weight matrix, the quadratic weighted kappa score is calculated as follows:","metadata":{}},{"cell_type":"code","source":"\ndef compute_quadratic_weighted_kappa(y_true, y_pred, max_rating):\n    \"\"\"\n    Calculate the quadratic weighted kappa score.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n               Actual ratings.\n    - y_pred : array-like of shape (n_samples,)\n               Predicted ratings.\n    - max_rating: int\n               Maximum possible rating.\n    Returns:\n    - kappa : float\n            Quadratic weighted kappa score.\n    \"\"\"\n    \n    # =========== 1. Creating the quadratic weights matrix W ======================\n    w_mat = np.zeros([max_rating, max_rating], dtype=int)\n    w_mat += np.arange(max_rating)\n    weights = (w_mat - w_mat.T) ** 2 / (max_rating - 1) ** 2\n    \n    # =========== 2. Creating the observed agreement matrix O ======================\n    # i indicate predicted score, j indicate actual score.\n    confusion = np.zeros((max_rating, max_rating))\n    for i, j in zip(y_true, y_pred):\n        confusion[i - 1, j - 1] += 1\n        \n    # =========== 3. Calculating the expected agreement Matrix E ==============\n    # Calculate the total number of samples\n    num_ratings = np.sum(confusion)\n\n    # Marginal sums for each rating\n    # Marginalized over true values (predicted histogram vector of outcomes)\n    marginal_pred = np.sum(confusion, axis=1) / num_ratings\n    # Marginalized over predicted values (actual histogram vector of outcomes)\n    marginal_true = np.sum(confusion, axis=0) / num_ratings\n\n    # Expected ratings matrix E\n    # to match the scale with confusion matrix, we multiply num_ratings\n    expected = np.outer(marginal_pred, marginal_true) * num_ratings\n\n    # ================ 4. Score calculation ===============================\n    observed_weighted_sum = np.sum(weights * confusion)\n    expected_weighted_sum = np.sum(weights * expected)\n\n    kappa = 1 - (observed_weighted_sum / expected_weighted_sum)\n\n    return {\n        \"kappa\": kappa,\n        \"observed_weighted_sum\": observed_weighted_sum,\n        \"expected_weighted_sum\": expected_weighted_sum,\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:08:31.917825Z","iopub.status.idle":"2024-04-25T12:08:31.918223Z","shell.execute_reply.started":"2024-04-25T12:08:31.91804Z","shell.execute_reply":"2024-04-25T12:08:31.91806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Check change in quadratic weighted kappa based on the closeness of the predicted score to the actual score","metadata":{}},{"cell_type":"code","source":"\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 2 # 2 is added to each element\nprint(compute_quadratic_weighted_kappa(y_true, y_pred, max_rating=MAX_RATING))\n# output: 0.11111111111111116\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 1 # 1 is added to each element\nprint(compute_quadratic_weighted_kappa(y_true, y_pred, max_rating=MAX_RATING))\n# output: 0.33333333333333337\n\n\n# As a result, the quadratic weighted kappa is higher when the predicted score is closer to the actual score.","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:08:31.920637Z","iopub.status.idle":"2024-04-25T12:08:31.921043Z","shell.execute_reply.started":"2024-04-25T12:08:31.920839Z","shell.execute_reply":"2024-04-25T12:08:31.920856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Appendix","metadata":{}},{"cell_type":"markdown","source":"You don't need to implement the kappa score by yourself. You can use the `sklearn.metrics.cohen_kappa_score` function like this:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 1 \nprint(cohen_kappa_score(y_true, y_pred, weights='quadratic', labels=[1, 2, 3, 4, 5, 6]))\n\ny_true = np.array([1, 1, 1, 2, 2, 2])\ny_pred = np.array([1, 1, 1, 2, 2, 2]) + 2\nprint(cohen_kappa_score(y_true, y_pred, weights='quadratic', labels=[1, 2, 3, 4, 5, 6]))\n\n# The results should be the same as the results of the function I implemented.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-25T12:08:31.922098Z","iopub.status.idle":"2024-04-25T12:08:31.922469Z","shell.execute_reply.started":"2024-04-25T12:08:31.922278Z","shell.execute_reply":"2024-04-25T12:08:31.922292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}