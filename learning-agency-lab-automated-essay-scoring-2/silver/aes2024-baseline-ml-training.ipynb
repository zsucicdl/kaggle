{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30684,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:white; font-size:180%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > TABLE OF CONTENTS<br><div>  \n* [IMPORTS](#1)\n* [INTRODUCTION](#2)\n    * [UTILITIES](#2.1)\n    * [CONFIGURATION](#2.2)    \n    * [FOREWORD](#2.3)\n    * [VERSION DETAILS](#2.4)\n* [PREPROCESSING](#3)\n* [MODEL TRAINING](#4)      \n* [PLANNED WAY FORWARD](#5)  ",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:white; font-size:180%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > KEY NOTE<br><div> \n    \nThis kernel is executed on Google Colab with the T4 GPU and high RAM settings, so please modify the code in a Kaggle environment and reuse <br>\n\n**KEY FEATURES** <br>\n1. Ability to add more models to the pipeline <br>\n2. Train-Inference separated <br>\n3. Vectorizer saved as objects <br>\n4. Configuration and utilities for easy experimentation \n    ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n\n# Installing select libraries:-\nfrom gc import collect;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nfrom IPython.display import display_html, clear_output;\nimport logging;\n\nfrom copy import deepcopy;\nimport pandas as pd, polars as pl, numpy as np;\nimport polars.selectors as cs;\n\nfrom os import path, walk, getpid\nfrom psutil import Process\nimport re\nfrom collections import Counter\nfrom itertools import product\n\nfrom colorama import Fore, Style, init;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\nimport joblib;\nimport os;\n\nfrom tqdm.notebook import tqdm;\nimport seaborn as sns;\nimport matplotlib.pyplot as plt;\nfrom matplotlib.colors import ListedColormap as LCM;\n%matplotlib inline\n\nfrom pprint import pprint;\nfrom functools import partial;\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nprint();\ncollect();\nclear_output();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\n# Importing model and pipeline specifics:-\nfrom category_encoders import OrdinalEncoder, OneHotEncoder;\n\n# Pipeline specifics:-\nfrom sklearn.preprocessing import (RobustScaler,\n                                   MinMaxScaler,\n                                   StandardScaler,\n                                   FunctionTransformer as FT,\n                                   PowerTransformer,\n                                  );\nfrom sklearn.impute import SimpleImputer as SI;\nfrom sklearn.model_selection import (RepeatedStratifiedKFold as RSKF,\n                                     StratifiedKFold as SKF,\n                                     StratifiedGroupKFold as SGKF,\n                                     KFold,\n                                     RepeatedKFold as RKF,\n                                     cross_val_score,\n                                     cross_val_predict\n                                    );\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline;\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin;\nfrom sklearn.compose import ColumnTransformer;\n\n# ML Model training:-\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score;\nfrom xgboost import DMatrix, XGBRegressor as XGBR;\nfrom lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR;\nfrom catboost import CatBoostRegressor as CBR, Pool;\nfrom sklearn.ensemble import HistGradientBoostingRegressor as HGBR, RandomForestRegressor as RFR;\n\n# Ensemble and tuning:-\nimport optuna;\nfrom optuna import Trial, trial, create_study;\nfrom optuna.pruners import HyperbandPruner;\nfrom optuna.samplers import TPESampler, CmaEsSampler;\noptuna.logging.set_verbosity = optuna.logging.ERROR;\noptuna.logging.disable_default_handler()\n\nclear_output();\nprint();\ncollect();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\n# Setting rc parameters in seaborn for plots and graphs-\n# Reference - https://matplotlib.org/stable/tutorials/introductory/customizing.html:-\n# To alter this, refer to matplotlib.rcParams.keys()\n\nsns.set({\"axes.facecolor\"       : \"#ffffff\",\n         \"figure.facecolor\"     : \"#ffffff\",\n         \"axes.edgecolor\"       : \"#000000\",\n         \"grid.color\"           : \"#ffffff\",\n         \"font.family\"          : ['Cambria'],\n         \"axes.labelcolor\"      : \"#000000\",\n         \"xtick.color\"          : \"#000000\",\n         \"ytick.color\"          : \"#000000\",\n         \"grid.linewidth\"       : 0.75,\n         \"grid.linestyle\"       : \"--\",\n         \"axes.titlecolor\"      : '#0099e6',\n         'axes.titlesize'       : 8.5,\n         'axes.labelweight'     : \"bold\",\n         'legend.fontsize'      : 7.0,\n         'legend.title_fontsize': 7.0,\n         'font.size'            : 7.5,\n         'xtick.labelsize'      : 7.5,\n         'ytick.labelsize'      : 7.5,\n        });\n\n# Setting global configuration for polars\npl.Config.activate_decimals(True).set_tbl_hide_column_data_types(True)\npl.Config(**dict(tbl_formatting = 'ASCII_FULL_CONDENSED',\n                 tbl_hide_column_data_types = True,\n                 tbl_hide_dataframe_shape = True,\n                 fmt_float = \"mixed\",\n                 tbl_cell_alignment = 'CENTER',\n                 tbl_hide_dtype_separator = True,\n                 tbl_cols = 100,\n                 tbl_rows = 50,\n                 fmt_str_lengths = 100,\n                )\n         )\n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config;\nset_config(transform_output = \"pandas\");\npd.set_option('display.max_columns', 50);\npd.set_option('display.max_rows', 50);\n\nprint();\ncollect();\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"2\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > INTRODUCTION<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"2.1\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: black; font-size:120%; text-align:left;padding:3.0px; background: #c2d6d6; border-bottom: 8px solid black\" > UTILITIES<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass Utility:\n    \"\"\"\n    This class serves to do the below-\n    1. Define method to print in color\n    2. Define the garbage cleaning process\n    \"\"\";\n\n    def PrintColor(self,text:str, color = Fore.BLUE, style = Style.BRIGHT):\n        \"Prints color outputs using colorama using a text F-string\";\n        print(style + color + text + Style.RESET_ALL)\n\n    def ScoreMetric(self, ytrue, ypred)-> float:\n        \"\"\"\n        This method calculates the custom metric from the imported script\n        Inputs- ytrue, ypred:- input truth and predictions\n        Output- float:- competition metric\n        \"\"\";\n\n        y_pred = np.uint8(np.around(np.clip(ypred, a_min = 1, a_max = 6)))\n        return cohen_kappa_score(np.uint8(ytrue), y_pred, weights = \"quadratic\")\n\n    def CleanMemory(self):\n        \"This method cleans the memory off unused objects and displays the cleaned state RAM usage\";\n\n        collect();\n        libc.malloc_trim(0)\n        pid        = getpid()\n        py         = Process(pid)\n        memory_use = py.memory_info()[0] / 2. ** 30\n        return f\"\\nRAM usage = {memory_use :.4} GB\"\n\nUtils = Utility()\nprint();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"2.2\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: black; font-size:120%; text-align:left;padding:3.0px; background: #c2d6d6; border-bottom: 8px solid black\" > CONFIGURATION<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "| Parameter         | Description                                             | Possible value choices|\n| ---               | ---                                                     | :-:                   |\n|  exp_nb           | Experiment Number                                       | integer               |\n|  version_nb       | Version Number                                          | integer               |\n|  test_req         | Are we testing syntax here?                             | Y/N                   |  \n|  test_sample_frac | Sample size for syntax test                             | float(0-1)/ int       |     \n|  gpu_switch       | GPU switch                                              | ON/OFF                |\n|  state            | Random state for most purposes                          | integer               |\n|  target           | Target column names                                     | string                |    \n|  path             | Path for input data files                               |                       |\n|  dtl_preproc_req  | Proprocessing required                                  | Y/N                   |    \n|  ftre_plots_req   | Feature plots required                                  | Y/N                   |\n|  ftre_imp_req     | Feature importance required                             | Y/N                   |   \n|  ML               | Machine Learning Models                                 | Y/N                   |\n|  nb_models        | Number of models                                        | integer               |\n|  n_splits         | Number of CV splits                                     | integer               |\n|  n_repeats        | Number of CV repeats                                    | integer               |\n|  nbrnd_erly_stp   | Number of early stopping rounds                         | integer               |\n|  mdl_cv_mthd      | Model CV method name                                    | RKF/ RSKF/ SKF/ KFold |\n|  ensemble_req     | Ensemble required                                       | Y/N                   |\n|  metric_obj       | Metric objective                                        | maximize/ minimize    |  \n|  ntrials          | Number of trials                                        | int                   |  ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass CFG:\n    \"\"\"\n    Configuration class for parameters and CV strategy for tuning and training\n    Some parameters may be unused here as this is a general configuration class\n    \"\"\";\n\n    # Data preparation:-\n    exp_nb             = 1;\n    version_nb         = 10;\n    test_req           = \"N\";\n    test_sample_frac   = 0.025;\n    gpu_switch         = \"ON\";\n    state              = 42;\n    target             = \"score\";\n    path               = f\"/input/learning-agency-lab-automated-essay-scoring-2\";\n    op_path            = f\"/content/drive/MyDrive/AES24/Interim\"\n\n    dtl_preproc_req    = \"Y\";\n    ftre_plots_req     = 'Y';\n    ftre_imp_req       = \"Y\";\n\n    # Model Training:-\n    ML                 = \"Y\";\n    nb_models          = 10;\n    n_splits           = 3 if test_req == \"Y\" else 12;\n    n_repeats          = 1 ;\n    nbrnd_erly_stp     = 100;\n    mdlcv_mthd         = 'RSKF';\n\n    a                  = 2.998\n    b                  = 1.042\n\n    # Ensemble:-\n    ensemble_req       = \"N\";\n    metric_obj         = 'maximize';\n    ntrials            = 10 if test_req == \"Y\" else 250;\n\n    # Global variables for plotting:-\n    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--',\n                           'color': 'lightgrey', 'linewidth': 0.75};\n    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': '#992600'};\n\nprint();\nUtils.PrintColor(f\"--> Configuration done!\\n\");\ncollect();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"2.3\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: black; font-size:120%; text-align:left;padding:3.0px; background: #c2d6d6; border-bottom: 8px solid black\" > FOREWORD<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "This competition aims to grade essays into 6 grades from 1-6 using a training corpus data. We are asked to use **Quadratic Kappa Score** as the metric. <br>\nScoring rubric is explained in detail [here](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf) as part of the competition overview and evaluation guidelines <br>\n\n### **KERNEL OBJECTIVE** <br>\nI delve into the data a bit here, use the public notebook to generate features and use them effectively to train ML models. <br>\nThis is the training kernel. I shall infer this separately and submit later <br>\n\n### **KERNEL SOURCES** <br>\nhttps://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments <br>\n\n### **MY CONTRIBUTION** <br>\n1. I created a Utility script to collate all preprocessing functions and use them in the train and inference kernels separately <br>\n2. I have created a class for the vectorizer instead of the function and lambda expression <br>\n3. I added a few more models and created an Optuna ensemble, CV score is better this way <br>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"2.4\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: black; font-size:120%; text-align:left;padding:3.0px; background: #c2d6d6; border-bottom: 8px solid black\" > VERSION DETAILS<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "|Experiment <br> Number| Version<br>Number | Version Details | CV score| Single/ Ensemble|Public LB Score|\n|----| :-: | --- | :-: | :-: |:-:|\n|1 | **V1** |* Public notebook features - 3361 features <br> * ML model training <br> * Optuna ensemble <br> * 10x1 RSKF|0.80812|Ensemble <br> Optuna|0.796|\n|1 | **V2** |* Public notebook features - 3237 features <br> * ML model training <br> * Optuna ensemble <br> * 10x1 RSKF|0.80631|Ensemble <br> Optuna|0.796|\n|1 | **V3** |* Public notebook features - 3237 features <br> * ML model training <br> * Optuna ensemble <br> * 10x1 Repeated K Fold |0.80683|Ensemble <br> Optuna|0.799|\n|1 | **V4** |* Public notebook features - 3237 features <br> * Retained infinity values <br> * ML model training <br> * Optuna ensemble <br> * 5x1 Repeated K Fold |0.80453|Ensemble <br> Optuna|0.796|\n|1 | **V5** |* Public notebook features - 3361 features <br> * Retained infinity values <br> * ML model training, 1 model with 10 random states <br> * Optuna ensemble <br> * 5x1 Repeated K Fold |0.80738|Ensemble <br> Optuna|0.796|\n|1 | **V6** |* Public notebook features - 8742 features <br> * Retained infinity values <br> * ML model training, 4 models <br> * Optuna ensemble <br> * 5x1 RSKF |0.80684|Ensemble <br> Optuna|0.796|\n|1 | **V9** |* Public notebook features - 13786 features <br> * Retained infinity values <br> * ML model training, 1 model <br> * Stacked with DeBERTAa large model <br> * 18x1 RSKF ||Stacking||",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"3\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > PREPROCESSING<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile \"fe.py\"\n\n# Feature engineering functions and classes to be used in the inference kernel also\nimport polars as pl, pandas as pd, numpy as np, re, joblib;\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer;\nfrom sklearn.base import RegressorMixin, ClassifierMixin, BaseEstimator, TransformerMixin\nfrom sklearn.metrics import cohen_kappa_score\nimport lightgbm as lgb\nimport os\n\nprint(f\"\\n---> Sourced from 'https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments'\\n\")\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x\n\ndef Paragraph_Preprocess(tmp):\n    tmp = tmp.explode('paragraph')\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n\ndef Paragraph_Eng(train_tmp):\n\n    paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n    aggs = [\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\")\n          for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n         ],\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\")\n          for i in [25,49]\n         ],\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ndef Sentence_Preprocess(tmp):\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    return tmp\n\ndef Sentence_Eng(train_tmp):\n\n    sentence_fea = ['sentence_len','sentence_word_cnt']\n    aggs = [\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\")\n          for i in [15,50,100,150,200,250,300]\n         ],\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ndef Word_Preprocess(tmp):\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    return tmp\n\ndef Word_Eng(train_tmp):\n    aggs = [\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ],\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ndef _maketk(x): return x\ndef _makepp(x): return x\n\nclass TFIDFMaker:\n    \"\"\"\n    This class creates/ implements a Tf-IDF and count vectorizer on the dataset provided\n    \"\"\"\n\n    def __init__(self, lbl: str, path: str, maketk = _maketk, makepp = _makepp):\n        \"\"\"\n        This method initializes the vectorizer/ loads the vectorizer based on the label\n        Inputs -\n        lbl - string - Train/ Test\n        path - string - path to save/ load the vectorizer\n        \"\"\"\n\n        self.lbl   = lbl\n        self.path  = path\n\n        if self.lbl.lower() == \"train\":\n            self.vectorizer = \\\n            TfidfVectorizer(tokenizer     =  maketk,\n                            preprocessor  =  makepp,\n                            token_pattern =  None,\n                            strip_accents = 'unicode',\n                            analyzer      = 'word',\n                            ngram_range   = (3,6),\n                            min_df        =  0.05,\n                            max_df        =  0.95,\n                            sublinear_tf  =  True,\n                           )\n\n            self.vectorizer_cnt  = \\\n            CountVectorizer(tokenizer     =  maketk,\n                            preprocessor  =  makepp,\n                            token_pattern =  None,\n                            strip_accents =  'unicode',\n                            analyzer      =  'word',\n                            ngram_range   = (2,3),\n                            min_df        = 0.10,\n                            max_df        = 0.85,\n                           )\n\n        else:\n            self.vectorizer     = joblib.load(os.path.join(self.path, \"vec\"))\n            self.vectorizer_cnt = joblib.load(os.path.join(self.path, \"cnt_vec\"))\n\n    def fit(self, X, y = None, **params):\n        \"This method fits the vectorizer for the training data\"\n        self.vectorizer.fit([i for i in X['full_text']])\n        joblib.dump(self.vectorizer, os.path.join(self.path, \"vec\"))\n\n        self.vectorizer_cnt.fit([i for i in X['full_text']])\n        joblib.dump(self.vectorizer_cnt, os.path.join(self.path, \"cnt_vec\"))\n        return self\n\n    def transform(self, X, y = None, **params):\n        \"This method transforms the provided data using the vectorizer\"\n\n        df1 = self.vectorizer.transform([i for i in X['full_text']])\n        df2 = self.vectorizer_cnt.transform([i for i in X['full_text']])\n        return (pd.DataFrame(df1.toarray(), dtype = np.float32).add_prefix(\"tfidf_\"),\n                pd.DataFrame(df2.toarray(), dtype = np.float32).add_prefix(\"tfid_cnt_\"),\n               )\n\n    def fit_transform(self, X, y = None, **params):\n        \"Defines the fit-transform process on the provided data\"\n\n        self.fit(X)\n        return self.transform(X)\n\n\ndef Make_Features(df, path: str, lbl: str,target: str = \"score\", maketk = _maketk, makepp = _makepp):\n    \"\"\"\n    This function uses all other previous functions and prepares the features for the provided dataset\n    \"\"\";\n\n    print(f\"\\n{'=' * 15} PREPROCESSING- {lbl.upper()} MODE {'=' * 15}\\n\")\n\n    tmp   = Paragraph_Preprocess(df)\n    feats = Paragraph_Eng(tmp)\n\n    if lbl == \"Train\":\n        feats[target] = df[target]\n\n    feature_names = list(filter(lambda x: x not in ['essay_id',target], feats.columns))\n    print(f\"1. Paragraph features = {feats.shape}\")\n\n    tmp   = Sentence_Preprocess(df)\n    feats = feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n    feature_names = list(filter(lambda x: x not in ['essay_id',target], feats.columns))\n    print(f\"2. Sentence features = {feats.shape}\")\n\n    tmp   = Word_Preprocess(df)\n    feats = feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n    feature_names = list(filter(lambda x: x not in ['essay_id','score'], feats.columns))\n    print(f\"3. Word features = {feats.shape}\")\n\n    tfidf = TFIDFMaker(lbl, path, maketk = _maketk, makepp = _makepp )\n    if lbl == \"Train\":\n        df_tfidf, df_cnt = tfidf.fit_transform(df)\n    else:\n        df_tfidf, df_cnt = tfidf.transform(df)\n\n    df_tfidf[\"essay_id\"] =  df[\"essay_id\"]\n    df_cnt[\"essay_id\"]   =  df[\"essay_id\"]\n    feats = feats.merge(df_tfidf, on='essay_id', how='left')\n    feats = feats.merge(df_cnt, on='essay_id', how='left')\n    print(f\"4. TFIDF features and count vectorizer = {feats.shape}\")\n\n    return feats\n\ndef ReduceMem(df: pd.DataFrame):\n    \"This method reduces memory for numeric columns in the dataframe\";\n\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"];\n    start_mem = df.memory_usage().sum() / 1024**2;\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min();\n            c_max = df[col].max();\n\n            if \"int\" in str(col_type):\n                if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    print(f\"Start - end memory:- {start_mem:5.2f} - {end_mem:5.2f} Mb\");\n    return df;",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from shutil import copyfile\nimport fe\nfrom fe import *",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\ntrain = \\\npl.read_csv(os.path.join(CFG.path, \"train.csv\")).\\\nwith_columns([( pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))])\n\nUtils.PrintColor(f\"\\nTrain data sample\\n\")\ndisplay(train.head(1))\n\ntest = \\\npl.read_csv(os.path.join(CFG.path, \"test.csv\")).\\\nwith_columns([( pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))])\n\nUtils.PrintColor(f\"\\nTest data sample\\n\")\ndisplay(test.head(1))\n\n# Utility script for feature engineering\nXYtrain = Make_Features(df = train,\n                        path = CFG.op_path,\n                        lbl = \"Train\",\n                        target = CFG.target,\n                       )\n\nXYtrain = ReduceMem(XYtrain)\n\nprint();\nXtest = Make_Features(df = test,\n                      lbl = \"test\",\n                      target = CFG.target,\n                      path = CFG.op_path,\n                     )\nXtest = ReduceMem(Xtest)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\n# Stacking the DeBERTA large model results herewith:-\ndf = \\\npd.DataFrame(data = joblib.load('/content/drive/MyDrive/AES24/Imports/oof.pkl'),\n             index = XYtrain[\"essay_id\"],\n             ).add_prefix(\"deberta_\")\n\nXYtrain = XYtrain.merge(df, how = \"left\", left_on = \"essay_id\", right_index = True)\n\nfor col in df.columns:\n    Xtest[col] = 0.5\n\n# Saving the train set for future usage:-\nXYtrain.to_parquet(os.path.join(CFG.op_path, f\"XYtrain_E{CFG.exp_nb}.parquet\"))\n\ndel df;\n_ = Utils.CleanMemory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%time\n\nfig, ax = plt.subplots(1,1, figsize = (7, 5))\n\nXYtrain[CFG.target].value_counts().sort_index(ascending = True).plot.bar(ax=ax, color = \"tab:blue\")\nax.set(xlabel = \"\", ylabel = \"\");\nax.set_title(f\"Target plot\", **CFG.title_specs)\nax.grid(**CFG.grid_specs)\nax.set_yticks(range(0, 7001, 250), labels = range(0, 7001, 250), fontsize = 7)\nax.set_xticks(range(0,6,1), labels = range(1,7,1), rotation = 0)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"4\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color: white; font-size:120%; text-align:left;padding:3.0px; background: maroon; border-bottom: 8px solid black\" > MODEL TRAINING<br><div>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass OptunaEnsembler:\n    \"\"\"\n    This is the Optuna ensemble class-\n    Source- https://www.kaggle.com/code/arunklenin/ps3e26-cirrhosis-survial-prediction-multiclass\n    \"\"\";\n\n    def __init__(self, model_mode:str = \"Regression\"):\n        \"\"\"\n        Key parameter:-\n        model_mode- string :-\n        Regression- uses the Cohen Kappa metric\n        Classification - uses the multiclass auc metric\n        \"\"\"\n\n        self.study        = None;\n        self.weights      = None;\n        self.random_state = CFG.state;\n        self.n_trials     = CFG.ntrials;\n        self.direction    = CFG.metric_obj;\n        self.model_mode   = model_mode;\n\n        if self.model_mode == \"Regression\":\n            self.ScoreMetric = Utils.ScoreMetric\n        else:\n            self.ScoreMetric = Utils.ClsfMetric\n\n    def _objective(self, trial, y_true, y_preds):\n        \"\"\"\n        This method defines the objective function for the ensemble\n        \"\"\";\n\n        if isinstance(y_preds, pd.DataFrame) or isinstance(y_preds, np.ndarray):\n            weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(y_preds.shape[-1])];\n            axis = 1;\n        elif isinstance(y_preds, list):\n            weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))];\n            axis = 0;\n\n        # Calculating the weighted prediction:-\n        weighted_pred  = np.average(np.array(y_preds), axis = axis, weights = weights);\n        score          = self.ScoreMetric(y_true, weighted_pred);\n        return score;\n\n    def fit(self, y_true, y_preds):\n        \"This method fits the Optuna objective on the fold level data\";\n\n        optuna.logging.set_verbosity = optuna.logging.ERROR;\n        self.study = \\\n        optuna.create_study(sampler    = TPESampler(seed = self.random_state),\n                            pruner     = HyperbandPruner(),\n                            study_name = \"Ensemble\",\n                            direction  = self.direction,\n                           );\n\n        obj = partial(self._objective, y_true = y_true, y_preds = y_preds);\n        self.study.optimize(obj, n_trials = self.n_trials);\n\n        if isinstance(y_preds, list):\n            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))];\n        else:\n            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(y_preds.shape[-1])];\n        clear_output();\n\n    def predict(self, y_preds):\n        \"This method predicts using the fitted Optuna objective\";\n\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict';\n\n        if isinstance(y_preds, list):\n            weighted_pred = np.average(np.array(y_preds), axis=0, weights = self.weights);\n        else:\n            weighted_pred = np.average(np.array(y_preds), axis=1, weights = self.weights);\n        return weighted_pred;\n\n    def fit_predict(self, y_true, y_preds):\n        \"\"\"\n        This method fits the Optuna objective on the fold data, then predicts the test set\n        \"\"\";\n        self.fit(y_true, y_preds);\n        return self.predict(y_preds);\n\n    def weights(self):\n        return self.weights;\n\nprint();\ncollect();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass LGBMSupport:\n    \"\"\"\n    This class supports the LGBM training with a custom metric and objective function\n    \"\"\"\n\n    def __init__(self, a, b):\n        \"\"\"\n        Initializing class level parameters\n        \"\"\"\n        self.a = a\n        self.b = b\n\n    def _LGBMetric(self, y_true, y_pred):\n        \"\"\"\n        This is the custom metric used in evaluation and early stopping\n        \"\"\";\n\n        y_true = y_true + self.a\n        y_pred = (y_pred + self.a).clip(1, 6).round()\n        qwk = cohen_kappa_score(np.around(y_true,0), y_pred, weights=\"quadratic\")\n        return 'QWK', qwk, True\n\n    def _LGBObj(self, y_true, y_pred):\n        \"This is the custom objective function for the LGBM\"\n\n        labels = y_true  + self.a\n        preds  = y_pred  + self.a\n        preds = preds.clip(1, 6)\n\n        f = 1/2 * np.sum((preds-labels)**2)\n        g = 1/2 * np.sum((preds- self.a)**2  + self.b)\n\n        df = preds - labels\n        dg = preds - self.a\n\n        grad = (df/g - f*dg/g**2)*len(labels)\n        hess = np.ones(len(labels))\n        return grad, hess\n\nclass MyLogger:\n    \"\"\"\n    This class helps to suppress logs in lightgbm and Optuna\n    Source - https://github.com/microsoft/LightGBM/issues/6014\n    \"\"\"\n\n    def init(self, logging_lbl: str):\n        self.logger = logging.getLogger(logging_lbl)\n        self.logger.setLevel(logging.ERROR)\n\n    def info(self, message):\n        pass\n\n    def warning(self, message):\n        pass\n\n    def error(self, message):\n        self.logger.error(message)\n\nclass VotingModelMaker(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators: list, weights: list, a = CFG.a, b = CFG.b):\n        super().__init__()\n        self.estimators = estimators\n        self.weights    = weights\n        self.a = a\n        self.b = b\n\n    def fit(self, X, y=None):\n        return self\n\n    def mdlpredict(self, X):\n        y_preds = \\\n        pd.DataFrame(columns = [f\"Est{i}\" for i in range(len((self.estimators)))],\n                     index = range(len(X))\n                    )\n\n        for i, estm in enumerate(self.estimators):\n            y_preds[f\"Est{i}\"] = estm.predict(X) + self.a\n\n        if self.weights != []:\n            return np.average(y_preds, axis=1, weights = self.weights)\n        else:\n            return np.mean(y_preds, axis=1,)\n\n    def mdlpredictproba(self, X):\n        y_preds = \\\n        pd.DataFrame(columns = [f\"Est{i}\" for i in range(len(range(self.estimators)))],\n                     index = range(len(X))\n                    )\n\n        for i, estm in enumerate(self.estimators):\n            y_preds[f\"Est{i}\"] = estm.predict_proba(X)[:,1]\n\n        if self.weights != []:\n            return np.average(y_preds, axis=1, weights = self.weights)\n        else:\n            return np.mean(y_preds, axis=1,)\n\nmdlsupport = LGBMSupport(a = CFG.a,b = CFG.b)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass MdlDeveloper(CFG):\n    \"\"\"\n    This class implements the training pipeline elements-\n    1. Initializes the Model predictions\n    2. Trains and infers models\n    3. Returns the OOF and model test set predictions\n    \"\"\";\n\n    def __init__(self, Xtrain, ytrain, ygrp, Xtest, sel_cols, cat_cols, model_mode,\n                 **kwarg\n                ):\n        \"\"\"\n        In this method, we initialize the below-\n        1. Train-test data, selected columns\n        2. Metric, custom scorer, model and cv object\n        3. Output tables for score and predictions\n        \"\"\";\n\n        self.Xtrain      = Xtrain\n        self.ytrain      = ytrain\n        self.y_grp       = ygrp\n        self.Xtest       = Xtest\n        self.sel_cols    = sel_cols\n        self.cat_cols    = cat_cols\n        self.model_mode  = model_mode\n        self.num_class   = 6\n\n        if self.model_mode == \"Regression\":\n            self.ScoreMetric = Utils.ScoreMetric\n        else:\n            self.ScoreMetric = Utils.ClsfMetric\n\n        self._DefineModels();\n        self.cv          = self.all_cv[self.mdlcv_mthd];\n\n        self.methods = list(self.Mdl_Master.keys());\n\n        if self.model_mode == \"Regression\":\n            self.methods = [c for c in self.methods if c.endswith(\"R\")]\n            self.OOF_Preds   = pd.DataFrame();\n            self.Mdl_Preds   = pd.DataFrame();\n\n        else:\n            self.methods   = [c for c in self.methods if c.endswith(\"C\")]\n            self.OOF_Preds = pd.DataFrame(columns = [f\"class{i}\" for i in range(self.num_class)])\n            self.Mdl_Preds = pd.DataFrame(columns = [f\"class{i}\" for i in range(self.num_class)])\n\n        self.Scores = pd.DataFrame(columns = self.methods + [\"Ensemble\"],\n                                   index = range(self.n_splits * self.n_repeats)\n                                  );\n\n        self.TrainScores = pd.DataFrame(columns = self.methods,\n                                        index = range(self.n_splits * self.n_repeats)\n                                       );\n\n        self.AllFittedModels = []\n\n        Utils.PrintColor(f\"\\n---> Selected model options-\");\n        try:\n            with np.printoptions(linewidth = 150):\n                pprint(np.array(self.methods), depth = 1, width = 100, indent = 5);\n        except:\n            pprint(self.methods, depth = 1, width = 100, indent = 5);\n\n    def _DefineModels(self):\n        \"\"\"\n        This method initiliazes models for the analysis\n        It also initializes the CV methods and class-weights that could be tuned going ahead.\n        \"\"\";\n\n        # Commonly used CV strategies for later usage:-\n        self.all_cv = \\\n        {'KF'  : KFold(n_splits = self.n_splits, shuffle = True, random_state= self.state),\n         'RKF' : RKF(n_splits   = self.n_splits, n_repeats = self.n_repeats, random_state= self.state),\n         'RSKF': RSKF(n_splits  = self.n_splits, n_repeats = self.n_repeats, random_state= self.state),\n         'SKF' : SKF(n_splits   = self.n_splits, shuffle = True, random_state= self.state),\n         'SGKF': SGKF(n_splits  = self.n_splits, shuffle= True, random_state= self.state),\n        };\n\n        self.Mdl_Master = \\\n        {\n         'LGBM1R':LGBMR(objective        = mdlsupport._LGBObj,\n                        metrics          = 'None',\n                        learning_rate    = 0.10,\n                        max_depth        = 6,\n                        num_leaves       = 10,\n                        colsample_bytree = 0.50,\n                        reg_alpha        = 0.10,\n                        reg_lambda       = 0.80,\n                        n_estimators     = 1024,\n                        random_state     = self.state,\n                        extra_trees      = True,\n                        class_weight     = 'balanced',\n                        verbosity        = - 1,\n                        device           = \"gpu\" if self.gpu_switch == \"ON\" else \"cpu\",\n                       ),\n\n        'LGBM1C':LGBMC(objective        = \"multiclass\",\n                       metrics          = 'auc_mu',\n                       num_class        = self.num_class,\n                       learning_rate    = 0.10,\n                       max_depth        = 6,\n                       num_leaves       = 10,\n                       colsample_bytree = 0.50,\n                       reg_alpha        = 0.10,\n                       reg_lambda       = 0.80,\n                       n_estimators     = 1024,\n                       random_state     = self.state,\n                       extra_trees      = True,\n                       class_weight     = 'balanced',\n                       verbosity        = - 1,\n                       device           = \"gpu\" if self.gpu_switch == \"ON\" else \"cpu\",\n                       ),\n        }\n        return self;\n\n    def PostProcessPred(self, ypred):\n        \"\"\"\n        This is an optional post-processing method\n        We clip the values of the target between 1 and 6\n        \"\"\";\n        return ypred;\n\n    def TrainMdl(self, target: str, test_preds_req: str = \"Y\", save_models = \"N\",):\n        \"\"\"\n        This method trains and infers from the model suite and returns the predictions and scores\n        It optionally predicts the test set too, if desired by the user\n        \"\"\";\n\n        # Initializing I-O:-\n        X,y  = self.Xtrain[self.sel_cols], self.ytrain.copy(deep = True);\n        Xt   = self.Xtest[self.sel_cols];\n\n        cols_drop  = [\"Source\", \"essay_id\",];\n        ens        = OptunaEnsembler(model_mode = self.model_mode);\n\n        self.FtreImp = pd.DataFrame(columns = self.methods,\n                                    index   = [c for c in self.sel_cols if c not in cols_drop]\n                                   ).fillna(0);\n\n        # Making CV folds:-\n        for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(self.cv.split(X, self.y_grp))):\n            Xtr  = X.iloc[train_idx].drop(columns = cols_drop, errors = 'ignore');\n            Xdev = X.iloc[dev_idx].drop(columns = cols_drop, errors = 'ignore');\n            ytr  = y.loc[y.index.isin(Xtr.index)];\n            ydev = y.loc[y.index.isin(Xdev.index)];\n\n            fitted_models = [];\n\n            # Initializing the OOF and test set predictions:-\n            if self.model_mode == \"Regression\":\n                oof_preds = pd.DataFrame(columns = self.methods, index = Xdev.index);\n                mdl_preds = pd.DataFrame(columns = self.methods, index = Xt.index);\n\n            else:\n                oof_preds = []\n                mdl_preds = []\n\n            Utils.PrintColor(f\"\\n{'=' * 5} FOLD {fold_nb + 1} {'=' * 5}\\n\");\n\n            # Initializing models across methods:-\n            for method in tqdm(self.methods):\n\n                model = self.Mdl_Master.get(method);\n\n                # Fitting the model:-\n                if \"CB\" in method:\n                    model.fit(Xtr, ytr,\n                              eval_set = [(Xdev, ydev)],\n                              verbose = 0,\n                              early_stopping_rounds = CFG.nbrnd_erly_stp,\n                             );\n\n                elif \"LGBM\" in method and method.endswith(\"R\"):\n                    model.fit(Xtr, ytr,\n                              eval_set = [(Xdev, ydev)],\n                              eval_names = [(\"Dev\")],\n                              eval_metric = mdlsupport._LGBMetric,\n                              callbacks = [log_evaluation(250),\n                                           early_stopping(stopping_rounds = self.nbrnd_erly_stp,\n                                                          verbose = False,),\n                                          ],\n                             );\n\n                elif \"LGBM\" in method and method.endswith(\"C\"):\n                    model.fit(Xtr, ytr,\n                              eval_set = [(Xdev, ydev)],\n                              eval_names = [(\"Dev\")],\n                              callbacks = [log_evaluation(250),\n                                           early_stopping(stopping_rounds = self.nbrnd_erly_stp,\n                                                          verbose = False,),\n                                          ],\n                             );\n\n                elif \"XGB\" in method:\n                     model.fit(Xtr, ytr,\n                               eval_set = [(Xdev, ydev)],\n                               verbose  = 0,\n                              );\n\n                else:\n                    model.fit(Xtr, ytr);\n\n                # Collating feature importance:-\n                try:\n                    self.FtreImp[method] += model.feature_importances_;\n                except:\n                    pass;\n\n                fitted_models.append(model);\n\n                # Collecting predictions and scores and post-processing OOF based on model method:-\n                if method.endswith(\"R\"):\n                    dev_preds    = self.PostProcessPred(model.predict(Xdev)) + self.a ;\n                    train_preds  = self.PostProcessPred(model.predict(Xtr)) + self.a ;\n                    tr_score     = self.ScoreMetric(ytr.values.flatten()  + self.a, train_preds,);\n                    score        = self.ScoreMetric(ydev.values.flatten() + self.a, dev_preds);\n\n                    oof_preds[method] = dev_preds;\n\n                    if test_preds_req == \"Y\":\n                        mdl_preds[method] = \\\n                        self.PostProcessPred(model.predict(Xt.drop(columns = cols_drop, errors = \"ignore\"))) + self.a;\n\n                else:\n                    dev_preds    = model.predict_proba(Xdev) ;\n                    train_preds  = model.predict_proba(Xtr) ;\n                    tr_score     = self.ScoreMetric(ytr.values, train_preds,);\n                    score        = self.ScoreMetric(ydev.values, dev_preds);\n\n                    oof_preds.append(dev_preds)\n\n                    if test_preds_req == \"Y\":\n                        test_preds = model.predict_proba(Xt.drop(columns = cols_drop, errors = \"ignore\"))\n                        mdl_preds.append(test_preds)\n                        del test_preds\n\n                Utils.PrintColor(f\"OOF = {score:.5f} | Train = {tr_score:.5f} | {method}\",color = Fore.CYAN);\n\n                # Integrating the predictions and scores:-\n                self.Scores.at[fold_nb, method]      = np.round(score, decimals= 6);\n                self.TrainScores.at[fold_nb, method] = np.round(tr_score, decimals= 6);\n\n            try:\n                del dev_preds, train_preds, tr_score, score;\n            except:\n                pass;\n\n            # Ensembling the predictions with post-processing:-\n            if self.ensemble_req == \"Y\" and self.model_mode == \"Regression\":\n                oof_preds[\"Ensemble\"]  = self.PostProcessPred(ens.fit_predict(ydev + self.a, oof_preds[self.methods]));\n                score                  = self.ScoreMetric(ydev + self.a, oof_preds[\"Ensemble\"].values);\n                oof_preds[\"Ensemble\"]  = oof_preds[\"Ensemble\"]\n                self.OOF_Preds         = pd.concat([self.OOF_Preds, oof_preds], axis = 0, ignore_index = False);\n                self.Scores.at[fold_nb, \"Ensemble\"] = np.round(score,6);\n\n                if test_preds_req == \"Y\":\n                    mdl_preds[\"Ensemble\"] = ens.predict(mdl_preds[self.methods]);\n                    self.Mdl_Preds        = pd.concat([self.Mdl_Preds, mdl_preds], axis = 0, ignore_index = False);\n\n                vote_model = VotingModelMaker(estimators = fitted_models, weights = ens.weights)\n\n                if save_models == \"Y\" and self.model_mode == \"Regression\":\n                    joblib.dump(vote_model,\n                                os.path.join(self.op_path, f\"VR_E{self.exp_nb}V{self.version_nb}_Fold{fold_nb}.model\")\n                               );\n                elif save_models == \"Y\" and self.model_mode == \"Classification\":\n                    joblib.dump(vote_model,\n                                os.path.join(self.op_path, f\"VC_E{self.exp_nb}V{self.version_nb}_Fold{fold_nb}.model\")\n                               );\n\n                self.AllFittedModels.append(vote_model)\n\n            if self.ensemble_req == \"Y\" and self.model_mode == \"Classification\":\n                ens_preds       = ens.fit_predict(ydev, oof_preds);\n                score           = self.ScoreMetric(ydev, ens_preds);\n                ens_preds       = pd.DataFrame(ens_preds, index = ydev.index, columns = self.OOF_Preds.columns)\n                self.OOF_Preds  = pd.concat([self.OOF_Preds, ens_preds], axis = 0, ignore_index = False)\n                self.Scores.at[fold_nb, \"Ensemble\"] = np.round(score,6);\n\n                if test_preds_req == \"Y\":\n                    test_preds      = pd.DataFrame(ens.predict(mdl_preds), index = Xt.index, columns = self.Mdl_Preds.columns)\n                    self.Mdl_Preds  = pd.concat([self.Mdl_Preds, test_preds], axis = 0, ignore_index = False);\n\n            elif self.ensemble_req != \"Y\" and self.model_mode == \"Regression\":\n                self.AllFittedModels.append(model)\n                self.OOF_Preds = pd.concat([self.OOF_Preds, oof_preds], axis = 0, ignore_index = False)\n\n                if test_preds_req == \"Y\":\n                    mdl_preds[f\"Ensemble\"]  = ens.predict(mdl_preds[self.methods]) + self.a\n                    self.Mdl_Preds = pd.concat([self.Mdl_Preds , mdl_preds], axis = 0, ignore_index = False)\n\n            elif self.ensemble_req != \"Y\" and self.model_mode == \"Classification\":\n                self.AllFittedModels.append(model)\n                self.OOF_Preds = pd.concat([self.OOF_Preds,\n                                            pd.DataFrame(oof_preds[0],\n                                                         index = ydev.index,\n                                                         columns = self.OOF_Preds.columns\n                                                         )\n                                            ],\n                                           axis = 0, ignore_index = False\n                                           )\n\n                if test_preds_req == \"Y\":\n                    self.Mdl_Preds  = pd.concat([self.Mdl_Preds,\n                                                 pd.DataFrame(ens.predict(mdl_preds),\n                                                              index = Xt.index,\n                                                              columns = self.Mdl_Preds.columns\n                                                              )\n                                                 ] , axis=0, ignore_index = False\n                                                )\n            else:\n                pass\n\n        # Averaging the predictions after all folds:-\n        self.OOF_Preds = self.OOF_Preds.groupby(level = 0).mean();\n\n        if self.model_mode == \"Regression\":\n            if test_preds_req == \"Y\" and self.ensemble == \"Y\":\n                self.Mdl_Preds = self.Mdl_Preds[self.methods + [\"Ensemble\"]].groupby(level=0).mean();\n            elif test_preds_req == \"Y\" and self.ensemble != \"Y\":\n                self.Mdl_Preds = self.Mdl_Preds[self.methods].groupby(level=0).mean();\n            elif test_preds_req != \"Y\":\n                pass;\n\n        elif self.model_mode == \"Classification\":\n            if test_preds_req == \"Y\":\n                self.Mdl_Preds = self.Mdl_Preds.groupby(level=0).mean();\n            elif test_preds_req != \"Y\":\n                pass;\n\n        else:\n            pass\n\n        if self.ensemble_req == \"Y\" and self.model_mode == \"Regression\":\n            joblib.dump(self.AllFittedModels,\n                        os.path.join(self.op_path, f\"VR_E{self.exp_nb}V{self.version_nb}\")\n                       );\n\n        elif self.ensemble_req == \"Y\" and self.model_mode == \"Classification\":\n            joblib.dump(self.AllFittedModels,\n                        os.path.join(self.op_path, f\"VC_E{self.exp_nb}V{self.version_nb}\")\n                       );\n\n        elif self.ensemble_req != \"Y\" and self.model_mode == \"Regression\":\n            vote_model = []\n            vote_model.append(VotingModelMaker(estimators = self.AllFittedModels, weights = []))\n\n            joblib.dump(vote_model,\n                        os.path.join(self.op_path, f\"VR_E{self.exp_nb}V{self.version_nb}\")\n                       );\n            del vote_model;\n\n        elif self.ensemble_req != \"Y\" and self.model_mode == \"Classification\":\n            vote_model = []\n            vote_model.append(VotingModelMaker(estimators = self.AllFittedModels, weights = []))\n\n            joblib.dump(vote_model,\n                        os.path.join(self.op_path, f\"VC_E{self.exp_nb}V{self.version_nb}\")\n                       );\n            del vote_model;\n\n        else:\n            pass\n\n        return self.OOF_Preds, self.Mdl_Preds, self.Scores, self.TrainScores;\n\nprint();\ncollect();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nclass DisplayMaker:\n    \"\"\"\n    This class plots the final scores and generates adjutant model utilities\n    \"\"\";\n\n    def __init__(self, target):\n        self.target   = target;\n        self.ensemble = CFG.ensemble_req\n\n    def DisplayAdjTbl(self, *args):\n        \"\"\"\n        This function displays pandas tables in an adjacent manner, sourced from the below link-\n        https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n        \"\"\";\n\n        html_str = '';\n        for df in args:\n            html_str += df.to_html();\n        display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True);\n        collect();\n\n    def DisplayScores(self, Scores: pd.DataFrame, TrainScores: pd.DataFrame, methods: list):\n        \"This method displays the scores and their means\";\n\n        if self.ensemble == \"Y\":\n            cols  = methods + [\"Ensemble\"]\n        else:\n            cols = methods\n        args = \\\n        [Scores.style.format(precision = 5).\\\n         background_gradient(cmap = \"mako\", subset = cols).\\\n         set_caption(f\"\\nOOF scores across methods and folds\\n\").\\\n         set_properties(**{'text-align': 'centre'}),\n\n         TrainScores.style.format(precision = 5).\\\n         background_gradient(cmap = \"mako\", subset = methods).\\\n         set_caption(f\"\\nTrain scores across methods and folds\\n\").\\\n         set_properties(**{'text-align': 'centre'})\n        ];\n\n        self.DisplayAdjTbl(*args);\n\n        print('\\n');\n        display(Scores.mean().to_frame().\\\n                transpose().\\\n                style.format(precision = 5).\\\n                background_gradient(cmap = \"mako\", axis=1, subset = Scores.columns).\\\n                set_caption(f\"\\nOOF mean scores across methods and folds\\n\").\\\n                set_properties(**{'text-align': 'centre'})\n               );\n\n    def MakeMLPlots(self, methods: list, FtreImp: pd.DataFrame):\n        \"\"\"\n        This method makes plots for the ML models, including feature importance\n        \"\"\";\n\n        fig, axes = plt.subplots(len(methods), 1, figsize = (12, len(methods) * 6),\n                                 gridspec_kw = {'hspace': 0.8, 'wspace': 0.2},\n                                );\n\n        for i, col in enumerate(methods):\n            try:\n                ax = axes[i];\n            except:\n                ax = axes\n\n            FtreImp[[col]].\\\n            sort_values(col, ascending = False).head(50).\\\n            plot.bar(ax = ax, color = '#0073e6');\n            ax.set_title(f\"{col} Importances\", **CFG.title_specs);\n            ax.set(xlabel = '', ylabel = '');\n\n        plt.tight_layout();\n        plt.show();\n\ncollect();\nprint();",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nl = MyLogger()\nl.init(logging_lbl = \"lightgbm_custom\")\nlgb.register_logger(l)\n\nif CFG.ML == \"Y\":\n    md = MdlDeveloper(XYtrain.drop(columns = CFG.target),\n                        XYtrain[CFG.target] - 1,\n                        XYtrain[CFG.target] - 1,\n                        Xtest,\n                        sel_cols = list(XYtrain.drop(columns = CFG.target).columns),\n                        cat_cols = [],\n                        model_mode = \"Classification\",\n                        );\n    OOF_Preds, Mdl_Preds, Scores, TrainScores = \\\n    md.TrainMdl(test_preds_req = \"N\", target = CFG.target, save_models = \"Y\");\n\n    print(\"\\n\\n\\n\");\n    disp = DisplayMaker(CFG.target);\n    disp.DisplayScores(Scores, TrainScores, methods = md.methods);\n    disp.MakeMLPlots(methods = list(md.methods), FtreImp = md.FtreImp)\n    _ = Utils.CleanMemory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nif CFG.ML == \"Y\":\n    XYtrain = pd.concat([XYtrain, OOF_Preds], axis=1)\n    for col in [f\"class{i}\" for i in range(6)]:\n        Xtest[col] = 0.5\n\n    Utils.PrintColor(f\"\\nTrain Test data shape = {XYtrain.shape} | {Xtest.shape}\\n\")\n\n    md = MdlDeveloper(XYtrain.drop(columns = CFG.target),\n                      XYtrain[CFG.target] - CFG.a,\n                      XYtrain[CFG.target].astype(np.uint8),\n                      Xtest,\n                      sel_cols = list(XYtrain.drop(columns = CFG.target).columns),\n                      cat_cols = [],\n                      model_mode = \"Regression\",\n                      );\n\n    OOF_Preds, Mdl_Preds, Scores, TrainScores = \\\n    md.TrainMdl(test_preds_req = \"N\", target = CFG.target, save_models = \"Y\");\n\n    OOF_Preds[CFG.target] = XYtrain[CFG.target].values\n    OOF_Preds.to_csv(os.path.join(CFG.op_path, f\"OOF_Preds_E{CFG.exp_nb}V{CFG.version_nb}.csv\"))\n    md.FtreImp.to_csv(os.path.join(CFG.op_path,f\"FtreImp_E{CFG.exp_nb}V{CFG.version_nb}.csv\"))\n\n    disp = DisplayMaker(CFG.target);\n    disp.DisplayScores(Scores, TrainScores, methods = list(md.methods));\n\n    print(\"\\n\\n\")\n    disp.MakeMLPlots(methods = list(md.methods), FtreImp = md.FtreImp)\n\n_ = Utils.CleanMemory()\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
