{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T20:58:16.564816Z","iopub.execute_input":"2024-05-01T20:58:16.565212Z","iopub.status.idle":"2024-05-01T20:58:16.874291Z","shell.execute_reply.started":"2024-05-01T20:58:16.565183Z","shell.execute_reply":"2024-05-01T20:58:16.873314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ~~Idea 1: What if we feed the rubric to a pretrained LLM?~~\nA: It's difficult to get working for a number of reasons. The rubric is not very semantically helpful and the LLM's default calibration is quite bad.\n### Idea 2: Let's finetune an additional classifier head on top of a lightweight pretrained LLM.","metadata":{}},{"cell_type":"markdown","source":"### Preliminary: Exploratory Data Analysis:","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\ntrain_df = pd.read_csv(DATA_DIR + \"train.csv\")\ntest_df = pd.read_csv(DATA_DIR + 'test.csv')\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:16.92543Z","iopub.execute_input":"2024-05-01T20:58:16.925928Z","iopub.status.idle":"2024-05-01T20:58:17.841122Z","shell.execute_reply.started":"2024-05-01T20:58:16.925899Z","shell.execute_reply":"2024-05-01T20:58:17.840186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examine manual features\n\ndef annotate_with_features(df):\n    mean = lambda x: sum(x) / len(x)\n    df[\"char_length\"] = df[\"full_text\"].apply(lambda x: len(x))\n    df[\"word_length\"] = df[\"full_text\"].apply(lambda x: len(x.split(' ')))\n    df[\"num_sentences\"] = df[\"full_text\"].apply(lambda x: len(x.split('.')))\n    df[\"mean_sentence_wordlength\"] = df[\"full_text\"].apply(lambda x: mean([len(i.split(' ')) for i in x.split('.')]))\n    df[\"mean_word_charlength\"] = df[\"full_text\"].apply(lambda x: mean([len(i) for i in x.split(' ')]))\n    return df\n\ntrain_df = annotate_with_features(train_df)\ntest_df = annotate_with_features(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:17.842698Z","iopub.execute_input":"2024-05-01T20:58:17.843012Z","iopub.status.idle":"2024-05-01T20:58:19.72665Z","shell.execute_reply.started":"2024-05-01T20:58:17.842986Z","shell.execute_reply":"2024-05-01T20:58:19.725676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:19.728349Z","iopub.execute_input":"2024-05-01T20:58:19.728709Z","iopub.status.idle":"2024-05-01T20:58:19.744164Z","shell.execute_reply.started":"2024-05-01T20:58:19.728674Z","shell.execute_reply":"2024-05-01T20:58:19.74291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:19.747211Z","iopub.execute_input":"2024-05-01T20:58:19.747572Z","iopub.status.idle":"2024-05-01T20:58:19.773532Z","shell.execute_reply.started":"2024-05-01T20:58:19.747535Z","shell.execute_reply":"2024-05-01T20:58:19.772554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like test.csv is just the first three rows of train.csv. We can discard it for now.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:19.775386Z","iopub.execute_input":"2024-05-01T20:58:19.775702Z","iopub.status.idle":"2024-05-01T20:58:19.811497Z","shell.execute_reply.started":"2024-05-01T20:58:19.775675Z","shell.execute_reply":"2024-05-01T20:58:19.81062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are the variables we extracted correlated with score?","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(train_df.drop(\"full_text\", axis = 1))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:19.812656Z","iopub.execute_input":"2024-05-01T20:58:19.812938Z","iopub.status.idle":"2024-05-01T20:58:45.337088Z","shell.execute_reply.started":"2024-05-01T20:58:19.812913Z","shell.execute_reply":"2024-05-01T20:58:45.336139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the top row, we can see that essays with more characters, more sentences, and longer words typcially have higher scores. It's not that long essays with polysyllabic words are always good - but it's nearly always the case that all good essays have decent length and word complexity.","metadata":{}},{"cell_type":"markdown","source":"### Local Evaluation Framework\nLet's write a local evaluation framework such that we don't have to upload our results to examine our model's performance. Faster iteration = faster improvement!\n\nThe metric used in this competition is quadratic weighted Kappa. To understand what this means, let's construct a dummy submission:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport random\n\nlabels = train_df[\"score\"]\nfake_preds = train_df[\"score\"].apply(lambda x: random.randint(1, 6))\ncmat = confusion_matrix(labels, fake_preds)\nsns.heatmap(cmat, annot = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:45.338236Z","iopub.execute_input":"2024-05-01T20:58:45.338533Z","iopub.status.idle":"2024-05-01T20:58:45.851036Z","shell.execute_reply.started":"2024-05-01T20:58:45.338507Z","shell.execute_reply":"2024-05-01T20:58:45.850153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nweight_matrix = np.zeros((6, 6))\nfor i in range(6):\n    for j in range(6):\n        weight_matrix[i, j] = (i-j)**2 / 25\nplt.suptitle(\"Weight penalty (e.g larger mispredictions are penalized more heavily)\")\nsns.heatmap(weight_matrix, annot = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:45.852324Z","iopub.execute_input":"2024-05-01T20:58:45.852678Z","iopub.status.idle":"2024-05-01T20:58:46.276987Z","shell.execute_reply.started":"2024-05-01T20:58:45.852647Z","shell.execute_reply":"2024-05-01T20:58:46.276038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've since created the confusion matrix $O$ and the weight matrix $W$. We then calculate the matrix of expected outcomes $E$ as the outer product of the value counts of the labels and predictions (sorry, I don't have a good intuition for what this operation does).\n\nWe then use the equation provided in *data* to calculate the quadratic weighted Kappa:\n\n$$ \\kappa = 1 - \\frac{\\sum_{i,j}W_{i, j} O_{i,j}}{\\sum_{i,j} W_{i,j} E_{i,j}}$$","metadata":{}},{"cell_type":"code","source":"label_count_vector, pred_count_vector = np.zeros(6), np.zeros(6)\nfor i in labels.value_counts().index:\n    label_count_vector[i-1] = labels.value_counts()[i]\nfor i in fake_preds.value_counts().index:\n    pred_count_vector[i-1] = fake_preds.value_counts()[i]\nexpected_val = np.outer(label_count_vector, pred_count_vector).astype(np.float32)\nexpected_val /= expected_val.sum()\ncmat = cmat.astype(np.float32)\ncmat /= cmat.sum()\n\nnumerator, denominator = 0, 0\nfor i in range(6):\n    for j in range(6):\n        numerator += weight_matrix[i, j] * cmat[i, j]\n        denominator += weight_matrix[i, j] * expected_val[i, j]\nweighted_kappa = (1 - (numerator/denominator))\nweighted_kappa","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:46.278234Z","iopub.execute_input":"2024-05-01T20:58:46.279244Z","iopub.status.idle":"2024-05-01T20:58:46.302602Z","shell.execute_reply.started":"2024-05-01T20:58:46.279208Z","shell.execute_reply":"2024-05-01T20:58:46.301571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wrapping it up in a function:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport random\nimport matplotlib.pyplot as plt\nimport torch as t\n\ndef quadratic_weighted_kappa(labels, pred):\n    cmat = confusion_matrix(y_true = labels, y_pred = pred, labels = range(6))\n    labels = pd.Series(labels, dtype = int); pred = pd.Series(pred, dtype = int)\n    weight_matrix = np.zeros((6, 6))\n    for i in range(6):\n        for j in range(6):\n            weight_matrix[i, j] = (i-j)**2 / 25\n            \n    label_count_vector, pred_count_vector = np.zeros(6), np.zeros(6)\n    for i in labels.value_counts().index:\n        label_count_vector[i-1] = labels.value_counts()[i]\n    for i in pred.value_counts().index:\n        pred_count_vector[i-1] = pred.value_counts()[i]\n    expected_val = np.outer(label_count_vector, pred_count_vector).astype(np.float32)\n    expected_val /= expected_val.sum()\n    cmat = cmat.astype(np.float32)\n    cmat /= cmat.sum()\n\n    numerator, denominator = 0, 0\n    for i in range(6):\n        for j in range(6):\n            numerator += weight_matrix[i, j] * cmat[i, j]\n            denominator += weight_matrix[i, j] * expected_val[i, j]\n    weighted_kappa = (1 - (numerator/denominator))\n    return weighted_kappa\n\ndef tensor_QWK(y_true: t.Tensor, y_pred: t.Tensor) -> t.Tensor:\n    assert y_true.shape == y_pred.shape\n    y_true = y_true.cpu().detach().numpy()\n    y_pred = y_pred.cpu().detach().numpy()\n    return quadratic_weighted_kappa(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:46.306282Z","iopub.execute_input":"2024-05-01T20:58:46.306647Z","iopub.status.idle":"2024-05-01T20:58:46.318785Z","shell.execute_reply.started":"2024-05-01T20:58:46.30661Z","shell.execute_reply":"2024-05-01T20:58:46.317747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's benchmark!","metadata":{}},{"cell_type":"code","source":"labels = train_df[\"score\"]\nrandom_preds = train_df[\"score\"].apply(lambda x: random.randint(1, 6))\nall_3s = train_df[\"score\"].apply(lambda x: 3)\nperfect_preds = train_df[\"score\"]\nprint(\"Random preds:\", quadratic_weighted_kappa(labels, random_preds))\nprint(\"All 3s:\", quadratic_weighted_kappa(labels, all_3s))\nprint(\"Perfect preds:\", quadratic_weighted_kappa(labels, perfect_preds))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:46.32026Z","iopub.execute_input":"2024-05-01T20:58:46.32141Z","iopub.status.idle":"2024-05-01T20:58:46.4025Z","shell.execute_reply.started":"2024-05-01T20:58:46.321372Z","shell.execute_reply":"2024-05-01T20:58:46.401426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like being off by a large margin is heavily penalized. Let's try to make sure that doesn't happen.","metadata":{}},{"cell_type":"markdown","source":"### Flan-T5 series:","metadata":{}},{"cell_type":"code","source":"import torch as t\nimport torch.nn.functional as F\nimport torch.nn as nn\n\ndevice = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_df, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:46.403914Z","iopub.execute_input":"2024-05-01T20:58:46.404756Z","iopub.status.idle":"2024-05-01T20:58:46.453972Z","shell.execute_reply.started":"2024-05-01T20:58:46.404718Z","shell.execute_reply":"2024-05-01T20:58:46.453005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport transformers\n\n# MODEL_NAME = \"pszemraj/flan-t5-large-grammar-synthesis\"\nMODEL_NAME = \"google/flan-t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, device_map=device, torch_dtype=t.float32)\ntokenizer.model_max_length=4096","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:46.455219Z","iopub.execute_input":"2024-05-01T20:58:46.455957Z","iopub.status.idle":"2024-05-01T20:58:59.111589Z","shell.execute_reply.started":"2024-05-01T20:58:46.455917Z","shell.execute_reply":"2024-05-01T20:58:59.110663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"After reading the following essay and completing the analytical rating form, assign a holistic score based on the rubric\nbelow. For the following evaluations you will need to use a grading scale between 1 (minimum) and 50\n(maximum). The distance between each grade should be considered equal. Please provide a single numerical score instead of \na qualitative assessment. Note: 1 should be poor elementary level writing, 50 should be university level writing.\nThe essay is as follows:\\n\"\"\"\n\ndecoder_prefix = \"The essay is a \"\ndecoder_input_ids = tokenizer(decoder_prefix, return_tensors = \"pt\").input_ids.to(device)\n\ndef get_scoring_logits(essay: str):\n    input_text = prompt + essay\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n    return model(input_ids = input_ids, decoder_input_ids = decoder_input_ids).logits[:, -1, :] # last token \n\ntokenizer_elements = []\nfor element in range(1, 7):\n    tokenizer_elements.append(tokenizer(str(element), return_tensors = \"pt\").input_ids[0][0].item())\n    \nsample_essay = train_df[\"full_text\"].iloc[0]\nlogits = get_scoring_logits(sample_essay)\nlogits.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:59.113012Z","iopub.execute_input":"2024-05-01T20:58:59.113277Z","iopub.status.idle":"2024-05-01T20:58:59.886074Z","shell.execute_reply.started":"2024-05-01T20:58:59.113253Z","shell.execute_reply":"2024-05-01T20:58:59.885147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the distribution of essay lengths (in tokens) such that we can define a good cutoff point to pad/truncate input sequences to.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm, trange\n\nlength_distribution = [len(tokenizer(essay).input_ids) for essay in tqdm(train_df[\"full_text\"])]\nimport seaborn as sns\nsns.histplot(length_distribution)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:58:59.887351Z","iopub.execute_input":"2024-05-01T20:58:59.887661Z","iopub.status.idle":"2024-05-01T20:59:40.251825Z","shell.execute_reply.started":"2024-05-01T20:58:59.887634Z","shell.execute_reply":"2024-05-01T20:59:40.250904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eyeballing the distribution above, 1250 looks like a good cutoff point; it'll include the majority of our essays while not overly limiting our batch size. Note: Other notebooks have used different token cutoff lengths. While different models use different tokenization schemes, tuning the essay cutoff length may be good to explore.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm, trange\nclass EssayDataset(Dataset):\n    def __init__(self, df, train = True):\n        self.df = df\n        self.train = train\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        essay = self.df[\"full_text\"].iloc[idx]\n        input_text = prompt + essay\n        input_ids = tokenizer(input_text, \n            return_tensors=\"pt\", \n            padding = \"max_length\", \n            max_length = 1250, \n            truncation = True).input_ids[0].to(device)\n        if self.train: \n            return input_ids, self.df[\"score\"].iloc[idx] - 1 # 0-indexed\n        return input_ids\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.linear = nn.Linear(model.config.vocab_size, 6).to(device).to(t.float32)\n    def forward(self, input_ids):\n        x = self.model(input_ids = input_ids, decoder_input_ids = decoder_input_ids.repeat(input_ids.shape[0], 1),\n                          output_hidden_states = False, output_attentions = False).logits[:, -1]#, tokenizer_elements]\n        return self.linear(x)\n    def get_score(self, input_ids):\n        logits = self.forward(input_ids)\n        return t.argmax(logits).item()\n\nclf = TransformerClassifier(model)\nfor param in model.parameters():\n    param.requires_grad = False\n# for param in model.get_submodule(\"lm_head\").parameters():\n#     param.requires_grad = True\n\nclass EMAMetric:\n    def __init__(self, value = None, gamma = 0.95, sigfigs = 3):\n        self.value = value\n        self.gamma = gamma\n        self.sigfigs = sigfigs\n    def set(self, update):\n        if self.value is None: \n            self.value = update\n        else: \n            self.value = self.gamma * self.value + (1 - self.gamma) * update\n        return self.value\n    def __repr__(self):\n        return str(round(self.value, self.sigfigs))\n\nimport collections\nclass History:\n    def __init__(self):\n        self.train_metrics = collections.defaultdict(list)\n        self.val_metrics = collections.defaultdict(list)\n    def append(self, **kwargs):\n        assert type(kwargs) == dict\n        assert all([isinstance(value, EMAMetric) for value in kwargs.values()]), \"All values must be EMAMetric objects\"\n        for key, value in kwargs.items():\n            if \"val\" in key:\n                self.val_metrics[key].append(value.value)\n            else:\n                self.train_metrics[key].append(value.value)\n    def plot(self):\n        fig, axes = plt.subplots(2, figsize = (10, 10))\n        sns.set()\n        for key, value in self.train_metrics.items():\n            sns.lineplot(value, label = key, ax = axes[0])\n        for key, value in self.val_metrics.items():\n            sns.lineplot(value, label = key, ax = axes[1])\n        plt.legend()\n        plt.show()\n\ndef train(train_df, val_df, optimizer = t.optim.AdamW(clf.parameters(), lr = 1e-5), BATCH_SIZE = 32, EPOCHS = 1):\n    train_metrics = {\n        \"loss\": EMAMetric(), \"acc\": EMAMetric(), \"QWK\": EMAMetric(),\n    }\n    val_metrics = {\n        \"val_loss\": EMAMetric(), \"val_acc\": EMAMetric(), \"val_QWK\": EMAMetric()\n    }\n    history = History()\n\n    train_dataset = EssayDataset(train_df)\n    val_dataset = EssayDataset(val_df)\n    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n    CELoss = nn.CrossEntropyLoss()\n    # scaler = t.cuda.amp.GradScaler()\n\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch + 1}\")\n        with tqdm(train_loader) as pbar:\n            for input_ids, target in pbar:\n                target = target.to(device)\n                # all_logits = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids.repeat(BATCH_SIZE, 1),\n                #     output_hidden_states = False, output_attentions = False)[\"logits\"]\n                # num_logits = all_logits[:, -1,  tokenizer_elements]\n                num_logits = clf(input_ids)\n                optimizer.zero_grad()\n                loss = CELoss(num_logits, target)\n                loss.backward()\n                optimizer.step()\n                acc = (t.argmax(num_logits, dim = -1) == target).to(t.float32).mean()\n                q_weighted_kappa = tensor_QWK(target, t.argmax(num_logits, dim = -1))\n                train_metrics[\"loss\"].set(loss.item())\n                train_metrics[\"acc\"].set(acc.item())\n                train_metrics[\"QWK\"].set(q_weighted_kappa)\n                history.append(**train_metrics)\n                desc = f\"Loss: {train_metrics['loss']}, Accuracy: {train_metrics['acc']}, QWK: {train_metrics['QWK']}\"\n                pbar.set_description(desc)\n        with t.no_grad():\n            with tqdm(val_loader) as pbar:\n                for input_ids, target in pbar:\n                    target = target.to(device)\n                    num_logits = clf(input_ids)\n                    loss = CELoss(num_logits, target)\n                    acc = (t.argmax(num_logits, dim = -1) == target).to(t.float32).mean()\n                    q_weighted_kappa = tensor_QWK(target, t.argmax(num_logits, dim = -1))\n                    val_metrics[\"val_loss\"].set(loss.item())\n                    val_metrics[\"val_acc\"].set(acc.item())\n                    val_metrics[\"val_QWK\"].set(q_weighted_kappa)\n                    history.append(**val_metrics)\n                    desc = f\"Val Loss: {val_metrics['val_loss']}, Val Accuracy: {val_metrics['val_acc']}, Val QWK: {val_metrics['val_QWK']}\"\n                    pbar.set_description(desc)\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:10:06.153674Z","iopub.execute_input":"2024-05-01T21:10:06.15432Z","iopub.status.idle":"2024-05-01T21:10:06.190467Z","shell.execute_reply.started":"2024-05-01T21:10:06.154285Z","shell.execute_reply":"2024-05-01T21:10:06.189536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the classifier head -- all other weights frozen.","metadata":{}},{"cell_type":"code","source":"history = train(train_df, val_df, EPOCHS = 5, BATCH_SIZE = 32)\nhistory.plot()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:10:07.349765Z","iopub.execute_input":"2024-05-01T21:10:07.350473Z","iopub.status.idle":"2024-05-01T21:10:17.394321Z","shell.execute_reply.started":"2024-05-01T21:10:07.350434Z","shell.execute_reply":"2024-05-01T21:10:17.39292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finetuning the whole model, including the transformer weights.","metadata":{}},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True\n\nhistory = train(train_df, val_df, EPOCHS = 5, BATCH_SIZE = 8, \n                optimizer = t.optim.AdamW(clf.parameters(), lr = 5e-7))\nhistory.plot()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:11:11.824914Z","iopub.execute_input":"2024-05-01T21:11:11.825774Z","iopub.status.idle":"2024-05-01T21:11:29.079872Z","shell.execute_reply.started":"2024-05-01T21:11:11.825729Z","shell.execute_reply":"2024-05-01T21:11:29.078622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.save(clf, \"transformer_classifier_May_1.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:48:30.250335Z","iopub.status.idle":"2024-05-01T20:48:30.250743Z","shell.execute_reply.started":"2024-05-01T20:48:30.250573Z","shell.execute_reply":"2024-05-01T20:48:30.250589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a submission file","metadata":{}},{"cell_type":"code","source":"test_ds = EssayDataset(test_df, train = False)\nscores = []\nfor input_ids in test_ds:\n    scores.append(clf.get_score(input_ids.unsqueeze(0)) + 1)\ntest_df[\"score\"] = scores\ntest_df[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:48:30.252175Z","iopub.status.idle":"2024-05-01T20:48:30.252581Z","shell.execute_reply.started":"2024-05-01T20:48:30.252377Z","shell.execute_reply":"2024-05-01T20:48:30.252392Z"},"trusted":true},"execution_count":null,"outputs":[]}]}