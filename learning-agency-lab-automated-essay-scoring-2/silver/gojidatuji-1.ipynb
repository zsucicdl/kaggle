{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T13:41:52.174986Z","iopub.execute_input":"2024-05-02T13:41:52.175384Z","iopub.status.idle":"2024-05-02T13:41:52.181033Z","shell.execute_reply.started":"2024-05-02T13:41:52.175353Z","shell.execute_reply":"2024-05-02T13:41:52.179614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv', index_col='essay_id')\ntest = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv', index_col='essay_id')\n\nsubmission = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T13:41:52.182904Z","iopub.execute_input":"2024-05-02T13:41:52.183777Z","iopub.status.idle":"2024-05-02T13:41:52.62932Z","shell.execute_reply.started":"2024-05-02T13:41:52.183738Z","shell.execute_reply":"2024-05-02T13:41:52.627959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values('score')\n\n# 並べ替え","metadata":{"execution":{"iopub.status.busy":"2024-05-02T13:41:52.630699Z","iopub.execute_input":"2024-05-02T13:41:52.631031Z","iopub.status.idle":"2024-05-02T13:41:52.640045Z","shell.execute_reply.started":"2024-05-02T13:41:52.631001Z","shell.execute_reply":"2024-05-02T13:41:52.638911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-05-02T13:41:52.64204Z","iopub.execute_input":"2024-05-02T13:41:52.642825Z","iopub.status.idle":"2024-05-02T13:41:52.653613Z","shell.execute_reply.started":"2024-05-02T13:41:52.642795Z","shell.execute_reply":"2024-05-02T13:41:52.652485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# textBlobを用いて、スペルミスの数を数えます\n\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\n#　nltk.download('wordnet')\ntrain['spelling_errors'] = train['full_text'].apply(lambda x: len(TextBlob(x).correct().words) - len(TextBlob(x).words))\nprint(train.head())\n\nprint(train['spelling_errors'].groupby(train['score']).mean())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T13:41:52.654761Z","iopub.execute_input":"2024-05-02T13:41:52.65508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# scoreで正規表現からスペルミスの数を比較してみる\ntrain['spelling_errors'] = train.full_text.str.count(r'\\b\\w{20,}\\b')\ntrain['spelling_errors'].groupby(train['score']).mean()\n\n\n'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['spelling_errors'] = test['full_text'].apply(lambda x: len(TextBlob(x).correct().words) - len(TextBlob(x).words))\n\ntest\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 文章ごとの語の数 \ntrain['word_count'] = train.full_text.str.count(r'\\w+')\ntest['word_count'] = test.full_text.str.count(r'\\w+')\n\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 語の数を比較してみる\n\ntrain['word_count'].groupby(train['score']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textstat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport textstat\n\n# Flesch-Kincaid Grade Levelを特徴量として追加する\n\ntrain['flesch_kincaid_grade'] = train.full_text.apply(textstat.flesch_kincaid_grade)\ntest['flesch_kincaid_grade'] = test.full_text.apply(textstat.flesch_kincaid_grade)\ntrain\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['flesch_kincaid_grade'].groupby(train['score']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom textblob import TextBlob\n\n# 感情分析のスコアを特徴量として追加する\n\ntrain['sentiment'] = train.full_text.apply(lambda x: TextBlob(x).sentiment.polarity)\ntest['sentiment'] = test.full_text.apply(lambda x: TextBlob(x).sentiment.polarity)\n\nprint(train['sentiment'].groupby(train['score']).mean())\ntrain\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n!pip install vaderSentiment\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# 感情分析のスコアを特徴量として追加する\n\nanalyzer = SentimentIntensityAnalyzer()\n\ntrain['vader_sentiment'] = train.full_text.apply(lambda x: analyzer.polarity_scores(x)['compound'])\n\ntest['vader_sentiment'] = test.full_text.apply(lambda x: analyzer.polarity_scores(x)['compound'])\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#　スペルミスの割合\ntrain['spelling_errors_rate'] = train['spelling_errors'] / train['word_count']\ntest['spelling_errors_rate'] = test['spelling_errors'] / test['word_count']\n\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# スペルミスの割合とスコアの関係を見ます\n\ntrain[['spelling_errors_rate', 'score']].groupby(train['score']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 誤字脱字のある単語を除いてみます\n\nimport nltk\nnltk.download('words')\n\nfrom nltk.corpus import words\n\ntrain['corrected_text'] = train['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word in words.words()]))\n\ntest['corrected_text'] = test['full_text'].apply(lambda x: ' '.join([word for word in x.split() if word in words.words()]))\n\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nltkヲ使って文章をトークン化する\nimport nltk\n\nnltk.download('punkt')\n\nfrom nltk.tokenize import word_tokenize\n\ndata['tokens'] = data['corrected_text'].apply(word_tokenize)\ntest['tokens'] = test['corrected_text'].apply(word_tokenize)\ndata\n\n# ストップワードを取り除く\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ndata['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\ntest['tokens'] = test['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# スペルミスのない正しい文章をストップワードを除いてみます\n\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\ndata['corrected_text'] = data['corrected_text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n\ntest['corrected_text'] = test['corrected_text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\n# 文章をトークン化する\nimport nltk\n\nnltk.download('punkt')\n\nfrom nltk.tokenize import word_tokenize\n\ntrain['tokens'] = train['full_text'].apply(word_tokenize)\ntest['tokens'] = test['full_text'].apply(word_tokenize)\n\n# ストップワードを取り除く\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\ntrain['tokens'] = train['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\ntest['tokens'] = test['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ストップワードを取り除いた後の語の数を特徴量として追加する\n\ntrain['word_count_no_stopwords'] = train['tokens'].apply(len)\ntest['word_count_no_stopwords'] = test['tokens'].apply(len)\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ストップワードを取り除いた後の語の数とスコアの関係を見ます\n\ntrain[['word_count_no_stopwords', 'score']].groupby(train['score']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['word_count_no_stopwords'] = test['tokens'].apply(len)\ntest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 語彙という意味で、tokenを集合にして、数値化したものを特徴量にします\n\ntrain['vocabulary'] = train['tokens'].apply(set)\n\ntest['vocabulary'] = test['tokens'].apply(set)\n\ntrain\n\n\n# 語彙の数\n\ntrain['vocabulary_count'] = train['vocabulary'].apply(len)\n\ntrain\n\n\ntest['vocabulary_count'] = test['vocabulary'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scoreで語彙の数を比較してみる\n\ntrain['vocabulary_count'].groupby(train['score']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(train['score']).astype(int)\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['full_text', 'tokens', 'vocabulary', 'score'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['full_text', 'tokens', 'vocabulary'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train,y, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lightgbmを使ってモデルを作成します\n\nimport lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\n\nlgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\nparams = {\n    'objective': 'multiclass',\n    'num_class': 6,\n    'metric': 'multi_logloss',\n    'verbosity': -1\n}\n\nmodel_0 = lgb.train(params, lgb_train, valid_sets=lgb_valid, num_boost_round=1000, early_stopping_rounds=10)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# テストデータを予測します\n\ny_pred = model_0.predict(test)\n\ny_pred\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 提出用のデータフレームを作成します\n\nsubmission['score'] = le.inverse_transform(np.argmax(y_pred, axis=1)).astype(int)\n\nsubmission\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 提出用のデータフレームを保存します\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1 = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_1.fit(train, y)\n\nmodel_2 = LineerRegression()\nmodel_2.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model_2.predict(X_valid)\ny_pred = le.inverse_transform(y_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cohen kappa scoreを用いて評価します\n\nfrom sklearn.metrics import cohen_kappa_score\n\ncohen_kappa_score(y_valid, y_pred, weights='quadratic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['full_text', 'tokens', 'vocabulary'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model_2.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"pred = le.inverse_transform(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['score'] = pred\n\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}