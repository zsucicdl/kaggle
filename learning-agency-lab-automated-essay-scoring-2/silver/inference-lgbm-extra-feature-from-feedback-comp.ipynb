{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 1164617,
     "sourceType": "datasetVersion",
     "datasetId": 659784
    },
    {
     "sourceId": 3712905,
     "sourceType": "datasetVersion",
     "datasetId": 2220792
    },
    {
     "sourceId": 4610416,
     "sourceType": "datasetVersion",
     "datasetId": 2680195
    },
    {
     "sourceId": 8522353,
     "sourceType": "datasetVersion",
     "datasetId": 5088694
    },
    {
     "sourceId": 177531229,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Feature from Feedback Competion",
   "metadata": {
    "_uuid": "571ffea4-062a-4c64-9f3e-3dd517aae5c0",
    "_cell_guid": "ee5d9776-3106-4e55-8f9c-e24130e03068",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "import sys\nsys.path.append(\"../input/omegaconf\")",
   "metadata": {
    "_uuid": "e5e63cf8-3b74-45ba-be75-dce7114130c9",
    "_cell_guid": "024d62a8-2576-4193-9b3d-c5457035b04e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:52:59.880735Z",
     "iopub.execute_input": "2024-05-28T15:52:59.881191Z",
     "iopub.status.idle": "2024-05-28T15:52:59.885947Z",
     "shell.execute_reply.started": "2024-05-28T15:52:59.88115Z",
     "shell.execute_reply": "2024-05-28T15:52:59.885069Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# basics\nimport os\nimport sys\nimport json\nfrom copy import deepcopy\nfrom itertools import chain\nfrom omegaconf import OmegaConf\n\n# Processing\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\n\n# ipython\nfrom IPython.display import display\nfrom IPython.core.debugger import set_trace",
   "metadata": {
    "_uuid": "96ddb08e-cecb-43dd-846e-4d6360cc61a8",
    "_cell_guid": "f3f44b01-5c32-408c-83b7-4a878ca736c6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:52:59.88784Z",
     "iopub.execute_input": "2024-05-28T15:52:59.888143Z",
     "iopub.status.idle": "2024-05-28T15:52:59.896949Z",
     "shell.execute_reply.started": "2024-05-28T15:52:59.888113Z",
     "shell.execute_reply": "2024-05-28T15:52:59.896055Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os \nos.makedirs(\"./datasets\", exist_ok=True)\nos.makedirs(\"./predictions\", exist_ok=True)",
   "metadata": {
    "_uuid": "86fbd70e-f9d6-4167-b05f-54f42e1dbe17",
    "_cell_guid": "f59b6e06-35e7-4c06-84ad-bb4fa2e178b2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:52:59.89805Z",
     "iopub.execute_input": "2024-05-28T15:52:59.898362Z",
     "iopub.status.idle": "2024-05-28T15:52:59.910506Z",
     "shell.execute_reply.started": "2024-05-28T15:52:59.89834Z",
     "shell.execute_reply": "2024-05-28T15:52:59.909747Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile eff_inference.yaml\n\nmodel:\n    backbone_path: /kaggle/input/debertav3xsmall/deberta-v3-xsmall\n    feature_extractor:\n        num_layers: 4\n    max_length: 448\n    target_names:\n        - cohesion\n        - syntax\n        - vocabulary\n        - phraseology\n        - grammar\n        - conventions\n    len_tokenizer: ???\n    loss_fn: mse\n\ninfer_params:\n    input_path: ../input/learning-agency-lab-automated-essay-scoring-2/test.csv\n    infer_bs: 4\n    agg_fn: mean",
   "metadata": {
    "_uuid": "2a98df3f-0aaa-4e76-af36-e717ab60fc80",
    "_cell_guid": "ca8c262d-67ca-42b5-b3f1-5b82d54014df",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:52:59.911543Z",
     "iopub.execute_input": "2024-05-28T15:52:59.911888Z",
     "iopub.status.idle": "2024-05-28T15:52:59.922742Z",
     "shell.execute_reply.started": "2024-05-28T15:52:59.911859Z",
     "shell.execute_reply": "2024-05-28T15:52:59.921883Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n!python /kaggle/input/feedback-utility/create_datasets_main.py \\\n--config_path eff_inference.yaml \\\n--save_path ./datasets/task_dataset_{rank} \\\n--rank=0",
   "metadata": {
    "_uuid": "b017d171-b94d-4099-ad58-c64def82be22",
    "_cell_guid": "3542a3df-9591-4320-9c6a-ff8c99856004",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:52:59.925165Z",
     "iopub.execute_input": "2024-05-28T15:52:59.925873Z",
     "iopub.status.idle": "2024-05-28T15:53:06.670906Z",
     "shell.execute_reply.started": "2024-05-28T15:52:59.92584Z",
     "shell.execute_reply": "2024-05-28T15:53:06.669792Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\n!python /kaggle/input/feedback-utility/create_datasets_main.py \\\n--config_path eff_inference.yaml \\\n--save_path ./datasets/task_dataset_{rank} \\\n--rank=1",
   "metadata": {
    "_uuid": "eca5e91b-7407-42ab-98f1-d31c0791d975",
    "_cell_guid": "973a7eab-e8fd-41c6-9646-f685fe977881",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:53:06.672274Z",
     "iopub.execute_input": "2024-05-28T15:53:06.672594Z",
     "iopub.status.idle": "2024-05-28T15:53:13.42547Z",
     "shell.execute_reply.started": "2024-05-28T15:53:06.672566Z",
     "shell.execute_reply": "2024-05-28T15:53:13.424436Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python /kaggle/input/feedback-utility/create_datasets_main.py \\\n--config_path eff_inference.yaml \\\n--save_path ./datasets/task_dataset_{rank} \\\n--rank=2",
   "metadata": {
    "_uuid": "1bf89eaf-299a-46f2-ae5f-ee6e6939ecc0",
    "_cell_guid": "4832997f-e7c3-4d89-9821-37773821a5d3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:54:34.405497Z",
     "iopub.execute_input": "2024-05-28T15:54:34.406151Z",
     "iopub.status.idle": "2024-05-28T15:54:41.110019Z",
     "shell.execute_reply.started": "2024-05-28T15:54:34.406116Z",
     "shell.execute_reply": "2024-05-28T15:54:41.109033Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!python /kaggle/input/feedback-utility/create_datasets_main.py \\\n--config_path eff_inference.yaml \\\n--save_path ./datasets/task_dataset_{rank} \\\n--rank=3",
   "metadata": {
    "_uuid": "313b33bd-eccf-49fe-85d6-5d0c353def04",
    "_cell_guid": "1310981a-61a7-4ded-8da7-e4e832a94b8a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:54:41.111934Z",
     "iopub.execute_input": "2024-05-28T15:54:41.112246Z",
     "iopub.status.idle": "2024-05-28T15:54:47.960988Z",
     "shell.execute_reply.started": "2024-05-28T15:54:41.112217Z",
     "shell.execute_reply": "2024-05-28T15:54:47.959874Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!sh /kaggle/input/feedback-utility/run.sh",
   "metadata": {
    "_uuid": "5a27787c-559a-46e5-a6d6-8411fd420a22",
    "_cell_guid": "3708eded-25e7-4c6f-8868-ae2f2bca66c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:54:47.962564Z",
     "iopub.execute_input": "2024-05-28T15:54:47.96293Z",
     "iopub.status.idle": "2024-05-28T15:55:24.656712Z",
     "shell.execute_reply.started": "2024-05-28T15:54:47.962901Z",
     "shell.execute_reply": "2024-05-28T15:55:24.655792Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "eff_df_0 = pd.read_csv(\"./predictions/eff_preds_0.csv\")\n# eff_df_0 = eff_df_0.reset_index(drop=True)\n\neff_df_1 = pd.read_csv(\"./predictions/eff_preds_1.csv\")\neff_df_2 = pd.read_csv(\"./predictions/eff_preds_2.csv\")\neff_df_3 = pd.read_csv(\"./predictions/eff_preds_3.csv\")\n# eff_df_1 = eff_df_1.reset_index(drop=True)\n\neff_df = pd.concat([eff_df_0, eff_df_1, eff_df_2, eff_df_3], axis=0)\neff_df = eff_df.sort_values(by=\"essay_id\")\neff_df = eff_df.reset_index(drop=True)",
   "metadata": {
    "_uuid": "faa0de30-92a7-493e-9f59-9549bd37f9a0",
    "_cell_guid": "f96f0c6c-a50a-419e-9396-c52b77e12ade",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:55:24.659063Z",
     "iopub.execute_input": "2024-05-28T15:55:24.659368Z",
     "iopub.status.idle": "2024-05-28T15:55:24.680335Z",
     "shell.execute_reply.started": "2024-05-28T15:55:24.65934Z",
     "shell.execute_reply": "2024-05-28T15:55:24.67968Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MODEL_WEIGHTS = [1.0] #[0.34, 0.33, 0.33]\nprint(f\"sum of weights {np.sum(MODEL_WEIGHTS)}\")\n\nsubmission_df = pd.DataFrame()\n\npred_dfs  = [  \n    eff_df,\n]\n\nTARGET_COLS = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n\nsubmission_df[\"essay_id\"] =  pred_dfs[0][\"essay_id\"].values\nfor model_idx, model_preds in enumerate(pred_dfs):\n    if model_idx == 0:\n        for target in TARGET_COLS:\n            submission_df[target]  =  MODEL_WEIGHTS[model_idx] * model_preds[target]\n    else:\n        for target in TARGET_COLS:\n            submission_df[target]  +=  MODEL_WEIGHTS[model_idx] * model_preds[target] \n\neff_df\neff_df = eff_df.drop_duplicates(subset=['essay_id'], keep='first')\neff_df",
   "metadata": {
    "_uuid": "c67c8189-a695-46f7-8f20-f7e3245ef409",
    "_cell_guid": "74c212bd-c8ca-4010-a4b3-2ec71eda97a1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:55:24.681351Z",
     "iopub.execute_input": "2024-05-28T15:55:24.681613Z",
     "iopub.status.idle": "2024-05-28T15:55:24.71038Z",
     "shell.execute_reply.started": "2024-05-28T15:55:24.68159Z",
     "shell.execute_reply": "2024-05-28T15:55:24.709485Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## feature engineering for text",
   "metadata": {
    "_uuid": "fb80be87-aba7-4d22-ae6e-4ba77bb87673",
    "_cell_guid": "0b7145d8-36d4-4431-a798-b0b2c588020c",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "import re\nimport polars as pl\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn.ensemble import VotingClassifier,VotingRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom tqdm.auto import tqdm,trange\nfrom lightgbm import log_evaluation, early_stopping\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nimport pickle",
   "metadata": {
    "_uuid": "7942498e-6c00-4e7c-b605-2ee9989f5a39",
    "_cell_guid": "46c4ae9c-9b78-48c6-9461-db4c0b941888",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:25:09.364855Z",
     "iopub.execute_input": "2024-05-28T16:25:09.365207Z",
     "iopub.status.idle": "2024-05-28T16:25:13.829878Z",
     "shell.execute_reply.started": "2024-05-28T16:25:09.36518Z",
     "shell.execute_reply": "2024-05-28T16:25:13.829095Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import warnings\nwarnings.filterwarnings('ignore')",
   "metadata": {
    "_uuid": "1a36c51e-46d4-41ed-af49-7f5c9c3d8651",
    "_cell_guid": "648f3797-c5f2-4a5d-ba78-9d57897627ce",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:25:13.831765Z",
     "iopub.execute_input": "2024-05-28T16:25:13.832214Z",
     "iopub.status.idle": "2024-05-28T16:25:13.836644Z",
     "shell.execute_reply.started": "2024-05-28T16:25:13.832181Z",
     "shell.execute_reply": "2024-05-28T16:25:13.83566Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class FeatureEngineering():\n    def __init__(self):\n        self.columns = [\n            (pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))\n        ]\n        self.train_dataset = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv').with_columns(self.columns)\n        self.test_dataset = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv').with_columns(self.columns)\n        # feature_eng\n        self.sentence_fea = ['sentence_len','sentence_word_cnt']\n        # feature_eng\n        self.paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n        self.vectorizer = TfidfVectorizer(tokenizer=lambda x: x,\n                                          preprocessor=lambda x: x,\n                                          token_pattern=None,\n                                          strip_accents='unicode',\n                                          analyzer = 'word',\n                                          ngram_range=(2,3),\n                                          min_df=0.05,\n                                          max_df=0.9,\n                                          sublinear_tf=True  \n        )\n    def removeHTML(self,x):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',x)\n    def dataPreprocessing(self,x):\n        x = x.lower()             # covert all letter to lower form\n        x = self.removeHTML(x)\n        x = re.sub(\"@\\w+\", '',x)\n        x = re.sub(\"'\\d+\", '',x)\n        x = re.sub(\"\\d+\", '',x)\n        x = re.sub(\"http\\w+\", '',x)\n        x = re.sub(r\"\\s+\", \" \",x) # replace any sequence of whitespace characters with a sigle whitespace\n        x = re.sub(r\"\\.+\", \".\",x) # replace any sequence of periods with a sigle periods\n        x = re.sub(r\"\\,+\", \",\",x) # replace any sequence of commas with a sigle comma\n        x = x.strip()\n        return x \n    def Paragraph_Preprocess(self,tmp):\n        tmp = tmp.explode('paragraph')\n        # preprocess\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(self.dataPreprocessing))\n        # paragraph_len\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x:len(x)).alias(\"paragraph_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('paragraph_len')>=25)\n        # paragraph_sentence_count/paragraph_word_count\n        tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split(\".\"))).alias(\"paragraph_sentence_cnt\"),\n                               pl.col('paragraph').map_elements(lambda x: len(x.split(\" \"))).alias(\"paragraph_word_cnt\")\n                              )\n        return tmp\n    def Paragraph_Eng(self,train_tmp):\n        aggs = [\n            # paragraph_len_cnt\n            *[pl.col('paragraph').filter(pl.col('paragraph_len')>=i)\n            .count().alias(f'paragraph_{i}_cnt') for i in [25,100,200,300,400,500,600,700]],\n            # other\n            *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in self.paragraph_fea],\n            *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in self.paragraph_fea],\n            *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in self.paragraph_fea],\n            *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in self.paragraph_fea],\n            *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in self.paragraph_fea],\n        ]\n        df = train_tmp.group_by([\"essay_id\"], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Paragraph_Eng +\",len(df.columns),\"features\")\n        return df\n    def Sentence_Preprocess(self,tmp):\n        tmp = tmp.with_columns(pl.col('full_text').map_elements(self.dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n        tmp = tmp.explode('sentence')\n        # sentence_len\n        tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('sentence_len')>=15)\n        # sentence_word_cnt\n        tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n\n        return tmp\n    def Sentence_Eng(self,train_tmp):\n        aggs = [\n            # sentence_cnt\n            *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n            # other\n            *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in self.sentence_fea],\n            *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in self.sentence_fea],\n            *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in self.sentence_fea],\n            *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in self.sentence_fea],\n            *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in self.sentence_fea],\n            ]\n        df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Sentence_Eng +\",len(df.columns),\"features\")\n        return df\n    # word feature\n    def Word_Preprocess(self,tmp):\n        tmp = tmp.with_columns(pl.col('full_text').map_elements(self.dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n        tmp = tmp.explode('word')\n        # word_len\n        tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n        # filter\n        tmp = tmp.filter(pl.col('word_len')!=0)\n\n        return tmp\n    # feature_eng\n    def Word_Eng(self,train_tmp):\n        aggs = [\n            # word_cnt\n            *[pl.col('word').filter(pl.col('word_len') >= i+1)\n              .count().alias(f\"word_{i+1}_cnt\") for i in range(15)], \n            # other\n            pl.col('word_len').max().alias(f\"word_len_max\"),\n            pl.col('word_len').mean().alias(f\"word_len_mean\"),\n            pl.col('word_len').std().alias(f\"word_len_std\"),\n            pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n            pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n            pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n        df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n        df = df.to_pandas()\n        print(\"done Word_Eng +\",len(df.columns),\"features\")\n        return df\n    def process(self):\n        tmp = self.Paragraph_Preprocess(self.train_dataset)\n        train_feats = self.Paragraph_Eng(tmp)\n        train_feats['score'] = self.train_dataset['score']\n        \n        tmp = self.Sentence_Preprocess(self.train_dataset)\n        train_feats = train_feats.merge(self.Sentence_Eng(tmp), on='essay_id', how='left')\n        \n        tmp = self.Word_Preprocess(self.train_dataset)\n        train_feats = train_feats.merge(self.Word_Eng(tmp), on='essay_id', how='left')\n        \n        train_tfid = self.vectorizer.fit_transform([i for i in self.train_dataset['full_text']])\n        dense_matrix = train_tfid.toarray()\n        df = pd.DataFrame(dense_matrix)\n        tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n        df.columns = tfid_columns\n#         print(df)\n#         print(\"----------------------------------------------------------\")\n        df['essay_id'] = train_feats['essay_id']\n        # merge\n        train_feats = train_feats.merge(df, on='essay_id', how='left')\n        print('feature_num: ',len(train_feats.columns)-2)\n        return train_feats\n    def process_test(self):\n        temp = self.Paragraph_Preprocess(self.test_dataset)\n        test_feats = self.Paragraph_Eng(temp)\n        \n        temp = self.Sentence_Preprocess(self.test_dataset)\n        test_feats = test_feats.merge(self.Sentence_Eng(temp), on='essay_id', how='left')\n        \n        temp = self.Word_Preprocess(self.test_dataset)\n        test_feats = test_feats.merge(self.Word_Eng(temp), on='essay_id', how='left')\n        \n        test_tfid = self.vectorizer.transform([i for i in self.test_dataset['full_text']])\n        dense_matrix = test_tfid.toarray()\n        df = pd.DataFrame(dense_matrix)\n        tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n        df.columns = tfid_columns\n#         print(df)\n        df['essay_id'] = test_feats['essay_id']\n        # merge\n        test_feats = test_feats.merge(df, on='essay_id', how='left')\n        print('feature_num: ',len(test_feats.columns)-2)\n        \n        return test_feats",
   "metadata": {
    "_uuid": "5e6f8a0e-ea6f-4029-b0bf-15fc4a684625",
    "_cell_guid": "cd051ade-0dc4-4be5-8d04-25bfcda25930",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:25:13.837866Z",
     "iopub.execute_input": "2024-05-28T16:25:13.838166Z",
     "iopub.status.idle": "2024-05-28T16:25:13.878615Z",
     "shell.execute_reply.started": "2024-05-28T16:25:13.838144Z",
     "shell.execute_reply": "2024-05-28T16:25:13.87776Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class LGBM():\n    def __init__(self):\n        self.data_train = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n        self.data_test = pl.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n        self.num_models = 3\n        self.acc_metrics = []\n        self.cohen_metrics = []\n        \n        # coef for cohen kappa score\n        self.a = 2.948\n        self.b = 1.092\n        \n        self.lgb_parameters = {  \n                                 'metrics': 'None',\n                                 'objective': self.qwk_obj,\n                                 'learning_rate': 0.05,\n                                 'max_depth': 5,\n                                 'num_leaves': 10, # should be a number smaller than \"max_depth\"^2\n                                 'colsample_bytree': 0.3,\n                                 'min_data_in_leaf': 100,\n                                 'reg_alpha': 0.7,\n                                 'reg_lambda' : 0.1,\n                                 'n_estimators': 700,\n                                 'extra_trees' : True,\n                                 'verbosity': -100,\n#                                  'device' : \"gpu\"\n        }\n        self.model = VotingRegressor(\n            estimators = [\n                            (f\"lgb_{i}\",lgb.LGBMRegressor(**self.lgb_parameters, random_state=i+40),)for i in range(self.num_models)\n                         ],\n                        n_jobs=-1\n        )\n        \n    def quadratic_weighted_kappa(self,y_true,y_pred):\n        y_true = y_true + self.a\n        y_pred = (y_pred + self.a).clip(1,6).round()\n#         print(y_true)\n#         print(y_pred)\n        qwk = cohen_kappa_score(y_true,y_pred,weights='quadratic')\n        \n        return \"QWK\",qwk,True\n    def qwk_obj(self,y_true,y_pred):\n        labels = y_true + self.a\n        preds = y_pred + self.a\n        preds = preds.clip(1,6)\n        f = 1/2 * np.sum((preds-labels)**2)\n        g = 1/2 * np.sum((preds-self.a)**2+self.b)\n        df = preds - labels\n        dg = preds - self.a\n        grad = (df/g - f*dg/g**2)*len(labels)\n        hess = np.ones(len(labels))\n        \n        return grad,hess\n    def split_folds(self, df):\n        feature_names = [col for col in df.columns if col not in ['essay_id', 'score']]\n        x = df[feature_names].values\n        y = df['score'].values\n        \n        kfold = KFold(n_splits=5, random_state=44, shuffle=True)\n        \n        return kfold.split(x, y)\n    \n    def fit(self, df,debug=False):\n        folds = self.split_folds(df)\n\n        for fold_id, (trn_idx, val_idx) in enumerate(folds):\n            if fold_id != 0 and debug==True:\n                break \n\n            X_train, X_val = df.iloc[trn_idx][feature_names], df.iloc[val_idx][feature_names]\n            Y_train, Y_val = df.iloc[trn_idx]['score'] - self.a, df.iloc[val_idx]['score'] - self.a\n            \n            print(f'\\nFold_{fold_id} Training ================================\\n')\n            \n            self.model.fit(X_train, Y_train)\n            pred_val = self.model.predict(X_val)\n            \n            df_tmp = df.iloc[val_idx][['essay_id', 'score']].copy()\n            df_tmp['pred'] = pred_val\n            \n            # plot confusion matrix\n            y_true = Y_val.values+np.ones_like(Y_val.shape)*self.a\n            y_pred = (pred_val + np.ones_like(pred_val)*self.a).clip(1,6).round()\n            cm = confusion_matrix(y_true,y_pred)\n            sns.heatmap(cm, \n                        annot=True,\n                        fmt='g', \n                        xticklabels=['1','2','3','4','5','6'],\n                        yticklabels=['1','2','3','4','6','6'])\n            plt.ylabel('Prediction',fontsize=13)\n            plt.xlabel('Actual',fontsize=13)\n            plt.title('Confusion Matrix',fontsize=17)\n            plt.show()\n                                  \n            cohen_score = self.quadratic_weighted_kappa(Y_val.values, df_tmp['pred'])\n            self.cohen_metrics.append(cohen_score[1])\n\n        average_cohen = np.mean(self.cohen_metrics)\n        print(f'Average Cohen all fold: {average_cohen:.4f}')\n    def save_model(self):\n        pass\n    def predict(self,df):\n        feature_names = list(filter(lambda x: x not in ['essay_id','score'], df.columns))\n        \n        predictions = self.model.predict(df[feature_names])\n        predictions += self.a\n        predictions = predictions.clip(1,6).round()\n#         predictions = self.model.predict(df[feature_names])\n        return predictions\n    def submit(self,df):\n        feature_names = list(filter(lambda x: x not in ['essay_id'], df.columns))\n        return self.data_test.select('essay_id').with_columns(score = (self.model.predict(df[feature_names])+self.a).clip(1, 6).round())",
   "metadata": {
    "_uuid": "ff81988f-0171-48c8-bdfd-a08580be391d",
    "_cell_guid": "c74e686d-61ab-4403-858a-8e3ec83d448b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:25:13.881074Z",
     "iopub.execute_input": "2024-05-28T16:25:13.881377Z",
     "iopub.status.idle": "2024-05-28T16:25:13.907571Z",
     "shell.execute_reply.started": "2024-05-28T16:25:13.881343Z",
     "shell.execute_reply": "2024-05-28T16:25:13.906499Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "FE = FeatureEngineering()\ntrain_feature = FE.process()\ntest_feature = FE.process_test()\ntest_feature = pd.merge(test_feature,eff_df,on='essay_id')\ntest_feature.shape",
   "metadata": {
    "_uuid": "35ea1db4-fe02-44b3-8b63-91a56bcbd5e9",
    "_cell_guid": "2142ead0-823c-4da0-a6cc-ba55800d22c1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:25:13.908815Z",
     "iopub.execute_input": "2024-05-28T16:25:13.909186Z",
     "iopub.status.idle": "2024-05-28T16:26:53.892512Z",
     "shell.execute_reply.started": "2024-05-28T16:25:13.909153Z",
     "shell.execute_reply": "2024-05-28T16:26:53.891283Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feature = test_feature.drop(['essay_id'],axis=1)\ntest_feature",
   "metadata": {
    "_uuid": "3ee18b2b-d37a-42dc-a9e8-c0bd199b028f",
    "_cell_guid": "f707ba41-4bab-4109-b7fe-c4dcded282af",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:26:53.89352Z",
     "iopub.status.idle": "2024-05-28T16:26:53.893866Z",
     "shell.execute_reply.started": "2024-05-28T16:26:53.893691Z",
     "shell.execute_reply": "2024-05-28T16:26:53.893704Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feature['essay_id']",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-28T16:27:21.232022Z",
     "iopub.execute_input": "2024-05-28T16:27:21.232749Z",
     "iopub.status.idle": "2024-05-28T16:27:21.242269Z",
     "shell.execute_reply.started": "2024-05-28T16:27:21.232718Z",
     "shell.execute_reply": "2024-05-28T16:27:21.24112Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## predict",
   "metadata": {
    "_uuid": "3ce0f15a-1789-475a-b170-0c00e72b4786",
    "_cell_guid": "249c21be-b29c-403b-9efd-c9bb06fe513c",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "from joblib import load\nmodel = load(f'/kaggle/input/train-lgbm-extra-feature-from-feedback-comp/saved_models/model.joblib')",
   "metadata": {
    "_uuid": "4dce8d6f-29fb-428e-b5ff-f143b8d6bf27",
    "_cell_guid": "71549784-e005-4e16-ad8f-c362b8f6ece5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T15:57:09.527056Z",
     "iopub.execute_input": "2024-05-28T15:57:09.527442Z",
     "iopub.status.idle": "2024-05-28T16:00:28.213682Z",
     "shell.execute_reply.started": "2024-05-28T15:57:09.527409Z",
     "shell.execute_reply": "2024-05-28T16:00:28.210716Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "pred = model.predict(test_feature)",
   "metadata": {
    "_uuid": "c48e6268-0967-41dc-9063-1ec9728add47",
    "_cell_guid": "c4adbeae-ac27-41ba-8792-41b74c68793a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:00:28.302476Z",
     "iopub.execute_input": "2024-05-28T16:00:28.303738Z",
     "iopub.status.idle": "2024-05-28T16:00:28.462943Z",
     "shell.execute_reply.started": "2024-05-28T16:00:28.303688Z",
     "shell.execute_reply": "2024-05-28T16:00:28.461481Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## submission",
   "metadata": {
    "_uuid": "0d3a6299-f6c6-41f4-af74-b8ba376c2fed",
    "_cell_guid": "33358252-67d6-4479-8d2a-ed31a8405e96",
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": "submission = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')\nsubmission['score'] = pred\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())",
   "metadata": {
    "_uuid": "3f042fa3-ddd3-4d65-b16a-ed39a6beb8a9",
    "_cell_guid": "6c615993-0f96-457e-b2ae-11360e45161e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:00:28.464559Z",
     "iopub.execute_input": "2024-05-28T16:00:28.464931Z",
     "iopub.status.idle": "2024-05-28T16:00:28.492028Z",
     "shell.execute_reply.started": "2024-05-28T16:00:28.464888Z",
     "shell.execute_reply": "2024-05-28T16:00:28.490918Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nfrom sklearn.model_selection import KFold\n\n# Example DataFrame\ndata = {\n    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'feature2': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'unique_column': [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]\n}\ndf = pd.DataFrame(data)\n\n# Number of unique values\nunique_values = df['unique_column'].unique()\nn_splits = len(unique_values)\n\n# Initialize GroupKFold\ngkf = KFold(n_splits=n_splits)\n\n# Create folds\nfolds = {}\nfor fold, (train_index, test_index) in enumerate(gkf.split(df, groups=df['unique_column'])):\n    train_df = df.iloc[train_index]\n    test_df = df.iloc[test_index]\n    folds[fold] = {'train': train_df, 'test': test_df}\n    print(f\"Fold {fold + 1} - Test Data (unique_column={test_df['unique_column'].unique()[0]}):\\n{test_df}\\n\")\n",
   "metadata": {
    "_uuid": "a71a6ae9-f7ae-4e00-b30b-bc0ade591a13",
    "_cell_guid": "78d5ae20-b2b5-4c48-a0d7-e776dde2c4ff",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-05-28T16:11:54.095683Z",
     "iopub.execute_input": "2024-05-28T16:11:54.096103Z",
     "iopub.status.idle": "2024-05-28T16:11:54.116602Z",
     "shell.execute_reply.started": "2024-05-28T16:11:54.096075Z",
     "shell.execute_reply": "2024-05-28T16:11:54.115534Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
