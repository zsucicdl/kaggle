{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8126207,
     "sourceType": "datasetVersion",
     "datasetId": 4791897
    },
    {
     "sourceId": 8141507,
     "sourceType": "datasetVersion",
     "datasetId": 4813598
    },
    {
     "sourceId": 8166166,
     "sourceType": "datasetVersion",
     "datasetId": 4832208
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ‚ö†Ô∏è Found an error in tfidf and count word vectorizer in many baseline NB, this notebook is showing it and (try to) explain why ‚ö†Ô∏è\n\nIn many baseline and high LB score public NB, there is probably an error in **word n-gram tokenizer**, that behaves like a **character n-gram tokenizer**.\n\nTo be noted that correcting the error **DOES NOT** improve the score, and you would probably want to keep char n-gram tokenizer or both ! \n\n* copied NB to show this error:\n    * https://www.kaggle.com/code/yongsukprasertsuk/0-818-deberta-v3-large-lgbm-baseline \n\n\n* other kernels probably affected:\n    * https://www.kaggle.com/code/hideyukizushi/aes2-deberta-lgbm-countvectorizer-lb-819\n    * https://www.kaggle.com/code/olyatsimboy/81-1-aes2-5folddeberta-lgbm-stacking \n    * https://www.kaggle.com/code/zulqarnainalipk/lgbm-deberta-explained\n\nMany thanks üôè to those author for those great kernels !",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd \n\nfrom datasets import Dataset\n\nimport gc\n\nfrom scipy.special import softmax\n\nMAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nMODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\nEVAL_BATCH_SIZE = 1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:33.129803Z",
     "iopub.execute_input": "2024-05-01T16:21:33.130215Z",
     "iopub.status.idle": "2024-05-01T16:21:37.108375Z",
     "shell.execute_reply.started": "2024-05-01T16:21:33.130182Z",
     "shell.execute_reply": "2024-05-01T16:21:37.107055Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importing necessary libraries\nimport pandas as pd \nfrom datasets import Dataset  \n\nimport gc\nimport lightgbm as lgb\nfrom sklearn.ensemble import VotingRegressor\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport spacy\nimport string\nimport random\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport polars as pl\nimport joblib",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:37.110575Z",
     "iopub.execute_input": "2024-05-01T16:21:37.111278Z",
     "iopub.status.idle": "2024-05-01T16:21:47.020248Z",
     "shell.execute_reply.started": "2024-05-01T16:21:37.111237Z",
     "shell.execute_reply": "2024-05-01T16:21:47.018944Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n\nnlp = spacy.load(\"en_core_web_sm\")\nwith open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\n\n# Display the first sample data in the training set\ntrain.head(1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:47.022152Z",
     "iopub.execute_input": "2024-05-01T16:21:47.023057Z",
     "iopub.status.idle": "2024-05-01T16:21:50.537155Z",
     "shell.execute_reply.started": "2024-05-01T16:21:47.023014Z",
     "shell.execute_reply": "2024-05-01T16:21:50.53547Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Features engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def count_spelling_errors(text):\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n    return spelling_errors\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:50.54031Z",
     "iopub.execute_input": "2024-05-01T16:21:50.540886Z",
     "iopub.status.idle": "2024-05-01T16:21:50.554139Z",
     "shell.execute_reply.started": "2024-05-01T16:21:50.540853Z",
     "shell.execute_reply": "2024-05-01T16:21:50.552327Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Paragraph Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# paragraph features\ndef remove_punctuation(text):\n    \"\"\"\n    Remove all punctuation from the input text.\n    \n    Args:\n    - text (str): The input text.\n    \n    Returns:\n    - str: The text with punctuation removed.\n    \"\"\"\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # removed for speed\n    # tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n    # tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n# modified for speed\nparagraph_fea2 = paragraph_fea\ndef Paragraph_Eng(train_tmp):\n    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],  \n        ]\n    \n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:50.555863Z",
     "iopub.execute_input": "2024-05-01T16:21:50.556311Z",
     "iopub.status.idle": "2024-05-01T16:21:59.471392Z",
     "shell.execute_reply.started": "2024-05-01T16:21:50.556279Z",
     "shell.execute_reply": "2024-05-01T16:21:59.470433Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ‚ö†Ô∏è (faulty) word TfidfVectorizer ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,6),\n            min_df=0.15, # modified for quicker runtime\n            max_df=0.85,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n\n# Convert to dataframe\ndf = pd.DataFrame(train_tfid.toarray())\n\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nprint(f'New features count: {len(tfid_columns)}')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:21:59.473035Z",
     "iopub.execute_input": "2024-05-01T16:21:59.474267Z",
     "iopub.status.idle": "2024-05-01T16:26:36.486036Z",
     "shell.execute_reply.started": "2024-05-01T16:21:59.474216Z",
     "shell.execute_reply": "2024-05-01T16:26:36.484222Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "‚¨áÔ∏è‚¨áÔ∏è Geting the feature names of the vectorizer shows that it's not word n-grams that are processed, but rather char n-grams that are counted (with spaces between chars).\n\nThis is probably due to the fact that identity (lambda x:x) is given as tokenizer, so words are not tokenized from full-text ? Tokenizer should only be overided by identity if text is already tokenized before. Perhaps vectorizer is receiving string (char sequence) instead of word sequence, so it behaves like a char ngram vectorizer...",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "vect_feat_names=vectorizer.get_feature_names_out()\nprint(vect_feat_names[100:150],end='\\n\\n')\nprint(vect_feat_names[500:550],end='\\n\\n')\nprint(vect_feat_names[2000:2050],end='\\n\\n')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:26:36.48859Z",
     "iopub.execute_input": "2024-05-01T16:26:36.489085Z",
     "iopub.status.idle": "2024-05-01T16:26:36.51026Z",
     "shell.execute_reply.started": "2024-05-01T16:26:36.489044Z",
     "shell.execute_reply": "2024-05-01T16:26:36.508569Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ‚ö†Ô∏è (faulty) word CountVectorizer ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.15,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nprint(f'New features count: {len(tfid_columns)}')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:26:36.512547Z",
     "iopub.execute_input": "2024-05-01T16:26:36.512934Z",
     "iopub.status.idle": "2024-05-01T16:28:11.78679Z",
     "shell.execute_reply.started": "2024-05-01T16:26:36.512903Z",
     "shell.execute_reply": "2024-05-01T16:28:11.784989Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vect_feat_names=vectorizer_cnt.get_feature_names_out()\nprint(vect_feat_names[100:150],end='\\n\\n')\nprint(vect_feat_names[500:550],end='\\n\\n')\nprint(vect_feat_names[700:750],end='\\n\\n')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:28:11.789153Z",
     "iopub.execute_input": "2024-05-01T16:28:11.789625Z",
     "iopub.status.idle": "2024-05-01T16:28:11.803087Z",
     "shell.execute_reply.started": "2024-05-01T16:28:11.789589Z",
     "shell.execute_reply": "2024-05-01T16:28:11.801081Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# üëå(corrected) Word n-grams TfidfVectorizer ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\nvectorizer_word = TfidfVectorizer(\n            #tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            #token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,6),\n            min_df=0.01, # modified for words\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer_word.fit_transform([i for i in train['full_text']])\n\n# Convert to dataframe\ndf = pd.DataFrame(train_tfid.toarray())\n\n# rename features\ntfid_columns = [ f'tfid_word_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nprint(f'New features count: {len(tfid_columns)}')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:28:11.807746Z",
     "iopub.execute_input": "2024-05-01T16:28:11.808221Z",
     "iopub.status.idle": "2024-05-01T16:29:56.054723Z",
     "shell.execute_reply.started": "2024-05-01T16:28:11.808185Z",
     "shell.execute_reply": "2024-05-01T16:29:56.05308Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vect_feat_names=vectorizer_word.get_feature_names_out()\nprint(vect_feat_names[100:150],end='\\n\\n')\nprint(vect_feat_names[500:550],end='\\n\\n')\nprint(vect_feat_names[1700:1750],end='\\n\\n')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:29:56.056568Z",
     "iopub.execute_input": "2024-05-01T16:29:56.057081Z",
     "iopub.status.idle": "2024-05-01T16:29:56.144666Z",
     "shell.execute_reply.started": "2024-05-01T16:29:56.057037Z",
     "shell.execute_reply": "2024-05-01T16:29:56.14315Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# \n#train_feats=train_feats.drop(columns=[col for col in train_feats.columns if col.startswith('tfid_word')])\n#train_feats.head(4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:29:56.146597Z",
     "iopub.execute_input": "2024-05-01T16:29:56.147013Z",
     "iopub.status.idle": "2024-05-01T16:29:56.160636Z",
     "shell.execute_reply.started": "2024-05-01T16:29:56.14698Z",
     "shell.execute_reply": "2024-05-01T16:29:56.159253Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# üëå(correct) Char n-grams TfidfVectorizer ‚¨áÔ∏è",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Using the **analyzer = 'char'** parameter ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter for CHAR\nvectorizer_char = TfidfVectorizer(\n            #tokenizer=lambda x: x, # corrected\n            preprocessor=lambda x: x.lower(),\n            #token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'char',\n            ngram_range=(3,6),\n            min_df=0.15, \n            max_df=0.85,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer_char.fit_transform([i for i in train['full_text']])\n\n# Convert to dataframe\ndf = pd.DataFrame(train_tfid.toarray())\n\n# rename features\ntfid_columns = [ f'tfid_char_{i}' for i in range(len(df.columns))] \n\ndf.columns = tfid_columns\n\ndf['essay_id'] = train_feats['essay_id']\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\n\nprint(f'new features: {len(tfid_columns)}')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:29:56.162139Z",
     "iopub.execute_input": "2024-05-01T16:29:56.162556Z",
     "iopub.status.idle": "2024-05-01T16:33:16.114505Z",
     "shell.execute_reply.started": "2024-05-01T16:29:56.162521Z",
     "shell.execute_reply": "2024-05-01T16:33:16.112834Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "vect_feat_names=vectorizer_char.get_feature_names_out()\nprint(vect_feat_names[100:150],end='\\n\\n')\nprint(vect_feat_names[500:550],end='\\n\\n')\nprint(vect_feat_names[1700:1750],end='\\n\\n')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T16:33:16.116412Z",
     "iopub.execute_input": "2024-05-01T16:33:16.116809Z",
     "iopub.status.idle": "2024-05-01T16:33:16.141406Z",
     "shell.execute_reply.started": "2024-05-01T16:33:16.116776Z",
     "shell.execute_reply": "2024-05-01T16:33:16.139706Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
