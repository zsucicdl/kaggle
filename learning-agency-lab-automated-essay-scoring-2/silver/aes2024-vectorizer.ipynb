{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **FOREWORD**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Hello everyone, <br>\nWe are almost into the half way point in the competition as on date! I hope this kernel finds you learning well and augmenting your knowlwdge hereby! <br>\n\nWe have witnessed a lot of good public work in the competition that provides a lot of insights into feature engineering and a lot of useful information too. Thanks to the contributors across the forums! <br> \n\nI notice that all of these kernels use lambda functions in the count vectorizer and TFIDF vectorizer, thus not being able to save them in the standard pickle format. As a result, these kernels resort to building the entire preprocessing pipeline in the train data while inferring and predicting the test set. This is superfluous and in my opinion, an opportunity cost. <br> \n\nWe do have a solution for this and is presented below.. <br> ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# **IMPORT**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time \n\nimport os\n\ntry:\n    os.mkdir(\"/kaggle/working/Cloudpickle\");\nexcept:\n    pass;\n\n!pip -q download cloudpickle -d \"/kaggle/working/Cloudpickle\";\n\nprint();",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T13:58:46.979636Z",
     "iopub.execute_input": "2024-05-14T13:58:46.980045Z",
     "iopub.status.idle": "2024-05-14T13:58:49.194532Z",
     "shell.execute_reply.started": "2024-05-14T13:58:46.980013Z",
     "shell.execute_reply": "2024-05-14T13:58:49.193191Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **IMPLEMENTATION**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time \n\n!pip install -q \"/kaggle/working/Cloudpickle/cloudpickle-3.0.0-py3-none-any.whl\"\nimport cloudpickle\n\nimport pandas as pd, numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom IPython.display import clear_output\nfrom gc import collect\n\nfrom tqdm.notebook import tqdm",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-05-14T13:58:49.196988Z",
     "iopub.execute_input": "2024-05-14T13:58:49.197592Z",
     "iopub.status.idle": "2024-05-14T13:59:04.384483Z",
     "shell.execute_reply.started": "2024-05-14T13:58:49.197556Z",
     "shell.execute_reply": "2024-05-14T13:59:04.383516Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We shall illustrate an example for this package using the vectorizers defined in the public datasets and kernels. This is an illustration notebook. Feel free to use this as input for your kernels, including the cloudpickle installation folder! <br>\nBest wishes!\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time \n\ntrain = pd.read_csv(f\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\",)\ntest  = pd.read_csv(f\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\",)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T13:59:04.386068Z",
     "iopub.execute_input": "2024-05-14T13:59:04.386832Z",
     "iopub.status.idle": "2024-05-14T13:59:04.823426Z",
     "shell.execute_reply.started": "2024-05-14T13:59:04.386799Z",
     "shell.execute_reply": "2024-05-14T13:59:04.821493Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time \n\nmyvec = \\\n[Pipeline(steps = [(\"Vec\", \n                    TfidfVectorizer(tokenizer=lambda x: x,\n                                    preprocessor=lambda x: x,\n                                    token_pattern=None,\n                                    strip_accents='unicode',\n                                    analyzer = 'word',\n                                    ngram_range=(3,6),\n                                    min_df=0.05,\n                                    max_df=0.95,\n                                    sublinear_tf=True,\n                                   )\n                   )\n                  ]\n         ),\n \n Pipeline(steps = [(\"CVec\", \n                    CountVectorizer(\n                        tokenizer=lambda x: x,\n                        preprocessor=lambda x: x,\n                        token_pattern=None,\n                        strip_accents='unicode',\n                        analyzer = 'word',\n                        ngram_range=(2,3),\n                        min_df=0.10,\n                        max_df=0.85,\n                    )\n                   )\n                  ]\n         ), \n];\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T13:59:04.82627Z",
     "iopub.execute_input": "2024-05-14T13:59:04.826724Z",
     "iopub.status.idle": "2024-05-14T13:59:04.848165Z",
     "shell.execute_reply.started": "2024-05-14T13:59:04.826684Z",
     "shell.execute_reply": "2024-05-14T13:59:04.846755Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time \n\nmysavedvec = [];\n\nfor i, xform in tqdm(enumerate(myvec)):\n    df = \\\n    pd.DataFrame(xform.fit_transform([i for i in train['full_text']]).\\\n                 toarray()\n                ).\\\n    add_prefix(f\"vec{i}_\")\n    \n    # Saving the vectorizer   \n    mysavedvec.append(xform)\n    print(f\"Vectorizer {i} sample output - {df.shape}\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T14:19:04.943269Z",
     "iopub.execute_input": "2024-05-14T14:19:04.943683Z",
     "iopub.status.idle": "2024-05-14T14:19:05.290711Z",
     "shell.execute_reply.started": "2024-05-14T14:19:04.943655Z",
     "shell.execute_reply": "2024-05-14T14:19:05.289641Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Let us save the vectorizer now. We use the cloudpickle library for the same. <br>\nWe create a folder named **SavedVec** in the working folder and dump the fitted vectorizers there <br>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time \n\ntry:\n    os.mkdir(f\"/kaggle/working/SavedVec\")\nexcept: \n    pass\nwith open(\"/kaggle/working/SavedVec/mysavedvec\", \"wb\") as f:\n    cloudpickle.dump(mysavedvec, f)\n    \nprint(f\"---> Saved the vectorizers\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T14:19:09.343424Z",
     "iopub.execute_input": "2024-05-14T14:19:09.344373Z",
     "iopub.status.idle": "2024-05-14T14:19:09.363391Z",
     "shell.execute_reply.started": "2024-05-14T14:19:09.34433Z",
     "shell.execute_reply": "2024-05-14T14:19:09.362276Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now, we can load these vectorizers from our saved path and use them in the test set <br>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# **INFERENCE**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time \n\nwith open(\"/kaggle/working/SavedVec/mysavedvec\", \"rb\") as f:\n    myloadedvec = cloudpickle.load(f)\n    \ndisplay(myloadedvec)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T14:19:11.622613Z",
     "iopub.execute_input": "2024-05-14T14:19:11.622993Z",
     "iopub.status.idle": "2024-05-14T14:19:11.651415Z",
     "shell.execute_reply.started": "2024-05-14T14:19:11.622963Z",
     "shell.execute_reply": "2024-05-14T14:19:11.650318Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time \n\nfor i, xform in tqdm(enumerate(myloadedvec)):\n    df = \\\n    pd.DataFrame(xform.transform([i for i in test['full_text']]).\\\n                 toarray()\n                ).\\\n    add_prefix(f\"vec{i}_\")\n    print(f\"Vectorizer {i} sample output = {df.shape}\")\n    \nprint();",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-14T14:19:42.438486Z",
     "iopub.execute_input": "2024-05-14T14:19:42.439055Z",
     "iopub.status.idle": "2024-05-14T14:19:42.551996Z",
     "shell.execute_reply.started": "2024-05-14T14:19:42.439Z",
     "shell.execute_reply": "2024-05-14T14:19:42.550923Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **OUTRO**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "We are done here! Hope you liked the idea! <br>\nAll the best for the challenge! <br>\n\nPlease feel free to use this and save your inference time for better work! <br>",
   "metadata": {}
  }
 ]
}
