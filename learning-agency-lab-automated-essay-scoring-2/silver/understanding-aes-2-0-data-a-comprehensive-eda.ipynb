{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np, gc \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport nltk\nimport matplotlib.pyplot as plt\nimport spacy\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nimport textstat\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T11:34:35.082383Z","iopub.execute_input":"2024-05-23T11:34:35.083115Z","iopub.status.idle":"2024-05-23T11:34:35.614386Z","shell.execute_reply.started":"2024-05-23T11:34:35.083044Z","shell.execute_reply":"2024-05-23T11:34:35.612857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this library is needed to assess readability of the essays\n!pip install textstat","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:35.616963Z","iopub.execute_input":"2024-05-23T11:34:35.617374Z","iopub.status.idle":"2024-05-23T11:34:51.463821Z","shell.execute_reply.started":"2024-05-23T11:34:35.617338Z","shell.execute_reply":"2024-05-23T11:34:51.462177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook serves as a valuable resource for individuals at various proficiency levels in Natural Language Processing (NLP). We begin with an introductory section dedicated to general Exploratory Data Analysis (EDA). Subsequently, in the second section, we delve into a comprehensive content-based analysis of the AES 2.0 competition data. The content based analysis includes topic modelling, sentiment analysis, named entity recognition analysis, readability level of the essays and other common tasks. \n\nThroughout our investigation, we discovered a noteworthy observation regarding the memory-intensive nature of NLP tasks, particularly when dealing with nlp(essays_string). Due to memory constraints and the risk of potential memory leakage issues, our analysis was limited to processing only a quarter of the available dataset.","metadata":{}},{"cell_type":"code","source":"#loading train data and visulizing first fews records \ntrain_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:51.465744Z","iopub.execute_input":"2024-05-23T11:34:51.466328Z","iopub.status.idle":"2024-05-23T11:34:52.318744Z","shell.execute_reply.started":"2024-05-23T11:34:51.466268Z","shell.execute_reply":"2024-05-23T11:34:52.317418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the essays object would be used frequent in the below sections \nessays = train_df['full_text']","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.320847Z","iopub.execute_input":"2024-05-23T11:34:52.321354Z","iopub.status.idle":"2024-05-23T11:34:52.332089Z","shell.execute_reply.started":"2024-05-23T11:34:52.321313Z","shell.execute_reply":"2024-05-23T11:34:52.330827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**General EDA**","metadata":{}},{"cell_type":"code","source":"print(\"Train shape\",train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.33675Z","iopub.execute_input":"2024-05-23T11:34:52.337325Z","iopub.status.idle":"2024-05-23T11:34:52.366177Z","shell.execute_reply.started":"2024-05-23T11:34:52.337285Z","shell.execute_reply":"2024-05-23T11:34:52.363509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary statistics of essay scores\nprint(\"Summary Statistics of Scores:\")\nprint(train_df['score'].describe())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.368154Z","iopub.execute_input":"2024-05-23T11:34:52.368575Z","iopub.status.idle":"2024-05-23T11:34:52.393778Z","shell.execute_reply.started":"2024-05-23T11:34:52.368523Z","shell.execute_reply":"2024-05-23T11:34:52.39031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing length of each assay\n# Add a new column to store the length of each essay\ntrain_df['essay_length'] = essays.apply(len)\n# Display essay_id and essay_length columns\nprint(train_df[['essay_id', 'essay_length']])","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.395532Z","iopub.execute_input":"2024-05-23T11:34:52.396285Z","iopub.status.idle":"2024-05-23T11:34:52.432251Z","shell.execute_reply.started":"2024-05-23T11:34:52.396237Z","shell.execute_reply":"2024-05-23T11:34:52.430917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Analyzing maxim and minimum lengths of the essays \n\n# Find maximum essay length\nmax_length = train_df['essay_length'].max()\n\n# Find minimum essay length\nmin_length = train_df['essay_length'].min()\n\nprint(\"Maximum essay length:\", max_length)\nprint(\"Minimum essay length:\", min_length)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.4348Z","iopub.execute_input":"2024-05-23T11:34:52.43562Z","iopub.status.idle":"2024-05-23T11:34:52.444223Z","shell.execute_reply.started":"2024-05-23T11:34:52.435572Z","shell.execute_reply":"2024-05-23T11:34:52.442918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visulizations for essay lengths \ntrain_df['essay_length'].value_counts().plot(kind='bar')\nplt.title('Analyzing Essay Lengths')\nplt.xlabel('Essay Length')\nplt.ylabel('No. of Essays')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:34:52.446486Z","iopub.execute_input":"2024-05-23T11:34:52.447055Z","iopub.status.idle":"2024-05-23T11:35:21.609203Z","shell.execute_reply.started":"2024-05-23T11:34:52.447002Z","shell.execute_reply":"2024-05-23T11:35:21.607236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution plot of essay scores\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df['score'], kde=True)\nplt.title(\"Analyzing Essay Scores\")\nplt.xlabel(\"Essay Score\")\nplt.ylabel(\"No. of Essays\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:21.612393Z","iopub.execute_input":"2024-05-23T11:35:21.613344Z","iopub.status.idle":"2024-05-23T11:35:22.28499Z","shell.execute_reply.started":"2024-05-23T11:35:21.613159Z","shell.execute_reply":"2024-05-23T11:35:22.283597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between essay length and scores\ncorr_matrix = train_df['score'].corr(train_df['essay_length'])\nprint(\"Correlation between Essay Length and Score:\", corr_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:22.286713Z","iopub.execute_input":"2024-05-23T11:35:22.287182Z","iopub.status.idle":"2024-05-23T11:35:22.299205Z","shell.execute_reply.started":"2024-05-23T11:35:22.28714Z","shell.execute_reply":"2024-05-23T11:35:22.297805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing correlation matrix\ncorr_matrix = train_df[['score', 'essay_length']].corr()\n# Plotting the correlation matrix heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 12})\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:22.300961Z","iopub.execute_input":"2024-05-23T11:35:22.301484Z","iopub.status.idle":"2024-05-23T11:35:22.623969Z","shell.execute_reply.started":"2024-05-23T11:35:22.30144Z","shell.execute_reply":"2024-05-23T11:35:22.622603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing essay length vs. scores\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=train_df, x='essay_length', y='score')\nplt.title(\"Essay Length vs. Essay Score Analysis\")\nplt.xlabel(\"Essay Length\")\nplt.ylabel(\"Essay Score\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:22.625725Z","iopub.execute_input":"2024-05-23T11:35:22.626215Z","iopub.status.idle":"2024-05-23T11:35:23.007455Z","shell.execute_reply.started":"2024-05-23T11:35:22.626181Z","shell.execute_reply":"2024-05-23T11:35:23.006331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Essay contents EDA**","metadata":{}},{"cell_type":"code","source":"# Generating a single string by combine all essays \nall_essay_str = ' '.join(essays)\n# Combine half of the essays into a single string for NER analysis\n#This process nlp(selected_essays) needs much memory os we took only 1/4 essays to avoid from memory \nselected_essays = ' '.join(essays.iloc[:len(train_df) // 4])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:23.013403Z","iopub.execute_input":"2024-05-23T11:35:23.013827Z","iopub.status.idle":"2024-05-23T11:35:23.114959Z","shell.execute_reply.started":"2024-05-23T11:35:23.013794Z","shell.execute_reply":"2024-05-23T11:35:23.112569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download the necessary NLTK resources (if not already downloaded)\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:23.117049Z","iopub.execute_input":"2024-05-23T11:35:23.117614Z","iopub.status.idle":"2024-05-23T11:35:23.341847Z","shell.execute_reply.started":"2024-05-23T11:35:23.117549Z","shell.execute_reply":"2024-05-23T11:35:23.340448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download VADER lexicon (if not already downloaded)\nnltk.download('vader_lexicon')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:23.343452Z","iopub.execute_input":"2024-05-23T11:35:23.343854Z","iopub.status.idle":"2024-05-23T11:35:23.357742Z","shell.execute_reply.started":"2024-05-23T11:35:23.343818Z","shell.execute_reply":"2024-05-23T11:35:23.355971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution of most common words**","metadata":{}},{"cell_type":"code","source":"# Tokenize the essays into words\nno_of_words = all_essay_str.split()\n\n# Count the frequency of each word\nwords_freq = Counter(no_of_words)\n\n# Extract the top 20 common words and their frequencies\ntop_words = words_freq.most_common(20)\n\n# Plot the word frequency distribution\nplt.figure(figsize=(10, 6))\nplt.bar([word[0] for word in top_words], [count[1] for count in top_words])\nplt.title('Top 20 Common Words in Essays')\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:23.359139Z","iopub.execute_input":"2024-05-23T11:35:23.35951Z","iopub.status.idle":"2024-05-23T11:35:26.136114Z","shell.execute_reply.started":"2024-05-23T11:35:23.359477Z","shell.execute_reply":"2024-05-23T11:35:26.134893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analyzing the distribution of sentence lengths in the essays.**","metadata":{}},{"cell_type":"code","source":"# Generating sentences from the essays\nsentens = []\nfor essay in essays:\n    sentens.extend(nltk.sent_tokenize(essay))\n\n# computing length of each sentence\nsenten_lens = [len(nltk.word_tokenize(sentence)) for sentence in sentens]\n\n# Graphical view of the distribution of sentence lengths\nplt.figure(figsize=(10, 6))\nplt.hist(senten_lens, bins=30, color='skyblue', edgecolor='black')\nplt.title('Investigating Sentence Lengths in Essays')\nplt.xlabel('Sentence Length')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:35:26.137743Z","iopub.execute_input":"2024-05-23T11:35:26.13841Z","iopub.status.idle":"2024-05-23T11:37:24.322177Z","shell.execute_reply.started":"2024-05-23T11:35:26.138337Z","shell.execute_reply":"2024-05-23T11:37:24.320669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Topic Modeling: Analyzing latent topics in the essays using Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF)**  ","metadata":{}},{"cell_type":"code","source":"# Create TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n\n# Create document-term matrix\ntfidf_matrix = tfidf_vectorizer.fit_transform(essays)\n\n# LDA\nlda = LatentDirichletAllocation(n_components=5, random_state=42)\nlda_topics = lda.fit_transform(tfidf_matrix)\n\n# NMF\nnmf = NMF(n_components=5, random_state=42)\nnmf_topics = nmf.fit_transform(tfidf_matrix)\n\n# Print the top words for each topic\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = f\"Topic #{topic_idx}: \"\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n\nprint(\"LDA Topics:\")\nprint_top_words(lda, tfidf_vectorizer.get_feature_names_out(), 10)\n\nprint(\"NMF Topics:\")\nprint_top_words(nmf, tfidf_vectorizer.get_feature_names_out(), 10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:37:24.323761Z","iopub.execute_input":"2024-05-23T11:37:24.324151Z","iopub.status.idle":"2024-05-23T11:38:24.704429Z","shell.execute_reply.started":"2024-05-23T11:37:24.324117Z","shell.execute_reply":"2024-05-23T11:38:24.702959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sentiment analysis**","metadata":{}},{"cell_type":"code","source":"# Initialize the sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Analyze the sentiment of each essay\nsentiment_scores = []\nfor essay in essays:\n    sentiment_scores.append(sid.polarity_scores(essay)['compound'])\n\n# Add sentiment scores to the dataframe\ntrain_df['sentiment_score'] = sentiment_scores\n\n# Overall sentiment analysis\npositive_count = sum(score > 0 for score in sentiment_scores)\nnegative_count = sum(score < 0 for score in sentiment_scores)\nneutral_count = len(sentiment_scores) - positive_count - negative_count\n\nprint(\"Sentiment Analysis:\")\nprint(\"Positive essays:\", positive_count)\nprint(\"Negative essays:\", negative_count)\nprint(\"Neutral essays:\", neutral_count)\n\n# Data for visualization\ncategories = ['Positive', 'Negative', 'Neutral']\ncounts = [positive_count, negative_count, neutral_count]\n\n# Create bar plot\nplt.figure(figsize=(8, 6))\nplt.bar(categories, counts, color=['green', 'red', 'blue'])\nplt.title('Sentiment Analysis')\nplt.xlabel('Sentiment')\nplt.ylabel('Number of Essays')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:38:24.706218Z","iopub.execute_input":"2024-05-23T11:38:24.70663Z","iopub.status.idle":"2024-05-23T11:40:04.892495Z","shell.execute_reply.started":"2024-05-23T11:38:24.706579Z","shell.execute_reply":"2024-05-23T11:40:04.89115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Named Entity Recognition (NER) Analysis**","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\n# Increasing max_length limit\nnlp.max_length = len(selected_essays)  \n\n# Processing the combined text with spaCy\ndoc = nlp(selected_essays)\n\n# Extracting named entities and count their occurrences\nnamed_entities = Counter([(ent.text, ent.label_) for ent in doc.ents])\n\n# Print the most common named entities and their counts\nprint(\"Most Common Named Entities:\")\nfor entity, count in named_entities.most_common(10):\n    print(f\"{entity[0]} ({entity[1]}): {count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:40:04.894274Z","iopub.execute_input":"2024-05-23T11:40:04.894794Z","iopub.status.idle":"2024-05-23T11:46:53.909107Z","shell.execute_reply.started":"2024-05-23T11:40:04.89475Z","shell.execute_reply":"2024-05-23T11:46:53.907676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Readability analysis**","metadata":{}},{"cell_type":"markdown","source":"**Assessing the readability level of the essays (Flesch-Kincaid, Gunning Fog).**","metadata":{}},{"cell_type":"code","source":"# Analyzing readability scores for each essay\nreadability_scores = []\nfor essay in essays:\n    # Compute Flesch-Kincaid Grade Level\n    fkg_score = textstat.flesch_kincaid_grade(essay)\n    \n    # Computing Gunning Fog Index\n    gunning_fog_score = textstat.gunning_fog(essay)\n    \n    # Add scores to list\n    readability_scores.append({'Flesch-Kincaid': fkg_score, 'Gunning Fog': gunning_fog_score})\n\n# Creating a dataframe to store the scores\nreadability_df = pd.DataFrame(readability_scores)\n\n# Create histograms for Flesch-Kincaid and Gunning Fog scores\nplt.figure(figsize=(10, 6))\nplt.hist(readability_df['Flesch-Kincaid'], bins=20, alpha=0.5, label='Flesch-Kincaid')\nplt.hist(readability_df['Gunning Fog'], bins=20, alpha=0.5, label='Gunning Fog')\nplt.title('Readability Scores Distribution')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n# Display the dataframe\nprint(readability_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T11:46:53.911005Z","iopub.execute_input":"2024-05-23T11:46:53.91143Z","iopub.status.idle":"2024-05-23T11:47:45.158361Z","shell.execute_reply.started":"2024-05-23T11:46:53.91138Z","shell.execute_reply":"2024-05-23T11:47:45.157082Z"},"trusted":true},"execution_count":null,"outputs":[]}]}