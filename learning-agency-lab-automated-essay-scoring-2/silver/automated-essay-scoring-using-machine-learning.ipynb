{
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8403792,
     "sourceType": "datasetVersion",
     "datasetId": 5000565
    },
    {
     "sourceId": 177475777,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 177473688,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 240.36083,
   "end_time": "2024-05-18T14:45:33.483916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T14:41:33.123086",
   "version": "2.3.4"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Project Title:\nAutomated Essay Scoring using Machine Learning\n\n### Steps:\n\n1. **Data Preparation**:\n   - Load and preprocess the dataset containing essays and corresponding scores.\n   - Split the dataset into training and testing sets.\n\n2. **Feature Extraction**:\n   - Use pre-trained language models (e.g., BERT, RoBERTa) to extract embeddings from the essays.\n   \n3. **Model Selection**:\n   - Choose a suitable machine learning model for essay scoring, such as a Multilayer Perceptron (MLP) or a recurrent neural network (RNN).\n\n4. **Cross-Validation**:\n   - Implement K-Fold cross-validation to evaluate model performance and ensure robustness.\n\n5. **Model Training**:\n   - Train the selected model on the training set using the extracted features and corresponding scores.\n\n6. **Model Evaluation**:\n   - Evaluate the trained model on the testing set using appropriate evaluation metrics such as quadratic weighted kappa.\n\n7. **Model Inference**:\n   - Load the trained model parameters.\n   - Make predictions on the test set using the extracted features.\n\n8. **Submission Generation**:\n   - Format the predictions into a submission file following the required template.\n   - Save the submission file in CSV format.\n\n### Algorithm Names:\n\n1. **Feature Extraction**: \n   - BERT Embeddings\n   - RoBERTa Embeddings\n\n2. **Model Selection**:\n   - Multilayer Perceptron (MLP)\n   - Recurrent Neural Network (RNN)\n\n3. **Evaluation Metrics**:\n   - Quadratic Weighted Kappa (QWK)\n   \n4. **Cross-Validation**:\n   - K-Fold Cross-Validation\n\n### Final Project Summary:\nThe project aims to automate essay scoring using machine learning techniques. It involves extracting features from essays using pre-trained language models, selecting and training appropriate machine learning models, evaluating model performance using cross-validation and evaluation metrics, and generating submissions for scoring predictions. The project explores various algorithms and techniques to achieve accurate and reliable essay scoring.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Downloads All important Path",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# The exclamation mark at the start is used to run shell commands in a Jupyter notebook environment.\n# pip is the package installer for Python, used to install packages.\n# install specifies that we want to install a package.\n# --find-links allows you to specify a directory to search for packages.\n# /kaggle/input/downgrade-pandas is the directory where pip should look for packages.\n# /kaggle/input/downgrade-pandas/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl is the specific package (pandas version 1.5.3) to install.\n!pip install --find-links /kaggle/input/downgrade-pandas /kaggle/input/downgrade-pandas/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 45.624606,
     "end_time": "2024-05-18T14:42:21.717485",
     "exception": false,
     "start_time": "2024-05-18T14:41:36.092879",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:16.308466Z",
     "iopub.execute_input": "2024-05-31T15:36:16.309137Z",
     "iopub.status.idle": "2024-05-31T15:36:36.056844Z",
     "shell.execute_reply.started": "2024-05-31T15:36:16.309103Z",
     "shell.execute_reply": "2024-05-31T15:36:36.055906Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "1. **`!`**:\n   - In a Jupyter notebook, the exclamation mark `!` is used to execute shell commands directly from the notebook cell.\n\n2. **`pip install`**:\n   - `pip` is the package installer for Python. It is used to install and manage software packages written in Python.\n   - `install` is a command used with `pip` to install specified packages.\n\n3. **`--find-links /kaggle/input/downgrade-pandas`**:\n   - `--find-links` is an option that tells `pip` to look for packages in the specified directory or URL.\n   - `/kaggle/input/downgrade-pandas` is the directory where `pip` should look for packages. This is a specific path in the Kaggle environment where the package files are stored.\n\n4. **`/kaggle/input/downgrade-pandas/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl`**:\n   - This is the path to the specific package file that you want to install. It is a wheel file (`.whl`), which is a built package format for Python.\n   - `pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl` is the name of the file, which indicates:\n     - `pandas`: the name of the package.\n     - `1.5.3`: the version of the package.\n     - `cp310`: compatible with CPython 3.10.\n     - `manylinux_2_17_x86_64.manylinux2014_x86_64`: specifies the platform and compatibility tags.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#Library and Dataset Load",
   "metadata": {
    "papermill": {
     "duration": 0.007518,
     "end_time": "2024-05-18T14:42:21.732864",
     "exception": false,
     "start_time": "2024-05-18T14:42:21.725346",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Importing the os module to interact with the operating system\nimport os",
   "metadata": {
    "papermill": {
     "duration": 1.192523,
     "end_time": "2024-05-18T14:42:22.933106",
     "exception": false,
     "start_time": "2024-05-18T14:42:21.740583",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.059228Z",
     "iopub.execute_input": "2024-05-31T15:36:36.060023Z",
     "iopub.status.idle": "2024-05-31T15:36:36.064218Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.059982Z",
     "shell.execute_reply": "2024-05-31T15:36:36.063211Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Setting the environment variable to specify which GPUs to use (0 and 1)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.065422Z",
     "iopub.execute_input": "2024-05-31T15:36:36.065747Z",
     "iopub.status.idle": "2024-05-31T15:36:36.077522Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.065717Z",
     "shell.execute_reply": "2024-05-31T15:36:36.076712Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importing necessary libraries\nimport numpy as np  # NumPy for numerical operations\nimport gc  # Garbage collector interface for memory management\nimport re  # Regular expressions for string matching and manipulation\nimport pandas as pd  # Pandas for data manipulation and analysis",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.078677Z",
     "iopub.execute_input": "2024-05-31T15:36:36.079228Z",
     "iopub.status.idle": "2024-05-31T15:36:36.449954Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.079197Z",
     "shell.execute_reply": "2024-05-31T15:36:36.449236Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Loading the training dataset from a CSV file into a Pandas DataFrame\ntrain = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n# Printing the shape (number of rows and columns) of the training DataFrame\nprint(\"Train shape\", train.shape)\n# Displaying the first few rows of the training DataFrame for a quick preview\ndisplay(train.head())\n# Printing an empty line for better readability in the output\nprint()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.452916Z",
     "iopub.execute_input": "2024-05-31T15:36:36.453392Z",
     "iopub.status.idle": "2024-05-31T15:36:36.826732Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.45336Z",
     "shell.execute_reply": "2024-05-31T15:36:36.825814Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Loading the testing dataset from a CSV file into a Pandas DataFrame\ntest = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n# Printing the shape (number of rows and columns) of the testing DataFrame\nprint(\"Test shape\", test.shape)\n# Displaying the first few rows of the testing DataFrame for a quick preview\ndisplay(test.head())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.827845Z",
     "iopub.execute_input": "2024-05-31T15:36:36.828199Z",
     "iopub.status.idle": "2024-05-31T15:36:36.84021Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.828171Z",
     "shell.execute_reply": "2024-05-31T15:36:36.839134Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n```\n\n1. **`import os`**:\n   - This imports the `os` module, which provides a way of using operating system-dependent functionality like reading or writing to the file system, managing environment variables, etc.\n\n2. **`os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"`**:\n   - This line sets the environment variable `CUDA_VISIBLE_DEVICES` to `\"0,1\"`.\n   - This is used to specify which GPU devices (by their IDs) should be visible to CUDA. Here, GPUs with IDs 0 and 1 are being made visible. This is often used in environments with multiple GPUs to control which ones a particular program should use.\n\n```python\nimport numpy as np, gc, re\nimport pandas as pd\n```\n\n3. **`import numpy as np`**:\n   - This imports the NumPy library, which is a powerful numerical computing library in Python, under the alias `np` for convenience.\n\n4. **`import gc`**:\n   - This imports the `gc` module, which provides an interface to the garbage collector. This can be used to control and interact with the garbage collection process.\n\n5. **`import re`**:\n   - This imports the `re` module, which provides support for regular expressions in Python. Regular expressions are used for matching patterns in strings.\n\n6. **`import pandas as pd`**:\n   - This imports the Pandas library, which is a powerful data manipulation and analysis library in Python, under the alias `pd` for convenience.\n\n```python\ntrain = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\nprint(\"Train shape\",train.shape)\ndisplay(train.head())\nprint()\n```\n\n7. **`train = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")`**:\n   - This reads a CSV file located at the specified path into a Pandas DataFrame named `train`.\n   - The path points to a CSV file in a Kaggle competition dataset directory.\n\n8. **`print(\"Train shape\",train.shape)`**:\n   - This prints the shape (number of rows and columns) of the `train` DataFrame to give a quick overview of the data's size.\n\n9. **`display(train.head())`**:\n   - This displays the first few rows of the `train` DataFrame. `display` is used here instead of `print` because it provides a better visual representation in Jupyter notebooks.\n\n10. **`print()`**:\n    - This prints an empty line for better readability in the output.\n\n```python\ntest = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\nprint(\"Test shape\",test.shape)\ndisplay(test.head())\n```\n\n11. **`test = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")`**:\n    - This reads a CSV file located at the specified path into a Pandas DataFrame named `test`.\n    - Similar to the previous line for the training data, this reads the testing data.\n\n12. **`print(\"Test shape\",test.shape)`**:\n    - This prints the shape (number of rows and columns) of the `test` DataFrame to give a quick overview of the data's size.\n\n13. **`display(test.head())`**:\n    - This displays the first few rows of the `test` DataFrame for a quick preview of the testing data.\n\nBy running this code, you are preparing your environment to use specific GPUs, importing necessary libraries, and loading and previewing training and testing datasets from CSV files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Importing StratifiedKFold from scikit-learn for stratified k-fold cross-validation\nfrom sklearn.model_selection import StratifiedKFold\n\n# Defining the number of folds for cross-validation\nFOLDS = 15\n# Initializing a new column \"fold\" in the training DataFrame with a default value of -1\ntrain[\"fold\"] = -1\n# Creating a StratifiedKFold object with the specified number of splits, shuffling, and a random state for reproducibility\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n# Splitting the data into stratified folds\nfor fold, (train_index, val_index) in enumerate(skf.split(train, train[\"score\"])):\n    # Assigning the fold number to the validation set indices\n    train.loc[val_index, \"fold\"] = fold\n\n# Printing the number of samples in each fold\nprint('Train samples per fold:')\n# Counting the occurrences of each fold value and sorting by fold number\nprint(train.fold.value_counts().sort_index())",
   "metadata": {
    "papermill": {
     "duration": 1.249232,
     "end_time": "2024-05-18T14:42:24.206997",
     "exception": false,
     "start_time": "2024-05-18T14:42:22.957765",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:36.841386Z",
     "iopub.execute_input": "2024-05-31T15:36:36.841648Z",
     "iopub.status.idle": "2024-05-31T15:36:37.303363Z",
     "shell.execute_reply.started": "2024-05-31T15:36:36.841628Z",
     "shell.execute_reply": "2024-05-31T15:36:37.302408Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Explanation of Each Section:\n\n1. **Importing the Necessary Module**:\n   ```python\n   # Importing StratifiedKFold from scikit-learn for stratified k-fold cross-validation\n   from sklearn.model_selection import StratifiedKFold\n   ```\n\n2. **Setting Up Folds**:\n   ```python\n   # Defining the number of folds for cross-validation\n   FOLDS = 15\n   # Initializing a new column \"fold\" in the training DataFrame with a default value of -1\n   train[\"fold\"] = -1\n   ```\n\n3. **Creating StratifiedKFold Object**:\n   ```python\n   # Creating a StratifiedKFold object with the specified number of splits, shuffling, and a random state for reproducibility\n   skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n   ```\n\n4. **Splitting Data into Stratified Folds**:\n   ```python\n   # Splitting the data into stratified folds\n   for fold, (train_index, val_index) in enumerate(skf.split(train, train[\"score\"])):\n       # Assigning the fold number to the validation set indices\n       train.loc[val_index, \"fold\"] = fold\n   ```\n\n5. **Printing Fold Distribution**:\n   ```python\n   # Printing the number of samples in each fold\n   print('Train samples per fold:')\n   # Counting the occurrences of each fold value and sorting by fold number\n   print(train.fold.value_counts().sort_index())\n   ```\n\n### Detailed Line-by-Line Comments:\n\n- **`from sklearn.model_selection import StratifiedKFold`**:\n  - Importing the `StratifiedKFold` class from the `sklearn.model_selection` module to perform stratified k-fold cross-validation, which ensures each fold is representative of the class distribution.\n\n- **`FOLDS = 15`**:\n  - Defining a constant `FOLDS` to specify the number of folds (15) for cross-validation.\n\n- **`train[\"fold\"] = -1`**:\n  - Adding a new column `fold` to the `train` DataFrame and initializing all its values to `-1`. This column will later store the fold assignments.\n\n- **`skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)`**:\n  - Creating a `StratifiedKFold` object with 15 splits, enabling shuffling of data before splitting, and setting a `random_state` for reproducibility.\n\n- **`for fold, (train_index, val_index) in enumerate(skf.split(train, train[\"score\"])):`**:\n  - Looping through the indices generated by the `skf.split()` method. The `skf.split()` method splits the `train` data into `FOLDS` folds based on the `score` column, ensuring stratified splits.\n  - `fold` is the fold number, `train_index` is the list of training indices for the current fold, and `val_index` is the list of validation indices for the current fold.\n\n- **`train.loc[val_index, \"fold\"] = fold`**:\n  - Assigning the current fold number to the `fold` column of the `train` DataFrame for the validation indices of the current fold.\n\n- **`print('Train samples per fold:')`**:\n  - Printing a header to indicate the start of the fold distribution output.\n\n- **`print(train.fold.value_counts().sort_index())`**:\n  - Printing the count of samples in each fold, sorted by fold number, to show the distribution of samples across the folds.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Generate AutoModel and AutoTokenizer",
   "metadata": {
    "papermill": {
     "duration": 0.009226,
     "end_time": "2024-05-18T14:42:24.22559",
     "exception": false,
     "start_time": "2024-05-18T14:42:24.216364",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Importing AutoModel and AutoTokenizer from the transformers library\nfrom transformers import AutoModel, AutoTokenizer\n\n# Importing torch and torch.nn.functional from the PyTorch library\nimport torch\nimport torch.nn.functional as F\n\n# Importing tqdm for progress bar functionality\nfrom tqdm import tqdm",
   "metadata": {
    "papermill": {
     "duration": 6.789362,
     "end_time": "2024-05-18T14:42:31.02308",
     "exception": false,
     "start_time": "2024-05-18T14:42:24.233718",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:37.304615Z",
     "iopub.execute_input": "2024-05-31T15:36:37.304942Z",
     "iopub.status.idle": "2024-05-31T15:36:39.701863Z",
     "shell.execute_reply.started": "2024-05-31T15:36:37.304902Z",
     "shell.execute_reply": "2024-05-31T15:36:39.70111Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "1. **Importing from `transformers`**:\n   ```python\n   # Importing AutoModel and AutoTokenizer from the transformers library\n   from transformers import AutoModel, AutoTokenizer\n   ```\n   - `AutoModel` is a generic model class from the `transformers` library. It allows you to load any pre-trained model.\n   - `AutoTokenizer` is a generic tokenizer class from the `transformers` library. It allows you to load any pre-trained tokenizer.\n   - The `transformers` library by Hugging Face provides state-of-the-art machine learning models, especially for natural language processing (NLP).\n\n2. **Importing from `torch`**:\n   ```python\n   # Importing torch and torch.nn.functional from the PyTorch library\n   import torch\n   import torch.nn.functional as F\n   ```\n   - `torch` is the main PyTorch library, used for tensor operations and various other functionalities.\n   - `torch.nn.functional` (imported as `F`) provides a range of functions useful for building neural networks, like activation functions, loss functions, and more.\n\n3. **Importing `tqdm`**:\n   ```python\n   # Importing tqdm for progress bar functionality\n   from tqdm import tqdm\n   ```\n   - `tqdm` is a library that provides fast, extensible progress bars for loops. It's useful for tracking the progress of tasks that involve iterating over datasets or any long-running processes.\n\n- **`from transformers import AutoModel, AutoTokenizer`**:\n  - Importing `AutoModel` and `AutoTokenizer` from the `transformers` library. These classes provide a convenient way to load pre-trained models and tokenizers for various NLP tasks without specifying the exact model or tokenizer architecture.\n\n- **`import torch`**:\n  - Importing the main PyTorch library. PyTorch is an open-source machine learning library used for applications such as natural language processing and computer vision.\n\n- **`import torch.nn.functional as F`**:\n  - Importing the `torch.nn.functional` module from PyTorch as `F`. This module contains functions that are used as operations on tensors, typically in the context of neural networks.\n\n- **`from tqdm import tqdm`**:\n  - Importing `tqdm`, a library for displaying progress bars. This is useful for monitoring the progress of loops and long-running processes, especially in data processing and training loops.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# model output and moving it to CPU memory",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def mean_pooling(model_output, attention_mask):\n    # Extracting the last hidden state from the model output and moving it to CPU memory\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    \n    # Expanding the attention mask dimensions to match the token embeddings\n    # This will make the attention mask compatible for element-wise multiplication with token embeddings\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    \n    # Performing element-wise multiplication of token embeddings and expanded attention mask\n    # Summing the embeddings along the sequence length (dim=1)\n    # Dividing by the sum of the expanded attention mask to compute the mean, while clamping to avoid division by zero\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )",
   "metadata": {
    "papermill": {
     "duration": 0.017157,
     "end_time": "2024-05-18T14:42:31.049218",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.032061",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:39.702967Z",
     "iopub.execute_input": "2024-05-31T15:36:39.7034Z",
     "iopub.status.idle": "2024-05-31T15:36:39.709947Z",
     "shell.execute_reply.started": "2024-05-31T15:36:39.703373Z",
     "shell.execute_reply": "2024-05-31T15:36:39.70879Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Explanation of Each Section:\n\n1. **Function Definition**:\n   ```python\n   def mean_pooling(model_output, attention_mask):\n   ```\n   - Defines a function named `mean_pooling` that takes two arguments: `model_output` and `attention_mask`.\n\n2. **Extracting Last Hidden State**:\n   ```python\n   # Extracting the last hidden state from the model output and moving it to CPU memory\n   token_embeddings = model_output.last_hidden_state.detach().cpu()\n   ```\n   - `model_output.last_hidden_state`: Accesses the last hidden state from the model output, which is typically a tensor of shape (batch_size, sequence_length, hidden_size).\n   - `.detach()`: Detaches the tensor from the computation graph, so no gradients will be calculated for it.\n   - `.cpu()`: Moves the tensor from GPU to CPU memory for further processing.\n\n3. **Expanding Attention Mask**:\n   ```python\n   # Expanding the attention mask dimensions to match the token embeddings\n   # This will make the attention mask compatible for element-wise multiplication with token embeddings\n   input_mask_expanded = (\n       attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n   )\n   ```\n   - `attention_mask.unsqueeze(-1)`: Adds an extra dimension to the attention mask tensor, changing its shape from (batch_size, sequence_length) to (batch_size, sequence_length, 1).\n   - `.expand(token_embeddings.size())`: Expands the attention mask to match the size of `token_embeddings`, so it can be used for element-wise multiplication. This changes the attention mask to have the same shape as `token_embeddings`.\n   - `.float()`: Converts the expanded attention mask to float type.\n\n4. **Mean Pooling Calculation**:\n   ```python\n   # Performing element-wise multiplication of token embeddings and expanded attention mask\n   # Summing the embeddings along the sequence length (dim=1)\n   # Dividing by the sum of the expanded attention mask to compute the mean, while clamping to avoid division by zero\n   return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n       input_mask_expanded.sum(1), min=1e-9\n   )\n   ```\n   - `torch.sum(token_embeddings * input_mask_expanded, 1)`: Performs element-wise multiplication of `token_embeddings` and `input_mask_expanded`, then sums along the sequence length dimension (dim=1).\n   - `input_mask_expanded.sum(1)`: Sums the expanded attention mask along the sequence length dimension to get the number of valid tokens for each sequence in the batch.\n   - `torch.clamp(..., min=1e-9)`: Clamps the sum to a minimum value of `1e-9` to avoid division by zero.\n   - The division computes the mean of the token embeddings, considering only the valid tokens (as indicated by the attention mask).\n\n### Summary:\nThis function `mean_pooling` performs mean pooling on the token embeddings produced by a transformer model. It uses the attention mask to ensure that padding tokens do not affect the mean calculation. The function first extracts the last hidden state from the model output, then uses the attention mask to calculate the mean of the valid token embeddings for each sequence in the batch.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Design and Develop PyTorch Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Defining a custom dataset class for embedding text using PyTorch's Dataset class\nclass EmbedDataset(torch.utils.data.Dataset):\n    # Initialization method to set up the dataset\n    def __init__(self, df, tokenizer, max_length):\n        # Storing the DataFrame after resetting its index\n        self.df = df.reset_index(drop=True)\n        # Storing the tokenizer to be used for tokenizing text\n        self.tokenizer = tokenizer\n        # Setting the maximum token length for padding/truncation\n        self.max = max_length\n\n    # Method to get the length of the dataset (number of samples)\n    def __len__(self):\n        return len(self.df)\n\n    # Method to get a single sample from the dataset\n    def __getitem__(self, idx):\n        # Retrieving the text at the specified index\n        text = self.df.loc[idx, \"full_text\"]\n        # Tokenizing the text using the provided tokenizer\n        tokens = self.tokenizer(\n            text,                       # The input text to tokenize\n            None,                       # No secondary input text\n            add_special_tokens=True,    # Add special tokens like [CLS] and [SEP]\n            padding='max_length',       # Pad the sequences to the maximum length\n            truncation=True,            # Truncate sequences to the maximum length\n            max_length=self.max,        # The maximum length for padding/truncation\n            return_tensors=\"pt\"         # Return PyTorch tensors\n        )\n        # Squeezing the tensor dimensions from (1, max_length) to (max_length)\n        tokens = {k: v.squeeze(0) for k, v in tokens.items()}\n        # Returning the tokenized input as the output\n        return tokens",
   "metadata": {
    "papermill": {
     "duration": 0.017755,
     "end_time": "2024-05-18T14:42:31.075745",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.05799",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:39.711093Z",
     "iopub.execute_input": "2024-05-31T15:36:39.711368Z",
     "iopub.status.idle": "2024-05-31T15:36:39.723347Z",
     "shell.execute_reply.started": "2024-05-31T15:36:39.711345Z",
     "shell.execute_reply": "2024-05-31T15:36:39.722435Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n### Class Definition\n- **Class Purpose**: The `EmbedDataset` class is a custom dataset class that extends PyTorch's `Dataset` class. It is designed to handle text data, tokenize it using a specified tokenizer, and prepare it for use in a machine learning model.\n\n### Initialization Method (`__init__`)\n- **Arguments**:\n  - `df`: A DataFrame containing the text data. This is typically a table where each row represents a text sample.\n  - `tokenizer`: An instance of a tokenizer from the `transformers` library, which is used to convert the text into token IDs that the model can process.\n  - `max_length`: The maximum length for token sequences. Texts longer than this length will be truncated, and shorter texts will be padded to this length.\n- **Purpose**: The `__init__` method initializes the dataset object. It stores the DataFrame, tokenizer, and maximum length as instance variables. It also resets the index of the DataFrame to ensure it starts from 0 and increments by 1.\n\n### Length Method (`__len__`)\n- **Purpose**: The `__len__` method returns the total number of samples in the dataset. It allows PyTorch's DataLoader to know how many samples there are in the dataset by simply calling this method.\n\n### Get Item Method (`__getitem__`)\n- **Arguments**:\n  - `idx`: An index that specifies which sample to retrieve from the dataset.\n- **Purpose**: The `__getitem__` method retrieves and processes a single sample from the dataset. It is called by PyTorch's DataLoader to get data during training or evaluation.\n\n### Detailed Steps in `__getitem__`:\n1. **Retrieve Text**: The method fetches the text data from the DataFrame at the specified index (`idx`). This text is usually in a column named `full_text`.\n\n2. **Tokenize Text**: The text is passed through the tokenizer, which converts the text into a sequence of token IDs. During this process:\n   - **Special Tokens**: The tokenizer adds special tokens (like `[CLS]` at the beginning and `[SEP]` at the end) which are required by some models.\n   - **Padding**: The sequence is padded to the maximum length specified during initialization. Padding ensures all sequences are of the same length, which is necessary for batch processing.\n   - **Truncation**: If the sequence is longer than the maximum length, it is truncated to fit within the specified length.\n   - **Return Tensors**: The tokenizer outputs the token IDs and other necessary components (like attention masks) as PyTorch tensors.\n\n3. **Squeeze Tensors**: The method processes the tensors to remove any unnecessary dimensions, converting them from 2D (batch_size, sequence_length) to 1D (sequence_length) for each token in the dictionary. This is typically needed because the tokenizer adds a batch dimension, even if there's only one sample.\n\n4. **Return Processed Tokens**: Finally, the method returns the processed tokenized data. This data includes the token IDs and possibly other components like attention masks, which are needed for model input.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Extract Embeddings",
   "metadata": {
    "papermill": {
     "duration": 0.008379,
     "end_time": "2024-05-18T14:42:31.092317",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.083938",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def get_embeddings(model_name='', max_length=1024, batch_size=32, compute_train=True, compute_test=True):\n    # Global variables for train and test dataframes\n    global train, test\n    \n    # Device for GPU processing\n    DEVICE = \"cuda:1\"  # EXTRACT EMBEDDINGS WITH GPU #2\n    \n    # Path to pre-trained model and tokenizer\n    path = \"/kaggle/input/download-huggingface-models/\"\n    disk_name = path + model_name.replace(\"/\", \"_\")\n    \n    # Load pre-trained model and tokenizer\n    model = AutoModel.from_pretrained(disk_name, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(disk_name, trust_remote_code=True)\n\n    # Create EmbedDataset and DataLoader for training data\n    ds_tr = EmbedDataset(train, tokenizer, max_length)\n    embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=False)\n    \n    # Create EmbedDataset and DataLoader for testing data\n    ds_te = EmbedDataset(test, tokenizer, max_length)\n    embed_dataloader_te = torch.utils.data.DataLoader(ds_te, batch_size=batch_size, shuffle=False)\n    \n    # Move model to GPU and set to evaluation mode\n    model = model.to(DEVICE)\n    model.eval()\n\n    # COMPUTE TRAIN EMBEDDINGS\n    all_train_text_feats = []\n    if compute_train:\n        # Iterate over batches of training data\n        for batch in tqdm(embed_dataloader_tr, total=len(embed_dataloader_tr)):\n            # Move batch to GPU\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            \n            # Compute embeddings with model inference\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled=True):\n                    model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n            \n            # Normalize embeddings\n            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n            sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            all_train_text_feats.extend(sentence_embeddings)\n    all_train_text_feats = np.array(all_train_text_feats)\n\n    # COMPUTE TEST EMBEDDINGS\n    all_test_text_feats = []\n    if compute_test:\n        # Iterate over batches of testing data\n        for batch in embed_dataloader_te:\n            # Move batch to GPU\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            \n            # Compute embeddings with model inference\n            with torch.no_grad():\n                with torch.cuda.amp.autocast(enabled=True):\n                    model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n            \n            # Normalize embeddings\n            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n            sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            all_test_text_feats.extend(sentence_embeddings)\n        all_test_text_feats = np.array(all_test_text_feats)\n    \n    # Clear memory\n    del ds_tr, ds_te\n    del embed_dataloader_tr, embed_dataloader_te\n    del model, tokenizer, model_output, sentence_embeddings, input_ids, attention_mask\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    # RETURN EMBEDDINGS\n    return all_train_text_feats, all_test_text_feats",
   "metadata": {
    "papermill": {
     "duration": 0.02622,
     "end_time": "2024-05-18T14:42:31.12706",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.10084",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:39.724889Z",
     "iopub.execute_input": "2024-05-31T15:36:39.725313Z",
     "iopub.status.idle": "2024-05-31T15:36:39.742071Z",
     "shell.execute_reply.started": "2024-05-31T15:36:39.725281Z",
     "shell.execute_reply": "2024-05-31T15:36:39.741126Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "1. **Setting Up Environment and Paths**:\n   - The function starts by setting up the environment, including the device (GPU), file paths, and loading the pre-trained model and tokenizer from the specified location.\n\n2. **Creating EmbedDataset and DataLoader**:\n   - Two instances of `EmbedDataset` are created for both the training and testing data.\n   - PyTorch `DataLoader` objects are initialized using these datasets. These loaders are used for iterating over the data in batches during the embedding extraction process.\n\n3. **Moving Model to GPU and Setting to Evaluation Mode**:\n   - The loaded model is moved to the specified GPU (`cuda:1`) and set to evaluation mode using `model.eval()`.\n\n4. **Computing Embeddings**:\n   - The function iterates over batches of data from the training and testing datasets.\n   - For each batch, it performs inference using the model to obtain sentence embeddings.\n   - The embeddings are then normalized and converted to NumPy arrays before being added to the respective lists (`all_train_text_feats` and `all_test_text_feats`).\n\n5. **Memory Management**:\n   - After computing embeddings, memory is cleared to release GPU resources using `del`, `gc.collect()`, and `torch.cuda.empty_cache()`.\n\n6. **Returning Embeddings**:\n   - Finally, the computed embeddings for both training and testing data are returned as NumPy arrays.\n\n### Main Points to Note:\n- **Memory Management**: The function efficiently manages memory to prevent memory leaks and optimize GPU usage by deleting unnecessary objects and clearing the GPU cache.\n- **Normalization**: Embeddings are normalized using L2 normalization to ensure consistent scales across embeddings.\n- **GPU Usage**: The function leverages GPU acceleration (`cuda:1`) for faster computation of embeddings.\n- **Data Iteration**: Data is iterated over in batches using PyTorch's DataLoader for efficient processing.\n- **Evaluation Mode**: The model is set to evaluation mode (`model.eval()`) to disable dropout layers and ensure consistent inference results.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# List of pre-trained transformer models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# List of pre-trained transformer models along with their corresponding parameters\nmodels = [\n    ('microsoft/deberta-base', 1024, 32),             # Microsoft DeBERTa Base model with max_length=1024, batch_size=32\n    ('microsoft/deberta-large', 1024, 8),             # Microsoft DeBERTa Large model with max_length=1024, batch_size=8\n    ('microsoft/deberta-v3-large', 1024, 8),          # Microsoft DeBERTa v3 Large model with max_length=1024, batch_size=8\n    ('allenai/longformer-base-4096', 1024, 32),       # AllenAI Longformer Base model with max_length=1024, batch_size=32\n    ('google/bigbird-roberta-base', 1024, 32),        # Google BigBird-RoBERTa Base model with max_length=1024, batch_size=32\n    ('google/bigbird-roberta-large', 1024, 8),        # Google BigBird-RoBERTa Large model with max_length=1024, batch_size=8\n]",
   "metadata": {
    "papermill": {
     "duration": 0.015905,
     "end_time": "2024-05-18T14:42:31.151518",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.135613",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:39.743075Z",
     "iopub.execute_input": "2024-05-31T15:36:39.743358Z",
     "iopub.status.idle": "2024-05-31T15:36:39.754782Z",
     "shell.execute_reply.started": "2024-05-31T15:36:39.743336Z",
     "shell.execute_reply": "2024-05-31T15:36:39.753954Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Directory path where embeddings are saved or loaded from\npath = \"/kaggle/input/essay-embeddings-v1/\"\n\n# Lists to store embeddings for all models\nall_train_embeds = []\nall_test_embeds = []\n\n# Loop through each model and associated parameters\nfor (model, max_length, batch_size) in models:\n    # Generate file name for the embeddings corresponding to the current model\n    name = path + model.replace(\"/\",\"_\") + \".npy\"\n    \n    # Check if embeddings file already exists\n    if os.path.exists(name):\n        # If embeddings file exists, load test embeddings directly\n        _, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=False)\n        # Load train embeddings from file\n        train_embed = np.load(name)\n        # Print message indicating loading of train embeddings\n        print(f\"Loading train embeddings for {name}\")\n    else:\n        # If embeddings file does not exist, compute both train and test embeddings\n        # Compute train and test embeddings using the get_embeddings function\n        print(f\"Computing train embeddings for {name}\")\n        train_embed, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=True)\n        # Save computed train embeddings to file\n        np.save(name, train_embed)\n    \n    # Append train and test embeddings to corresponding lists\n    all_train_embeds.append(train_embed)\n    all_test_embeds.append(test_embed)\n\n# Clean up memory by deleting variables train_embed and test_embed\ndel train_embed, test_embed",
   "metadata": {
    "papermill": {
     "duration": 56.552404,
     "end_time": "2024-05-18T14:43:27.712567",
     "exception": false,
     "start_time": "2024-05-18T14:42:31.160163",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:36:39.756032Z",
     "iopub.execute_input": "2024-05-31T15:36:39.75641Z",
     "iopub.status.idle": "2024-05-31T15:37:36.378223Z",
     "shell.execute_reply.started": "2024-05-31T15:36:39.756376Z",
     "shell.execute_reply": "2024-05-31T15:37:36.3772Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "1. **Path Definition**:\n   ```python\n   path = \"/kaggle/input/essay-embeddings-v1/\"\n   ```\n   - This line defines the directory path where embeddings are either saved or loaded from.\n\n2. **Initialization of Lists**:\n   ```python\n   all_train_embeds = []\n   all_test_embeds = []\n   ```\n   - These lists (`all_train_embeds` and `all_test_embeds`) are initialized to store embeddings for all models.\n\n3. **Looping Through Models**:\n   ```python\n   for (model, max_length, batch_size) in models:\n   ```\n   - This loop iterates over each tuple in the `models` list, unpacking the model name, maximum sequence length, and batch size for each iteration.\n\n4. **Generating File Name**:\n   ```python\n   name = path + model.replace(\"/\",\"_\") + \".npy\"\n   ```\n   - This line generates the file name for the embeddings corresponding to the current model by replacing the slashes in the model name with underscores and appending the \".npy\" extension.\n\n5. **Checking if Embeddings File Exists**:\n   ```python\n   if os.path.exists(name):\n   ```\n   - This conditional statement checks if the embeddings file already exists in the specified path.\n\n6. **Loading Existing Embeddings**:\n   ```python\n   _, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=False)\n   train_embed = np.load(name)\n   ```\n   - If the embeddings file exists, test embeddings are loaded directly using `get_embeddings`, and train embeddings are loaded from the `.npy` file.\n\n7. **Computing New Embeddings**:\n   ```python\n   else:\n       train_embed, test_embed = get_embeddings(model_name=model, max_length=max_length, batch_size=batch_size, compute_train=True)\n       np.save(name, train_embed)\n   ```\n   - If the embeddings file does not exist, both train and test embeddings are computed using the `get_embeddings` function. Train embeddings are then saved to the `.npy` file for future use.\n\n8. **Appending Embeddings to Lists**:\n   ```python\n   all_train_embeds.append(train_embed)\n   all_test_embeds.append(test_embed)\n   ```\n   - Train and test embeddings are appended to their respective lists for each model iteration.\n\n9. **Memory Cleanup**:\n   ```python\n   del train_embed, test_embed\n   ```\n   - Finally, the `train_embed` and `test_embed` variables are deleted to free up memory.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Combine Feature Embeddings",
   "metadata": {
    "papermill": {
     "duration": 0.009079,
     "end_time": "2024-05-18T14:43:27.731202",
     "exception": false,
     "start_time": "2024-05-18T14:43:27.722123",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Concatenate train embeddings horizontally along axis 1\nall_train_embeds = np.concatenate(all_train_embeds, axis=1)\n\n# Concatenate test embeddings horizontally along axis 1\nall_test_embeds = np.concatenate(all_test_embeds, axis=1)\n\n# Perform garbage collection to free up memory\ngc.collect()\n\n# Print the shape of the concatenated train embeddings\nprint('Our concatenated train embeddings have shape', all_train_embeds.shape)",
   "metadata": {
    "papermill": {
     "duration": 0.312154,
     "end_time": "2024-05-18T14:43:28.052282",
     "exception": false,
     "start_time": "2024-05-18T14:43:27.740128",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:37:36.382307Z",
     "iopub.execute_input": "2024-05-31T15:37:36.38276Z",
     "iopub.status.idle": "2024-05-31T15:37:36.672235Z",
     "shell.execute_reply.started": "2024-05-31T15:37:36.382733Z",
     "shell.execute_reply": "2024-05-31T15:37:36.671344Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define the MLP model\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n        super(MLP, self).__init__()\n        # Define the first fully connected layer\n        self.fc1 = nn.Linear(input_size, hidden_size1)\n        # Define the ReLU activation function\n        self.relu = nn.ReLU()\n        # Define the second fully connected layer\n        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n        # Define the ReLU activation function\n        self.relu = nn.ReLU()\n        # Define the output fully connected layer\n        self.fc3 = nn.Linear(hidden_size2, output_size)\n\n    def forward(self, x):\n        # Forward pass through the network\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n# Training function for the MLP model\ndef train_MLP(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        # Iterate over training batches\n        for inputs, labels in train_loader:\n            # Move inputs and labels to the appropriate device (GPU or CPU)\n            inputs, labels = inputs.to(device), labels.to(device)\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(inputs)\n            # Compute the loss\n            loss = criterion(outputs, labels)\n            # Backward pass\n            loss.backward()\n            # Update weights\n            optimizer.step()\n            # Accumulate the loss\n            running_loss += loss.item() * inputs.size(0)\n        \n        # Move validation data to the appropriate device\n        X_valid_tensor = X_valid_tensor.to(device)\n        # Perform validation by passing validation data through the model\n        preds = torch.argmax(model(X_valid_tensor), dim=1)\n        # Compute the QWK score for validation predictions\n        score = comp_score(y_valid, (preds + 1).cpu())    \n        # Compute average epoch loss\n        epoch_loss = running_loss / len(train_loader.dataset)\n        # Print training progress\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\" + f\" => QWK score: {score}\")\n\n# Hyperparameters\ninput_size = 5376       # Input vector dimensionality\nhidden_size1 = 3200     # Size of the first hidden layer\nhidden_size2 = 1600     # Size of the second hidden layer\noutput_size = 6         # Number of output classes\nlearning_rate = 0.001   # Learning rate\nnum_epochs = 8          # Number of training epochs\nbatch_size = 128        # Batch size\n\n# Initialize the model, loss function, and optimizer\nmodel = MLP(input_size, hidden_size1, hidden_size2, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)",
   "metadata": {
    "papermill": {
     "duration": 0.196074,
     "end_time": "2024-05-18T14:43:28.257655",
     "exception": false,
     "start_time": "2024-05-18T14:43:28.061581",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:37:36.673505Z",
     "iopub.execute_input": "2024-05-31T15:37:36.67379Z",
     "iopub.status.idle": "2024-05-31T15:37:36.859972Z",
     "shell.execute_reply.started": "2024-05-31T15:37:36.673766Z",
     "shell.execute_reply": "2024-05-31T15:37:36.859238Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n1. **Model Definition**:\n   - The script defines a Multilayer Perceptron (MLP) neural network using PyTorch's `nn.Module` class. This MLP consists of three fully connected layers (`fc1`, `fc2`, `fc3`) with ReLU activation functions between them. The `forward` method defines the forward pass of the network.\n\n2. **Training Function**:\n   - The `train_MLP` function is responsible for training the MLP model. It iterates over the specified number of epochs, performing forward and backward passes for each batch of training data. It also computes and accumulates the loss during training. After each epoch, it evaluates the model's performance on the validation set and prints the training progress.\n\n3. **Device Selection**:\n   - The code checks for the availability of a CUDA-enabled GPU and sets the device accordingly. This allows for GPU acceleration if a compatible GPU is available, otherwise, it falls back to CPU execution.\n\n4. **Hyperparameters**:\n   - Hyperparameters such as input size, hidden layer sizes, output size, learning rate, number of epochs, and batch size are defined. These parameters control the architecture and training process of the MLP model.\n\n5. **Loss Function and Optimizer**:\n   - The script specifies the cross-entropy loss function (`nn.CrossEntropyLoss`) and the Adam optimizer (`optim.Adam`) for training the MLP model. These components are essential for computing the loss and updating the model's weights during the training process.\n\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Design training and validation sets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import cohen_kappa_score\n\n# Function to compute the quadratic weighted kappa score\ndef comp_score(y_true, y_pred):\n    m = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    return m\n\n# Compute the sizes of the training and validation sets\ntrain_size = int(0.7 * len(train))\nvalid_size = len(train) - train_size\n\n# Shuffle the indices of the dataset\nindices = np.random.permutation(len(train))\n\n# Split the dataset into training and validation sets based on the specified proportions\ntrain_indices = indices[:train_size]\nvalid_indices = indices[train_size:]\n\n# Extract features and labels for training and validation sets\nX_train = all_train_embeds[train_indices,]\ny_train = train.loc[train_indices,'score'].values\nX_valid = all_train_embeds[valid_indices,]\ny_valid = train.loc[valid_indices,'score'].values\nX_test = all_test_embeds\n\n# Convert training and validation data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train)\ny_train_tensor = torch.tensor(y_train - 1)  # Adjust labels to start from 0\nX_valid_tensor = torch.tensor(X_valid)\ny_valid_tensor = torch.tensor(y_valid)\n\n# Create TensorDataset and DataLoader for training and validation data\ntrain_data = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_data = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\nval_loader = torch.utils.data.DataLoader(val_data, shuffle=False)\n\n# Call the train_MLP function to train the MLP model\n# train_MLP(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid)",
   "metadata": {
    "papermill": {
     "duration": 0.263716,
     "end_time": "2024-05-18T14:43:28.530394",
     "exception": false,
     "start_time": "2024-05-18T14:43:28.266678",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:37:36.860939Z",
     "iopub.execute_input": "2024-05-31T15:37:36.861183Z",
     "iopub.status.idle": "2024-05-31T15:37:37.116886Z",
     "shell.execute_reply.started": "2024-05-31T15:37:36.861162Z",
     "shell.execute_reply": "2024-05-31T15:37:37.116049Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport torch\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Function to compute the quadratic weighted kappa score\ndef comp_score(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Training function for the MLP model\ndef train_MLP(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            # Move inputs and labels to the appropriate device\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        model.eval()\n        with torch.no_grad():\n            # Move validation data to the appropriate device\n            X_valid_tensor = X_valid_tensor.to(device)\n            preds = torch.argmax(model(X_valid_tensor), dim=1)\n            score = comp_score(y_valid, (preds + 1).cpu())\n            epoch_loss = running_loss / len(train_loader.dataset)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\" + f\" => QWK score: {score}\")\n    return model\n\n# Assume you have defined model, criterion, optimizer, all_train_embeds, train, all_test_embeds, batch_size, num_epochs\n\n# Initialize 10-fold cross-validation\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nscores = []\n\n# Perform 10-fold cross-validation\nfor fold, (train_indices, valid_indices) in enumerate(kf.split(all_train_embeds)):\n    print(f'Fold {fold+1}')\n    \n    # Split data into training and validation sets\n    X_train = all_train_embeds[train_indices]\n    y_train = train.loc[train_indices, 'score'].values\n    X_valid = all_train_embeds[valid_indices]\n    y_valid = train.loc[valid_indices, 'score'].values\n    \n    # Convert data to PyTorch tensors\n    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    y_train_tensor = torch.tensor(y_train - 1, dtype=torch.long)  # Adjust labels to start from 0\n    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n    y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n    \n    # Create TensorDataset and DataLoader for training and validation data\n    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    val_data = TensorDataset(X_valid_tensor, y_valid_tensor)\n    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n    \n    # Reinitialize the model\n    model = MLP(input_size, hidden_size1, hidden_size2, output_size)  # Replace with your model class\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters())  # Choose optimizer as needed\n\n    # Train the model\n    model = train_MLP(model, criterion, optimizer, train_loader, num_epochs, X_valid_tensor, y_valid_tensor)\n    \n    # Validate the model\n    model.eval()\n    with torch.no_grad():\n        X_valid_tensor = X_valid_tensor.to(device)\n        valid_preds = model(X_valid_tensor).argmax(dim=1).cpu().numpy()\n\n    model_path = f'/kaggle/working/model_fold_{fold+1}.pth'\n    torch.save(model.state_dict(), model_path)\n    print(f'Model saved at {model_path}')\n    \n    # Compute score\n    score = comp_score(y_valid, valid_preds + 1)\n    scores.append(score)\n    print(f'Score for fold {fold+1}: {score}')\n\n# Compute the mean score for 10 folds\nmean_score = np.mean(scores)\nprint(f'Mean score: {mean_score}')",
   "metadata": {
    "papermill": {
     "duration": 120.335045,
     "end_time": "2024-05-18T14:45:28.874946",
     "exception": false,
     "start_time": "2024-05-18T14:43:28.539901",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:37:37.118349Z",
     "iopub.execute_input": "2024-05-31T15:37:37.118946Z",
     "iopub.status.idle": "2024-05-31T15:39:38.552827Z",
     "shell.execute_reply.started": "2024-05-31T15:37:37.118891Z",
     "shell.execute_reply": "2024-05-31T15:39:38.551764Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n1. **Imports**:\n   - The script imports necessary libraries such as NumPy, PyTorch, and scikit-learn's `cohen_kappa_score` for evaluating the model's performance.\n\n2. **Function Definitions**:\n   - `comp_score`: Computes the quadratic weighted kappa score between true labels and predicted labels. This function is used to evaluate the performance of the model during training.\n   - `train_MLP`: Function for training the MLP model. It iterates over the specified number of epochs, performing forward and backward passes for each batch of training data. After each epoch, it evaluates the model's performance on the validation set and prints the training progress.\n\n3. **Initialization**:\n   - Hyperparameters and necessary variables such as the number of epochs, batch size, and KFold cross-validation splitter are initialized.\n\n4. **K-Fold Cross-Validation**:\n   - The script uses K-Fold cross-validation with 10 folds to train and evaluate the model on different subsets of the data.\n   - Within each fold loop:\n     - The data is split into training and validation sets.\n     - PyTorch tensors and data loaders are created for both training and validation sets.\n     - The model is reinitialized, optimizer is chosen, and the model is trained using the `train_MLP` function.\n     - After training, the model's predictions on the validation set are computed, and the model is saved.\n     - The performance score for the fold is computed and printed.\n   - After all folds are completed, the mean score over 10 folds is computed and printed.\n\n5. **Explanation**:\n   - The script demonstrates a complete pipeline for training and evaluating an MLP model using K-Fold cross-validation. It ensures robust evaluation by training and testing the model on different subsets of the data. The mean score over multiple folds provides a more reliable estimate of the model's performance.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Loading models and making predictions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inference Phase: Loading models and making predictions\n\n# Convert test embeddings to PyTorch tensors and move to the appropriate device\nall_test_embeds_tensor = torch.tensor(all_test_embeds, dtype=torch.float32)\nall_test_embeds_tensor = all_test_embeds_tensor.to(device)\n\n# List to store predictions from each fold\nall_preds = []\n\n# Loop through each fold\nfor fold in range(10):\n    # Reinitialize the model\n    model = MLP(input_size, hidden_size1, hidden_size2, output_size)  # Replace with your model class\n    model.to(device)\n    \n    # Load the trained model parameters\n    model_path = f'/kaggle/working/model_fold_{fold+1}.pth'\n    model.load_state_dict(torch.load(model_path))\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Make predictions on test data\n    with torch.no_grad():\n        test_preds = model(all_test_embeds_tensor).argmax(dim=1).cpu().numpy()\n    \n    # Append predictions from this fold to the list\n    all_preds.append(test_preds + 1)  # Incrementing labels by 1 to match original indexing\n\n# Compute the final prediction for each sample\nfinal_preds = np.mean(all_preds, axis=0).round().astype(int)  # Using majority voting for aggregation",
   "metadata": {
    "papermill": {
     "duration": 2.59857,
     "end_time": "2024-05-18T14:45:31.489622",
     "exception": false,
     "start_time": "2024-05-18T14:45:28.891052",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:39:38.554028Z",
     "iopub.execute_input": "2024-05-31T15:39:38.554317Z",
     "iopub.status.idle": "2024-05-31T15:39:40.872133Z",
     "shell.execute_reply.started": "2024-05-31T15:39:38.554292Z",
     "shell.execute_reply": "2024-05-31T15:39:40.871111Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n1. **Model Inference**:\n   - This section loads the trained models saved during the cross-validation process and uses them to make predictions on the test data.\n   - The test data embeddings are converted to PyTorch tensors and moved to the appropriate device (CPU or GPU).\n   - For each fold:\n     - The model is reinitialized.\n     - The saved model parameters are loaded from the corresponding file.\n     - The model is set to evaluation mode (`model.eval()`) to disable dropout and batch normalization layers.\n     - With no gradient calculation, predictions are made on the test data using the loaded model.\n     - The predicted labels are converted to NumPy arrays and appended to `all_preds`.\n\n2. **Combining Predictions**:\n   - After predictions from all folds are collected in `all_preds`, the final prediction for each sample is computed.\n   - In this implementation, the final prediction for each sample is calculated by averaging the predictions from all folds (`np.mean(all_preds, axis=0)`).\n   - The averaged predictions are rounded to the nearest integer and cast to integers, representing the final predicted class labels.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Create Submission CSV",
   "metadata": {
    "papermill": {
     "duration": 0.014506,
     "end_time": "2024-05-18T14:45:31.558071",
     "exception": false,
     "start_time": "2024-05-18T14:45:31.543565",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Assign final predictions to test_preds variable\ntest_preds = final_preds\n\n# Print the shape of test_preds array\nprint('Test preds shape:', test_preds.shape)\n\n# Print the first 3 test predictions\nprint('First 3 test preds:', test_preds[:3])\n",
   "metadata": {
    "papermill": {
     "duration": 0.02267,
     "end_time": "2024-05-18T14:45:31.59574",
     "exception": false,
     "start_time": "2024-05-18T14:45:31.57307",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:39:40.873295Z",
     "iopub.execute_input": "2024-05-31T15:39:40.873588Z",
     "iopub.status.idle": "2024-05-31T15:39:40.879225Z",
     "shell.execute_reply.started": "2024-05-31T15:39:40.873564Z",
     "shell.execute_reply": "2024-05-31T15:39:40.878406Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Read the sample submission file\nsub = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n\n# Assign the final predictions to the \"score\" column of the submission dataframe\nsub[\"score\"] = test_preds\n\n# Convert the \"score\" column to int32 data type\nsub.score = sub.score.astype('int32')\n\n# Save the submission dataframe to a CSV file\nsub.to_csv(\"submission.csv\", index=False)\n\n# Print the shape of the submission dataframe\nprint(\"Submission shape:\", sub.shape)\n\n# Display the first few rows of the submission dataframe\nsub.head()",
   "metadata": {
    "papermill": {
     "duration": 0.035029,
     "end_time": "2024-05-18T14:45:31.646382",
     "exception": false,
     "start_time": "2024-05-18T14:45:31.611353",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-05-31T15:39:40.880482Z",
     "iopub.execute_input": "2024-05-31T15:39:40.880746Z",
     "iopub.status.idle": "2024-05-31T15:39:40.902849Z",
     "shell.execute_reply.started": "2024-05-31T15:39:40.880723Z",
     "shell.execute_reply": "2024-05-31T15:39:40.901985Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
