{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":6258399,"sourceType":"datasetVersion","datasetId":3596984}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summery: Featuring engineering + LGBM\nThis notebook is a modified from notebook provided by YE_AI [link](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=171264491). If you like my notebook remember to like YE_AI notebook also. \n\n***\n\n### Modification Done:\n1. Add sum, kurtosis, Quartile 1 and Quartile 3 to paragraph and sentence features.\n2. Add spelling mistake counting and Extra text processing.\n3. Add TFIDF vector.\n4. Add TFIDF word. \n\n***\n\n### Score:\n| Setup | Validation score | LB score |\n| :--- | :--- | :-- |\n| LGBM <br>+ feature engineering (Paragraph, sentence & word based features) | 0.7351538 | 0.780 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) | 0.7428280 | 0.782 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing | 0.7493708 | 0.790 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`) | 0.8022457 | 0.797 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Word TFIDF feature (`whole text is taken for TFIDF`) | 0.8019827 | 0.797 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Word TFIDF feature (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.8026114 | 0.796 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Update Word TFIDF feature (min_df=0.15 & max_df=0.85) (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.7996054 | 0.799 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Update Extra text processing [**[Link](#extra-feature)**] <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Update Word TFIDF feature (min_df=0.15 & max_df=0.85) (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.8006839 | 0.802 |\n\n**Extra text processing**:\n> * Contraction Expension e.g. I'll --> i will, this is added as a text processing step.\n> * Punctuation removal is applied:\n    - When extra features are generation [**[Link](#extra-feature)**]\n> \n> **Note**: The <u>Extra text processing</u> is taken from the notebook [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817)\n\n***\n\n### References:\n* Paragraph, sentence & word based features [source](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799).\n* Spelling mistake counting [here](https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect).\n* Extra text processing [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817?scriptVersionId=173223907&cellId=11).\n* TFIDF vector [here](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=172203959&cellId=16).\n* TFIDF word [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17).","metadata":{}},{"cell_type":"markdown","source":"# 1. Import modules","metadata":{}},{"cell_type":"code","source":"!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T13:49:14.017866Z","iopub.execute_input":"2024-05-14T13:49:14.018294Z","iopub.status.idle":"2024-05-14T13:49:48.814826Z","shell.execute_reply.started":"2024-05-14T13:49:14.018255Z","shell.execute_reply":"2024-05-14T13:49:48.813478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json, string\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\n# logging setting \n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:49:48.824923Z","iopub.execute_input":"2024-05-14T13:49:48.82532Z","iopub.status.idle":"2024-05-14T13:49:59.640778Z","shell.execute_reply.started":"2024-05-14T13:49:48.82527Z","shell.execute_reply":"2024-05-14T13:49:59.639675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load dataset and initial configuration","metadata":{}},{"cell_type":"code","source":"class PATHS:\n    train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n    test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\n    sub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:49:59.642287Z","iopub.execute_input":"2024-05-14T13:49:59.642554Z","iopub.status.idle":"2024-05-14T13:49:59.649804Z","shell.execute_reply.started":"2024-05-14T13:49:59.642524Z","shell.execute_reply":"2024-05-14T13:49:59.648255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    seed = 42\n    num_labels = 6","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:49:59.653662Z","iopub.execute_input":"2024-05-14T13:49:59.653984Z","iopub.status.idle":"2024-05-14T13:49:59.664125Z","shell.execute_reply.started":"2024-05-14T13:49:59.653918Z","shell.execute_reply":"2024-05-14T13:49:59.662787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(PATHS.train_path)\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:49:59.666114Z","iopub.execute_input":"2024-05-14T13:49:59.666376Z","iopub.status.idle":"2024-05-14T13:50:00.115558Z","shell.execute_reply.started":"2024-05-14T13:49:59.666338Z","shell.execute_reply":"2024-05-14T13:50:00.114405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(PATHS.test_path)\ntest.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:00.117246Z","iopub.execute_input":"2024-05-14T13:50:00.118135Z","iopub.status.idle":"2024-05-14T13:50:00.133054Z","shell.execute_reply.started":"2024-05-14T13:50:00.118095Z","shell.execute_reply":"2024-05-14T13:50:00.131905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Data preprocessing functions definations","metadata":{}},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\n\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Remove \\xa0\n    x = x.replace(u'\\xa0',' ')\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef remove_punctuation(text):\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:00.134507Z","iopub.execute_input":"2024-05-14T13:50:00.134803Z","iopub.status.idle":"2024-05-14T13:50:00.172884Z","shell.execute_reply.started":"2024-05-14T13:50:00.134765Z","shell.execute_reply":"2024-05-14T13:50:00.171478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Paragraph based feature\n\n<a id='paragraph-feature'></a>","metadata":{}},{"cell_type":"code","source":"# TODO: can be fixed by keeping \"\\n\" and removed empty paragraph entries\ncolumns = [(pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))]\ntrain = pl.from_pandas(train).with_columns(columns)\ntest = pl.from_pandas(test).with_columns(columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:00.174603Z","iopub.execute_input":"2024-05-14T13:50:00.175104Z","iopub.status.idle":"2024-05-14T13:50:00.365517Z","shell.execute_reply.started":"2024-05-14T13:50:00.175072Z","shell.execute_reply":"2024-05-14T13:50:00.364175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea],\n    ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\n\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:00.367507Z","iopub.execute_input":"2024-05-14T13:50:00.367825Z","iopub.status.idle":"2024-05-14T13:50:18.58945Z","shell.execute_reply.started":"2024-05-14T13:50:00.367785Z","shell.execute_reply":"2024-05-14T13:50:18.588748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Sentence based features\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    return tmp\n\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:18.590509Z","iopub.execute_input":"2024-05-14T13:50:18.591344Z","iopub.status.idle":"2024-05-14T13:50:36.785852Z","shell.execute_reply.started":"2024-05-14T13:50:18.591314Z","shell.execute_reply":"2024-05-14T13:50:36.784806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Word based feature\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:50:36.787171Z","iopub.execute_input":"2024-05-14T13:50:36.787406Z","iopub.status.idle":"2024-05-14T13:51:00.760524Z","shell.execute_reply.started":"2024-05-14T13:50:36.787378Z","shell.execute_reply":"2024-05-14T13:51:00.759791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.5 Character TFIDF feature:\n\nFor TFIDF vector generation we use TfidfVectorizer provided by [sickit-learn liberay](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n\n#### Terms:\n* **TF (Term frequency)**: Number of time a term occur in a document / Total number of term in the document.\n* **DF (Document frequency)**: Number of document where the term appear / Total number of document.\n* **IDF (Inverse Document Frequency)**: 1 / Document frequency\n***\n### TfidfVectorizer parameters:\n* **tokenizer**: Is set to `lambda x: x` which means the text will be passed as it is. \n* **preprocessor**: Is set to `lambda x: x` which means the text will be passed as it is.\n* **token_pattern**: Is not set to `None` means word will be taken as token as it is without any word-level processing.\n* **strip_accents**: Ts set to `unicode` which means include unicode characters during preprocessing step.\n* **analyzer**: Ts set to `word` which means the feature (terms or token) will be the words\n* **ngram_range**: ngram_range equal to `(1, 2)` which means unigrams and bigrams\n* **min_df**: Is equal to `0.05` means ignore terms that occur in less the 5% of documents.\n* **max_df**: Is equal to `0.95` means ignore terms that occur in more them 95% of documents.\n* **sublinear_tf**: Is equal to `True` means replace tf with 1 + log(tf)\n\n##### Note:\n* **tokenizer=lambda x: x**: \"`words are not tokenized from full-text? Tokenizer should only be overided by identity if text is already tokenized before. Perhaps vectorizer is receiving string (char sequence) instead of word sequence, so it behaves like a char ngram vectorizer`\" qouted from notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=11)","metadata":{}},{"cell_type":"code","source":"# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n\nprint(\"#\"*80)\nvect_feat_names=vectorizer.get_feature_names_out()\nprint(vect_feat_names[100:110])\nprint(\"#\"*80, \"\\n\\n\")\n\n# Convert to array\ndense_matrix = train_tfid.toarray()\n\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:51:00.761903Z","iopub.execute_input":"2024-05-14T13:51:00.76278Z","iopub.status.idle":"2024-05-14T13:52:40.39847Z","shell.execute_reply.started":"2024-05-14T13:51:00.762738Z","shell.execute_reply":"2024-05-14T13:52:40.397443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='tfidf-word-feature'></a>\n## 3.6 Word TFIDF feature:\n\nSource notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17)","metadata":{}},{"cell_type":"code","source":"stopwords_list = stopwords.words('english')\n# TfidfVectorizer parameter\nword_vectorizer = TfidfVectorizer(\n    strip_accents='ascii',\n    analyzer = 'word',\n    ngram_range=(1,1),\n    min_df=0.15,\n    max_df=0.85,\n    sublinear_tf=True,\n    stop_words=stopwords_list,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\nprocessed_text = train.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntrain_tfid = word_vectorizer.fit_transform([i for i in processed_text])\n\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# rename features\ntfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_w_columns\ndf['essay_id'] = train_feats['essay_id']\n\ndf.head()\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:52:40.403535Z","iopub.execute_input":"2024-05-14T13:52:40.403809Z","iopub.status.idle":"2024-05-14T13:53:06.589Z","shell.execute_reply.started":"2024-05-14T13:52:40.403778Z","shell.execute_reply":"2024-05-14T13:53:06.588005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='extra-feature'></a>\n## 3.7 Extra features:\nReference: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect","metadata":{}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self) -> None:\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        self.spellchecker = SpellChecker()\n\n    def spelling(self, text):\n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n        return amount_miss\n    \n    def count_sym(self, text, sym):\n        sym_count = 0\n        for l in text:\n            if l == sym:\n                sym_count += 1\n        return sym_count\n\n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        \n        # preprocessing the text\n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n        \n        # Text tokenization\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        \n        # essay length\n        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        \n        # essay word count\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        \n        # essay unique word count\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        \n        # essay sentence count\n        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('.')))\n        \n        # essay paragraph count\n        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('\\n\\n')))\n        \n        # count misspelling\n        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        print(\"Spelling mistake count done\")\n        \n        # ratio fullstop / text_length ** new\n        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n        \n        # ratio comma / text_length ** new\n        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n        \n        return data\n    \npreprocessor = Preprocessor()\ntmp = preprocessor.run(train.to_pandas(), mode=\"train\")\ntrain_feats = train_feats.merge(tmp, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:53:06.590717Z","iopub.execute_input":"2024-05-14T13:53:06.591421Z","iopub.status.idle":"2024-05-14T13:54:42.003394Z","shell.execute_reply.started":"2024-05-14T13:53:06.591378Z","shell.execute_reply":"2024-05-14T13:54:42.002692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.8 Test dataset featurization","metadata":{}},{"cell_type":"code","source":"# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# Word Tfidf\nprocessed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntest_w_tfid = word_vectorizer.transform([i for i in processed_text])\ndense_matrix = test_w_tfid.toarray()\ndf_w = pd.DataFrame(dense_matrix)\ntfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df_w.columns))]\ndf_w.columns = tfid_w_columns\ndf_w['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df_w, on='essay_id', how='left')\n\n# Extra feature\ntmp = preprocessor.run(test.to_pandas(), mode=\"train\")\ntest_feats = test_feats.merge(tmp, on='essay_id', how='left')\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.004791Z","iopub.execute_input":"2024-05-14T13:54:42.005064Z","iopub.status.idle":"2024-05-14T13:54:42.216819Z","shell.execute_reply.started":"2024-05-14T13:54:42.005034Z","shell.execute_reply":"2024-05-14T13:54:42.215762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data preparation","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Add k-fold details","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\nfor i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"])):\n    train_feats.loc[val_index, \"fold\"] = i\nprint(train_feats.shape)\n# train_feats.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.218148Z","iopub.execute_input":"2024-05-14T13:54:42.218362Z","iopub.status.idle":"2024-05-14T13:54:42.23871Z","shell.execute_reply.started":"2024-05-14T13:54:42.218337Z","shell.execute_reply":"2024-05-14T13:54:42.237727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Feature selection","metadata":{}},{"cell_type":"code","source":"target = \"score\"\ntrain_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"] + [target]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.240492Z","iopub.execute_input":"2024-05-14T13:54:42.240701Z","iopub.status.idle":"2024-05-14T13:54:42.244696Z","shell.execute_reply.started":"2024-05-14T13:54:42.240675Z","shell.execute_reply":"2024-05-14T13:54:42.243988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats.drop(columns=train_drop_columns).head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.245585Z","iopub.execute_input":"2024-05-14T13:54:42.245801Z","iopub.status.idle":"2024-05-14T13:54:42.417922Z","shell.execute_reply.started":"2024-05-14T13:54:42.245774Z","shell.execute_reply":"2024-05-14T13:54:42.416889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.419266Z","iopub.execute_input":"2024-05-14T13:54:42.420001Z","iopub.status.idle":"2024-05-14T13:54:42.424402Z","shell.execute_reply.started":"2024-05-14T13:54:42.41997Z","shell.execute_reply":"2024-05-14T13:54:42.423394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats.drop(columns=test_drop_columns).head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.426171Z","iopub.execute_input":"2024-05-14T13:54:42.426404Z","iopub.status.idle":"2024-05-14T13:54:42.454045Z","shell.execute_reply.started":"2024-05-14T13:54:42.426366Z","shell.execute_reply":"2024-05-14T13:54:42.452978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Training","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Evaluation function and loss function defination ","metadata":{}},{"cell_type":"code","source":"# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = (y_true + a).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\n\ndef qwk_param_calc(y):\n    a = y.mean()\n    b = (y ** 2).mean() - a**2\n    return np.round(a, 4), np.round(b, 4)\n# a = 2.948\n# b = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.455778Z","iopub.execute_input":"2024-05-14T13:54:42.456101Z","iopub.status.idle":"2024-05-14T13:54:42.466654Z","shell.execute_reply.started":"2024-05-14T13:54:42.456064Z","shell.execute_reply":"2024-05-14T13:54:42.465899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Training LGBMRegressor model","metadata":{}},{"cell_type":"code","source":"models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=CFG.seed, verbosity = - 1\n    )\n    \n    a, b = qwk_param_calc(train_feats[train_feats[\"fold\"] != fold][\"score\"])\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold].drop(columns=train_drop_columns)\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T13:54:42.468061Z","iopub.execute_input":"2024-05-14T13:54:42.468986Z","iopub.status.idle":"2024-05-14T14:00:58.365029Z","shell.execute_reply.started":"2024-05-14T13:54:42.468907Z","shell.execute_reply":"2024-05-14T14:00:58.36374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Validating LGBMRegressor model","metadata":{}},{"cell_type":"code","source":"preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nv_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {v_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:00:58.372056Z","iopub.execute_input":"2024-05-14T14:00:58.372334Z","iopub.status.idle":"2024-05-14T14:00:59.999639Z","shell.execute_reply.started":"2024-05-14T14:00:58.372305Z","shell.execute_reply":"2024-05-14T14:00:59.998532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.4 Testing and collecting prediction","metadata":{}},{"cell_type":"code","source":"# predecting for 5 models\npreds = []\nfor fold, model in enumerate(models):\n    X_eval_cv = test_feats.drop(columns=test_drop_columns)\n    # pred = model.predict(X_eval_cv)\n    pred = model.predict(X_eval_cv) + a\n    preds.append(pred)\n\n# Combining the 5 model results\nfor i, pred in enumerate(preds):\n    test_feats[f\"score_pred_{i}\"] = pred\ntest_feats[\"score\"] = np.round(test_feats[[f\"score_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1),0).astype('int32')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:01:00.001069Z","iopub.execute_input":"2024-05-14T14:01:00.001362Z","iopub.status.idle":"2024-05-14T14:01:00.100196Z","shell.execute_reply.started":"2024-05-14T14:01:00.001323Z","shell.execute_reply":"2024-05-14T14:01:00.099257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:01:00.10138Z","iopub.execute_input":"2024-05-14T14:01:00.102786Z","iopub.status.idle":"2024-05-14T14:01:00.128334Z","shell.execute_reply.started":"2024-05-14T14:01:00.102755Z","shell.execute_reply":"2024-05-14T14:01:00.127029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Submission","metadata":{}},{"cell_type":"code","source":"test_feats[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:01:00.129861Z","iopub.execute_input":"2024-05-14T14:01:00.130197Z","iopub.status.idle":"2024-05-14T14:01:00.140693Z","shell.execute_reply.started":"2024-05-14T14:01:00.130159Z","shell.execute_reply":"2024-05-14T14:01:00.139534Z"},"trusted":true},"execution_count":null,"outputs":[]}]}