{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 6258399,
     "sourceType": "datasetVersion",
     "datasetId": 3596984
    },
    {
     "sourceId": 8533513,
     "sourceType": "datasetVersion",
     "datasetId": 5044889
    }
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Summery: Featuring engineering + LGBM\nThis notebook is a modified from notebook provided by YE_AI [link](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=171264491). If you like my notebook remember to like YE_AI notebook also. \n\n***\n\n### Modification Done:\n1. Add sum, kurtosis, Quartile 1 and Quartile 3 to paragraph and sentence features.\n2. Add spelling mistake counting and Extra text processing.\n3. Add TFIDF vector.\n4. Add TFIDF word. \n\n***\n\n### Score:\n| Setup | Validation score | LB score |\n| :--- | :--- | :-- |\n| LGBM <br>+ feature engineering (Paragraph, sentence & word based features) | 0.7351538 | 0.780 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) | 0.7428280 | 0.782 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing | 0.7493708 | 0.790 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`) | 0.8022457 | 0.797 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Word TFIDF feature (`whole text is taken for TFIDF`) | 0.8019827 | 0.797 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Word TFIDF feature (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.8026114 | 0.796 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Extra text processing <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Update Word TFIDF feature (min_df=0.15 & max_df=0.85) (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.7996054 | 0.799 |\n| LGBM <br>+ feature engineering (Paragraph, sentence, word based features & spelling mistake counts) <br>+ Update Extra text processing [**[Link](#extra-feature)**] <br>+ Character TFIDF feature (`whole text is taken for TFIDF`)<br>+ Update Word TFIDF feature (min_df=0.15 & max_df=0.85) (`whole text is taken for TFIDF`) <br>+ calc a and b values based on Fold | 0.8006839 | 0.802 |\n\n**Extra text processing**:\n> * Contraction Expension e.g. I'll --> i will, this is added as a text processing step.\n> * Punctuation removal is applied:\n    - When extra features are generation [**[Link](#extra-feature)**]\n> \n> **Note**: The <u>Extra text processing</u> is taken from the notebook [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817)\n\n***\n\n### References:\n* Paragraph, sentence & word based features [source](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799).\n* Spelling mistake counting [here](https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect).\n* Extra text processing [here](https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817?scriptVersionId=173223907&cellId=11).\n* TFIDF vector [here](https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-cv-0-799-lb-0-799?scriptVersionId=172203959&cellId=16).\n* TFIDF word [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Import modules",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-05-29T16:28:26.561818Z",
     "iopub.execute_input": "2024-05-29T16:28:26.56211Z",
     "iopub.status.idle": "2024-05-29T16:29:01.183076Z",
     "shell.execute_reply.started": "2024-05-29T16:28:26.562083Z",
     "shell.execute_reply": "2024-05-29T16:29:01.181361Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport polars as pl\nimport warnings\nimport logging\nimport os\nimport shutil\nimport json, string\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset,load_dataset, load_from_disk\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, disable_progress_bar\nfrom sklearn.metrics import mean_squared_error\nimport torch\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nimport spacy\nimport re\nfrom spellchecker import SpellChecker\nimport lightgbm as lgb\n\n# logging setting \n\nwarnings.simplefilter(\"ignore\")\nlogging.disable(logging.ERROR)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \ndisable_progress_bar()\ntqdm.pandas()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:01.187675Z",
     "iopub.execute_input": "2024-05-29T16:29:01.188026Z",
     "iopub.status.idle": "2024-05-29T16:29:01.206691Z",
     "shell.execute_reply.started": "2024-05-29T16:29:01.187988Z",
     "shell.execute_reply": "2024-05-29T16:29:01.205031Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Load dataset and initial configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PATHS:\n    train_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv'\n    test_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv'\n    sub_path = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:01.20877Z",
     "iopub.execute_input": "2024-05-29T16:29:01.209131Z",
     "iopub.status.idle": "2024-05-29T16:29:01.223095Z",
     "shell.execute_reply.started": "2024-05-29T16:29:01.209088Z",
     "shell.execute_reply": "2024-05-29T16:29:01.221482Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class CFG:\n    n_splits = 5\n    seed = 42\n    num_labels = 6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:01.228058Z",
     "iopub.execute_input": "2024-05-29T16:29:01.228776Z",
     "iopub.status.idle": "2024-05-29T16:29:01.236185Z",
     "shell.execute_reply.started": "2024-05-29T16:29:01.228737Z",
     "shell.execute_reply": "2024-05-29T16:29:01.234625Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv(\"/kaggle/input/persuade-2-0/AES2-persuade.csv\")\n# train = train[(train[\"src\"] == \"kaggle-only\") | (train[\"src\"] == \"kaggle-persuade\")]\n\ntrain[\"fold\"] = 1\ntrain[(train[\"src\"] == \"kaggle-only\") | (train[\"src\"] == \"kaggle-persuade\")][\"fold\"] = 0\n\ntrain = train[[\"essay_id\", \"full_text\", \"score\", \"prompt_label\", \"fold\"]]\ntrain.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:01.238498Z",
     "iopub.execute_input": "2024-05-29T16:29:01.238896Z",
     "iopub.status.idle": "2024-05-29T16:29:02.1425Z",
     "shell.execute_reply.started": "2024-05-29T16:29:01.238853Z",
     "shell.execute_reply": "2024-05-29T16:29:02.141452Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test = pd.read_csv(PATHS.test_path)\ntest.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.144373Z",
     "iopub.execute_input": "2024-05-29T16:29:02.144647Z",
     "iopub.status.idle": "2024-05-29T16:29:02.158234Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.144616Z",
     "shell.execute_reply": "2024-05-29T16:29:02.156648Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.1 Data preprocessing functions definations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\n\ncList = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",  ## --> he had or he would\n    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",   ## --> I had or I would\n    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",   ## --> It had or It would\n    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",   ## --> It had or It would\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n    \"we'd\": \"we had\",\n    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n    \"when's\": \"when is\",\"when've\": \"when have\",\n    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n    \"you're\": \"you are\",  \"you've\": \"you have\"\n}\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef dataPreprocessing(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Remove \\xa0\n    x = x.replace(u'\\xa0',' ')\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n#     x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x\n\ndef remove_punctuation(text):\n    # string.punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef dataPreprocessing_w_contract_punct_remove(x):\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    x = removeHTML(x)\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = expandContractions(x)\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r'[^\\w\\s.,;:\"\"''?!]', '', x)\n    x = remove_punctuation(x)\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.160105Z",
     "iopub.execute_input": "2024-05-29T16:29:02.160337Z",
     "iopub.status.idle": "2024-05-29T16:29:02.188769Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.160311Z",
     "shell.execute_reply": "2024-05-29T16:29:02.18759Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 Paragraph based feature\n\n<a id='paragraph-feature'></a>",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TODO: can be fixed by keeping \"\\n\" and removed empty paragraph entries\ncolumns = [(pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))]\ntrain = pl.from_pandas(train).with_columns(columns)\ntest = pl.from_pandas(test).with_columns(columns)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.192275Z",
     "iopub.execute_input": "2024-05-29T16:29:02.193019Z",
     "iopub.status.idle": "2024-05-29T16:29:02.207589Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.192932Z",
     "shell.execute_reply": "2024-05-29T16:29:02.206777Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea],\n    ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\n\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.209377Z",
     "iopub.execute_input": "2024-05-29T16:29:02.209646Z",
     "iopub.status.idle": "2024-05-29T16:29:02.495568Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.209606Z",
     "shell.execute_reply": "2024-05-29T16:29:02.494226Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.3 Sentence based features\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    return tmp\n\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.501933Z",
     "iopub.execute_input": "2024-05-29T16:29:02.502242Z",
     "iopub.status.idle": "2024-05-29T16:29:02.774302Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.502212Z",
     "shell.execute_reply": "2024-05-29T16:29:02.773403Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.4 Word based feature\n\nsource: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:02.775714Z",
     "iopub.execute_input": "2024-05-29T16:29:02.776215Z",
     "iopub.status.idle": "2024-05-29T16:29:03.117196Z",
     "shell.execute_reply.started": "2024-05-29T16:29:02.776184Z",
     "shell.execute_reply": "2024-05-29T16:29:03.11579Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5 Character TFIDF feature:\n\nFor TFIDF vector generation we use TfidfVectorizer provided by [sickit-learn liberay](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n\n#### Terms:\n* **TF (Term frequency)**: Number of time a term occur in a document / Total number of term in the document.\n* **DF (Document frequency)**: Number of document where the term appear / Total number of document.\n* **IDF (Inverse Document Frequency)**: 1 / Document frequency\n***\n### TfidfVectorizer parameters:\n* **tokenizer**: Is set to `lambda x: x` which means the text will be passed as it is. \n* **preprocessor**: Is set to `lambda x: x` which means the text will be passed as it is.\n* **token_pattern**: Is not set to `None` means word will be taken as token as it is without any word-level processing.\n* **strip_accents**: Ts set to `unicode` which means include unicode characters during preprocessing step.\n* **analyzer**: Ts set to `word` which means the feature (terms or token) will be the words\n* **ngram_range**: ngram_range equal to `(1, 2)` which means unigrams and bigrams\n* **min_df**: Is equal to `0.05` means ignore terms that occur in less the 5% of documents.\n* **max_df**: Is equal to `0.95` means ignore terms that occur in more them 95% of documents.\n* **sublinear_tf**: Is equal to `True` means replace tf with 1 + log(tf)\n\n##### Note:\n* **tokenizer=lambda x: x**: \"`words are not tokenized from full-text? Tokenizer should only be overided by identity if text is already tokenized before. Perhaps vectorizer is receiving string (char sequence) instead of word sequence, so it behaves like a char ngram vectorizer`\" qouted from notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=11)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,3),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n\nprint(\"#\"*80)\nvect_feat_names=vectorizer.get_feature_names_out()\nprint(vect_feat_names[100:110])\nprint(\"#\"*80, \"\\n\\n\")\n\n# Convert to array\ndense_matrix = train_tfid.toarray()\n\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n\n# rename features\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\n\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:03.118572Z",
     "iopub.execute_input": "2024-05-29T16:29:03.118795Z",
     "iopub.status.idle": "2024-05-29T16:29:04.329668Z",
     "shell.execute_reply.started": "2024-05-29T16:29:03.11877Z",
     "shell.execute_reply": "2024-05-29T16:29:04.328586Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id='tfidf-word-feature'></a>\n## 3.6 Word TFIDF feature:\n\nSource notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=17)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "stopwords_list = stopwords.words('english')\n# TfidfVectorizer parameter\nword_vectorizer = TfidfVectorizer(\n    strip_accents='ascii',\n    analyzer = 'word',\n    ngram_range=(1,1),\n    min_df=0.15,\n    max_df=0.85,\n    sublinear_tf=True,\n    stop_words=stopwords_list,\n)\n# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\nprocessed_text = train.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntrain_tfid = word_vectorizer.fit_transform([i for i in processed_text])\n\n# Convert to array\ndense_matrix = train_tfid.toarray()\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\n# rename features\ntfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_w_columns\ndf['essay_id'] = train_feats['essay_id']\n\ndf.head()\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:04.330799Z",
     "iopub.execute_input": "2024-05-29T16:29:04.331679Z",
     "iopub.status.idle": "2024-05-29T16:29:04.696267Z",
     "shell.execute_reply.started": "2024-05-29T16:29:04.331643Z",
     "shell.execute_reply": "2024-05-29T16:29:04.69484Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a id='extra-feature'></a>\n## 3.7 Extra features:\nReference: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class Preprocessor:\n    def __init__(self) -> None:\n        self.twd = TreebankWordDetokenizer()\n        self.STOP_WORDS = set(stopwords.words('english'))\n        self.spellchecker = SpellChecker()\n\n    def spelling(self, text):\n        wordlist=text.split()\n        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n        return amount_miss\n    \n    def count_sym(self, text, sym):\n        sym_count = 0\n        for l in text:\n            if l == sym:\n                sym_count += 1\n        return sym_count\n\n    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n        \n        # preprocessing the text\n        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n        \n        # Text tokenization\n        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n        \n        # essay length\n        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n        \n        # essay word count\n        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n        \n        # essay unique word count\n        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n        \n        # essay sentence count\n        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('.')))\n        \n        # essay paragraph count\n        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('\\n\\n')))\n        \n        # count misspelling\n        data[\"splling_err_num\"] = data[\"processed_text\"].progress_apply(self.spelling)\n        print(\"Spelling mistake count done\")\n        \n        # ratio fullstop / text_length ** new\n        data[\"fullstop_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\".\")/len(x))\n        \n        # ratio comma / text_length ** new\n        data[\"comma_ratio\"] = data[\"full_text\"].apply(lambda x: x.count(\",\")/len(x))\n        \n        return data\n    \npreprocessor = Preprocessor()\ntmp = preprocessor.run(train.to_pandas(), mode=\"train\")\ntrain_feats = train_feats.merge(tmp, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:04.69785Z",
     "iopub.execute_input": "2024-05-29T16:29:04.698079Z",
     "iopub.status.idle": "2024-05-29T16:29:05.960055Z",
     "shell.execute_reply.started": "2024-05-29T16:29:04.698052Z",
     "shell.execute_reply": "2024-05-29T16:29:05.958859Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.8 Test dataset featurization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# Word Tfidf\nprocessed_text = test.to_pandas()[\"full_text\"].progress_apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\ntest_w_tfid = word_vectorizer.transform([i for i in processed_text])\ndense_matrix = test_w_tfid.toarray()\ndf_w = pd.DataFrame(dense_matrix)\ntfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df_w.columns))]\ndf_w.columns = tfid_w_columns\ndf_w['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df_w, on='essay_id', how='left')\n\n# Extra feature\ntmp = preprocessor.run(test.to_pandas(), mode=\"train\")\ntest_feats = test_feats.merge(tmp, on='essay_id', how='left')\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:05.962908Z",
     "iopub.execute_input": "2024-05-29T16:29:05.963252Z",
     "iopub.status.idle": "2024-05-29T16:29:06.17069Z",
     "shell.execute_reply.started": "2024-05-29T16:29:05.963212Z",
     "shell.execute_reply": "2024-05-29T16:29:06.169351Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Data preparation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.1 Add k-fold details",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# skf = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n# for i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"], train_feats[\"prompt_label\"])):\n#     train_feats.loc[val_index, \"fold\"] = i\n# print(train_feats.shape)\n# train_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.172379Z",
     "iopub.execute_input": "2024-05-29T16:29:06.172715Z",
     "iopub.status.idle": "2024-05-29T16:29:06.178253Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.172682Z",
     "shell.execute_reply": "2024-05-29T16:29:06.176968Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Feature selection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "target = \"score\"\ntrain_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\", \"prompt_label\"] + [target]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.179872Z",
     "iopub.execute_input": "2024-05-29T16:29:06.180143Z",
     "iopub.status.idle": "2024-05-29T16:29:06.189703Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.180114Z",
     "shell.execute_reply": "2024-05-29T16:29:06.188468Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_feats.drop(columns=train_drop_columns).head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.191188Z",
     "iopub.execute_input": "2024-05-29T16:29:06.191448Z",
     "iopub.status.idle": "2024-05-29T16:29:06.226216Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.191422Z",
     "shell.execute_reply": "2024-05-29T16:29:06.224995Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.228333Z",
     "iopub.execute_input": "2024-05-29T16:29:06.228801Z",
     "iopub.status.idle": "2024-05-29T16:29:06.232632Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.228772Z",
     "shell.execute_reply": "2024-05-29T16:29:06.231789Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats.drop(columns=test_drop_columns).head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.233757Z",
     "iopub.execute_input": "2024-05-29T16:29:06.234222Z",
     "iopub.status.idle": "2024-05-29T16:29:06.265923Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.234188Z",
     "shell.execute_reply": "2024-05-29T16:29:06.264502Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Training",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.1 Evaluation function and loss function defination ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = (y_true + a).round()\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\n\ndef qwk_param_calc(y):\n    a = y.mean()\n    b = (y ** 2).mean() - a**2\n    return np.round(a, 4), np.round(b, 4)\n# a = 2.948\n# b = 1.092",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.267734Z",
     "iopub.execute_input": "2024-05-29T16:29:06.268028Z",
     "iopub.status.idle": "2024-05-29T16:29:06.281512Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.267996Z",
     "shell.execute_reply": "2024-05-29T16:29:06.280565Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.2 Training LGBMRegressor model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "models = []\n\ncallbacks = [\n    lgb.log_evaluation(period=25), \n    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n]\nfor fold in range(CFG.n_splits):\n\n    model = lgb.LGBMRegressor(\n        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n        n_estimators=1024, random_state=CFG.seed, verbosity = - 1\n    )\n    \n    a, b = qwk_param_calc(train_feats[train_feats[\"fold\"] != fold][\"score\"])\n    \n    # Take out the training and validation sets for 5 kfold segmentation separately\n    X_train = train_feats[train_feats[\"fold\"] != fold].drop(columns=train_drop_columns)\n    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n\n    X_eval = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n\n    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n    print(f\"Fold {fold} a: {a}  ;;  b: {b}\")\n    # Training model\n    lgb_model = model.fit(\n        X_train, y_train,\n        eval_names=['train', 'valid'],\n        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n        eval_metric=quadratic_weighted_kappa,\n        callbacks=callbacks\n    )\n    models.append(model)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.283539Z",
     "iopub.execute_input": "2024-05-29T16:29:06.28415Z",
     "iopub.status.idle": "2024-05-29T16:29:06.929509Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.284108Z",
     "shell.execute_reply": "2024-05-29T16:29:06.927613Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.3 Validating LGBMRegressor model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "preds, trues = [], []\n    \nfor fold, model in enumerate(models):\n    X_eval_cv = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n\n    pred = model.predict(X_eval_cv) + a\n    \n    trues.extend(y_eval_cv)\n    preds.extend(np.round(pred, 0))\n\nv_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n\nprint(f\"Validation score : {v_score}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.931041Z",
     "iopub.status.idle": "2024-05-29T16:29:06.931808Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.931482Z",
     "shell.execute_reply": "2024-05-29T16:29:06.931508Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5.4 Testing and collecting prediction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# predecting for 5 models\npreds = []\nfor fold, model in enumerate(models):\n    X_eval_cv = test_feats.drop(columns=test_drop_columns)\n    # pred = model.predict(X_eval_cv)\n    pred = model.predict(X_eval_cv) + a\n    preds.append(pred)\n\n# Combining the 5 model results\nfor i, pred in enumerate(preds):\n    test_feats[f\"score_pred_{i}\"] = pred\ntest_feats[\"score\"] = np.round(test_feats[[f\"score_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1),0).astype('int32')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.933507Z",
     "iopub.status.idle": "2024-05-29T16:29:06.934339Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.934046Z",
     "shell.execute_reply": "2024-05-29T16:29:06.934079Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "test_feats.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.936217Z",
     "iopub.status.idle": "2024-05-29T16:29:06.93777Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.937337Z",
     "shell.execute_reply": "2024-05-29T16:29:06.937373Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "test_feats[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T16:29:06.939597Z",
     "iopub.status.idle": "2024-05-29T16:29:06.93999Z",
     "shell.execute_reply.started": "2024-05-29T16:29:06.939784Z",
     "shell.execute_reply": "2024-05-29T16:29:06.939802Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
