{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8242528,"sourceType":"datasetVersion","datasetId":4889642},{"sourceId":173832210,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>1. Initial Setting</b></div>","metadata":{}},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Import Libraries</b>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc \nimport ctypes\nimport random\nimport time\nimport string\nimport re\nfrom tqdm import tqdm\nimport pickle\n\nimport pandas as pd, numpy as np\nimport polars as pl # For Feature Engineering\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import words\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import VotingRegressor\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # For GPU T4x2\n\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:50.971214Z","iopub.execute_input":"2024-04-25T11:24:50.972007Z","iopub.status.idle":"2024-04-25T11:24:56.108831Z","shell.execute_reply.started":"2024-04-25T11:24:50.971955Z","shell.execute_reply":"2024-04-25T11:24:56.107327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Configure</b>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    SEED = 2024\n    VER = 1\n    LOAD_MODELS_FROM = '/kaggle/input/aes2-catboost/'\n    LOAD_FEATURES_FROM = '/kaggle/input/aes2-catboost/train_feats_1.csv'\n    BASE_PATH = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:56.112085Z","iopub.execute_input":"2024-04-25T11:24:56.112739Z","iopub.status.idle":"2024-04-25T11:24:56.119348Z","shell.execute_reply.started":"2024-04-25T11:24:56.112695Z","shell.execute_reply":"2024-04-25T11:24:56.11801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Clean Memory</b>","metadata":{}},{"cell_type":"code","source":"Clean = True\n\ndef clean_memory():\n    if Clean:\n        ctypes.CDLL('libc.so.6').malloc_trim(0)\n        gc.collect()\n        \nclean_memory()  ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:56.121221Z","iopub.execute_input":"2024-04-25T11:24:56.12161Z","iopub.status.idle":"2024-04-25T11:24:56.244162Z","shell.execute_reply.started":"2024-04-25T11:24:56.121582Z","shell.execute_reply":"2024-04-25T11:24:56.242448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b><span style='color:#FFA500'> | </span> Seed Everything</b>","metadata":{}},{"cell_type":"code","source":"def seed_everything(): # To proudce simliar result in each run \n    random.seed(CFG.SEED)\n    np.random.seed(CFG.SEED)\n    os.environ['PYTHONHASHSEED'] = str(CFG.SEED)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:56.249093Z","iopub.execute_input":"2024-04-25T11:24:56.249938Z","iopub.status.idle":"2024-04-25T11:24:56.265581Z","shell.execute_reply.started":"2024-04-25T11:24:56.24988Z","shell.execute_reply":"2024-04-25T11:24:56.263854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>2. Road and Read Data</b></div>","metadata":{}},{"cell_type":"code","source":"def seed_everything(): # To proudce simliar result in each run \n    random.seed(CFG.SEED)\n    np.random.seed(CFG.SEED)\n    os.environ['PYTHONHASHSEED'] = str(CFG.SEED)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:56.267566Z","iopub.execute_input":"2024-04-25T11:24:56.268365Z","iopub.status.idle":"2024-04-25T11:24:56.279746Z","shell.execute_reply.started":"2024-04-25T11:24:56.268321Z","shell.execute_reply":"2024-04-25T11:24:56.278809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(CFG.BASE_PATH + 'train.csv')\ndf_train = df_train.sort_values(by='essay_id')\n\nprint('Shape of Train: ', df_train.shape)\nprint(display(df_train.head()))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:56.281685Z","iopub.execute_input":"2024-04-25T11:24:56.285328Z","iopub.status.idle":"2024-04-25T11:24:57.226686Z","shell.execute_reply.started":"2024-04-25T11:24:56.285247Z","shell.execute_reply":"2024-04-25T11:24:57.224785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(CFG.BASE_PATH + 'test.csv')\ndf_test = df_test.sort_values(by='essay_id')\n\nprint('Shape of Test: ', df_test.shape)\nprint(display(df_test.head()))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:57.228734Z","iopub.execute_input":"2024-04-25T11:24:57.229301Z","iopub.status.idle":"2024-04-25T11:24:57.253012Z","shell.execute_reply.started":"2024-04-25T11:24:57.229256Z","shell.execute_reply":"2024-04-25T11:24:57.251232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.path.join(CFG.BASE_PATH,'train.csv')\n# CFG.BASE_PATH + 'train.csv'\n\ntrain = pl.from_pandas(df_train).with_columns(\n    pl.col('full_text').str.split(by='\\n\\n').alias('paragraph'))\ntest = pl.from_pandas(df_test).with_columns(\n    pl.col('full_text').str.split(by='\\n\\n').alias('paragraph')\n)\n\nschema_train = train.schema # MetaData\nschema_test = test.schema # MetaData","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:57.254976Z","iopub.execute_input":"2024-04-25T11:24:57.25636Z","iopub.status.idle":"2024-04-25T11:24:57.564293Z","shell.execute_reply.started":"2024-04-25T11:24:57.256305Z","shell.execute_reply":"2024-04-25T11:24:57.562423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>3. FeatureGenerator</b></div>","metadata":{}},{"cell_type":"markdown","source":"ðŸ“Œ **Features & TFIDF**\n\nhttps://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments","metadata":{}},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',x) # html -> ''\n\ndef dataPreprocessing(x):\n    \n    x = x.lower()\n    # remove html\n    x = removeHTML(x)\n    \n    #remove string with starting @\n    x = re.sub(\"@\\w+\",'',x)\n    \n    # remove number\n    x = re.sub(\"'\\d+\",'',x)\n    x = re.sub(\"\\d+\",'',x)\n    \n    # remove url\n    x = re.sub(\"http\\w+\",'',x)\n    \n    # Replace consecutive empty spaces with a single empty spaces\n    x = re.sub(r'\\s+', \" \", x)\n    # Replace consecutive commas and periods with a single comma and period\n    x = expandContractions(x)\n    x = re.sub(r'\\.+', \".\",x)\n    x = re.sub(r'\\,+', \",\",x)\n    x = re.sub(r'[^\\w\\s.,;:\"''?!]', '', x)\n    # Remove empty character at the beginning and end\n    x = x.strip()\n    return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q --no-index /kaggle/input/aes-2-misspelled-whl/pyspellchecker-0.8.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:24:57.579965Z","iopub.execute_input":"2024-04-25T11:24:57.580872Z","iopub.status.idle":"2024-04-25T11:25:14.400714Z","shell.execute_reply.started":"2024-04-25T11:24:57.580805Z","shell.execute_reply":"2024-04-25T11:25:14.398529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef count_misspelled_words(text):\n    misspelled_words = spell.unknown(text.split())\n    return len(misspelled_words)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:25:14.403211Z","iopub.execute_input":"2024-04-25T11:25:14.40372Z","iopub.status.idle":"2024-04-25T11:25:14.662152Z","shell.execute_reply.started":"2024-04-25T11:25:14.403682Z","shell.execute_reply":"2024-04-25T11:25:14.660681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paragraph_features = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt',\"paragraph_comma_cnt\",'paragraph_misspelled_cnt']\n\ndef Paragraph_Features(x):\n    # Expand the paragraph list to several lines of data\n    x = x.explode('paragraph') \n   \n    print('Paragraph Preprocessing')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(dataPreprocessing)\n    )\n    print('Caculate the length of each paragraph')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\")\n    )\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: count_misspelled_words(x)).alias('paragraph_misspelled_cnt')\n    )\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: x.count(',')).alias(\"paragraph_comma_cnt\")\n    )\n    print('Caculate the number of sentences and words in each paragraph')\n    x = x.with_columns(\n         pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias('paragraph_sentence_cnt'),\n         pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias('paragraph_word_cnt')\n    )\n    return x\n\ndef Paragraph_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f'paragraph_{i}_cnt') for i in [100,150, 200, 250, 300, 350, 400, 450, 500, 600, 800]],\n    *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f'paragraph_{i}_cnt_v2') for i in [100,200]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 300) & (pl.col('paragraph_len') > 100)).count().alias(f'short_paragraph_cnt')],    \n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 500) & (pl.col('paragraph_len') > 300)).count().alias(f'mid_paragraph_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_len') <= 700) & (pl.col('paragraph_len') > 500)).count().alias(f'long_paragraph_cnt')],\n    *[pl.col('paragraph').filter(pl.col('paragraph_sentence_cnt') >= i).count().alias(f'paragraph_sentence_{i}_cnt') for i in [2,4,6,8,10]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 4) & (pl.col('paragraph_sentence_cnt') > 2)).count().alias(f'short_paragraph_sentence_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 8) & (pl.col('paragraph_sentence_cnt') > 4)).count().alias(f'mid_paragraph_sentence_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_sentence_cnt') <= 10) & (pl.col('paragraph_sentence_cnt') > 8)).count().alias(f'long_paragraph_sentence_cnt')],    \n    *[pl.col('paragraph').filter(pl.col('paragraph_word_cnt') >= i).count().alias(f'paragraph_word_{i}_cnt') for i in [20,40,60,90,120]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 40) & (pl.col('paragraph_word_cnt') > 20)).count().alias(f'short_paragraph_word_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 90) & (pl.col('paragraph_word_cnt') > 40)).count().alias(f'mid_paragraph_word_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_word_cnt') <= 120) & (pl.col('paragraph_word_cnt') > 90)).count().alias(f'long_paragraph_word_cnt')],\n    *[pl.col('paragraph').filter(pl.col('paragraph_comma_cnt') >= i).count().alias(f'paragraph_comma_{i}_cnt') for i in [1,2,3,4,5]],\n    *[pl.col('paragraph').filter(pl.col('paragraph_misspelled_cnt') >= i).count().alias(f'paragraph_misspelled_{i}_cnt') for i in [4,8,12,16]],\n    *[pl.col('paragraph').filter(pl.col('paragraph_misspelled_cnt') <= i).count().alias(f'paragraph_misspelled_{i}_cnt_v2') for i in [2,4]],\n    *[pl.col('paragraph').filter((pl.col('paragraph_misspelled_cnt') <= 8) & (pl.col('paragraph_misspelled_cnt') > 4)).count().alias(f'short_paragraph_misspelled_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_misspelled_cnt') <= 12) & (pl.col('paragraph_misspelled_cnt') > 8)).count().alias(f'mid_paragraph_misspelled_cnt')],\n    *[pl.col('paragraph').filter((pl.col('paragraph_misspelled_cnt') <= 16) & (pl.col('paragraph_misspelled_cnt') > 12)).count().alias(f'long_paragraph_misspelled_cnt')],\n    *[pl.col('paragraph').count().alias('paragraph_cnt')], \n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in paragraph_features],\n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in paragraph_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in paragraph_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in paragraph_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in paragraph_features],\n    *[pl.col(feat).quantile(0.25).alias(f'{feat}_q1') for feat in paragraph_features],\n    *[pl.col(feat).quantile(0.75).alias(f'{feat}_q3') for feat in paragraph_features],   \n    ]\n      \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:25:14.663945Z","iopub.execute_input":"2024-04-25T11:25:14.664395Z","iopub.status.idle":"2024-04-25T11:25:14.706078Z","shell.execute_reply.started":"2024-04-25T11:25:14.664358Z","shell.execute_reply":"2024-04-25T11:25:14.704702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_features = ['sentence_len','sentence_word_cnt','sentence_misspelled_cnt']\n\ndef Sentence_Features(x):\n    print('Preprocess full_text and use periods to segment sentences in the text')\n    x = x.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x)).str.split(\".\").alias('sentence')\n    )\n    x = x.explode('sentence')\n    \n    print('Caculate the length of a sentence') \n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: len(x)).alias('sentence_len'))\n    x = x.filter(pl.col('sentence_len') > 3)\n    x = x.with_columns(\n         pl.col('sentence').map_elements(lambda x: count_misspelled_words(x)).alias('sentence_misspelled_cnt')\n    ) \n    x = x.with_columns(\n       pl.col('sentence').map_elements(lambda x: len(x.replace(' ', ''))).alias('only_sentence_len'))\n    print('Count the number of words in each sentence')\n    x = x.with_columns(\n        pl.col('sentence').map_elements(lambda x: len(x.split(\" \"))).alias(\"sentence_word_cnt\"))\n    return x\n\ndef Sentence_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f'sentence_{i}_cnt') for i in [40,60,70,80,100,120,140]],    \n    *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f'sentence_{i}_cnt_v2') for i in [10,20,30]],\n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 70) & (pl.col('sentence_len') > 40)).count().alias(f'short_sentence_cnt')],    \n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 100) & (pl.col('sentence_len') > 70)).count().alias(f'mid_sentence_cnt')], \n    *[pl.col('sentence').filter((pl.col('sentence_len') <= 140) & (pl.col('sentence_len') > 100)).count().alias(f'long_sentence_cnt')],\n    *[pl.col('sentence').filter(pl.col('only_sentence_len') >= i).count().alias(f'only_sentence_{i}_cnt') for i in [40,60,80,100,120]],    \n    *[pl.col('sentence').filter((pl.col('only_sentence_len') <= 60) & (pl.col('only_sentence_len') > 40)).count().alias(f'short_only_sentence_cnt')],    \n    *[pl.col('sentence').filter((pl.col('only_sentence_len') <= 100) & (pl.col('only_sentence_len') > 60)).count().alias(f'mid_only_sentence_cnt')], \n    *[pl.col('sentence').filter((pl.col('only_sentence_len') <= 120) & (pl.col('only_sentence_len') > 100)).count().alias(f'long_only_sentence_cnt')],\n    *[pl.col('sentence').filter(pl.col('sentence_word_cnt') >= i).count().alias(f'sentence_word_{i}_cnt') for i in [10,15,20,25]], \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 15) & (pl.col('sentence_word_cnt') > 10)).count().alias(f'short_sentence_word_cnt')],    \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 20) & (pl.col('sentence_word_cnt') > 15)).count().alias(f'mid_sentence_word_cnt')], \n    *[pl.col('sentence').filter((pl.col('sentence_word_cnt') <= 25) & (pl.col('sentence_word_cnt') > 20)).count().alias(f'long_sentence_word_cnt')],    \n    *[pl.col('sentence').count().alias('sentence_cnt')],    \n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in sentence_features],\n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in sentence_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in sentence_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in sentence_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in sentence_features], \n    *[pl.col(feat).quantile(0.25).alias(f'{feat}_q1') for feat in sentence_features], \n    *[pl.col(feat).quantile(0.75).alias(f'{feat}_q3') for feat in sentence_features],     \n    ]\n    \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.with_columns(\n     *[(pl.col(f'sentence_{i}_cnt')/pl.col('sentence_cnt')).alias(f'sentence_{i}_cnt_ratio') for i in [40,60,70,80,100,120,140]],    \n    *[(pl.col('short_sentence_cnt')/pl.col('sentence_cnt')).alias(f'short_sentence_cnt_ratio')],\n    *[(pl.col('mid_sentence_cnt')/pl.col('sentence_cnt')).alias(f'mid_sentence_cnt_ratio')],\n    *[(pl.col('long_sentence_cnt')/pl.col('sentence_cnt')).alias(f'long_sentence_cnt_ratio')],    \n    ).sort('essay_id')\n    \n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:25:14.708294Z","iopub.execute_input":"2024-04-25T11:25:14.708844Z","iopub.status.idle":"2024-04-25T11:25:14.74185Z","shell.execute_reply.started":"2024-04-25T11:25:14.708797Z","shell.execute_reply":"2024-04-25T11:25:14.740353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_features = ['word_len',]\n\ndef Word_Features(x):\n    print('Preprocess full_text and use spaces to seperate words fro the text')\n\n    x = x.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x)).str.split(\" \").alias('word')\n    )\n    x = x.explode('word')\n    \n    print('Caculate the length of a word') \n    x = x.with_columns(\n        pl.col('word').map_elements(lambda x: len(x)).alias('word_len'))\n    x = x.filter(pl.col('word_len')>0)\n\n    return x\n\ndef Word_aggregation(x):    \n    \n    print('Aggregation')\n    aggs = [\n    *[pl.col('word').filter(pl.col('word_len') >= i).count().alias(f'word_{i}_cnt') for i in [3,4,5,6,7,8,10]],\n    *[pl.col('word').filter(pl.col('word_len') <= i).count().alias(f'word_{i}_cnt_v2') for i in [1,2,3]],\n    *[pl.col('word').filter((pl.col('word_len') <= 4) & (pl.col('word_len') > 2)).count().alias(f'short_word_cnt')],\n    *[pl.col('word').filter((pl.col('word_len') <= 6) & (pl.col('word_len') > 4)).count().alias(f'mid_word_cnt')], \n    *[pl.col('word').filter((pl.col('word_len') <= 10) & (pl.col('word_len') > 6)).count().alias(f'long_word_cnt')],     \n    *[pl.col('word').count().alias('word_cnt')],  \n    *[pl.col(feat).max().alias(f'{feat}_max') for feat in word_features],    \n    *[pl.col(feat).mean().alias(f'{feat}_mean') for feat in word_features],\n    *[pl.col(feat).min().alias(f'{feat}_min') for feat in word_features],\n    *[pl.col(feat).std().alias(f'{feat}_std') for feat in word_features],\n    *[pl.col(feat).sum().alias(f'{feat}_sum') for feat in word_features],\n    *[pl.col(feat).quantile(0.25).alias(f'{feat}_q1') for feat in word_features],\n    *[pl.col(feat).quantile(0.75).alias(f'{feat}_q3') for feat in word_features],   \n    ]\n    \n    df = x.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.with_columns(\n    *[(pl.col(f'word_{i}_cnt')/pl.col('word_cnt')).alias(f'word_{i}_cnt_ratio') for i in [3,4,5,6,7,8,10]], \n    *[(pl.col(f'word_{i}_cnt_v2')/pl.col('word_cnt')).alias(f'word_{i}_cnt_v2_ratio') for i in [1,2,3]], \n    *[(pl.col(f'word_{i}_cnt')/pl.col('word_2_cnt_v2')).alias(f'word_{i}_pre2_ratio') for i in [3,4,5,6,7,8,10]],    \n    *[(pl.col(f'word_{i}_cnt')/pl.col('word_3_cnt_v2')).alias(f'word_{i}_pre3_ratio') for i in [3,4,5,6,7,8,10]],  \n    *[(pl.col(f'short_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'short_word_ratio_{i}') for i in [1,2,3]], \n    *[(pl.col(f'mid_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'mid_word_ratio_{i}') for i in [1,2,3]], \n    *[(pl.col(f'long_word_cnt')/ pl.col(f'word_{i}_cnt_v2')).alias(f'long_word_ratio_{i}') for i in [1,2,3]],     \n      \n    ).sort(\"essay_id\")\n    \n \n    df = df.to_pandas() # polars -> pandas\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:25:14.744184Z","iopub.execute_input":"2024-04-25T11:25:14.744686Z","iopub.status.idle":"2024-04-25T11:25:14.772444Z","shell.execute_reply.started":"2024-04-25T11:25:14.744641Z","shell.execute_reply":"2024-04-25T11:25:14.77104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    tokenizer = lambda x: x,\n    preprocessor = lambda x: x,\n    token_pattern=None,\n    strip_accents='unicode',\n    analyzer= 'word',\n    ngram_range = (1,5),\n    min_df =0.05,\n    max_df=0.95,\n    sublinear_tf = True # Term Frequency Log Scaling \n    )\n\n# TfidfVectorizer parameter\ntrain_a = train.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x))\n    )\n# Fit all datasets into TfidfVectorizer\ntrain_tfid = vectorizer.fit_transform([i for i in train_a['full_text']])\n\n              \n# Convert to array\ndense_matrix = train_tfid.toarray()\n\n# Convert to dataframe\ndf = pd.DataFrame(dense_matrix)\ndf.columns = [f'tfidf_{i}' for i in range(len(df.columns))]\n\n\ndf['essay_id'] = df_train['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:25:14.774204Z","iopub.execute_input":"2024-04-25T11:25:14.774663Z","iopub.status.idle":"2024-04-25T11:27:00.447772Z","shell.execute_reply.started":"2024-04-25T11:25:14.774628Z","shell.execute_reply":"2024-04-25T11:27:00.446285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n      tokenizer=lambda x: x,\n      preprocessor=lambda x: x,\n      token_pattern=None,\n      strip_accents='unicode',\n      analyzer = 'word',\n      ngram_range=(1,4),\n      min_df=0.10,\n      max_df=0.85,)\n    \n# TfidfVectorizer parameter\n\n# Fit all datasets into TfidfVectorizer\ntrain_b = train.with_columns(\n        pl.col('full_text').map_elements(lambda x: dataPreprocessing(x))\n     )\ntrain_cnt = vectorizer_cnt.fit_transform([i for i in train_b['full_text']])\n\n              \n# Convert to array\ndense_matrix2 = train_cnt.toarray()\n\n# Convert to dataframe\ndf2 = pd.DataFrame(dense_matrix2)\ndf2.columns = [f'cnt_{i}' for i in range(len(df2.columns))]\n\n\ndf2['essay_id'] = df_train['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:27:00.449257Z","iopub.execute_input":"2024-04-25T11:27:00.449579Z","iopub.status.idle":"2024-04-25T11:28:34.031493Z","shell.execute_reply.started":"2024-04-25T11:27:00.449551Z","shell.execute_reply":"2024-04-25T11:28:34.030003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_FEATURES_FROM is None: \n    \n    train_feats1 = Paragraph_Features(train)\n    train_feats1 = Paragraph_aggregation(train_feats1)\n    train_feats2 = Sentence_Features(train)\n    train_feats2 = Sentence_aggregation(train_feats2)\n    train_feats3 = Word_Features(train)\n    train_feats3 = Word_aggregation(train_feats3)\n    \n    train_feats = train_feats1.merge(train_feats2, on='essay_id', how='left')\n    train_feats = train_feats.merge(train_feats3, on='essay_id', how='left')\n    train_feats = train_feats.merge(df, on='essay_id', how='left')\n    train_feats = train_feats.merge(df2, on='essay_id', how='left')\n    train_feats['score'] = df_train['score'].values\nelse:\n    None","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:28:34.033312Z","iopub.execute_input":"2024-04-25T11:28:34.03369Z","iopub.status.idle":"2024-04-25T11:29:38.249048Z","shell.execute_reply.started":"2024-04-25T11:28:34.033661Z","shell.execute_reply":"2024-04-25T11:29:38.247386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_FEATURES_FROM is None: \n    print('Save train_feats.csv')\n    train_feats.to_csv(f'train_feats_{CFG.VER}.csv', index=False)\nelse: \n    print('Load train_feats.csv')\n    train_feats = pd.read_csv(CFG.LOAD_FEATURES_FROM)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:21:17.317885Z","iopub.execute_input":"2024-04-25T06:21:17.318255Z","iopub.status.idle":"2024-04-25T06:22:11.756149Z","shell.execute_reply.started":"2024-04-25T06:21:17.318224Z","shell.execute_reply":"2024-04-25T06:22:11.753532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_feats.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.254536Z","iopub.execute_input":"2024-04-25T11:29:38.254916Z","iopub.status.idle":"2024-04-25T11:29:38.283668Z","shell.execute_reply.started":"2024-04-25T11:29:38.254887Z","shell.execute_reply":"2024-04-25T11:29:38.282039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFA500\"><b><span style='color:#FFA500'></span></b> <b>4. CatBoost Model</b></div>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.285988Z","iopub.execute_input":"2024-04-25T11:29:38.286422Z","iopub.status.idle":"2024-04-25T11:29:38.292901Z","shell.execute_reply.started":"2024-04-25T11:29:38.286387Z","shell.execute_reply":"2024-04-25T11:29:38.2912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.948\nb = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.295023Z","iopub.execute_input":"2024-04-25T11:29:38.295484Z","iopub.status.idle":"2024-04-25T11:29:38.309844Z","shell.execute_reply.started":"2024-04-25T11:29:38.295451Z","shell.execute_reply":"2024-04-25T11:29:38.308401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport optuna\n\nimport catboost \nfrom catboost import CatBoostRegressor, Pool\nprint('Catboost Version: ', catboost.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.311413Z","iopub.execute_input":"2024-04-25T11:29:38.311817Z","iopub.status.idle":"2024-04-25T11:29:38.939948Z","shell.execute_reply.started":"2024-04-25T11:29:38.311787Z","shell.execute_reply":"2024-04-25T11:29:38.938762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ“Œ **Optuna Tutorial** \n\nhttps://www.kaggle.com/code/corochann/optuna-tutorial-for-hyperparameter-optimization#3.-Make-%22study%22-and-let-optimize!","metadata":{}},{"cell_type":"markdown","source":"![](https://optuna.org/assets/img/pruning-example-with-caption.png)","metadata":{}},{"cell_type":"code","source":"'''\ndef cat_objective(trial):\n\n    params = { \n          'verbose'      : 0,\n          'random_state' : CFG.SEED, \n          'loss_function' : 'MultiClass', \n          'learning_rate' : trial.suggest_float('learning_rate', 0.001, 0.5), \n          'depth' : trial.suggest_int('depth', 5, 10),\n    }\n\n    train_x, valid_x, train_y, valid_y = train_test_split(train_feats[FEATURES], train_feats[TARGET], test_size=0.2, random_state=CFG.SEED)\n    train_pool = Pool(\n          data = train_x,\n          label = train_y\n    )    \n\n    valid_pool = Pool(\n          data = valid_x,\n          label = valid_y\n    )\n\n    model  = CatBoostClassifier(**params)\n\n    model.fit(train_pool,\n          eval_set = valid_pool,  \n           )\n    oof = model.predict(valid_pool)\n    cv = cohen_kappa_score(valid_y, oof, weights=\"quadratic\")\n\n    \n    return cv '''","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.941911Z","iopub.execute_input":"2024-04-25T11:29:38.942749Z","iopub.status.idle":"2024-04-25T11:29:38.954811Z","shell.execute_reply.started":"2024-04-25T11:29:38.942711Z","shell.execute_reply":"2024-04-25T11:29:38.95335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nstudy = optuna.create_study(direction='minimize', study_name='Classification') \nstudy.optimize(cat_objective, n_trials=10, show_progress_bar=True)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.956429Z","iopub.execute_input":"2024-04-25T11:29:38.957025Z","iopub.status.idle":"2024-04-25T11:29:38.965319Z","shell.execute_reply.started":"2024-04-25T11:29:38.95699Z","shell.execute_reply":"2024-04-25T11:29:38.964065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'Best Trial: score{study.best_value}, param{study.best_params}')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.967104Z","iopub.execute_input":"2024-04-25T11:29:38.967582Z","iopub.status.idle":"2024-04-25T11:29:38.979118Z","shell.execute_reply.started":"2024-04-25T11:29:38.967546Z","shell.execute_reply":"2024-04-25T11:29:38.977542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = train_feats.select_dtypes(include=['object','category']).columns.tolist()\nFEATURES = [col for col in train_feats.columns if col not in categorical_columns + ['score']]\nTARGET = 'score'","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:29:38.981261Z","iopub.execute_input":"2024-04-25T11:29:38.981736Z","iopub.status.idle":"2024-04-25T11:29:38.996309Z","shell.execute_reply.started":"2024-04-25T11:29:38.981695Z","shell.execute_reply":"2024-04-25T11:29:38.99484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def catboost():\n    all_oof = [] \n    all_true = []\n    \n    \n    skf = StratifiedKFold(n_splits=15, random_state=CFG.SEED, shuffle=True)\n    for i, (train_index, valid_index) in enumerate(skf.split(train_feats, train_feats[TARGET])):\n\n        print('#'*25)\n        print(f'### Fold {i+1}')\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n        \n        model = CatBoostRegressor(\n        iterations=1000,\n        learning_rate = 0.1,\n        depth = 5,\n        subsample=0.8,\n        l2_leaf_reg = 1,\n        task_type = 'CPU',\n        objective = 'RMSE',\n        eval_metric = 'RMSE',\n        random_state = CFG.SEED,\n        )\n\n        train_pool = Pool(\n          data = np.clip(train_feats.loc[train_index, FEATURES].fillna(0),\n                      0, 10000),\n          label = train_feats.loc[train_index, TARGET]\n        )    \n\n        valid_pool = Pool(\n          data = np.clip(train_feats.loc[valid_index, FEATURES].fillna(0),\n                      0,10000),\n          label = train_feats.loc[valid_index, TARGET]\n        )\n\n        model.fit(train_pool, verbose=100,\n              eval_set = valid_pool,\n              early_stopping_rounds=75\n\n           )\n\n        # Save Model\n        pickle.dump(model, open(f'CAT_v{CFG.VER}_f{i}.pkl', 'wb'))\n\n        oof = model.predict(valid_pool)\n        all_oof.append(oof)\n        all_true.append(train_feats.loc[valid_index, TARGET])\n\n        del train_pool, valid_pool, oof, model\n    clean_memory()\n\n    all_oof = np.concatenate(all_oof)\n    all_true = np.concatenate(all_true)\n\n    oof = pd.DataFrame(all_oof.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = cohen_kappa_score(true[0], oof[0].clip(1,6).round(), weights=\"quadratic\")\n    print('CV Score for Low Catboost = ',cv)\n    cm = confusion_matrix(true[0], oof[0].clip(1,6).round(), labels=[x for x in range(1,7)])\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[x for x in range(1,7)])   \n    disp.plot()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T11:42:32.278259Z","iopub.execute_input":"2024-04-25T11:42:32.278859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_MODELS_FROM is None:\n    print('Training CATBoost')\n    catboost()\nelse: \n    None\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.LOAD_MODELS_FROM:\n    model = pickle.load(open(f'{CFG.LOAD_MODELS_FROM}CAT_v{CFG.VER}_f0.pkl', 'rb'))\nelse: \n    model = pickle.load(open(f'CAT_v{CFG.VER}_f0.pkl', 'rb'))\n\ndf_importance = pd.DataFrame({\n        'features_name': FEATURES,\n        'importance': model.feature_importances_,\n    })\ndf_importance = df_importance.sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(12,6))\nplt.bar(data=df_importance.head(30), x='features_name', height='importance', color='pink', edgecolor='black')\nplt.title('Distribution of Feature Importance of Catboost')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.778127Z","iopub.status.idle":"2024-04-25T06:22:11.778487Z","shell.execute_reply.started":"2024-04-25T06:22:11.778315Z","shell.execute_reply":"2024-04-25T06:22:11.77833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_a = test.with_columns(\n         pl.col('full_text').map_elements(lambda x: dataPreprocessing(x))\n     )\ntest_tfid = vectorizer.transform([i for i in test_a['full_text']])\ndense_matrix = test_tfid.toarray()\ndf3 = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfidf_{i}' for i in range(len(df3.columns))]\ndf3.columns = tfid_columns\ndf3['essay_id'] = df_test['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.779786Z","iopub.status.idle":"2024-04-25T06:22:11.780434Z","shell.execute_reply.started":"2024-04-25T06:22:11.780239Z","shell.execute_reply":"2024-04-25T06:22:11.780257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_b = test.with_columns(\n         pl.col('full_text').map_elements(lambda x: dataPreprocessing(x))\n     )\ntest_cnt = vectorizer_cnt.transform([i for i in test_b['full_text']])\ndense_matrix = test_cnt.toarray()\ndf4 = pd.DataFrame(dense_matrix)\ncnt_columns = [ f'cnt_{i}' for i in range(len(df4.columns))]\ndf4.columns = cnt_columns\ndf4['essay_id'] = df_test['essay_id']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \ntest_feats1 = Paragraph_Features(test)\ntest_feats1 = Paragraph_aggregation(test_feats1)\ntest_feats2 = Sentence_Features(test)\ntest_feats2 = Sentence_aggregation(test_feats2)\ntest_feats3 = Word_Features(test)\ntest_feats3 = Word_aggregation(test_feats3)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.781879Z","iopub.status.idle":"2024-04-25T06:22:11.782328Z","shell.execute_reply.started":"2024-04-25T06:22:11.782099Z","shell.execute_reply":"2024-04-25T06:22:11.782113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats = test_feats1.merge(test_feats2, on='essay_id', how='left')\ntest_feats = test_feats.merge(test_feats3, on='essay_id', how='left')\ntest_feats = test_feats.merge(df3, on='essay_id', how='left')\ntest_feats = test_feats.merge(df4, on='essay_id', how='left')\nprint('Shape of test_feats:', test_feats.shape)\ndisplay(test_feats.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.783566Z","iopub.status.idle":"2024-04-25T06:22:11.783923Z","shell.execute_reply.started":"2024-04-25T06:22:11.783749Z","shell.execute_reply":"2024-04-25T06:22:11.783763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\ncategorical_columns = test_feats.select_dtypes(include=['object','category']).columns.tolist()\nFEATURES = [col for col in test_feats.columns if col not in categorical_columns]\n\n\nfor i in range(15):\n    print(f'Fold {i+1}')\n    if CFG.LOAD_MODELS_FROM:\n        model = pickle.load(open(f'{CFG.LOAD_MODELS_FROM}CAT_v{CFG.VER}_f{i}.pkl', 'rb'))\n    else: \n        model = pickle.load(open(f'CAT_v{CFG.VER}_f{i}.pkl', 'rb'))\n        \n        \n    pred= model.predict(test_feats[FEATURES])\n\n    \n    preds.append(pred)\npred = np.mean(preds,axis=0)          \n   ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.786607Z","iopub.status.idle":"2024-04-25T06:22:11.78717Z","shell.execute_reply.started":"2024-04-25T06:22:11.786883Z","shell.execute_reply":"2024-04-25T06:22:11.786906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'essay_id': df_test.essay_id.values})\nsub[TARGET] = pred.clip(1,6).round() \nsub.to_csv('submission.csv',index=False)\nprint('Submission shape', sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:22:11.788769Z","iopub.status.idle":"2024-04-25T06:22:11.789322Z","shell.execute_reply.started":"2024-04-25T06:22:11.789034Z","shell.execute_reply":"2024-04-25T06:22:11.789077Z"},"trusted":true},"execution_count":null,"outputs":[]}]}