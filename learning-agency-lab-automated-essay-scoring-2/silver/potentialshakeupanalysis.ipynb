{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8570577,
     "sourceType": "datasetVersion",
     "datasetId": 5124602
    }
   ],
   "dockerImageVersionId": 30715,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Potential Shake Up Analysis\n\nIn this competition, many of us are having a hard time finding a good CV/LB correlation.\n\nAs always, the big final question is \"Should we trust CV or LB\" ?\n\nIn this notebook, I analyze the behaviour of two different (different parameters) but similar (same architecture) model that I trained.\nThey both have very close overall OOF CV scores around 0.84, but have different LB scores (it won't be fun otherwise!).\n\nLet's see what we can learn from that.\n\nThe whole idea behind this analysis is that the models have similar CV and similar architectures, so they should essentially be equivalent.\nAny difference between the two can be considered as an observation of the noise of our metric and problem.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import cohen_kappa_score\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\n\ndf_train = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n\nmodel_1_oof = pd.read_csv(\"/kaggle/input/shakeupanalysis/model_1_oof.csv\")\nmodel_2_oof = pd.read_csv(\"/kaggle/input/shakeupanalysis/model_2_oof.csv\")\n\ndf_train[\"model_1\"] = model_1_oof[\"0\"]\ndf_train[\"model_2\"] = model_2_oof[\"0\"]\n\ncv_model_1 = cohen_kappa_score(np.clip(np.rint(df_train[\"model_1\"]), 1, 6),\n                                 df_train.score, weights='quadratic')\n\ncv_model_2 = cohen_kappa_score(np.clip(np.rint(df_train[\"model_2\"]), 1, 6),\n                                 df_train.score, weights='quadratic')\n\nprint(f\"Model 1 CV: {cv_model_1:.4f} -> LB score (fold 1) 0.805\")\nprint(f\"Model 2 CV: {cv_model_2:.4f} -> LB score (fold 1) 0.809\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T14:05:41.390294Z",
     "iopub.execute_input": "2024-05-31T14:05:41.39132Z",
     "iopub.status.idle": "2024-05-31T14:05:41.915866Z",
     "shell.execute_reply.started": "2024-05-31T14:05:41.391272Z",
     "shell.execute_reply": "2024-05-31T14:05:41.91456Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We know that the test set has approximately 8K rows, with 30% (2400 rows) being in Public LB and 70% (5600 rows) being in Private LB.\n\nSo let's randomly select a Public and a Private LB among the train data, and repeat that 1000 times to compute a few bootstrapped statistics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "models = [\"model_1\", \"model_2\"]\npublic_results = {}\nfor col in models:\n    public_results[col] = []\n\nprivate_results = {}\nfor col in models:\n    private_results[col] = []\n    \n\n\nfor rand_iter in tqdm(range(1000)):\n    \n    # Here take random indices to form the Public Leaderboard\n    rand_idx = np.random.choice(np.arange(len(df_train)), 2400, replace=False)\n    \n    # On the remaining indices, let's form a Private Leaderboard\n    private_idx = np.array([i for i in range(len(df_train)) if i not in rand_idx])\n    private_idx = np.random.choice(private_idx, 5600, replace=False)\n    \n    for col in models:\n\n        local_score = cohen_kappa_score(np.clip(np.rint(df_train[col]), 1, 6)[rand_idx],\n                                       df_train.score.values[rand_idx], weights='quadratic')\n        public_results[col].append(local_score)\n        private_score = cohen_kappa_score(np.clip(np.rint(df_train[col]), 1, 6)[private_idx],\n                                       df_train.score.values[private_idx], weights='quadratic')\n        private_results[col].append(private_score)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T13:24:15.540572Z",
     "iopub.execute_input": "2024-05-31T13:24:15.541914Z",
     "iopub.status.idle": "2024-05-31T13:27:08.978906Z",
     "shell.execute_reply.started": "2024-05-31T13:24:15.541876Z",
     "shell.execute_reply": "2024-05-31T13:27:08.977417Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Public LB variability\n\nThe two plots bellow tell us multiple things.\n\nFirst that depending on the Public LB, the scores can vary quite a lot from ~0.825 to ~ 85.5.\nThis is somewhat expected as some cases must be harder than others so the scores vary, this also could explain the CV/LB gap that most of us are experiencing.\n\nSecondly, we can see that the scores between two equally good models can differ of 0.01 with a standard deviation ~0.004.\nThis tells us that a difference in score <0.004 in the Public Leaderboard is not really significant. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "x_values = public_results[\"model_1\"]\ny_values = public_results[\"model_2\"]\n\n# Find the minimum and maximum values\nmin_val = min(np.min(x_values), np.min(y_values))-0.001\nmax_val = max(np.max(x_values), np.max(y_values))+0.001\n\n# Create the scatter plot\nplt.scatter(x_values, y_values)\n\n# Set the same limits for both axes\nplt.xlim(min_val, max_val)\nplt.ylim(min_val, max_val)\n\n# Plot the lines y=x, y=x+0.005, y=x-0.005\nx_line = np.linspace(min_val, max_val, 100)\nplt.plot(x_line, x_line, 'r--', label='y = x')  # Line y = x\nplt.plot(x_line, x_line + 0.005, 'g--', label='y = x + 0.005')  # Line y = x + 0.005\nplt.plot(x_line, x_line - 0.005, 'b--', label='y = x - 0.005')  # Line y = x - 0.005\n\n# Add legend\nplt.legend()\nplt.title(\"Public LB variability\")\nplt.xlabel(\"Model 1\")\nplt.ylabel(\"Model 2\")\n# Display the plot\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T13:35:56.467922Z",
     "iopub.execute_input": "2024-05-31T13:35:56.468419Z",
     "iopub.status.idle": "2024-05-31T13:35:56.908728Z",
     "shell.execute_reply.started": "2024-05-31T13:35:56.468383Z",
     "shell.execute_reply": "2024-05-31T13:35:56.907413Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.hist([x - y for x, y in zip(public_results[\"model_1\"], public_results[\"model_2\"])], bins=100);\nplt.title(f\"Public difference between the two models (std: {np.std([(x - y) for x, y in zip(public_results['model_1'],public_results['model_2'])]):.2e})\")\n\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T13:35:56.910931Z",
     "iopub.execute_input": "2024-05-31T13:35:56.911329Z",
     "iopub.status.idle": "2024-05-31T13:35:57.329835Z",
     "shell.execute_reply.started": "2024-05-31T13:35:56.911294Z",
     "shell.execute_reply": "2024-05-31T13:35:57.328361Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Private LB variability\n\nIf we plot the same thing but for private LB scores what can we say ?\n\nWe can see from the histogram bellow that having more data reduces the deviation between the two models private scores and that a difference of 0.002 on the private LB should be significant.\n\nHowever, since there are almost 2000 participants in this competition, we can expect that some of us would have a \"lucky boost\" as big as 0.005 on the Private Leader Board.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "x_values = private_results[\"model_1\"]\ny_values = private_results[\"model_2\"]\n\n# Find the minimum and maximum values\nmin_val = min(np.min(x_values), np.min(y_values))-0.001\nmax_val = max(np.max(x_values), np.max(y_values))+0.001\n\n# Create the scatter plot\nplt.scatter(x_values, y_values)\n\n# Set the same limits for both axes\nplt.xlim(min_val, max_val)\nplt.ylim(min_val, max_val)\n\n# Plot the lines y=x, y=x+0.005, y=x-0.005\nx_line = np.linspace(min_val, max_val, 100)\nplt.plot(x_line, x_line, 'r--', label='y = x')  # Line y = x\nplt.plot(x_line, x_line + 0.005, 'g--', label='y = x + 0.005')  # Line y = x + 0.005\nplt.plot(x_line, x_line - 0.005, 'b--', label='y = x - 0.005')  # Line y = x - 0.005\n\n# Add legend\nplt.legend()\nplt.title(\"Private LB variability\")\nplt.xlabel(\"Model 1\")\nplt.ylabel(\"Model 2\")\n# Display the plot\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T13:38:38.581097Z",
     "iopub.execute_input": "2024-05-31T13:38:38.582186Z",
     "iopub.status.idle": "2024-05-31T13:38:38.974519Z",
     "shell.execute_reply.started": "2024-05-31T13:38:38.58214Z",
     "shell.execute_reply": "2024-05-31T13:38:38.973087Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "plt.hist([x - y for x, y in zip(private_results[\"model_1\"], private_results[\"model_2\"])], bins=100);\nplt.title(f\"Private difference between the two models (std: {np.std([(x - y) for x, y in zip(private_results['model_1'],private_results['model_2'])]):.2e})\")\n\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-31T13:39:18.400138Z",
     "iopub.execute_input": "2024-05-31T13:39:18.400662Z",
     "iopub.status.idle": "2024-05-31T13:39:18.880869Z",
     "shell.execute_reply.started": "2024-05-31T13:39:18.400624Z",
     "shell.execute_reply": "2024-05-31T13:39:18.879668Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Conclusion\n\nThis analysis is just here to give some insights about a potential shakeup and is by no mean a proof of anything.\n\nHowever, since the Public Leaderboard of this competition is quite crowdy around 0.820-0.822 (mostly because of public notebooks) I would say that competitors with Public Scores above 0.826 are (almost) certain of finishing above the public notebooks.\nObviously, things are more complicated when looking at the public notebooks scores as they are by design created to overfit the LB. Considering this, I would say that anyone with a solution independant of the top public notebooks with a score > 0.815 can have reasonnable hope to outperform this baseline on the Private Leaderboard.\n\nFinally, we are still a long way to go before the end of the competition and gold zone will probably require a score >0.830 but at the moment I would definitely foresee a large shake up!\n\nHappy kaggling!",
   "metadata": {}
  }
 ]
}
