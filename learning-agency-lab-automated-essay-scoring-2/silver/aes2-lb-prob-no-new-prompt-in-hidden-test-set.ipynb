{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3900
    },
    {
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 28083
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-05-06T02:18:19.350754Z",
     "iopub.execute_input": "2024-05-06T02:18:19.351399Z",
     "iopub.status.idle": "2024-05-06T02:18:25.024679Z",
     "shell.execute_reply.started": "2024-05-06T02:18:19.351367Z",
     "shell.execute_reply": "2024-05-06T02:18:25.023645Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# # model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n# # model_name = \"/kaggle/input/mistral-7b-instruct-v0.2-gptq/transformers/1/1/Mistral-7B-Instruct-v0.2-GPTQ\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_name,\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"auto\",\n#     trust_remote_code=True,\n# )\nmodel_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:18:25.02663Z",
     "iopub.execute_input": "2024-05-06T02:18:25.027107Z",
     "iopub.status.idle": "2024-05-06T02:21:15.104364Z",
     "shell.execute_reply.started": "2024-05-06T02:18:25.027073Z",
     "shell.execute_reply": "2024-05-06T02:21:15.103584Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:21:15.105359Z",
     "iopub.execute_input": "2024-05-06T02:21:15.105764Z",
     "iopub.status.idle": "2024-05-06T02:21:15.110388Z",
     "shell.execute_reply.started": "2024-05-06T02:21:15.105738Z",
     "shell.execute_reply": "2024-05-06T02:21:15.109472Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "prompt_format = \"\"\"\n# Instruction\nYour task is to predict the topic from the list of the given text:\n1. Car-free cities\n2. Does the electoral college work\n3. A Cowboy Who Rode the Waves\n4. Exploring Venus\n5. Driverless cars\n6. Facial action coding system\n7. The Face on Mars\n8. None of the above\n\n# Input\nThe text is: \n```text\n{text}\n```\n\n# Output\nReturn JSON object only. Dont GIVE Any Explaination!!\n```json\n{{\"match_number\": }}\n```\n\"\"\".strip()\nprompt = prompt_format.format(text=\"\"\"People always wish they had the same technology that they have seen in movies, or the best new piece of technology that is all over social media. However, nobody seems to think of the risks that these kinds of new technologies may have. Cars have been around for many decades, and now manufacturers are starting to get on the bandwagon and come up with the new and improved technology that they hope will appeal to everyone. As of right now, it seems as though the negative characteristics of these cars consume the positive idea that these manufacturers have tried to convey.\\n\\nCurrently, this new technology in cars has a very long way to go before being completely \"driverless\".\"\"\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:21:15.112828Z",
     "iopub.execute_input": "2024-05-06T02:21:15.113277Z",
     "iopub.status.idle": "2024-05-06T02:21:15.123828Z",
     "shell.execute_reply.started": "2024-05-06T02:21:15.113244Z",
     "shell.execute_reply": "2024-05-06T02:21:15.123012Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# messages = [\n#     {\"role\": \"user\", \"content\": prompt}\n# ]\n\n# input_ids = tokenizer.apply_chat_template(\n#     messages,\n#     add_generation_prompt=True,\n#     return_tensors=\"pt\"\n# ).to(model.device)\n\n# terminators = [\n#     tokenizer.eos_token_id,\n#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n# ]\n\n# outputs = model.generate(\n#     input_ids,\n#     max_new_tokens=256,\n#     eos_token_id=terminators,\n#     do_sample=True,\n#     temperature=0.6,\n#     top_p=0.9,\n# )\n# response = outputs[0][input_ids.shape[-1]:]\n# print(tokenizer.decode(response, skip_special_tokens=True))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-05T16:01:01.785434Z",
     "iopub.execute_input": "2024-05-05T16:01:01.786208Z",
     "iopub.status.idle": "2024-05-05T16:01:07.455641Z",
     "shell.execute_reply.started": "2024-05-05T16:01:01.786172Z",
     "shell.execute_reply": "2024-05-05T16:01:07.45473Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def chat_for_complation(prompt):\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n#     encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n# #     text = f\"\"\"\n# #     <s>[INST] <<SYS>>\n# # {prompt}\n# # <<SYS>> [/INST]\n# #     \"\"\".strip()\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    terminators = [\n        tokenizer.eos_token_id,\n        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n    outputs = model.generate(\n        input_ids,\n        max_new_tokens=32,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.1,\n        top_p=0.9,\n    )\n    response = outputs[0][input_ids.shape[-1]:]\n    return tokenizer.decode(response, skip_special_tokens=True)\n\n#     encodeds = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n#     model_inputs = encodeds\n#     generated_ids = model.generate(input_ids, max_new_tokens=128, do_sample=True, temperature=0.1)\n#     decoded = tokenizer.batch_decode(generated_ids)[0]\n#     return decoded[decoded.index(\"[/INST]\"):][len(\"[/INST]\"):-len(\"</s>\")].strip()\n#     return decoded",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:21:15.124964Z",
     "iopub.execute_input": "2024-05-06T02:21:15.125605Z",
     "iopub.status.idle": "2024-05-06T02:21:15.136209Z",
     "shell.execute_reply.started": "2024-05-06T02:21:15.125568Z",
     "shell.execute_reply": "2024-05-06T02:21:15.135418Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import json\n\nprompt_format = \"\"\"\n# Instruction\nYour task is to predict the topic from the list of the given text:\n1. Car-free cities\n2. Does the electoral college work\n3. A Cowboy Who Rode the Waves\n4. Exploring Venus\n5. Driverless cars\n6. Facial action coding system\n7. The Face on Mars\n8. None of the above\n\n# Input\nThe text is: \n```text\n{text}\n```\n\n# Output\nReturn JSON object only.\n```json\n{{\"match_number\": }}\n```\n\"\"\".strip()\n\n\ndef chat_for_match(full_text):\n    text = \" \".join(full_text.strip().split()[:128])\n    prompt = prompt_format.format(text=text)\n    completion = chat_for_complation(prompt)\n    if \"{\" not in completion or \"}\" not in completion:\n        return -1\n    try:\n        data = json.loads(completion[completion.find(\"{\"): completion.find(\"}\") + 1])\n    except Exception as e:\n        return -1\n    if \"match_number\" not in data:\n        return -1\n    try:\n        return int(data[\"match_number\"])\n    except Exception:\n        return -1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:21:15.137237Z",
     "iopub.execute_input": "2024-05-06T02:21:15.137491Z",
     "iopub.status.idle": "2024-05-06T02:21:15.152617Z",
     "shell.execute_reply.started": "2024-05-06T02:21:15.13747Z",
     "shell.execute_reply": "2024-05-06T02:21:15.151646Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\n\n\nDEBUG = False\nif DEBUG:\n    df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n    df = df.sample(100)\nelse:\n    df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n    df = df.sample(min(1200, len(df)))\n\ndf[\"topic\"] = df[\"full_text\"].progress_apply(chat_for_match)\n# print(df.shape[0], df[\"matched\"].sum())\ndf[\"topic\"].value_counts()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-06T02:27:57.066178Z",
     "iopub.execute_input": "2024-05-06T02:27:57.066775Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "d = df[\"topic\"].value_counts()\nprint(dict(zip(d.index, d.values)))\nother_type = dict(zip(d.index, d.values)).get(8, 0)\nif other_type > 100:\n    raise ValueError(\"many other prompt!\")\nelse:\n    sub = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n    sub.to_csv(\"submission.csv\", index=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
