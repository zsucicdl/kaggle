{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 8204502,
     "sourceType": "datasetVersion",
     "datasetId": 4860630
    },
    {
     "sourceId": 8207741,
     "sourceType": "datasetVersion",
     "datasetId": 4860826
    },
    {
     "sourceId": 26140,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 22003
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Customizable pytorch LLM training pipeline\n\n## Why this notebook ?\n\nLike many, I'm trying to improve my skills and understanding of LLMs.\nThere are new things popping out everyday and it is hard to keep up.\nThere are many great resources to finetune an LLM in a few lines of code (transformers, trl, auto-train, ...).\nHowever, I don't feel like learning much when using these ready-to-train libraries.\nSo I am here proposing a simple self-contained notebook where I try as much as possible to make all the subtelties visible and customizable to the final users, without of course reinventing the wheel.\n\nAs I said, I am currently learning all this and there might be errors or easy improvements to my pipeline: if you find some do not hesitate to let me know so that we can all improve!\n\n## Ideas for Customization\n\nHere is a few ideas to go beyond this notebook:\n- try other LLM backbones\n- add a more complex prediction head\n- explore different loss functions\n- try other pooling method\n\n## Things that still don't work\nDon't hesitate to let me know how this could work!\n\n- loading AND training the LLM in float16, bfloat16, or int8, int4: I still don't know what needs to be done inside the pipeline to make this work (it would change everything in terms of memory needed)\n- Save only the peft weights + the custom layers to save memory when saving the model\n\n## Next steps to explore\nI'll try to find the time to explore and share similar notebooks with:\n- qlora\n- torchtune\n- what else? share your ideas in comments!\n\n**Happy Kaggling!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# install latest libraries\n! pip install -q /kaggle/input/lal-scoring-wheels/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n! pip install -q /kaggle/input/lal-scoring-wheels/transformers-4.40.0-py3-none-any.whl --no-deps\n! pip install -q /kaggle/input/lal-scoring-wheels/peft-0.10.0-py3-none-any.whl --no-deps\n! pip install -q /kaggle/input/lal-scoring-wheels/accelerate-0.29.3-py3-none-any.whl --no-deps",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:31:28.407867Z",
     "iopub.execute_input": "2024-04-23T13:31:28.408448Z",
     "iopub.status.idle": "2024-04-23T13:31:46.19069Z",
     "shell.execute_reply.started": "2024-04-23T13:31:28.408417Z",
     "shell.execute_reply": "2024-04-23T13:31:46.189481Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport time\nimport datetime",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:31:46.192673Z",
     "iopub.execute_input": "2024-04-23T13:31:46.192976Z",
     "iopub.status.idle": "2024-04-23T13:31:50.491948Z",
     "shell.execute_reply.started": "2024-04-23T13:31:46.19295Z",
     "shell.execute_reply": "2024-04-23T13:31:50.491118Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Experiment configuration\nOnce your set up is finalized you only need to play with hyperparameters to find the best model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class Config():\n    def __init__(self):\n        # Parameters related to the problem\n        self.num_classes = 1 # 1 class for regression\n        \n        # Parameters related to network\n        self.architecture = {\"backbone\": \"/kaggle/input/gemma/transformers/1.1-2b-it/1\",\n                             \"params\": {}}\n        self.remove_layers = 8 # number of layer to remove to make the model smaller\n        self.freeze_layers = None # number of layers to freeze to reduce number of training parameters\n        self.use_lora = True\n        self.lora_config = {\"r\": 16, # rank of the decomposed matrix (higher means less memory saving)\n                            \"lora_alpha\": 32, # scaling factor, should be 2xr according to https://www.entrypointai.com/blog/lora-fine-tuning/\n                            \"lora_dropout\": 0.1, # usual dropout\n                            # make sure that you name correctly your modules according to your backbone\n                            # you should spot the linear layers in the attention blocks\n                            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                                               \"gate_proj\", \"up_proj\", \"down_proj\",],\n                           }\n        \n        self.token_info = {\"padding\" :\"longest\", # batch are going to be the length of longest sequence\n                           \"max_length\" : 256, # trained with 1024 locally, lowered to train within kaggle notebook\n                           \"truncation\": True,\n                           \"pad_to_multiple_of\" : 512 # I heard that modern GPUs are fastest with multiple of 512? is that True?\n                          }\n\n        # Parameters related to training\n        self.max_epochs = 1 # number of epochs\n\n        self.initial_lr =1e-4\n        self.optimizer_name = \"AdamW\" #\"AdamW\" # try 8 bit adam AdamW8bit     \n        self.optimizer_params = {\"lr\": self.initial_lr, \n                                 \"weight_decay\":1e-2\n                                }\n        self.loss_config = {\"loss_name\" : \"MSELoss\",\n                            \"reduction\":\"mean\",\n                           }\n        \n        self.scheduler_name = \"OneCycleLR\"\n        self.steps_per_epochs = -1 # this is automatically overwritten\n        self.scheduler_params={\n                              \"max_lr\":self.optimizer_params[\"lr\"] if type(self.optimizer_params)==dict else self.optimizer_params[-1][\"lr\"],\n                               \"div_factor\":10,\n                              \"steps_per_epoch\": self.steps_per_epochs,\n                              \"final_div_factor\":1e2, #1e2\n                               \"anneal_strategy\":\"cos\", #\"cos\"\n                               \"three_phase\" : False,\n                              \"pct_start\":0.1, #0.3\n                              \"epochs\": self.max_epochs}\n        \n        \n        self.eval_on_train = False # You might want to compute the exact metric on training set to monitor overfitting\n        self.batch_size = 1 # Let's start small\n        self.gradient_accumulation = 16 // self.batch_size # this allows you to train with low batch size but compute gradients on more that a few samples\n        self.mixed_precision = True\n        self.num_workers = 2 # I think num_workers for kaggle environment should be kept low\n        self.pin_memory = True\n        self.clip_value = 10.0\n\n        # parameters related to logs\n        self.verbose = 1 # how often do you want to compute the competition metric?\n        self.save_path = \"/kaggle/working/\"\n\nPATH_TO_DATA = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2\"\nexp_config = Config()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:31:50.493063Z",
     "iopub.execute_input": "2024-04-23T13:31:50.493461Z",
     "iopub.status.idle": "2024-04-23T13:31:50.507033Z",
     "shell.execute_reply.started": "2024-04-23T13:31:50.493436Z",
     "shell.execute_reply": "2024-04-23T13:31:50.506034Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Define datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from dataclasses import dataclass\nfrom torch.utils.data import DataLoader, Dataset\nfrom typing import Optional, Union, Any\nfrom transformers import DataCollatorWithPadding\n\n\nfrom transformers import AutoTokenizer\n\ndef define_tokenizer(cfg):\n    \"\"\"\n    Let's use basic AutoTokenizer\n    \"\"\"\n\n    tokenizer = AutoTokenizer.from_pretrained(cfg.architecture[\"backbone\"])    \n    return tokenizer\n    \nclass LALDataset(Dataset):\n    \"\"\"\n    There are simpler ways of creating a dataset nowadays (using datasets library for example).\n    But I prefer to define it that way as I feel more in control of what is actually happening.\n    Here the dataset is very simple, but more customization could be done.\n    \n    If there is a good reason not to do that and use more recent methods please let me know!\n    \"\"\"\n    def __init__(self, df, config, inference, remove=True):\n        \"\"\"\n        df: pandas dataframe\n        config: experiment config\n        inference (bool): are we in inference mode ?\n        remove (bool): should we remove unecessary columns that might not colate correctly?\n        \"\"\"\n        self.df = df\n        # tokenizer needs to be defined as it's used by datacollator\n        self.tokenizer = define_tokenizer(config)\n        self.inference = inference\n        self.config = config\n        self.remove = remove\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, row_idx):\n        \n        full_text = self.df.loc[row_idx, \"full_text\"]\n        \n        tokenized_text = self.tokenizer(full_text,\n                                        return_offsets_mapping=False, # mostly needed for entity recognition\n                                        truncation=self.config.token_info[\"truncation\"],\n                                        max_length=self.config.token_info[\"max_length\"])\n        \n        labels = self.df.loc[row_idx, \"score\"]\n            \n        # here we append eos token at the end which will work as a CLS token\n        # note that this must be the last token as GPT like models have a causal attention\n        tokenized_text.input_ids.append(self.tokenizer.eos_token_id)\n        tokenized_text.attention_mask.append(1)\n       \n        out_dict = {\n                \"input_ids\": tokenized_text.input_ids,\n                \"attention_mask\": tokenized_text.attention_mask,\n                \"labels\": torch.Tensor([labels])\n            }\n        return out_dict\n\n\ndef define_loader(dataset,\n                  config,\n                  inference,\n                 ):\n    \"\"\"\n    Use config and inference mode to create dataloader for train and test.\n    \"\"\"\n    num_workers = config.num_workers\n    pin_memory = config.pin_memory\n    \n    # collate_fn = None\n    # we use here a basic data collator\n    collate_fn = DataCollatorWithPadding(tokenizer=dataset.tokenizer,\n                                         padding=config.token_info[\"padding\"],\n                                         max_length=config.token_info[\"max_length\"],\n                                         pad_to_multiple_of=config.token_info[\"pad_to_multiple_of\"]\n                                    )\n\n\n    loader = DataLoader(\n                dataset,\n                batch_size=config.batch_size,\n                shuffle=not inference,\n                drop_last=not inference,\n                num_workers=num_workers,\n                pin_memory=pin_memory, \n                collate_fn=collate_fn,\n                # worker_init_fn=worker_init_fn,\n            )\n    return loader\n\n\ndef get_dataset_and_loader(df, config, inference, remove=True):\n    \"\"\"\n    Returns both dataset and dataloader\n    \"\"\"\n    dataset = LALDataset(df, config, inference, remove=remove)\n    loader = define_loader(dataset, config, inference)\n    return dataset, loader\n\ndef create_loaders(df, train_idx, valid_idx, config, eval_on_train):\n    \n    # You can set larger max length for inference\n    valid_config = copy.deepcopy(config)\n    valid_config.token_info['max_length'] = config.token_info['max_length']\n    \n    _, train_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n                                        config=config,\n                                        inference=False,\n                                        )\n\n    _, valid_dl = get_dataset_and_loader(df=df.iloc[valid_idx].reset_index(drop=True),\n                                        config=valid_config,\n                                        inference=True)\n\n    if eval_on_train:\n        _, train_aux_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n                                                config=valid_config,\n                                                inference=True)\n\n        eval_loaders = [train_aux_dl, valid_dl]\n        eval_names = [\"train\", \"valid\"]\n    else:\n        eval_loaders = [valid_dl]\n        eval_names = [\"valid\"]\n    return train_dl, valid_dl, eval_loaders, eval_names",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:31:50.50911Z",
     "iopub.execute_input": "2024-04-23T13:31:50.509408Z",
     "iopub.status.idle": "2024-04-23T13:32:03.060313Z",
     "shell.execute_reply.started": "2024-04-23T13:31:50.509376Z",
     "shell.execute_reply": "2024-04-23T13:32:03.059503Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Define network's architecture\n\nHere we use a simple architecture composed of an LLM backbone finetuned with LORA and a linear head.\nWe only use the last eos_token to predict the final score.\n\nYou can easily customize this architecture as you would do with any torch.nn.Module:\n- add metadata as inputs: tf-idf, num_words, engineered features etc...\n- make the final head more complicated (MLP with activations etc...)\n- try pooling methods insread of eos_oken pooling etc...",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoConfig\n\nclass CustomLLM(torch.nn.Module):\n    \"\"\"\n    Here is where you can customize your architecture\n    \"\"\"\n    def __init__(self, cfg, eos_token_id):\n        super().__init__()\n        self.num_classes = cfg.num_classes\n        self.eos_token_id = eos_token_id\n        self.model_config = AutoConfig.from_pretrained(\n                cfg.architecture[\"backbone\"],\n            )\n\n        self.activation = torch.nn.Identity() # Activation could be differnt\n        # get a backbone for our network\n        # Here let's go with AutoModelForCausalLM, it does not matter as we take remove the final layers\n        self.backbone  = AutoModelForCausalLM.from_pretrained(cfg.architecture[\"backbone\"],\n                                                                    device_map=\"cpu\", #\"cuda\",\n                                                                    load_in_4bit=False, #cfg.load_in_4bit,\n                                                                    torch_dtype=torch.float32, # let's leave bfloat16 for later (not working)\n                                                                    **cfg.architecture[\"params\"])\n        # remove the head as we are going to use a custom head\n        self.backbone.lm_head = torch.nn.Identity()\n\n        \n        if cfg.remove_layers is not None:\n            # we only remove the last layers as they can be superfluous: https://arxiv.org/html/2403.17887v1\n            self.backbone.layers = self.backbone.model.layers[:-cfg.remove_layers]          \n        \n        if hasattr(cfg, \"num_layers_to_freeze\"):\n            print(f\"freezing {cfg.num_layers_to_freez} layers.\")\n            if cfg.num_layers_to_freeze > 0:\n                if cfg.freeze_embeddings:\n                    # should you train embeddings?\n                    for param in self.backbone.embed_tokens.parameters():\n                        param.requires_grad = False\n                # Here the first layers are frozen: only remaining last layers will be trained\n                for layer in self.backbone.model.layers[:cfg.num_layers_to_freeze]:\n                    for param in layer.parameters():\n                        param.requires_grad = False\n                \n        if cfg.use_lora:\n            # Here we apply lora from peft library\n            peft_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False, # this does not seem to change anything -> how are we suppose to use it properly?\n                r=cfg.lora_config[\"r\"],\n                lora_alpha=cfg.lora_config[\"lora_alpha\"],\n                lora_dropout=cfg.lora_config[\"lora_dropout\"],\n                target_modules=cfg.lora_config[\"target_modules\"],\n                # modules_to_save=[\"lstm_head\", \"final_linear\"]\n                # bnb_4bit_compute_dtype=torch.bfloat16, # leave this for later       \n            )\n            \n            self.backbone = get_peft_model(self.backbone, peft_config)\n        else:\n            print(\"NOT USING LORA\")\n            \n        # this is for gradient checkpoint, left for later\n        # self.transformers_model.gradient_checkpointing_enable()\n                    \n        self.final_linear = torch.nn.Linear(self.model_config.hidden_size, cfg.num_classes)\n        \n        if cfg.use_lora:\n            self.backbone.print_trainable_parameters()\n        \n    def forward(self, batch):\n        x = batch[\"input_ids\"] # (bs, num_tokens)\n        # this assumes that you only have one eos_token per example\n        eos_positions = torch.argwhere(x == self.eos_token_id)[:, 1]\n        x = self.backbone(\n            input_ids=x,\n            attention_mask=batch[\"attention_mask\"],\n        )[\"logits\"] # (bs, num_tokens, hidden_size)\n        \n        # we are only interested in the eos_token\n        x = x[torch.arange(x.shape[0]), eos_positions] # (bs, hidden_size)\n        \n        logits = self.final_linear(x) # (bs, num_classes)\n\n        return {\"logits\": logits}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:03.061655Z",
     "iopub.execute_input": "2024-04-23T13:32:03.062204Z",
     "iopub.status.idle": "2024-04-23T13:32:03.993984Z",
     "shell.execute_reply.started": "2024-04-23T13:32:03.062176Z",
     "shell.execute_reply": "2024-04-23T13:32:03.992995Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Training recipe\n\nYou may need to change this if you make significant changes in your modelling apporach",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from dataclasses import dataclass\nfrom typing import List, Any, Dict\nfrom torch.nn.utils import clip_grad_norm_\nfrom abc import abstractmethod\nfrom sklearn.base import BaseEstimator\nimport json\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport copy\n\n# Layers to which we do not want to apply weight decay with AdamW\nALL_LAYERNORM_LAYERS = [torch.nn.LayerNorm, torch.nn.Embedding]\n\n\ndef get_parameter_names(network, forbidden_layer_types):\n        \"\"\"\n        Returns the names of the model parameters that are not inside a forbidden layer.\n        \"\"\"\n        result = []\n        for name, child in network.named_children():\n            result += [\n                f\"{name}.{n}\"\n                for n in get_parameter_names(child, forbidden_layer_types)\n                if not isinstance(child, tuple(forbidden_layer_types))\n            ]\n        # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n        result += list(network._parameters.keys())\n        return result\n    \ndef define_loss_function(loss_config):\n    \"\"\"\n    Basic torch loss functions or locally defined loss\n    \"\"\"\n    copy_config = copy.copy(loss_config)\n    loss_name = copy_config.pop('loss_name')\n    try:\n        loss_fn = getattr(torch.nn, loss_name)(**copy_config)\n    except AttributeError:\n        try:\n            loss_fn = globals().get(loss_name)(copy_config)\n        except:\n            raise NotImplementedError(\"Unkown loss function :\", loss_name)\n    return loss_fn\n\ndef prepare_log_folder(log_path):\n    \"\"\"\n    Utility function to create experiment folder\n    Creates the directory for logging.\n    Logs will be saved at log_path/date_of_day/exp_id\n\n    Args:\n        log_path (str): Directory\n\n    Returns:\n        str: Path to the created log folder\n    \"\"\"\n    today = str(datetime.date.today())\n    log_today = os.path.join(log_path, today)\n\n    if not os.path.exists(log_today):\n        Path(log_today).mkdir(parents=True)\n\n    exp_id = (\n        np.max([int(f) if str(f).isdigit() else -1 for f in os.listdir(log_today)]) + 1\n        if len(os.listdir(log_today))\n        else 0\n    )\n    log_folder = os.path.join(log_today, f\"{exp_id}\")\n\n    assert not os.path.exists(log_folder), \"Experiment already exists\"\n    os.mkdir(log_folder)\n    print(\"Saving logs at :\", log_folder)\n    return log_folder\n\ndef save_config(config, folder):\n    \"\"\"\n    Saves a config as a json, copies data and model configs.\n\n    Args:\n        config (Config): Config.\n        folder (str): Folder to save at.\n    \"\"\"\n    with open(os.path.join(folder, \"config.json\"), \"w\") as f:\n        json.dump(config.__dict__.copy(), f)\n\n@dataclass\nclass AbstractBaseModel(BaseEstimator):\n    \"\"\" Abstract class for scikit-like model.\n        Allows to build upon to train, infer, save, load etc..\n    \"\"\"\n\n    network: torch.nn.Module = None\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    seed: int = 42\n    mixed_precision: bool = False\n\n    def __post_init__(self):\n        torch.manual_seed(self.seed)\n        self.network.to(self.device)\n\n    def fit(\n        self,\n        train_dataloader,\n        eval_loaders=None,\n        eval_names=None,\n        eval_metric=None,\n        loss_config=None,\n        max_epochs=100,\n        callbacks=None,\n        optimizer_name=\"Adam\",\n        optimizer_params={\"lr\": 1e-3},\n        gradient_accumulation=None,\n        scheduler_name=None,\n        scheduler_params=None,\n        mixed_precision=False,\n        clip_value=None,\n        log_path=None,\n        verbose=1,\n    ):\n        \"\"\"\n        Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        eval_loaders for validation.\n        Parameters\n        ----------\n        train_dataloader : Dataloader\n            Train set\n        eval_loader : list of dataloaders\n            The last one is used for early stopping\n        eval_name : list of str\n            List of eval set names.\n        eval_metric : list of str\n            List of evaluation metrics.\n            The last metric is used for early stopping.\n        loss_name : Name\n            a PyTorch loss function name\n        max_epochs : int\n            Maximum number of epochs during training\n        num_workers : int\n            Number of workers used in torch.utils.data.DataLoader\n        drop_last : bool\n            Whether to drop last batch during training\n        pin_memory: bool\n            Whether to set pin_memory to True or False during training\n        from_unsupervised: unsupervised trained model\n            Use a previously self supervised model as starting weights\n        clip_value: float (default to None)\n            Gradient clipping\n        \"\"\"\n        # update model name\n\n        self.max_epochs = max_epochs\n        self._stop_training = False\n        self.optimizer_name = optimizer_name\n        self.optimizer_params = optimizer_params\n        eval_loaders = eval_loaders if eval_loaders else []       \n        self.mixed_precision = mixed_precision\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.gradient_accumulation = gradient_accumulation\n        self.metrics = eval_metric\n        \n        if loss_config is None:\n            raise(NotImplementedError, \"Please specifify a loss\")\n        else:\n            self.loss_fn = define_loss_function(loss_config)\n        \n        self._set_optimizer()\n        \n        # scheduler\n        self.scheduler_fn = getattr(torch.optim.lr_scheduler, scheduler_name) # this will only accept torch schedulers\n        self.scheduler_params = copy.copy(scheduler_params)\n        self.scheduler = self.scheduler_fn(self._optimizer, **self.scheduler_params)\n        \n        \n        # Training loop over epochs\n        start_time = time.time()\n        for epoch_idx in range(self.max_epochs):\n            self.epoch_idx = epoch_idx\n            epoch_loss, epoch_lr = self._train_epoch(train_dataloader)\n            msg = f\"epoch {epoch_idx:<3} | lr: {epoch_lr:.2e} | loss: {epoch_loss:.4f} \"\n            # Apply predict epoch to all eval sets\n            if ((self.verbose != 0) and (epoch_idx % self.verbose == 0)) or (epoch_idx==self.max_epochs-1):\n                for eval_name, valid_dataloader in zip(eval_names, eval_loaders):\n                    with torch.no_grad():\n                        prob_pred, prob_true, scores = self._predict_epoch(eval_name, valid_dataloader)\n                    for metric_name, metric_score in scores:\n                        msg += f\"| {metric_name:<3} ({eval_name}): {metric_score:.4f} \"\n            total_time = int(time.time() - start_time)\n            msg += f\"|  {str(datetime.timedelta(seconds=total_time)) + 's':<6}\"\n            print(msg)\n        print(\"End of training!\")\n        self.network.eval()\n        return prob_pred, prob_true\n        \n    def predict_proba(self, dataloader, return_target=False):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n        X : a :tensor: `torch.Tensor`\n            Input data\n        Returns\n        -------\n        predictions : np.array\n            Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        results_prob = []\n        results_targets = []\n        pbar = tqdm(dataloader,\n                     leave=False,\n                     total=len(dataloader),\n                     desc=f'Inference')\n        \n        with torch.no_grad():\n            for batch in pbar:\n                out_probs = self._predict_batch(batch).cpu()\n                results_prob.append(out_probs)\n\n                if return_target:\n                    targets = batch[\"labels\"]\n                    targets = targets.to(\"cpu\").detach()\n                    results_targets.append(targets)\n\n        res_prob = self.stack_preds(results_prob)                \n        if return_target:\n            res_target = self.stack_targets(results_targets)\n            return res_prob, res_target\n        else:\n            return res_prob\n\n    def save_model(self, path, model_name):\n        \"\"\"\n        Save the model somewhere\n\n        Users can specify both the path and model_name\n        If no model_name given an automatic one will be creted\n        \"\"\"\n        Path(path).mkdir(parents=True, exist_ok=True)\n        # Save state_dict with half precision for less gpu usage during inference: same result if trained with mixed precision\n        torch.save(self.network.half().state_dict(), Path(path).joinpath(f\"{model_name}.pt\"))\n        # torch.save(self.network.state_dict(), Path(path).joinpath(f\"{model_name}.pt\"))\n        return\n\n\n    def _train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n        train_loader : a :class: `torch.utils.data.Dataloader`\n            DataLoader with train set\n        \"\"\"\n        self.network.train()\n        num_iter_epoch = len(train_loader)\n        pbar = tqdm(enumerate(train_loader),\n                                     leave=False,\n                                     total=len(train_loader),\n                                     desc=f'train epoch {self.epoch_idx}')\n        \n        epoch_loss = 0\n        for batch_idx, batch in pbar:\n            batch_loss = self._train_batch(batch, batch_idx, num_iter_epoch)\n            epoch_loss = (train_loader.batch_size*batch_idx*epoch_loss + train_loader.batch_size*batch_loss) / (train_loader.batch_size*(batch_idx+1))            \n            pbar.set_description(f'train epoch {self.epoch_idx}: loss {epoch_loss:.3f}', refresh=True)\n            # update scheduler\n            self.scheduler.step()\n\n        epoch_lr = self._optimizer.param_groups[-1][\"lr\"]\n        return epoch_loss, epoch_lr\n\n    def _train_batch(self, batch, batch_idx, num_iter_epoch):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n        batch_logs : dict\n            Dictionnary with \"batch_size\" and \"loss\".\n        \"\"\"\n        self._send_batch_to_device(batch)\n                                   \n        with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n            # use mixed precision for float16 training\n            y = batch[\"labels\"]\n            batch_logs = {\"batch_size\": y.shape[0]}\n            out_probs = self.network(batch)\n            # computing loss with division by gradient accumulation\n            loss = self.loss_fn(out_probs[\"logits\"], y.unsqueeze(-1)) / self.gradient_accumulation\n            self.scaler.scale(loss).backward()\n            \n        if ((batch_idx + 1) % self.gradient_accumulation == 0) or ((batch_idx + 1)==num_iter_epoch):\n            # Perform backward pass and optimization\n            if self.clip_value is not None:\n                self.scaler.unscale_(self._optimizer)\n                clip_grad_norm_(self.network.parameters(), max_norm=self.clip_value)\n\n            self.scaler.step(self._optimizer)\n            self.scaler.update()\n            # set the gradients to 0 only when calling step\n            self._optimizer.zero_grad(set_to_none=True)\n        return loss.detach().item()\n\n    def _predict_epoch(self, name, loader):\n        \"\"\"\n        Predict an epoch and update metrics.\n        Parameters\n        ----------\n        name : str\n            Name of the validation set\n        loader : torch.utils.data.Dataloader\n                DataLoader with validation set\n        \"\"\"\n        prob_pred, prob_true = self.predict_proba(loader, return_target=True)\n        \n        scores = []\n        for metric_fn in self.metrics:\n            metric_score = metric_fn(prob_true, prob_pred)\n            scores.append((metric_fn._name, metric_score))\n        # need to compute metrics here\n        return prob_pred, prob_true, scores\n\n    def stack_preds(self, list_prob):\n        return torch.vstack(list_prob)\n\n    def stack_targets(self, list_prob):\n        return torch.hstack(list_prob)\n\n    def _send_batch_to_device(self, batch):\n        for key, value in batch.items():\n            batch[key] = value.to(self.device)\n            \n    def _predict_batch(self, batch):\n        \"\"\"\n        Predict one batch of data.\n        \"\"\"\n        with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n            self._send_batch_to_device(batch)\n            # compute model output\n            out_probs = self.network(batch)[\"logits\"]\n            # apply activation\n            if isinstance(self.network, torch.nn.DataParallel):\n                # deal with data parrallel\n                out_probs = self.network.module.activation(out_probs)\n            else:\n                out_probs = self.network.activation(out_probs)\n            \n        return out_probs.detach()\n\n    def _set_optimizer(self):\n        \"\"\"Setup optimizer.\"\"\"\n        \n        name = self.optimizer_name\n\n        # disable decay for layer norm\n        decay_parameters = get_parameter_names(self.network, ALL_LAYERNORM_LAYERS)\n        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.network.named_parameters() if (n in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": self.optimizer_params[\"weight_decay\"],\n            },\n            {\n                \"params\": [\n                    p for n, p in self.network.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        other_params = self.optimizer_params.copy()\n        _ = other_params.pop(\"weight_decay\")\n                \n        self._optimizer = getattr(torch.optim, name)(optimizer_grouped_parameters, **other_params)        \n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.mixed_precision)\n        return\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:03.995409Z",
     "iopub.execute_input": "2024-04-23T13:32:03.995692Z",
     "iopub.status.idle": "2024-04-23T13:32:04.076247Z",
     "shell.execute_reply.started": "2024-04-23T13:32:03.995668Z",
     "shell.execute_reply": "2024-04-23T13:32:04.075326Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Metrics to track\n\nHere you can define metrics you want to track during model training (every epoch)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import (\n    mean_squared_error\n)\nclass RMSE:\n    \"\"\"\n    Root Mean Squared Error.\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"rmse\"\n\n\n    def __call__(self, y_true, y_score):\n        \"\"\"\n        Compute MSE (Mean Squared Error) of predictions.\n\n        Parameters\n        ----------\n        y_true : np.ndarray\n            Target matrix or vector\n        y_score : np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n        float\n            MSE of predictions vs targets.\n        \"\"\"\n        return mean_squared_error(y_true.numpy(), y_score.numpy(), squared=False)\n\n\nimport numpy as np\nfrom numba import jit \n\n# @jit\ndef qwk6(a1, a2, max_rat=6):\n    \"\"\"\n    Comp metric adapted from CPMP: https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/145105\n    \"\"\"\n    assert(len(a1) == len(a2))\n    \n    a1 = a1.astype(np.int64).reshape(-1)\n    # take closest integer for continuous predictions\n    a2 = np.rint(a2).astype(np.int64).reshape(-1)\n    # a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n    return (1 - o / e)\n\nclass QWK:\n    def __init__(self):\n        self._name = \"qwk\"\n    def __call__(self, y_true, y_pred, max_rat=6):\n        return qwk6(y_true.numpy(), y_pred.numpy())\n\n\n# from sklearn.metrics import cohen_kappa_score\n\n\n# class ScikitQWK:\n#     \"\"\"\n#     Competition metric with scikit\n#     \"\"\"\n\n#     def __init__(self):\n#         self._name = \"scikit_qwk\"\n#     def __call__(self, y_true, y_pred):\n#         y_pred = np.rint(y_pred) # convert predictions to closest integer\n#         return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:04.07764Z",
     "iopub.execute_input": "2024-04-23T13:32:04.077969Z",
     "iopub.status.idle": "2024-04-23T13:32:04.718601Z",
     "shell.execute_reply.started": "2024-04-23T13:32:04.077943Z",
     "shell.execute_reply": "2024-04-23T13:32:04.717767Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Puting everything together for training one fold\n\nThis is just a simple function that will allow you to train one fold and save the corresponding configs and model checkpoint.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def update_sched_params(config, train_loader):\n    \"\"\"\n    This helper function allows to define steps per epoch dynamically\n\n    Parameters\n    ----------\n    - config : experiment config\n    - train_loader : train data loader used for this fold\n    \"\"\"\n    nb_epochs = config.max_epochs\n    is_per_epoch = config.scheduler_params.get(\"steps_per_epoch\", None)\n\n    if is_per_epoch is not None:\n        if is_per_epoch <= 0:\n            # this means automatic number of steps\n            config.scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n        # else use the defined value\n    \n    \n    # for get_cosine_schedule_with_warmup\n    warmup_ratio = config.scheduler_params.pop(\"warmup_ratio\", None)\n\n    if warmup_ratio is not None:\n        num_train_steps = int(len(train_loader) * nb_epochs)\n        num_warmup_steps = int(num_train_steps * warmup_ratio)\n        config.scheduler_params[\"num_warmup_steps\"] = num_warmup_steps\n        config.scheduler_params[\"num_training_steps\"] = num_train_steps\n        # else use the defined value\n    return config\n\ndef train_fold(df,\n               train_idx,\n               valid_idx,\n               config,\n               fold_nb):\n\n    print(\"Num train and valid samples:\", train_idx.shape[0], valid_idx.shape[0])\n    config = copy.deepcopy(config)\n    train_dl, valid_dl, eval_loaders, eval_names =  create_loaders(df,\n                                                                   train_idx,\n                                                                   valid_idx,\n                                                                   config,\n                                                                   eval_on_train=config.eval_on_train\n                                                                   )\n    log_folder = prepare_log_folder(config.save_path)\n    # add the eos_token_id to config\n    config.eos_token_id = train_dl.dataset.tokenizer.eos_token_id\n    save_config(config, log_folder)\n\n    \n    network = CustomLLM(config, train_dl.dataset.tokenizer.eos_token_id)\n    model = AbstractBaseModel(network=network)\n        \n    # update scheduler\n    config = update_sched_params(config, train_dl)\n\n    prob_pred, prob_true = model.fit(train_dl,\n                                      eval_loaders= eval_loaders,\n                                      eval_names=eval_names,\n                                      eval_metric=[RMSE(), QWK()], #  , ScikitQWK()\n                                      loss_config=config.loss_config, \n                                      max_epochs=config.max_epochs,\n                                      callbacks=None,\n                                      optimizer_name=config.optimizer_name,\n                                      optimizer_params=config.optimizer_params,\n                                      scheduler_name=config.scheduler_name,\n                                      scheduler_params=config.scheduler_params,\n                                      gradient_accumulation=config.gradient_accumulation,\n                                      mixed_precision=config.mixed_precision,\n                                      clip_value=config.clip_value,\n                                      verbose=config.verbose,\n             )\n\n    # prob_pred, prob_true = model.predict_proba(loader, return_target=True)\n    \n    model.save_model(path=log_folder, model_name=f\"fold_{fold_nb}\")\n    torch.cuda.empty_cache()\n        \n    return prob_pred, prob_true",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:04.719638Z",
     "iopub.execute_input": "2024-04-23T13:32:04.71994Z",
     "iopub.status.idle": "2024-04-23T13:32:04.73361Z",
     "shell.execute_reply.started": "2024-04-23T13:32:04.719909Z",
     "shell.execute_reply": "2024-04-23T13:32:04.732621Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Training: 5 fold cross validation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# use stratified kfold\nfrom sklearn.model_selection import StratifiedKFold\nimport os\n\n# download training data\ndf_train = pd.read_csv(os.path.join(PATH_TO_DATA, \"train.csv\"))\n\nTRAIN = False # switch to False for inference and True for training\nINFERENCE = True\nDEBUG = False\n\nif TRAIN:\n    if DEBUG:\n        df_train = df_train[:50]\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    for fold_nb, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train.score)):\n        prob_pred, prob_true = train_fold(df_train,\n                                           train_idx,\n                                           valid_idx,\n                                           exp_config,\n                                           fold_nb=fold_nb,\n                                           )\n        break",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:04.734973Z",
     "iopub.execute_input": "2024-04-23T13:32:04.735289Z",
     "iopub.status.idle": "2024-04-23T13:32:05.615626Z",
     "shell.execute_reply.started": "2024-04-23T13:32:04.735258Z",
     "shell.execute_reply": "2024-04-23T13:32:05.614489Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example of what you should see (trained on my personal setup)\n\n# Num train and valid samples: 13845 3462\n# Saving logs at : ../../logs/essay_scoring/2024-04-23/2\n# trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n# epoch 0   | lr: 5.87e-05 | loss: 0.0662 | rmse (valid): 0.5429 | qwk (valid): 0.8105 |  0:45:41s\n# epoch 1   | lr: 1.00e-07 | loss: 0.0301 | rmse (valid): 0.5167 | qwk (valid): 0.8358 |  1:31:20s\n# End of training!",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:05.618669Z",
     "iopub.execute_input": "2024-04-23T13:32:05.619022Z",
     "iopub.status.idle": "2024-04-23T13:32:05.623298Z",
     "shell.execute_reply.started": "2024-04-23T13:32:05.618994Z",
     "shell.execute_reply": "2024-04-23T13:32:05.6224Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Inference\n\nHere is where you can perform simple inference from a previously trained checkpoint.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SavedConfig:\n    \"\"\"\n    Placeholder to load a config from a saved json\n    \"\"\"\n    def __init__(self, dic):\n        for k, v in dic.items():\n            setattr(self, k, v)\n\ndef load_model(path, model_name, override_backbone=None):\n    \"\"\"\n    Load a previsouly trained model\n    \"\"\"\n    # get saved configurations\n    with open(os.path.join(path, \"config.json\"), \"r\") as f:\n        saved_configs = json.load(f)\n\n    saved_configs = SavedConfig(saved_configs)\n    \n    if override_backbone is not None:\n        saved_configs.architecture = {\"backbone\": override_backbone,\n                             \"params\": {}}\n    # create network\n    network = CustomLLM(saved_configs, saved_configs.eos_token_id)\n    # load trained weights\n    state_dict = torch.load(os.path.join(path, f\"{model_name}.pt\"))\n\n    network.load_state_dict(state_dict)     \n\n    # create a model\n    clf = AbstractBaseModel(network=network,\n                            mixed_precision=saved_configs.mixed_precision)\n    clf.network.eval()\n    return saved_configs, clf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:05.624391Z",
     "iopub.execute_input": "2024-04-23T13:32:05.624669Z",
     "iopub.status.idle": "2024-04-23T13:32:05.634442Z",
     "shell.execute_reply.started": "2024-04-23T13:32:05.624638Z",
     "shell.execute_reply": "2024-04-23T13:32:05.633655Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MODEL_PATH = \"/kaggle/input/simple-model-training/2/\" #\"your/path/to/pretrained/here\"\nMODEL_NAME = \"fold_0\"\n\nif INFERENCE:\n    # this loads saved configs and model\n    # my model was trained with MAX_LENGTH=1024 on my local machine\n    # I need to override the backbone as I did not train on kaggle notebooks\n    saved_configs, saved_model = load_model(MODEL_PATH, MODEL_NAME, override_backbone=\"/kaggle/input/gemma/transformers/1.1-2b-it/1\")\n    # make sure to use batch size of 1 to limit memory consumption\n    saved_configs.batch_size=1\n    \n    df_test = pd.read_csv(os.path.join(PATH_TO_DATA, \"test.csv\"))\n    # let's generated a dummy 'score' column to match the train.csv\n    df_test[\"score\"] = -1\n    # generate the test dataset and datalaoder\n    ds_test, dl_test = get_dataset_and_loader(df_test, saved_configs, inference=True)\n    \n    test_preds = saved_model.predict_proba(dl_test).numpy()\n    df_sub = pd.DataFrame()\n    df_sub[\"essay_id\"] = df_test[\"essay_id\"]\n    # convert prediction to integers\n    df_sub[\"score\"] = np.rint(test_preds).astype(int)\n    # save submission\n    df_sub.to_csv(\"submission.csv\", index=None)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-23T13:32:05.635666Z",
     "iopub.execute_input": "2024-04-23T13:32:05.635982Z",
     "iopub.status.idle": "2024-04-23T13:33:46.124376Z",
     "shell.execute_reply.started": "2024-04-23T13:32:05.635953Z",
     "shell.execute_reply": "2024-04-23T13:33:46.123507Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
