{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-20T19:24:55.310758Z","iopub.execute_input":"2024-05-20T19:24:55.311472Z","iopub.status.idle":"2024-05-20T19:24:56.477112Z","shell.execute_reply.started":"2024-05-20T19:24:55.311427Z","shell.execute_reply":"2024-05-20T19:24:56.476017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Agency Lab - Automated Essay Scoring 2.0\n* Data Exploration\n* Data Setup\n* Baseline Model Training\n* Submission","metadata":{}},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ntest_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:56.478768Z","iopub.execute_input":"2024-05-20T19:24:56.479171Z","iopub.status.idle":"2024-05-20T19:24:57.332344Z","shell.execute_reply.started":"2024-05-20T19:24:56.479144Z","shell.execute_reply":"2024-05-20T19:24:57.331456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"# Overview\ntrain_df.info()\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.333347Z","iopub.execute_input":"2024-05-20T19:24:57.3342Z","iopub.status.idle":"2024-05-20T19:24:57.370033Z","shell.execute_reply.started":"2024-05-20T19:24:57.334171Z","shell.execute_reply":"2024-05-20T19:24:57.368935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datatypes of each column\nprint(train_df.dtypes)\nprint(test_df.dtypes)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.372932Z","iopub.execute_input":"2024-05-20T19:24:57.373366Z","iopub.status.idle":"2024-05-20T19:24:57.379772Z","shell.execute_reply.started":"2024-05-20T19:24:57.37333Z","shell.execute_reply":"2024-05-20T19:24:57.378844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First Few Rows\nprint(train_df.head())\nprint(test_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.381265Z","iopub.execute_input":"2024-05-20T19:24:57.381553Z","iopub.status.idle":"2024-05-20T19:24:57.399041Z","shell.execute_reply.started":"2024-05-20T19:24:57.381528Z","shell.execute_reply":"2024-05-20T19:24:57.397971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary Statistics\nprint(train_df.describe())\nprint(test_df.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.400394Z","iopub.execute_input":"2024-05-20T19:24:57.401358Z","iopub.status.idle":"2024-05-20T19:24:57.423643Z","shell.execute_reply.started":"2024-05-20T19:24:57.401325Z","shell.execute_reply":"2024-05-20T19:24:57.422674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\nprint(train_df.isnull().sum())\nprint(test_df.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.424684Z","iopub.execute_input":"2024-05-20T19:24:57.424951Z","iopub.status.idle":"2024-05-20T19:24:57.436252Z","shell.execute_reply.started":"2024-05-20T19:24:57.424929Z","shell.execute_reply":"2024-05-20T19:24:57.435116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique values\nprint(train_df.nunique())\nprint(test_df.nunique())","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.437381Z","iopub.execute_input":"2024-05-20T19:24:57.437654Z","iopub.status.idle":"2024-05-20T19:24:57.560018Z","shell.execute_reply.started":"2024-05-20T19:24:57.437632Z","shell.execute_reply":"2024-05-20T19:24:57.559021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Create the bar plot\nplt.figure(figsize=(10, 6))  \nax = sns.countplot(x='score', data=train_df, palette='plasma')\n\n# Add titles and labels\nax.set_title('Distribution of Scores in Train Dataset', fontsize=16)\nax.set_xlabel('Score', fontsize=12)\nax.set_ylabel('Count', fontsize=12)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:57.561687Z","iopub.execute_input":"2024-05-20T19:24:57.561984Z","iopub.status.idle":"2024-05-20T19:24:59.361323Z","shell.execute_reply.started":"2024-05-20T19:24:57.561959Z","shell.execute_reply":"2024-05-20T19:24:59.360098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"code","source":"import re\n\nclass TextPreprocessor:\n    def __init__(self):\n        pass\n    \n    def to_lower(self, text):\n        return text.lower()\n    \n    def remove_special_chars(self, text):\n        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n        return text\n    \n    def remove_new_lines_whitespace(self, text):\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n    \n    def preprocess(self, text):\n        text = self.to_lower(text)\n        #text = self.remove_special_chars(text)\n        #text = self.remove_new_lines_whitespace(text)\n        return text","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.36661Z","iopub.execute_input":"2024-05-20T19:24:59.366958Z","iopub.status.idle":"2024-05-20T19:24:59.373487Z","shell.execute_reply.started":"2024-05-20T19:24:59.366931Z","shell.execute_reply":"2024-05-20T19:24:59.372291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing preprocessor\npreprocessor = TextPreprocessor()\n\nsample_text = \"Hello!!! This is an example: test for Pre-processing, with sp&cial char@cters.\"\nprocessed_text = preprocessor.preprocess(sample_text)\nprint(processed_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.375201Z","iopub.execute_input":"2024-05-20T19:24:59.375624Z","iopub.status.idle":"2024-05-20T19:24:59.394583Z","shell.execute_reply.started":"2024-05-20T19:24:59.375587Z","shell.execute_reply":"2024-05-20T19:24:59.393306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply preprocessor\ntrain_df['full_text'] = train_df['full_text'].apply(preprocessor.preprocess)\ntest_df['full_text'] = test_df['full_text'].apply(preprocessor.preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.395962Z","iopub.execute_input":"2024-05-20T19:24:59.396424Z","iopub.status.idle":"2024-05-20T19:24:59.537887Z","shell.execute_reply.started":"2024-05-20T19:24:59.396389Z","shell.execute_reply":"2024-05-20T19:24:59.536818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First Few Rows\nprint(train_df['full_text'])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.539435Z","iopub.execute_input":"2024-05-20T19:24:59.539722Z","iopub.status.idle":"2024-05-20T19:24:59.545996Z","shell.execute_reply.started":"2024-05-20T19:24:59.539699Z","shell.execute_reply":"2024-05-20T19:24:59.544924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df['full_text'])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.547256Z","iopub.execute_input":"2024-05-20T19:24:59.547684Z","iopub.status.idle":"2024-05-20T19:24:59.558718Z","shell.execute_reply.started":"2024-05-20T19:24:59.547656Z","shell.execute_reply":"2024-05-20T19:24:59.557675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Generating 4 New Feature Columns","metadata":{}},{"cell_type":"code","source":"class TextFeatureExtractor:\n    def __init__(self):\n        pass\n    \n    def word_count(self, text):\n        return len(text.split())\n    \n    def sentence_count(self, text):\n        return len(text.split('.'))\n    \n    def ave_word_length(self, text):\n        \"\"\"Return the average length of words in a text.\"\"\"\n        words = text.split()\n        total_length = 0\n        for word in words:\n            total_length += len(word)\n        if len(words) == 0:\n            return 0\n        else:\n            return total_length / len(words)\n    \n    def lexical_diversity(self, text):\n        \"\"\"\n        Return the lexical diversity of a text.\n        Lexical diversity is a measure of how many different words are used in a text.\n        It's calculated as the ratio of the number of unique words to the total number of words in the text.\n        \"\"\"\n        words = text.split()\n        if len(words) == 0:\n            return 0\n        return len(set(words)) / len(words)\n    \n    def extract_features(self, text):\n        features = {\n            'word_count': self.word_count(text),\n            'sentence_count': self.sentence_count(text),\n            'ave_word_length': self.avg_word_length(text),\n            'lexical_diversity': self.lexical_diversity(text)\n        }\n        return features\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.560122Z","iopub.execute_input":"2024-05-20T19:24:59.560489Z","iopub.status.idle":"2024-05-20T19:24:59.571049Z","shell.execute_reply.started":"2024-05-20T19:24:59.560454Z","shell.execute_reply":"2024-05-20T19:24:59.569905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_text_features(df, text_column):\n    extractor = TextFeatureExtractor()\n    for feature in ['word_count', 'sentence_count', 'ave_word_length', 'lexical_diversity']:\n        df[feature] = df[text_column].apply(lambda x: getattr(extractor, feature)(x))\n\napply_text_features(train_df, 'full_text')\napply_text_features(test_df, 'full_text')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:24:59.572865Z","iopub.execute_input":"2024-05-20T19:24:59.573177Z","iopub.status.idle":"2024-05-20T19:25:01.840852Z","shell.execute_reply.started":"2024-05-20T19:24:59.573152Z","shell.execute_reply":"2024-05-20T19:25:01.839792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:01.842207Z","iopub.execute_input":"2024-05-20T19:25:01.842541Z","iopub.status.idle":"2024-05-20T19:25:01.862373Z","shell.execute_reply.started":"2024-05-20T19:25:01.842514Z","shell.execute_reply":"2024-05-20T19:25:01.861196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:01.865422Z","iopub.execute_input":"2024-05-20T19:25:01.865954Z","iopub.status.idle":"2024-05-20T19:25:01.878736Z","shell.execute_reply.started":"2024-05-20T19:25:01.865912Z","shell.execute_reply":"2024-05-20T19:25:01.877412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Each Feature","metadata":{}},{"cell_type":"code","source":"print(train_df[['word_count', 'sentence_count', 'ave_word_length', 'lexical_diversity']].describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:01.880723Z","iopub.execute_input":"2024-05-20T19:25:01.881153Z","iopub.status.idle":"2024-05-20T19:25:01.905659Z","shell.execute_reply.started":"2024-05-20T19:25:01.881092Z","shell.execute_reply":"2024-05-20T19:25:01.904462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,8))\n\nfeature_titles = ['Word Count', 'Sentence Count', 'Average Word Length', 'Lexical Diversity']\n\nfor i, feature in enumerate(['word_count', 'sentence_count', 'ave_word_length', 'lexical_diversity'],1\n):\n    plt.subplot(2,2,i)\n    plt.hist(train_df[feature], bins=20, color='blue',alpha=0.7)\n    plt.title(feature_titles[i - 1])\n\nplt.tight_layout()\nplt.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:01.907137Z","iopub.execute_input":"2024-05-20T19:25:01.907554Z","iopub.status.idle":"2024-05-20T19:25:03.333795Z","shell.execute_reply.started":"2024-05-20T19:25:01.907517Z","shell.execute_reply":"2024-05-20T19:25:03.332572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preparation","metadata":{}},{"cell_type":"markdown","source":"### Scaling the features","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, cohen_kappa_score\nfrom sklearn.model_selection import KFold, GridSearchCV\n\n# Isolate the essay_id for the submission file\ntest_essay_ids = test_df['essay_id']\n\n# Exclude 'essay_id' from features\nfeatures_scale = ['word_count', 'sentence_count', 'ave_word_length', 'lexical_diversity']\nscaler = StandardScaler()\n# Scaling\ntrain_df[features_scale] = scaler.fit_transform(train_df[features_scale])\ntest_df[features_scale] = scaler.transform(test_df[features_scale]) # don't fit\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:03.335336Z","iopub.execute_input":"2024-05-20T19:25:03.335756Z","iopub.status.idle":"2024-05-20T19:25:03.936152Z","shell.execute_reply.started":"2024-05-20T19:25:03.335718Z","shell.execute_reply":"2024-05-20T19:25:03.934756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(train_df)\n#print(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:03.937632Z","iopub.execute_input":"2024-05-20T19:25:03.937941Z","iopub.status.idle":"2024-05-20T19:25:03.942806Z","shell.execute_reply.started":"2024-05-20T19:25:03.937917Z","shell.execute_reply":"2024-05-20T19:25:03.941631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Vectorization\n\nTF-IDF (Term Frequency-Inverse Document Frequency)","metadata":{}},{"cell_type":"code","source":"# Vectorize text data\ntfid_vectorizer = TfidfVectorizer(max_features=5000)\nX_train_tfidf = tfid_vectorizer.fit_transform(train_df['full_text'])\nX_test_tfidf = tfid_vectorizer.transform(test_df['full_text'])  # Use transform here, don't fit\n\n# Convert scaled features into sparse format\nfeatures_train = csr_matrix(train_df[features_scale])\nfeatures_test = csr_matrix(test_df[features_scale])\n\n# Combine TF-IDF features with additional features\nX_train_combined = hstack([X_train_tfidf, features_train])\nX_test_combined = hstack([X_test_tfidf, features_test])\n\n# Labels\ny_train = train_df['score']\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:03.944025Z","iopub.execute_input":"2024-05-20T19:25:03.944409Z","iopub.status.idle":"2024-05-20T19:25:09.084393Z","shell.execute_reply.started":"2024-05-20T19:25:03.944381Z","shell.execute_reply":"2024-05-20T19:25:09.083194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(X_train_combined)\n#print(X_test_combined)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:09.085847Z","iopub.execute_input":"2024-05-20T19:25:09.08637Z","iopub.status.idle":"2024-05-20T19:25:09.091815Z","shell.execute_reply.started":"2024-05-20T19:25:09.086336Z","shell.execute_reply":"2024-05-20T19:25:09.090506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\n### Baseline Model with LGBM\nI chose LightGBM for its efficiency in handling multiclass classification tasks on large datasets and paired it with Optuna for hyperparameter optimization. Optuna streamlines the model tuning process through systematic trials, optimizing parameters for better performance and robustness. This combination ensures the model is both powerful and practical for predictive accuracy. ","metadata":{}},{"cell_type":"code","source":"%%time\n\n## Optuna version\nimport optuna\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import train_test_split\n\n# Labels for xgboost\ny_train_xg = train_df['score'] -1\n\ndef objective(trial):\n    # Suggest values for the hyperparameters\n    param = {\n        'objective': 'multiclass',  # Specify multiclass classification\n        'metric': 'multi_logloss',  # Suitable for multiclass classification\n        'num_class': len(np.unique(y_train)),  # Number of classes\n        'verbosity': -1,\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n        'force_row_wise': True\n    }\n    \n    # Split data for validation\n    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_combined, y_train_xg, test_size=0.2, random_state=42)\n    \n    # Create a LightGBM classifier with suggested parameters\n    clf = LGBMClassifier(**param)\n    clf.fit(X_train_split, y_train_split)\n    preds = clf.predict(X_val_split)\n    \n    # Use Cohen's Kappa Score for evaluation\n    kappa = cohen_kappa_score(y_val_split, preds, weights='quadratic')\n    return kappa\n\n# Create a study object and specify the optimization direction\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)  # adjust the number of trials\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:25:09.093734Z","iopub.execute_input":"2024-05-20T19:25:09.094257Z","iopub.status.idle":"2024-05-20T19:34:48.51754Z","shell.execute_reply.started":"2024-05-20T19:25:09.094174Z","shell.execute_reply":"2024-05-20T19:34:48.514532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Train the final model with the best parameters\nbest_params = trial.params\nbest_params['force_row_wise'] = True\nprint(best_params)\nfinal_model = LGBMClassifier(**best_params)\nfinal_model.fit(X_train_combined, y_train_xg)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-20T19:34:48.520519Z","iopub.execute_input":"2024-05-20T19:34:48.521405Z","iopub.status.idle":"2024-05-20T19:37:41.953531Z","shell.execute_reply.started":"2024-05-20T19:34:48.521349Z","shell.execute_reply":"2024-05-20T19:37:41.952447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the test set\ny_test_pred = final_model.predict(X_test_combined)\n\ny_test_pred = y_test_pred + 1\n\nsubmission_df = pd.DataFrame({\n    'essay_id': test_df['essay_id'],\n    'score': y_test_pred\n})\n\nprint(\"DataFrame shape:\", submission_df.shape)\nprint(\"DataFrame sample:\", submission_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:37:41.955137Z","iopub.execute_input":"2024-05-20T19:37:41.955525Z","iopub.status.idle":"2024-05-20T19:37:41.968067Z","shell.execute_reply.started":"2024-05-20T19:37:41.955494Z","shell.execute_reply":"2024-05-20T19:37:41.966695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission file\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Files in current directory:\", os.listdir('.'))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T19:37:41.973589Z","iopub.execute_input":"2024-05-20T19:37:41.973982Z","iopub.status.idle":"2024-05-20T19:37:41.985404Z","shell.execute_reply.started":"2024-05-20T19:37:41.973951Z","shell.execute_reply":"2024-05-20T19:37:41.9837Z"},"trusted":true},"execution_count":null,"outputs":[]}]}