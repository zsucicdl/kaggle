{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":45890,"databundleVersionId":4966396,"sourceType":"competition"},{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"}],"dockerImageVersionId":30369,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import *\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-16T19:08:11.081965Z","iopub.execute_input":"2024-04-16T19:08:11.082479Z","iopub.status.idle":"2024-04-16T19:08:12.402625Z","shell.execute_reply.started":"2024-04-16T19:08:11.082378Z","shell.execute_reply":"2024-04-16T19:08:12.401562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction and aim of the notebook\n\nQuadratic Weighted Kappa is a measure used to evaluate the **agreement between two outcomes in a classification problem (or between two annotators' ratings), taking into account the possibility of agreement by chance.**<br>\nA perfect score `1` is granted when both the predictions and actuals are the same, whereas `0` is when the predictions are very far away (for instance having all 4s and we are predicting all 0s).\n\nIn this notebook, we will **explore the metric and its calculation**, so that hopefully the [sklearn implementation](https://github.com/scikit-learn/scikit-learn/blob/7db5b6a98/sklearn/metrics/_classification.py#L598) `sklearn.metrics.cohen_kappa_score` when `weights='Quadratic'` would make sense to you.\n\n```\n    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)    (1)\n    n_classes = confusion.shape[0]      \n    sum0 = np.sum(confusion, axis=0)                                                    (2)\n    sum1 = np.sum(confusion, axis=1) \n    expected = np.outer(sum0, sum1) / np.sum(sum0)                                      \n\n    if weights is None:\n        w_mat = np.ones([n_classes, n_classes], dtype=int)\n        w_mat.flat[:: n_classes + 1] = 0\n    elif weights == \"linear\" or weights == \"quadratic\":\n        w_mat = np.zeros([n_classes, n_classes], dtype=int)                             (3)\n        w_mat += np.arange(n_classes)\n        if weights == \"linear\":\n            w_mat = np.abs(w_mat - w_mat.T)\n        else:\n            w_mat = (w_mat - w_mat.T) ** 2                                              \n    else:\n        raise ValueError(\"Unknown kappa weighting type.\")\n\n    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)                            (4)\n    return 1 - k\n\n```","metadata":{}},{"cell_type":"markdown","source":"# Quadratic Weighted Kappa","metadata":{"_uuid":"0b5047f5a15cd776b781164c574734e01c9a051d"}},{"cell_type":"markdown","source":"According to the **competition evaluation section**:\n\n<img src='https://i.imgur.com/Dk8lQi0.png' width=750>\n\nLet's try to make sense of all that, **breaking the metric into smaller steps.**","metadata":{"_uuid":"81940bd3a8c9af891028a428468880059f3cf4cc"}},{"cell_type":"markdown","source":"## Calculation","metadata":{"heading_collapsed":true,"_uuid":"c944b46efc3310b77b079bbd12d2dedc5dbff5b1"}},{"cell_type":"markdown","source":"Let's assume we have two vectors representing the ground truth (actuals) and our classifier's predictions (preds). Numbers can be also be thought as labels or rating given by a human, it doens't change how we are going to build the metric.","metadata":{"hidden":true,"_uuid":"fccbeb55d5b45ab5a40fcdc7986e47118c68a98e"}},{"cell_type":"code","source":"actuals = np.array([0, 0, 4, 3, 2, 4, 1, 1, 2, 1])\npreds   = np.array([0, 2, 3, 0, 0, 4, 1, 1, 3, 1])","metadata":{"hidden":true,"_uuid":"b3d9aa9ada12887791fc222060763878b765fd12","execution":{"iopub.status.busy":"2024-04-16T19:08:12.404618Z","iopub.execute_input":"2024-04-16T19:08:12.405423Z","iopub.status.idle":"2024-04-16T19:08:12.41099Z","shell.execute_reply.started":"2024-04-16T19:08:12.405381Z","shell.execute_reply":"2024-04-16T19:08:12.410129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **first step (according to the Kaggle docs)** is to create a **confusion matrix O between predicted and actuals.** We'll be using `confusion_matrix` from `sklearn` as follows:","metadata":{}},{"cell_type":"code","source":"O = confusion_matrix(actuals, preds)\nO","metadata":{"hidden":true,"_uuid":"c3b1aba627aa9bd2002a360812b3be62c6eab97f","execution":{"iopub.status.busy":"2024-04-16T19:08:12.412538Z","iopub.execute_input":"2024-04-16T19:08:12.41317Z","iopub.status.idle":"2024-04-16T19:08:12.437273Z","shell.execute_reply.started":"2024-04-16T19:08:12.413133Z","shell.execute_reply":"2024-04-16T19:08:12.435968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize it to get a better sense of what it is happening:","metadata":{}},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(actuals, preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.440517Z","iopub.execute_input":"2024-04-16T19:08:12.441396Z","iopub.status.idle":"2024-04-16T19:08:12.770492Z","shell.execute_reply.started":"2024-04-16T19:08:12.441344Z","shell.execute_reply":"2024-04-16T19:08:12.769132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above, for instance, we see that we are doing **quite a good job predicting the label 1, not quite the same for label 3.**<br>\nThis first step is also what `sklearn` does in **(1)** above.\n\nThe **2nd step** is to construct a **weight matrix `w`** which calculates the weight between the actuals and predicted: **predictions that are far away from actuals are penalized more than those closer to the actuals.**","metadata":{}},{"cell_type":"code","source":"# Shape of the confusion matrix, the same as \nn = O.shape[0]\n\n# Init zero weight matrix\nw = np.zeros((n,n))\nw","metadata":{"hidden":true,"_uuid":"a8f5f67d8b71b88cfd56dc8b2be904a2aa5193a6","execution":{"iopub.status.busy":"2024-04-16T19:08:12.772199Z","iopub.execute_input":"2024-04-16T19:08:12.77333Z","iopub.status.idle":"2024-04-16T19:08:12.789136Z","shell.execute_reply.started":"2024-04-16T19:08:12.773286Z","shell.execute_reply":"2024-04-16T19:08:12.782878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll **apply the formula reported in the evaluation section** as follows:","metadata":{}},{"cell_type":"code","source":"for i in range(n):\n    for j in range(n):\n        w[i][j] = ((i-j)**2)/(n-1)**2","metadata":{"hidden":true,"_uuid":"83482f3a469878ff16683f066a4a9ad70498c3f0","execution":{"iopub.status.busy":"2024-04-16T19:08:12.791506Z","iopub.execute_input":"2024-04-16T19:08:12.792594Z","iopub.status.idle":"2024-04-16T19:08:12.800439Z","shell.execute_reply.started":"2024-04-16T19:08:12.792485Z","shell.execute_reply":"2024-04-16T19:08:12.798955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.round(w,2)","metadata":{"hidden":true,"_uuid":"83c46fee9c4941f0d0f7775a301261c48349e25b","execution":{"iopub.status.busy":"2024-04-16T19:08:12.802546Z","iopub.execute_input":"2024-04-16T19:08:12.803671Z","iopub.status.idle":"2024-04-16T19:08:12.815798Z","shell.execute_reply.started":"2024-04-16T19:08:12.803623Z","shell.execute_reply":"2024-04-16T19:08:12.814159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All values on the **diagonal have penalty 0** since we have perfect agreement, whereas **predictions and actuals further away from each other are penalised the most.**\n\n`sklearn` comes up with that matrix in a different (and elegant) way using **(3)**, reported here:\n\n```\nw_mat = np.zeros([n_classes, n_classes], dtype=int)\nw_mat += np.arange(n_classes)\nif weights == \"linear\":\n    w_mat = np.abs(w_mat - w_mat.T)\nelse:\n    w_mat = (w_mat - w_mat.T) ** 2\n```\n\nLet's try to understand what it does. Let's recreate the empty matrix and create an increasing range of numbers:","metadata":{"hidden":true,"_uuid":"a072e0e8a4c72cb3c235ced789ab80d306b2aa21"}},{"cell_type":"code","source":"w = np.zeros((n,n))\nw += np.arange(n)\n\nw","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.817136Z","iopub.execute_input":"2024-04-16T19:08:12.818095Z","iopub.status.idle":"2024-04-16T19:08:12.828477Z","shell.execute_reply.started":"2024-04-16T19:08:12.818032Z","shell.execute_reply":"2024-04-16T19:08:12.827332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like it's calculating exactly what we did before with `for i in range(n)`.\nThen we have:","metadata":{}},{"cell_type":"code","source":" w.T","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.830974Z","iopub.execute_input":"2024-04-16T19:08:12.832274Z","iopub.status.idle":"2024-04-16T19:08:12.850084Z","shell.execute_reply.started":"2024-04-16T19:08:12.832215Z","shell.execute_reply":"2024-04-16T19:08:12.848831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which is just the transposed of `w`, so that this calculation:","metadata":{}},{"cell_type":"code","source":"w = (w - w.T) ** 2\nw","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.854555Z","iopub.execute_input":"2024-04-16T19:08:12.855305Z","iopub.status.idle":"2024-04-16T19:08:12.865167Z","shell.execute_reply.started":"2024-04-16T19:08:12.85525Z","shell.execute_reply":"2024-04-16T19:08:12.864124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"resembles our\n\n```\nfor i in range(n):\n    for j in range(n):\n        w[i][j] = ((i-j)**2)\n```","metadata":{}},{"cell_type":"markdown","source":"We simply need to divide by $(N-1)^2$ to get the exact same result we got before. However, this is not necessary (that's why `sklearn` doesn't have it), since later we'll do $w*O / w*E$, so the denominator $(N-1)^2$ will cancel out. Let's stick to the plan and do it anyways.","metadata":{}},{"cell_type":"code","source":"w = w / (n-1)**2\nw","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.866584Z","iopub.execute_input":"2024-04-16T19:08:12.867182Z","iopub.status.idle":"2024-04-16T19:08:12.878374Z","shell.execute_reply.started":"2024-04-16T19:08:12.867146Z","shell.execute_reply":"2024-04-16T19:08:12.877361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **3rd step is to calculate E**, which is the **outer product between the actual counts of outcomes and the predicted.** This is **(2)** in the sklearn code:","metadata":{}},{"cell_type":"code","source":"actual_sum = np.sum(O, axis=1)\npredicted_sum = np.sum(O, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.879901Z","iopub.execute_input":"2024-04-16T19:08:12.880318Z","iopub.status.idle":"2024-04-16T19:08:12.890684Z","shell.execute_reply.started":"2024-04-16T19:08:12.880279Z","shell.execute_reply":"2024-04-16T19:08:12.889541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Actuals value counts: {actual_sum}, Prediction value counts: {predicted_sum}')","metadata":{"hidden":true,"_uuid":"c8881fdab187f7d471046de59800764fbac79a15","execution":{"iopub.status.busy":"2024-04-16T19:08:12.892255Z","iopub.execute_input":"2024-04-16T19:08:12.892999Z","iopub.status.idle":"2024-04-16T19:08:12.905276Z","shell.execute_reply.started":"2024-04-16T19:08:12.892955Z","shell.execute_reply":"2024-04-16T19:08:12.904018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the actuals (rows of the confusion matrix), in fact we can see that we have:\n- 2 values with label 0\n- 3 values with label 1\n- 2 value with label 2\n- 1 with label 3\n- 2 values with label 4","metadata":{"hidden":true,"_uuid":"405056fbbe6ac3f0f95dc457e5295191a6c7b859"}},{"cell_type":"code","source":"E = np.outer(actual_sum, predicted_sum)/O.sum()\nE","metadata":{"hidden":true,"_uuid":"e670e97a025bc8c4d85e265f8d52e2a97e3469fe","execution":{"iopub.status.busy":"2024-04-16T19:08:12.907009Z","iopub.execute_input":"2024-04-16T19:08:12.907789Z","iopub.status.idle":"2024-04-16T19:08:12.925905Z","shell.execute_reply.started":"2024-04-16T19:08:12.907746Z","shell.execute_reply":"2024-04-16T19:08:12.924429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**But why do we need the outer product?**<br>\nLet's try to understand first what we mean with **expected outcome.**\n\nThis value is defined as the value **expected to be achieved based on the confusion matrix.** Given the confusion matrix calculated before:","metadata":{}},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(actuals, preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:12.927823Z","iopub.execute_input":"2024-04-16T19:08:12.929109Z","iopub.status.idle":"2024-04-16T19:08:13.290399Z","shell.execute_reply.started":"2024-04-16T19:08:12.929027Z","shell.execute_reply":"2024-04-16T19:08:13.288029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The expected outcome is calculated as follows, starting from cell 1:\n- 2 (1 + 1 = 2) instances were labeled as 0 according to ground truth (first row), and 3 (1 + 1 + 1) instances were classified as 0 by the classifier. Considering 10 total instances in the confusion matrix, this results in a value of 0.6 (2 * 3 / 10 = 0.6).\n\nMoving to the right:\n- 2 instances were labeled as 0 according to ground truth, and 3 instances were classified as 1 by the classifier. We divide by 10 again and we still obtain 0.6\n\nAnd so on.\n**The fact that we are multiplying a row by each column is actually the outer product between two vectors!**<br>\nIf we have two vectors:\n\n$$\n\\mathbf{v}=\\left[\\begin{array}{c}\nv_{1} \\\\\nv_{2} \\\\\n\\vdots \\\\\nv_{n}\n\\end{array}\\right], \\mathbf{w}=\\left[\\begin{array}{c}\nw_{1} \\\\\nw_{2} \\\\\n\\vdots \\\\\nw_{m}\n\\end{array}\\right]\n$$\n\nthe outer product is:\n\n$$\n\\underset{\\underset{\\scriptstyle\\text{}}{\\scriptstyle}}{\\mathbf{v} \\otimes \\mathbf{w}}=\\left[\\begin{array}{cccc}\nv_{1} w_{1} & v_{1} w_{2} & \\cdots & v_{1} w_{m} \\\\\nv_{2} w_{1} & v_{2} w_{2} & \\cdots & v_{2} w_{m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n} w_{1} & v_{n} w_{2} & \\cdots & v_{n} w_{m}\n\\end{array}\\right]\n$$\nIf we then divide by the total instances (10), we **get the expected outcome for each cell.**","metadata":{}},{"cell_type":"markdown","source":"Let's now **normalize E and O**:","metadata":{}},{"cell_type":"code","source":"O = O/O.sum()\nE = E/E.sum()","metadata":{"hidden":true,"_uuid":"ee4cf77bed65391f990be3e34363e06afb5653a9","execution":{"iopub.status.busy":"2024-04-16T19:08:13.294474Z","iopub.execute_input":"2024-04-16T19:08:13.2955Z","iopub.status.idle":"2024-04-16T19:08:13.303851Z","shell.execute_reply.started":"2024-04-16T19:08:13.295433Z","shell.execute_reply":"2024-04-16T19:08:13.301892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now we are ready to **calculate the Kappa as indicated by Kaggle!**","metadata":{}},{"cell_type":"code","source":"k = np.sum(w * O) / np.sum(w * E)\n1-k","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:13.306151Z","iopub.execute_input":"2024-04-16T19:08:13.306683Z","iopub.status.idle":"2024-04-16T19:08:13.321269Z","shell.execute_reply.started":"2024-04-16T19:08:13.30663Z","shell.execute_reply":"2024-04-16T19:08:13.319151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare this result with the `sklearn` implementation:","metadata":{}},{"cell_type":"code","source":"cohen_kappa_score(preds, actuals, weights='quadratic')","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:13.323884Z","iopub.execute_input":"2024-04-16T19:08:13.324961Z","iopub.status.idle":"2024-04-16T19:08:13.339812Z","shell.execute_reply.started":"2024-04-16T19:08:13.324897Z","shell.execute_reply":"2024-04-16T19:08:13.337937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intuitions and examples on imbalanced dataset\n\nAccording to [Wikipedia](https://en.wikipedia.org/wiki/Cohen%27s_kappa#Interpreting-magnitude), some Kappa values can be defined (according to Landis and Koch) as:\n\n- 0 no agreement\n- 0.01 – 0.20 Slight\n- 0.21 – 0.40 Fair\n- 0.41 – 0.60 Moderate\n- 0.61 – 0.80 Substantial \n- 0.81 – 0.99 Almost perfect\n\nWhat about **imbalanced datasets?** Let's try to create one:","metadata":{}},{"cell_type":"code","source":"actuals = np.concatenate([np.zeros(100000), np.ones(10)])\npreds   = np.concatenate([np.zeros(100010)])\n\nConfusionMatrixDisplay.from_predictions(actuals, preds)\n\nprint(classification_report(preds, actuals))\nprint('Kappa score:', cohen_kappa_score(preds, actuals, weights='quadratic'))","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:13.342648Z","iopub.execute_input":"2024-04-16T19:08:13.343811Z","iopub.status.idle":"2024-04-16T19:08:14.110656Z","shell.execute_reply.started":"2024-04-16T19:08:13.343746Z","shell.execute_reply":"2024-04-16T19:08:14.109472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that we are not able to predict any positive class, **Kappa = 0 is reassuring!** What about being able to predict correctly **1 instance of the positive class?**","metadata":{"execution":{"iopub.status.busy":"2023-02-01T16:07:24.811561Z","iopub.execute_input":"2023-02-01T16:07:24.812398Z","iopub.status.idle":"2023-02-01T16:07:24.824501Z","shell.execute_reply.started":"2023-02-01T16:07:24.812357Z","shell.execute_reply":"2023-02-01T16:07:24.823347Z"}}},{"cell_type":"code","source":"actuals = np.concatenate([np.zeros(100000), np.ones(10)])\npreds   = np.concatenate([np.zeros(100009), np.ones(1)])\n\nConfusionMatrixDisplay.from_predictions(actuals, preds)\n\nprint(classification_report(preds, actuals))\nprint('Kappa score:', cohen_kappa_score(preds, actuals, weights='quadratic'))","metadata":{"execution":{"iopub.status.busy":"2024-04-16T19:08:14.112481Z","iopub.execute_input":"2024-04-16T19:08:14.113241Z","iopub.status.idle":"2024-04-16T19:08:14.731768Z","shell.execute_reply.started":"2024-04-16T19:08:14.113192Z","shell.execute_reply":"2024-04-16T19:08:14.730112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A Kappa of **0.18 indicates not a great result**, which is a good thing given that we are **not doing well on the positive class.**","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIn conclusion, the Quadratic Weighted Kappa is a popular metric used to measure the inter-rater agreement between annotators (or labels, if we consider the truth and the predictions as annotators). It is a robust measure that can also take into account the severity of the disagreement thanks to the weighting part.\n\nI hope this helped you understand better this competition's metric!","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- https://www.kaggle.com/code/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps/notebook\n- https://datascience.stackexchange.com/questions/1108/kappa-near-to-60-in-unbalanced-110-data-set\n- https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english","metadata":{}}]}