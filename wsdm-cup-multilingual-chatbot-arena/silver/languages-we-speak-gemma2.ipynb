{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":85984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on November 19, 2024. By Marília Prata, mpwolke","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T21:12:04.153489Z","iopub.execute_input":"2024-11-19T21:12:04.153944Z","iopub.status.idle":"2024-11-19T21:12:05.360752Z","shell.execute_reply.started":"2024-11-19T21:12:04.153905Z","shell.execute_reply":"2024-11-19T21:12:05.359315Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Noam Chomsky on Language Aquisition\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/7Cgpfw4z8cw\" title=\"Noam Chomsky on Language Aquisition\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n\"Language sets us apart other animals communicate but they don't have anything approaching the sophisticated grammar of human languages.  \n\n\"Young children become Adept in a new language very quickly since the dawn of philosophy thinkers have argued about whether or not we have innate idea is whether we \nare born knowing things as Plat believed. Or rather as John Lock and other empiricists argued the mind is a blank slate on which experience writes.\"\n\n\"The american linguist Chomsky gave a Twist to this debate in the 1960s by demonstrating that children learning to speak just don't have enough information to form the complex grammatical maneuvers that allow them to generate unlimited new and original sentences yet.\n\nChomsky's hypothesis was that there are inborn structures in our brain what he called a language acquisition device or LAD. Which gives us a natural propensity organize the spoken language that we hear in various grammatical ways. Without that, we couldn't get started as language Learners.\"go\n\nhttps://www.youtube.com/watch?v=7Cgpfw4z8cw","metadata":{}},{"cell_type":"code","source":"#Read One parquet file. Obviously, it's big.\n\ntrain = pd.read_parquet(\"../input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\ntrain.tail(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:16:18.575135Z","iopub.execute_input":"2024-11-19T22:16:18.575507Z","iopub.status.idle":"2024-11-19T22:16:20.260829Z","shell.execute_reply.started":"2024-11-19T22:16:18.575473Z","shell.execute_reply":"2024-11-19T22:16:20.259707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What languages we \"speak\" in this data.","metadata":{}},{"cell_type":"code","source":"train['language']. value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:17:08.482049Z","iopub.execute_input":"2024-11-19T22:17:08.482447Z","iopub.status.idle":"2024-11-19T22:17:08.503991Z","shell.execute_reply.started":"2024-11-19T22:17:08.482411Z","shell.execute_reply":"2024-11-19T22:17:08.50254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Only Six rows of Hindi????\n\nhindi = train[(train['language']=='Hindi')].reset_index(drop=True)\nhindi.tail(10)\n\n#train.loc[(train[\"language\"] == \"Hindi\")]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:48:29.165814Z","iopub.execute_input":"2024-11-19T22:48:29.166246Z","iopub.status.idle":"2024-11-19T22:48:29.185215Z","shell.execute_reply.started":"2024-11-19T22:48:29.166209Z","shell.execute_reply":"2024-11-19T22:48:29.184112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## हेलो। नमस्कार। मैं बात करता हूँ sir\n\nHello, hello. I will talk to you Sir (Google translate)","metadata":{}},{"cell_type":"code","source":"#On train row 41870 is 4th row on Hindi\n\ntrain['prompt'][41870]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:29:02.017533Z","iopub.execute_input":"2024-11-19T22:29:02.01797Z","iopub.status.idle":"2024-11-19T22:29:02.025105Z","shell.execute_reply.started":"2024-11-19T22:29:02.017934Z","shell.execute_reply":"2024-11-19T22:29:02.023971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Google Translate: Hello. Namaste. I am speaking to you sir. \n\n\"Hello. Namaskar. I am speaking to you sir. Sir, I can hear your voice. Hello, yes, yes, sir Amar, I am speaking to you from Tarmiya sir. Sir, as we recently ran a campaign. Right of ORS and Banerjee. Yes, I have sent its report in the group sir. So, sir, we have shown that report on Facebook. Five thousand four hundred and thirty three times. Five thousand thirty four people have seen it from Ajmer. Total clicks on the post are thirty five and the click on your store link is twenty nine. Okay, so this is the detail sir related to your third campaign. So, further, your campaign is pending, so we will proceed on that as well. And then, as we talked to you, right, you had made payment for Lagera ji India Private Limited. Hmm hmm sir, so, like Mamta Medical Stores, we will merge it with this. Going further, like your campaigns which are pending, they will be provided to you in this.\"","metadata":{}},{"cell_type":"markdown","source":"![](https://yolainebodin.com/wp-content/uploads/2017/12/Quote-A-language-is-not-just-words...-Noam-Chomsky.png)Yolaine Bodin","metadata":{}},{"cell_type":"markdown","source":"#Keras installation","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n!pip install -q -U kagglehub --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:35:09.893502Z","iopub.execute_input":"2024-11-19T22:35:09.893946Z","iopub.status.idle":"2024-11-19T22:35:45.976825Z","shell.execute_reply.started":"2024-11-19T22:35:09.89391Z","shell.execute_reply":"2024-11-19T22:35:45.975012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:35:58.938725Z","iopub.execute_input":"2024-11-19T22:35:58.939151Z","iopub.status.idle":"2024-11-19T22:36:12.236437Z","shell.execute_reply.started":"2024-11-19T22:35:58.939109Z","shell.execute_reply":"2024-11-19T22:36:12.235421Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kaggle Secrets","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#Make yours and Add copy to clipboard\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_licorne\")\n\n#Gabriel's line\n#from kaggle_secrets import UserSecretsClient\n#user_secrets = UserSecretsClient()\n#os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n#os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:36:34.36664Z","iopub.execute_input":"2024-11-19T22:36:34.367498Z","iopub.status.idle":"2024-11-19T22:36:35.209939Z","shell.execute_reply.started":"2024-11-19T22:36:34.367456Z","shell.execute_reply":"2024-11-19T22:36:35.208792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass Config:\n    seed = 42\n    dataset_path = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena\"\n    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 5 # Original is 10 number of epochs to train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:38:31.533238Z","iopub.execute_input":"2024-11-19T22:38:31.534038Z","iopub.status.idle":"2024-11-19T22:38:31.541761Z","shell.execute_reply.started":"2024-11-19T22:38:31.533983Z","shell.execute_reply":"2024-11-19T22:38:31.54062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T22:38:41.042049Z","iopub.execute_input":"2024-11-19T22:38:41.042432Z","iopub.status.idle":"2024-11-19T22:38:41.048435Z","shell.execute_reply.started":"2024-11-19T22:38:41.0424Z","shell.execute_reply":"2024-11-19T22:38:41.047294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Don't change anything on Template line. Just the rows (in blue)\n#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ntemplate = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\nhindi[\"prompt\"] = hindi.apply(lambda row: template.format(Category=row.model_a,\n                                                             Question=row.prompt,\n                                                             Answer=row.response_a), axis=1)\ndata = hindi.prompt.tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:11:29.774329Z","iopub.execute_input":"2024-11-19T23:11:29.774869Z","iopub.status.idle":"2024-11-19T23:11:29.786255Z","shell.execute_reply.started":"2024-11-19T23:11:29.77482Z","shell.execute_reply":"2024-11-19T23:11:29.785146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Template utility function","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ndef colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:11:49.004941Z","iopub.execute_input":"2024-11-19T23:11:49.005312Z","iopub.status.idle":"2024-11-19T23:11:49.011749Z","shell.execute_reply.started":"2024-11-19T23:11:49.00528Z","shell.execute_reply":"2024-11-19T23:11:49.010437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma Causal","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ngemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:14:12.264676Z","iopub.execute_input":"2024-11-19T23:14:12.265104Z","iopub.status.idle":"2024-11-19T23:15:37.879206Z","shell.execute_reply.started":"2024-11-19T23:14:12.265069Z","shell.execute_reply":"2024-11-19T23:15:37.877871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Define the specialized class","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, category, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                Category=category,\n                Question=question,\n                Answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:16:20.248505Z","iopub.execute_input":"2024-11-19T23:16:20.248979Z","iopub.status.idle":"2024-11-19T23:16:20.256685Z","shell.execute_reply.started":"2024-11-19T23:16:20.248939Z","shell.execute_reply":"2024-11-19T23:16:20.255321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:16:44.828565Z","iopub.execute_input":"2024-11-19T23:16:44.829028Z","iopub.status.idle":"2024-11-19T23:16:44.981875Z","shell.execute_reply.started":"2024-11-19T23:16:44.82899Z","shell.execute_reply":"2024-11-19T23:16:44.980953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:17:00.85116Z","iopub.execute_input":"2024-11-19T23:17:00.8516Z","iopub.status.idle":"2024-11-19T23:17:00.859454Z","shell.execute_reply.started":"2024-11-19T23:17:00.851534Z","shell.execute_reply":"2024-11-19T23:17:00.85818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:17:23.793792Z","iopub.execute_input":"2024-11-19T23:17:23.794218Z","iopub.status.idle":"2024-11-19T23:17:24.308353Z","shell.execute_reply.started":"2024-11-19T23:17:23.794184Z","shell.execute_reply":"2024-11-19T23:17:24.307274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma_causal_lm\n\r\nEpochs!","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#set sequence length cf. config (512)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:18:33.72029Z","iopub.execute_input":"2024-11-19T23:18:33.72078Z","iopub.status.idle":"2024-11-19T23:54:46.165739Z","shell.execute_reply.started":"2024-11-19T23:18:33.720734Z","shell.execute_reply":"2024-11-19T23:54:46.163165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T23:55:46.385382Z","iopub.execute_input":"2024-11-19T23:55:46.386029Z","iopub.status.idle":"2024-11-19T23:55:46.393795Z","shell.execute_reply.started":"2024-11-19T23:55:46.385984Z","shell.execute_reply":"2024-11-19T23:55:46.3925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verify some rows ","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = hindi.iloc[4]\ngemma_qa.query(row.model_a,row.prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:06:57.829158Z","iopub.execute_input":"2024-11-20T00:06:57.829754Z","iopub.status.idle":"2024-11-20T00:07:41.521258Z","shell.execute_reply.started":"2024-11-20T00:06:57.829712Z","shell.execute_reply":"2024-11-20T00:07:41.519976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = hindi.iloc[2]\ngemma_qa.query(row.model_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = hindi.iloc[0]\ngemma_qa.query(row.model_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = hindi.iloc[5]\ngemma_qa.query(row.model_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The Languages We Speak Affect Our Perceptions of the World\n\nAuthor: O.YU. MYKHAILYUK, H.YA. POHLOD\n\n\"The article presents review of studies on interaction between language and thought and the way it influences our understanding of the world. This is an interesting and important new direction for the research on language and thought, as our society becomes increasingly global and has an increased need for clear communication across languages and cultures. With a better understanding of how language and thought interact to influence how we understand and communicate about the world, we may begin to better understand how to communicate across languages and cultures, in ways beyond simple translation, that allow us to understand the various nuances of different languages and cultures.\" \n\n\"Everyone who is multilingual talks about languages changing the way they think, affecting their personality. Each language is another personality as each language has its own distinctive way of expressing ideas. Varying grammar structures force the speaker to rethink how they emphasise certain ideas, and words can have different etymologies which, even if only on a subconscious level, affects the associations you have with them.\"\n\n\"a) Are you  unable to  think about  things you  don't have words for, or  do you lack words for them because you don't think about them? Part of the problem is that there is more involved than just language and thought; there is also culture. Your culture—the traditions, lifestyle, habits, and so on that you pick up from the people you live and interact with—shapes the way you think, and also shapes the way you talk.\"\n\n\"b) Are cultural practices a product of language, or is language a product of cultural practices? No matter what the case may be, learning another language is the best way to break free from whatever psychological shackles your mother tongue has placed on you.  What  is  clear  from  the  vast  research  literature  on  language  and  thought  is  that  language  and thought seem to  interact in some  way to  influence  how  we  understand  and communicate about  the world. \n\n\"Moreover, if the new **language is very different from your own**, it may give you **insight into another culture** and **another way of life**.\" \n\nhttps://www.researchgate.net/publication/283783768_The_Languages_We_Speak_Affect_Our_Perceptions_of_the_World","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"gemma-2-2b-it\" #model_a\nquestion = \"हेलो। नमस्कार। मैं बात करता हूँ sir\"  #Hello. Hello. I will talk to you sir\ngemma_qa.query(category,question)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:08:08.699198Z","iopub.execute_input":"2024-11-20T00:08:08.699703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt hindi index2\n\nउस समुच्चय का चयन करें जिसमेंसंख्याएँ उसी प्रकार संबंधित हैं जिस प्रकार निम्नलिखित समुच्चयोंकी\\nसंख्याएँ संबंधित हैं।\\n(55, 11, 25)\\n(64,16, 16)\n\nGoogle Translate\n\nSelect the set in which the numbers are related in the same way as the numbers of the following sets.\\n(55, 11, 25)\\n(64,16, 16)","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"chatgpt-4o-latest-20240903\" #model_a row index2\nquestion = \"उस समुच्चय का चयन करें जिसमेंसंख्याएँ उसी प्रकार संबंधित हैं जिस प्रकार निम्नलिखित समुच्चयोंकी\\nसंख्याएँ संबंधित हैं।\\n(55, 11, 25)\\n(64,16, 16)\"  #How many cover arts do we have?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt hindi index1\n\nआपका नया नया जब लगे अभी आपकी सैलरी कम है लेकिन उसमें से भी आप हर महीने दो-तीन ₹4000 बच्चा रहे हो क्योंकि आगे जाके या तो आपको कर लेनी है या घरवालों के लिए घर लेना है उसका डॉ \n\nGoogle Translate:\n\nWhen you feel that your salary is low right now, but even from that you are saving ₹4000 every month because later you will either have to do business or buy a house for your family.","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"jamba-1.5-mini\" #model_a row index 1\nquestion = \"आपका नया नया जब लगे अभी आपकी सैलरी कम है लेकिन उसमें से भी आप हर महीने दो-तीन ₹4000 बच्चा रहे हो क्योंकि आगे जाके या तो आपको कर लेनी है या घरवालों के लिए घर लेना है उसका डॉ \"  #How many cover arts do we have?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Save the model","metadata":{}},{"cell_type":"code","source":"preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\ngemma_causal_lm.save_to_preset(preset_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Acknowledgements:\r\n\r\nGabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook","metadata":{}}]}