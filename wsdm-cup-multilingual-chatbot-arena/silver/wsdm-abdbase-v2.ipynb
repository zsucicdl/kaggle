{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":209406605,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:#FF6347; font-family:'Poppins', sans-serif; color:#FFFFFF; font-size:140%; text-align:center; border: 2px solid #FF4500; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">WSDM || ABDBASE || STARTER</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\n\nfrom sklearn.base import clone\nimport optuna\nimport os\n\nfrom tqdm import tqdm\nimport category_encoders as ce\nfrom IPython.display import clear_output\nfrom scipy.sparse import hstack\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nimport string\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport lightgbm as lgb\nfrom lightgbm import early_stopping  \nfrom catboost import CatBoostRegressor, CatBoostClassifier, Pool\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\nSEED = 42\nn_splits = 5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:38:20.619055Z","iopub.execute_input":"2024-11-24T19:38:20.61944Z","iopub.status.idle":"2024-11-24T19:38:24.848024Z","shell.execute_reply.started":"2024-11-24T19:38:20.619404Z","shell.execute_reply":"2024-11-24T19:38:24.846928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#FF6347; font-family:'Poppins', sans-serif; color:#FFFFFF; font-size:140%; text-align:center; border: 2px solid #FF4500; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">Preprocess</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nsp = '/kaggle/input/abdbase/AbdML/main.py'\ntp = '/kaggle/working/main.py'\n\nwith open(sp, 'r', encoding='utf-8') as file:\n    content = file.read()\nwith open(tp, 'w', encoding='utf-8') as file:\n    file.write(content)\n\nfrom main import AbdBase\n\ntrain = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\ntest = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\nsample = pd.read_csv('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/sample_submission.csv')\n\ntrain['winner'] = train['winner'].map({\"model_a\": 0, \"model_b\": 1})\ndrop_cols = ['model_a', 'model_b', 'language', 'scored']\n\ntrain = train.drop(columns=drop_cols, errors='ignore')\ntest = test.drop(columns=drop_cols, errors='ignore')\n\ntrain['id'] = train['id'].astype('category')\ntest['id'] = test['id'].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:38:24.849817Z","iopub.execute_input":"2024-11-24T19:38:24.850371Z","iopub.status.idle":"2024-11-24T19:38:28.110438Z","shell.execute_reply.started":"2024-11-24T19:38:24.850335Z","shell.execute_reply":"2024-11-24T19:38:28.109187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nstop_words = set(stopwords.words('english'))\n\ndef text_stat(df, txt_col):\n    for col in tqdm(txt_col, desc=\"Processing text columns\"):\n\n        df[f'{col}_length'] = df[col].apply(len)\n        df[f'{col}_word_count'] = df[col].apply(lambda x: len(x.split()))\n        df[f'{col}_char_count'] = df[col].apply(lambda x: sum([len(word) for word in x.split()]))\n        df[f'{col}_avg_word_length'] = df[f'{col}_char_count'] / df[f'{col}_word_count']\n        \n        df[f'{col}_punctuation_count'] = df[col].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n        df[f'{col}_capitalized_count'] = df[col].apply(lambda x: sum([1 for word in x.split() if word.isupper()]))\n        df[f'{col}_special_char_count'] = df[col].apply(lambda x: sum([1 for char in x if not char.isalnum() and not char.isspace()]))\n        df[f'{col}_stopwords_count'] = df[col].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n        df[f'{col}_unique_word_count'] = df[col].apply(lambda x: len(set(x.split())))\n        df[f'{col}_lexical_diversity'] = df[f'{col}_unique_word_count'] / df[f'{col}_word_count']\n\n        df[f'{col}_word_length_mean'] = df[col].apply(lambda x: np.mean([len(word) for word in x.split()]))\n        df[f'{col}_word_length_median'] = df[col].apply(lambda x: np.median([len(word) for word in x.split()]))\n        df[f'{col}_word_length_max'] = df[col].apply(lambda x: max([len(word) for word in x.split()], default=0))\n        df[f'{col}_word_length_min'] = df[col].apply(lambda x: min([len(word) for word in x.split()], default=0))\n\n        df[f'{col}_sentence_length_mean'] = df[col].apply(lambda x: np.mean([len(sentence.split()) for sentence in x.split('.') if sentence.strip()]))\n        df[f'{col}_sentence_length_median'] = df[col].apply(lambda x: np.median([len(sentence.split()) for sentence in x.split('.') if sentence.strip()]))\n        df[f'{col}_sentence_length_max'] = df[col].apply(lambda x: max([len(sentence.split()) for sentence in x.split('.') if sentence.strip()], default=0))\n        df[f'{col}_sentence_length_min'] = df[col].apply(lambda x: min([len(sentence.split()) for sentence in x.split('.') if sentence.strip()], default=0))\n    \n    df['response_length_diff_a_b'] = df['response_a_length'] - df['response_b_length']\n    df['response_length_diff_b_a'] = df['response_b_length'] - df['response_a_length']\n    df['response_length_ratio_a_b'] = df['response_a_length'] / (df['response_b_length'] + 1e-6)  \n    df['response_length_ratio_b_a'] = df['response_b_length'] / (df['response_a_length'] + 1e-6)  \n    \n    return df\n\ntxt_col = ['prompt', 'response_a', 'response_b']\n\ntrain = text_stat(train, txt_col)\ntest = text_stat(test, txt_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:38:28.112463Z","iopub.execute_input":"2024-11-24T19:38:28.113001Z","iopub.status.idle":"2024-11-24T19:40:18.153446Z","shell.execute_reply.started":"2024-11-24T19:38:28.112941Z","shell.execute_reply":"2024-11-24T19:40:18.152432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ndef tf_fe(train, test, text_columns, max_features=3000, analyzer='char_wb'):\n    \n    train_features = []\n    test_features = []\n    \n    for col in tqdm(text_columns, desc=\"Processing text columns\", unit=\"col\"):\n        vectorizer = TfidfVectorizer(analyzer=analyzer, max_features=max_features)\n        train_tfidf_col = vectorizer.fit_transform(train[col])\n        test_tfidf_col = vectorizer.transform(test[col])\n        train_tfidf_col = pd.DataFrame(train_tfidf_col.toarray(), columns=[f\"tfidf_{col}_{i}\" for i in range(train_tfidf_col.shape[1])])\n        test_tfidf_col = pd.DataFrame(test_tfidf_col.toarray(), columns=[f\"tfidf_{col}_{i}\" for i in range(test_tfidf_col.shape[1])])\n        train_features.append(train_tfidf_col)\n        test_features.append(test_tfidf_col)\n    \n    train_with_tfidf = pd.concat([train, *train_features], axis=1)\n    test_with_tfidf = pd.concat([test, *test_features], axis=1)\n    \n    return train_with_tfidf, test_with_tfidf\n\ntxt_col = ['prompt', 'response_a', 'response_b']\ntrain, test = tf_fe(train, test, txt_col)\n\ntrain = train.drop(columns=txt_col, errors='ignore')\ntest = test.drop(columns=txt_col, errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:40:18.154629Z","iopub.execute_input":"2024-11-24T19:40:18.154987Z","iopub.status.idle":"2024-11-24T19:42:23.265441Z","shell.execute_reply.started":"2024-11-24T19:40:18.154954Z","shell.execute_reply":"2024-11-24T19:42:23.264318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#FF6347; font-family:'Poppins', sans-serif; color:#FFFFFF; font-size:140%; text-align:center; border: 2px solid #FF4500; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">AbdBase | LGBM</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nSEED = 42\n\nbase = AbdBase(train_data=train, test_data=test, target_column='winner',gpu=False,\n                 problem_type=\"classification\", metric=\"accuracy\", seed=SEED,\n                 n_splits=5,early_stop=True,num_classes=2,test_prob=True,\n                 fold_type='SKF',weights=None,tf_vec=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:42:23.267493Z","iopub.execute_input":"2024-11-24T19:42:23.267833Z","iopub.status.idle":"2024-11-24T19:42:24.332699Z","shell.execute_reply.started":"2024-11-24T19:42:23.267799Z","shell.execute_reply":"2024-11-24T19:42:24.331583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nParams = {'n_estimators': 2083, 'learning_rate': 0.02516607127550297, 'max_depth': 11, 'num_leaves': 31,'n_jobs':-1,\n          'min_child_samples': 42, 'subsample': 0.8085392166316496, 'colsample_bytree': 0.6281848449949525,\n          'lambda_l1': 4.02155452669029, 'lambda_l2': 0.14096175149815865, 'min_gain_to_split': 0.2960660809801552}\n\nmeanOFFL, meanTestL = base.Train_ML(Params,'LGBM',e_stop=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:44:35.025034Z","iopub.execute_input":"2024-11-24T19:44:35.025432Z","iopub.status.idle":"2024-11-24T19:48:46.415992Z","shell.execute_reply.started":"2024-11-24T19:44:35.025397Z","shell.execute_reply":"2024-11-24T19:48:46.414759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#FF6347; font-family:'Poppins', sans-serif; color:#FFFFFF; font-size:140%; text-align:center; border: 2px solid #FF4500; border-radius:15px; padding: 15px; box-shadow: 5px 5px 20px rgba(0, 0, 0, 0.5); font-weight: bold; letter-spacing: 1px; text-transform: uppercase;\">Submission</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nsample['winner'] = np.round(meanTestL).astype('int')\nsample['winner'] = sample['winner'].map({0: 'model_a', 1: 'model_b'})\n\nsample.to_csv('submission.csv', index = False)\nsample.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T19:49:44.586397Z","iopub.execute_input":"2024-11-24T19:49:44.586829Z","iopub.status.idle":"2024-11-24T19:49:44.608447Z","shell.execute_reply.started":"2024-11-24T19:49:44.58678Z","shell.execute_reply":"2024-11-24T19:49:44.607298Z"}},"outputs":[],"execution_count":null}]}