{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:#E3F6FC; font-family:'Caveat', cursive; color:#4A4E69; font-size:140%; text-align:center; border: 3px dashed #A0CED9; border-radius:25px; padding: 15px; box-shadow: 3px 3px 15px rgba(74, 78, 105, 0.4); font-weight: bold; letter-spacing: 1.5px; text-transform: uppercase;\">WSDM GET STARTER</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\n\nfrom sklearn.base import clone\nimport optuna\nimport os\n\nfrom tqdm import tqdm\nimport category_encoders as ce\nfrom IPython.display import clear_output\nfrom scipy.sparse import hstack\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nimport string\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, early_stopping, LGBMClassifier\nfrom lightgbm import early_stopping  \nfrom catboost import CatBoostRegressor, CatBoostClassifier, Pool\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:07:59.29997Z","iopub.execute_input":"2024-11-22T18:07:59.30044Z","iopub.status.idle":"2024-11-22T18:08:03.764674Z","shell.execute_reply.started":"2024-11-22T18:07:59.300388Z","shell.execute_reply":"2024-11-22T18:08:03.76352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#E3F6FC; font-family:'Caveat', cursive; color:#4A4E69; font-size:140%; text-align:center; border: 3px dashed #A0CED9; border-radius:25px; padding: 15px; box-shadow: 3px 3px 15px rgba(74, 78, 105, 0.4); font-weight: bold; letter-spacing: 1.5px; text-transform: uppercase;\">Load Data</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\ntest = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\nsample = pd.read_csv('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/sample_submission.csv')\n\ntrain['id'] = train['id'].astype('category')\ntest['id'] = test['id'].astype('category')\n\ntrain['winner'] = train['winner'].map({\n    \"model_a\": 0,\n     \"model_b\": 1\n     })\n\ncols_to_drops = ['model_a', 'model_b', 'language', 'scored']\ntrain = train.drop(columns=cols_to_drops, errors='ignore')\ntest = test.drop(columns=cols_to_drops, errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:08:03.766865Z","iopub.execute_input":"2024-11-22T18:08:03.767628Z","iopub.status.idle":"2024-11-22T18:08:06.724361Z","shell.execute_reply.started":"2024-11-22T18:08:03.767575Z","shell.execute_reply":"2024-11-22T18:08:06.723071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#E3F6FC; font-family:'Caveat', cursive; color:#4A4E69; font-size:140%; text-align:center; border: 3px dashed #A0CED9; border-radius:25px; padding: 15px; box-shadow: 3px 3px 15px rgba(74, 78, 105, 0.4); font-weight: bold; letter-spacing: 1.5px; text-transform: uppercase;\">Preprocessing Text Columns</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Import the set of English stopwords from NLTK\nstop_words = set(stopwords.words('english'))\n\n# Function to compute various text statistics for specified columns in a DataFrame\ndef text_stat(df, text_columns):\n    \"\"\"\n    Compute a variety of text statistics for each text column in the dataset.\n    \n    Args:\n    df (pd.DataFrame): The DataFrame to process.\n    txt_col (list): List of text column names to compute statistics for.\n\n    Returns:\n    pd.DataFrame: DataFrame with added text statistic features.\n    \"\"\"\n    # Loop through each text column in the provided list\n    for col in tqdm(text_columns, desc=\"Processing text columns\"):\n        \n        # Calculate the length of each text entry\n        df[f'{col}_length'] = df[col].apply(len)\n        \n        # Calculate the word count for each text entry\n        df[f'{col}_word_count'] = df[col].apply(lambda x: len(x.split()))\n        \n        # Calculate the total character count (excluding spaces) for all words in each entry\n        df[f'{col}_char_count'] = df[col].apply(lambda x: sum([len(word) for word in x.split()]))\n        \n        # Calculate the average word length for each text entry\n        df[f'{col}_avg_word_length'] = df[f'{col}_char_count'] / df[f'{col}_word_count']\n        \n        # Count the number of punctuation marks in each text entry\n        df[f'{col}_punctuation_count'] = df[col].apply(lambda x: sum([1 for char in x if char in string.punctuation]))\n        \n        # Count the number of fully capitalized words in each text entry\n        df[f'{col}_capitalized_count'] = df[col].apply(lambda x: sum([1 for word in x.split() if word.isupper()]))\n        \n        # Count the number of special characters (non-alphanumeric and non-space) in each entry\n        df[f'{col}_special_char_count'] = df[col].apply(lambda x: sum([1 for char in x if not char.isalnum() and not char.isspace()]))\n        \n        # Count the number of stopwords in each text entry\n        df[f'{col}_stopwords_count'] = df[col].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n        \n        # Count the number of unique words in each text entry\n        df[f'{col}_unique_word_count'] = df[col].apply(lambda x: len(set(x.split())))\n        \n        # Calculate the lexical diversity (unique words divided by total word count)\n        df[f'{col}_lexical_diversity'] = df[f'{col}_unique_word_count'] / df[f'{col}_word_count']\n        \n        # Calculate the mean word length in each text entry\n        df[f'{col}_word_length_mean'] = df[col].apply(lambda x: np.mean([len(word) for word in x.split()]))\n        \n        # Calculate the median word length in each text entry\n        df[f'{col}_word_length_median'] = df[col].apply(lambda x: np.median([len(word) for word in x.split()]))\n        \n        # Calculate the maximum word length in each text entry\n        df[f'{col}_word_length_max'] = df[col].apply(lambda x: max([len(word) for word in x.split()], default=0))\n        \n        # Calculate the minimum word length in each text entry\n        df[f'{col}_word_length_min'] = df[col].apply(lambda x: min([len(word) for word in x.split()], default=0))\n    \n    return df\n\n# List of text columns to process\ntext_columns = ['prompt', 'response_a', 'response_b']\n\n# Compute text statistics for the train dataset\ntrain = text_stat(train, text_columns)\n\n# Compute text statistics for the test dataset\ntest = text_stat(test, text_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:08:06.725655Z","iopub.execute_input":"2024-11-22T18:08:06.726085Z","iopub.status.idle":"2024-11-22T18:09:38.677444Z","shell.execute_reply.started":"2024-11-22T18:08:06.726055Z","shell.execute_reply":"2024-11-22T18:09:38.676286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Function to process text columns using TF-IDF vectorization and add the features to the original dataset\ndef TF_IDF_Features_Text_columns(train, test, text_columns, max_features=3000, analyzer='char_wb'):\n    \"\"\"\n    Process text columns in the train and test datasets using TF-IDF vectorization.\n    \n    Args:\n    train (pd.DataFrame): Training dataset.\n    test (pd.DataFrame): Test dataset.\n    text_columns (list): List of text column names to process.\n    max_features (int): Maximum number of features to extract for each column.\n    analyzer (str): Tokenization mode for TF-IDF ('word', 'char', or 'char_wb').\n\n    Returns:\n    pd.DataFrame, pd.DataFrame: Updated train and test datasets with added TF-IDF features.\n    \"\"\"\n    \n    train_features = []  # List to store TF-IDF features for the train dataset\n    test_features = []   # List to store TF-IDF features for the test dataset\n    \n    # Loop through each text column to process it\n    for col in tqdm(text_columns, desc=\"Processing text columns\", unit=\"col\"):\n        # Initialize TF-IDF vectorizer with specified parameters\n        vectorizer = TfidfVectorizer(analyzer=analyzer, max_features=max_features)\n        \n        # Fit the vectorizer on the train column and transform train and test columns\n        train_tfidf_col = vectorizer.fit_transform(train[col])\n        test_tfidf_col = vectorizer.transform(test[col])\n        \n        # Convert the sparse TF-IDF matrices to DataFrames with meaningful column names\n        train_tfidf_col = pd.DataFrame(\n            train_tfidf_col.toarray(), \n            columns=[f\"tfidf_{col}_{i}\" for i in range(train_tfidf_col.shape[1])]\n        )\n        test_tfidf_col = pd.DataFrame(\n            test_tfidf_col.toarray(), \n            columns=[f\"tfidf_{col}_{i}\" for i in range(test_tfidf_col.shape[1])]\n        )\n        \n        # Append the processed TF-IDF DataFrames to their respective lists\n        train_features.append(train_tfidf_col)\n        test_features.append(test_tfidf_col)\n    \n    # Concatenate the original train dataset with the newly generated TF-IDF features\n    train_with_tfidf = pd.concat([train, *train_features], axis=1)\n    \n    # Concatenate the original test dataset with the newly generated TF-IDF features\n    test_with_tfidf = pd.concat([test, *test_features], axis=1)\n    \n    return train_with_tfidf, test_with_tfidf\n\n# Specify the text columns to process\ntext_columns = ['prompt', 'response_a', 'response_b']\n\n# Apply the TF-IDF feature engineering function to the train and test datasets\ntrain, test = TF_IDF_Features_Text_columns(train, test, text_columns)\n\n# Drop the original text columns from both datasets as they are now represented by TF-IDF features\ntrain = train.drop(columns=text_columns, errors='ignore')\ntest = test.drop(columns=text_columns, errors='ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:09:38.680604Z","iopub.execute_input":"2024-11-22T18:09:38.681159Z","iopub.status.idle":"2024-11-22T18:11:46.542836Z","shell.execute_reply.started":"2024-11-22T18:09:38.681103Z","shell.execute_reply":"2024-11-22T18:11:46.541412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#E3F6FC; font-family:'Caveat', cursive; color:#4A4E69; font-size:140%; text-align:center; border: 3px dashed #A0CED9; border-radius:25px; padding: 15px; box-shadow: 3px 3px 15px rgba(74, 78, 105, 0.4); font-weight: bold; letter-spacing: 1.5px; text-transform: uppercase;\">Modeling</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Define constants for reproducibility and splitting\nSEED = 42\nn_splits = 5\n\n# Separate features and target variable from the training dataset\nX = train.drop(['winner'], axis=1)  # Features\ny = train['winner']                 # Target variable\n\ndef TrainML(model, X_test_data):\n    \"\"\"\n    Train a machine learning model using StratifiedKFold cross-validation.\n    \n    Args:\n    model: The machine learning model to train.\n    X_test_data (pd.DataFrame): Test dataset for generating predictions.\n\n    Returns:\n    mean_test_preds (np.array): Mean predictions on the test data across all folds.\n    \"\"\"\n    n_splits = 5  # Number of folds for StratifiedKFold\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n\n    # Lists to store accuracy scores and predictions for each fold\n    train_accuracy_scores = []\n    val_accuracy_scores = []\n    test_preds_list = []\n    trained_models = []  # To store trained models for potential later use\n\n    # Iterate through each fold\n    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), \n                                                     desc=\"Training Folds\", total=n_splits)):\n        # Split data into training and validation sets for the current fold\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Define early stopping callback to prevent overfitting\n        callbacks = [early_stopping(stopping_rounds=40, verbose=False)]\n\n        # Train the model on the training set\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=callbacks)\n        trained_models.append(model)  # Save the trained model\n\n        # Predict on the training and validation sets\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        # Calculate accuracy scores for training and validation\n        train_accuracy = accuracy_score(y_train, y_train_pred)\n        val_accuracy = accuracy_score(y_val, y_val_pred)\n\n        # Append accuracy scores to the respective lists\n        train_accuracy_scores.append(train_accuracy)\n        val_accuracy_scores.append(val_accuracy)\n\n        # Predict probabilities on the test data and save them\n        test_preds = model.predict_proba(X_test_data)[:, 1]  # Get probability for the positive class\n        test_preds_list.append(test_preds)\n\n        # Print fold-specific accuracy scores\n        print(f\"Fold {fold+1} - Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n        clear_output(wait=True)  # Clear the output to keep the console clean\n\n    # Calculate the mean predictions on the test data across all folds\n    mean_test_preds = np.mean(test_preds_list, axis=0)\n\n    # Print summary of scores\n    print(\"\\n--- Final Mean Scores ---\")\n    print(f\"Mean Train Accuracy: {np.mean(train_accuracy_scores):.4f}\")\n    print(f\"Mean Validation Accuracy: {np.mean(val_accuracy_scores):.4f}\")\n\n    # Create a DataFrame to summarize fold-wise accuracy scores\n    results_df = pd.DataFrame({\n        'Fold': np.arange(1, n_splits + 1),\n        'Train Accuracy': train_accuracy_scores,\n        'Validation Accuracy': val_accuracy_scores\n    })\n\n    # Display fold-wise scores\n    print(\"\\n=== Fold-wise Accuracy Scores ===\")\n    print(results_df)\n\n    return mean_test_preds  # Return the mean test predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:15:20.901754Z","iopub.execute_input":"2024-11-22T18:15:20.902514Z","iopub.status.idle":"2024-11-22T18:15:22.440357Z","shell.execute_reply.started":"2024-11-22T18:15:20.902469Z","shell.execute_reply":"2024-11-22T18:15:22.43858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nLightParams = {'n_estimators': 2860, 'learning_rate': 0.022544116997360492, 'max_depth': 11, 'num_leaves': 31, 'min_child_samples': 42, 'subsample': 0.8085392166316496,\n 'colsample_bytree': 0.6281848449949525, 'lambda_l1': 4.02155452669029, 'lambda_l2': 0.14096175149815865, 'min_gain_to_split': 0.2960660809801552,'n_jobs':-1}\n\nLight_Model = LGBMClassifier(**LightParams, verbose=-1, random_state=SEED)\n\nmean_preds = TrainML(Light_Model, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:15:22.442291Z","iopub.execute_input":"2024-11-22T18:15:22.442645Z","iopub.status.idle":"2024-11-22T18:31:05.231324Z","shell.execute_reply.started":"2024-11-22T18:15:22.442614Z","shell.execute_reply":"2024-11-22T18:31:05.229996Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"background-color:#E3F6FC; font-family:'Caveat', cursive; color:#4A4E69; font-size:140%; text-align:center; border: 3px dashed #A0CED9; border-radius:25px; padding: 15px; box-shadow: 3px 3px 15px rgba(74, 78, 105, 0.4); font-weight: bold; letter-spacing: 1.5px; text-transform: uppercase;\">Submission</p>","metadata":{}},{"cell_type":"code","source":"%%time\n\nsample['winner'] = np.round(mean_preds).astype('int')\nsample['winner'] = sample['winner'].map({0: 'model_a', 1: 'model_b'})\n\nsample.to_csv('submission.csv', index = False)\nsample.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T18:31:32.241827Z","iopub.execute_input":"2024-11-22T18:31:32.24237Z","iopub.status.idle":"2024-11-22T18:31:32.270991Z","shell.execute_reply.started":"2024-11-22T18:31:32.242329Z","shell.execute_reply":"2024-11-22T18:31:32.269785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}