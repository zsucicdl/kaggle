{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **The model parameters were obtained using Optuna.**\n \n1. Add 'response_a_len','response_b_len' etc. they are valueable.Accuracy increased by 0.01.\n\n2. I think simple classification models are not effective in solving this problem, and the following perspectives should be considered:\n    (1)Accuracy of content: Check if the answer accurately and correctly answers the question.\n    (2)Integrity of content: Evaluate whether the answer comprehensively covers all aspects of the question.\n    (3)Standardization of language: Evaluate whether the language expression of the answer is clear, accurate, and appropriate, and whether there are grammar or spelling errors.\n    \n    So,this code is for reference only. Please consider other more suitable methods.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:01:43.460159Z","iopub.execute_input":"2024-11-21T06:01:43.460645Z","iopub.status.idle":"2024-11-21T06:01:43.467479Z","shell.execute_reply.started":"2024-11-21T06:01:43.460609Z","shell.execute_reply":"2024-11-21T06:01:43.466169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\")\nprint(\"Read dataset completed\")\n\n\n#train['prompt_len']=train['prompt'].apply(len)\n#train['response_a_len']=train['response_a'].apply(len)\n#train['response_b_len']=train['response_b'].apply(len)\n#train['response_a_len/prompt_len']=train['response_a_len']/train['prompt_len']\n#train['response_b_len/prompt_len']=train['response_b_len']/train['prompt_len']\n#train['response_b_len/response_a_len']=train['response_b_len']/train['response_a_len']\n\n#test['prompt_len']=test['prompt'].apply(len)\n#test['response_a_len']=test['response_a'].apply(len)\n#test['response_b_len']=test['response_b'].apply(len)\n#test['response_a_len/prompt_len']=test['response_a_len']/test['prompt_len']\n#test['response_b_len/prompt_len']=test['response_b_len']/test['prompt_len']\n#test['response_b_len/response_a_len']=test['response_b_len']/test['response_a_len']","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:01:43.471984Z","iopub.execute_input":"2024-11-21T06:01:43.472883Z","iopub.status.idle":"2024-11-21T06:01:44.775697Z","shell.execute_reply.started":"2024-11-21T06:01:43.472811Z","shell.execute_reply":"2024-11-21T06:01:44.774233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_model_a = train[train['winner'] == 'model_a']\ndf_model_a.drop(columns=['response_b'], inplace=True)\ndf_model_a.rename(columns={'response_a': 'response'}, inplace=True)\n\n\ndf_model_b = train[train['winner'] == 'model_b']\ndf_model_b.drop(columns=['response_a'], inplace=True)\ndf_model_b.rename(columns={'response_b': 'response'}, inplace=True)\n\ntrain = pd.concat([df_model_a, df_model_b], ignore_index=True)\n\ntrain['prompt_len']=train['prompt'].apply(len)\ntrain['response_len']=train['response'].apply(len)\ntrain['response_len/prompt_len']=train['response_len']/train['prompt_len']\n\ntrain['winner'] = train['winner'].map({\"model_a\": 0, \"model_b\": 1})\n\n#print(train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T06:01:44.778057Z","iopub.execute_input":"2024-11-21T06:01:44.778426Z","iopub.status.idle":"2024-11-21T06:01:44.902947Z","shell.execute_reply.started":"2024-11-21T06:01:44.778394Z","shell.execute_reply":"2024-11-21T06:01:44.901774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_a = test.drop('response_b', axis=1)\ntest_a.rename(columns={'response_a': 'response'}, inplace=True)\ntest_a['prompt_len']=test_a['prompt'].apply(len)\ntest_a['response_len']=test_a['response'].apply(len)\ntest_a['response_len/prompt_len']=test_a['response_len']/test_a['prompt_len']\n#print(test_a.head())\n\ntest_b = test.drop('response_a', axis=1)\ntest_b.rename(columns={'response_b': 'response'}, inplace=True)\ntest_b['prompt_len']=test_b['prompt'].apply(len)\ntest_b['response_len']=test_b['response'].apply(len)\ntest_b['response_len/prompt_len']=test_b['response_len']/test_b['prompt_len']\n#print(test_b.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T06:01:44.904791Z","iopub.execute_input":"2024-11-21T06:01:44.905617Z","iopub.status.idle":"2024-11-21T06:01:44.932162Z","shell.execute_reply.started":"2024-11-21T06:01:44.905552Z","shell.execute_reply":"2024-11-21T06:01:44.929483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('prompt_feats', TfidfVectorizer(analyzer = 'char_wb',max_features=1000), 'prompt'),\n        ('response', TfidfVectorizer(analyzer = 'char_wb',max_features=1000), 'response'),\n        #('response_a_feats', TfidfVectorizer(analyzer = 'char_wb',max_features=3000), 'response_a'),\n        #('response_b_feats', TfidfVectorizer(analyzer = 'char_wb',max_features=3000), 'response_b')\n    ]\n)\nprint(\"TfidfVectorizer is woking...\")\ntrain_tfidf = preprocessor.fit_transform(train)\ntest_a_tfidf = preprocessor.transform(test_a)\ntest_b_tfidf = preprocessor.transform(test_b)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:01:44.935481Z","iopub.execute_input":"2024-11-21T06:01:44.935996Z","iopub.status.idle":"2024-11-21T06:02:56.261089Z","shell.execute_reply.started":"2024-11-21T06:01:44.935948Z","shell.execute_reply":"2024-11-21T06:02:56.25988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"######################################################################\nnum_features = train_tfidf.shape[1]\nnew_columns = [f\"tfidf{i+1}\" for i in range(num_features)]\n\ntrain_tfidf = pd.DataFrame(train_tfidf.toarray(), columns=new_columns)\ntrain_tfidf = pd.concat([train[['prompt_len','response_len','response_len/prompt_len']],train_tfidf], axis=1)\n\ntest_a_tfidf = pd.DataFrame(test_a_tfidf.toarray(), columns=new_columns)\ntest_a_tfidf = pd.concat([test_a[['prompt_len','response_len','response_len/prompt_len']],test_a_tfidf], axis=1)\n\ntest_b_tfidf = pd.DataFrame(test_b_tfidf.toarray(), columns=new_columns)\ntest_b_tfidf = pd.concat([test_b[['prompt_len','response_len','response_len/prompt_len']],test_b_tfidf], axis=1)\n\nprint(\"Feature engineering is completed\")\n#print(train_tfidf.columns.value_counts())\n#print(test_tfidf.columns.value_counts())\n######################################################################\n","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:02:56.262628Z","iopub.execute_input":"2024-11-21T06:02:56.263008Z","iopub.status.idle":"2024-11-21T06:02:58.822788Z","shell.execute_reply.started":"2024-11-21T06:02:56.262977Z","shell.execute_reply":"2024-11-21T06:02:58.821434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=train_tfidf\ny=train['winner']","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:02:58.824373Z","iopub.execute_input":"2024-11-21T06:02:58.824687Z","iopub.status.idle":"2024-11-21T06:02:58.828804Z","shell.execute_reply.started":"2024-11-21T06:02:58.824656Z","shell.execute_reply":"2024-11-21T06:02:58.827796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb_param = {'num_leaves': 72, 'max_depth': 20, 'learning_rate': 0.04011765418908751, 'n_estimators': 244, 'reg_alpha': 2.2150749155666984, 'reg_lambda': 19.795375399670494,\n        'random_state': 42, 'verbose': -1}\nxgb_param = {'n_estimators': 454, 'learning_rate': 0.06466238370778891, 'max_depth': 2, 'reg_lambda': 27.280518858342674, 'min_data_in_leaf': 34,\n        'random_state': 42, 'verbose': -1}\ncat_param = {'depth': 7, 'learning_rate': 0.014180537144799797, 'n_estimators': 1980, 'reg_lambda': 0.6903912535122932,\n        'random_state': 42, 'verbose': 0}\n\nlgb_model = lgb.LGBMClassifier(**lgb_param)\nxgb_model = xgb.XGBClassifier(**xgb_param)\ncat_model = CatBoostClassifier(**cat_param)","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:02:58.8303Z","iopub.execute_input":"2024-11-21T06:02:58.830653Z","iopub.status.idle":"2024-11-21T06:02:58.848035Z","shell.execute_reply.started":"2024-11-21T06:02:58.830623Z","shell.execute_reply":"2024-11-21T06:02:58.846891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training model is working...\")\nmodels = [('lgb', lgb_model), ('xgb', xgb_model), ('cat', cat_model)]\nweights = [1, 1, 1] \nvoting_clf = VotingClassifier(estimators=models, voting='soft', weights=weights)\nvoting_clf.fit(X, y)\n\n\n# Accuracy of train Dataset\ny_pred = voting_clf.predict(X)\naccuracy = accuracy_score(y, y_pred)\nprint(f\"Accuracy of train Dataset: {accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:02:58.849282Z","iopub.execute_input":"2024-11-21T06:02:58.849766Z","iopub.status.idle":"2024-11-21T06:24:19.565132Z","shell.execute_reply.started":"2024-11-21T06:02:58.849721Z","shell.execute_reply":"2024-11-21T06:24:19.563925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Prediction is being executed...\")\ny_pred_a = voting_clf.predict(test_a_tfidf)\ny_pred_b = voting_clf.predict(test_b_tfidf)\n#y_pred_labels = ['model_a' if label == 0 else 'model_b' for label in y_pred]\ny_pred_labels = np.where(y_pred_a > y_pred_b, 'model_a', 'model_b')","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:24:19.566426Z","iopub.execute_input":"2024-11-21T06:24:19.566753Z","iopub.status.idle":"2024-11-21T06:24:20.099845Z","shell.execute_reply.started":"2024-11-21T06:24:19.566722Z","shell.execute_reply":"2024-11-21T06:24:20.09794Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission= pd.DataFrame({'id': test['id'], 'winner': y_pred_labels})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-21T06:24:20.104308Z","iopub.execute_input":"2024-11-21T06:24:20.104864Z","iopub.status.idle":"2024-11-21T06:24:20.116122Z","shell.execute_reply.started":"2024-11-21T06:24:20.104778Z","shell.execute_reply":"2024-11-21T06:24:20.114501Z"},"trusted":true},"outputs":[],"execution_count":null}]}