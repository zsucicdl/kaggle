{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"competition","sourceId":86946,"databundleVersionId":10131489},{"sourceType":"modelInstanceVersion","sourceId":85984,"databundleVersionId":9247149,"modelInstanceId":72244},{"sourceType":"modelInstanceVersion","sourceId":85986,"databundleVersionId":9247152,"modelInstanceId":72246}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on November 18, 2024. By Mar√≠lia Prata, mpwolke.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:34:26.437079Z","iopub.execute_input":"2024-11-18T23:34:26.437588Z","iopub.status.idle":"2024-11-18T23:34:29.351074Z","shell.execute_reply.started":"2024-11-18T23:34:26.437535Z","shell.execute_reply":"2024-11-18T23:34:29.349974Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Competition Citation\n\n@misc{wsdm-cup-multilingual-chatbot-arena,\n\r\n    author = {Wei-lin Chiang and Evan Frick and Lisa Dunlap and Anastasios Angelopoulos and Joseph E. Gonzalez and Ion Stoica and Sohier Dane and Maggie Demkin and Nate Keating}\n    ,\r\n    title = {WSDM Cup - Multilingual Chatbot Arena\n    },\r\n    year = {2024},\r\n    howpublished = {\\url{https://kaggle.com/competitions/wsdm-cup-multilingual-chatbot-aren\n    a}},\r\n    note = {Kaggle}\r\n}","metadata":{}},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQa9psK1q-xrMPxO-9oLH20PB8BCFu05HSgvg&s)WSDM - Call for Papers","metadata":{}},{"cell_type":"markdown","source":"## WSDM (Web Search and Data Mining)\n\n\"WSDM is a highly selective conference that includes invited talks, as well as refereed full papers. WSDM publishes original, high-quality papers related to search and data mining on the Web and the Social Web, with an emphasis on practical yet principled novel models of search and data mining, algorithm design and analysis, economic implications, and in-depth experimental analysis of accuracy and performance.\"\n\n**List of Topics**\n\n* Web Search\n\n\"Algorithms for web-scale search; Adversarial search; Search user interfaces and interaction; Distributed search, metasearch, peer-to-peer search; Local and mobile search; Multimedia web search, cross-lingual search; Query analysis and query processing; Search benchmarking and evaluation; Search user behavior and log analysis; Search with Foundation Models.\"\n\n\n* Web Mining and Content Analysis\n\n\"Crawling and indexing web content; Web recommender systems and algorithms; Clustering, classification, and summarization of web data; Data, entity, event, and relationship extraction; Knowledge acquisition and automatic construction of knowledge bases; Large-scale graph analysis; Semantic search, faceted search, and knowledge graphs; Multimodal data mining; Scalable algorithms for mining web data; Opinion mining and sentiment analysis; Web traffic and log analysis; Web measurements, web evolution, and web models.\"\n\n* Web of Things, Ubiquitous and Mobile Computing\n\n\"Algorithms for web-scale search; Adversarial search; Search user interfaces and interaction; Distributed search, metasearch, peer-to-peer search; Local and mobile search; Multimedia web search, cross-lingual search; Query analysis and query processing; Search benchmarking and evaluation; Search user behavior and log analysis; Search with Foundation Models.\"\n\n* Privacy, Fairness, Interpretability\n\n\"Fairness and accountability in ranking, recommendations and advertising; Explainability in web systems; Model and algorithm transparency; Interpretable models of individual and social behavior; Web search and data mining under privacy constraints; Fairness and interpretability in applications of web mining for social good.\"\n\n* Social Networks\n\n\"Link prediction and community detection; Social network analysis and graph algorithms; Computational social science; Influence and viral marketing in social networks; Social sensing; Searching social and real-time content; Social network dynamics; Sampling, experiments, and evaluation in social networks; Social media analysis: blogs and friendship networks; Social network analysis, theories, models and applications; Social reputation and trust.\"\n\n* Intelligent Assistants\n\n\"Voice search, conversational search, and dialog systems; Personal assistants, dialogue models, and conversational interaction; Task-driven search; Zero-query and implicit search.\"\n\n* Crowdsourcing and Human Computation\n\n\"Collaborative search and question answering; Human-in-the-Loop and Collaborative Human-AI systems.\"\n\n* Emerging and Creative Applications\n\n\"Mental health and well-being support systems; Web mining for social good; Systems and algorithms for urban applications such as smart cities/buildings/etc; Online education systems; Monitoring and prevention of epidemics; Social chatbots.\"\n\n* Information Integrity\n\n\"Systems and algorithms for monitoring and detection of misinformation and fake news; Prevalence and virality of misinformation; Misinformation sources and origins; Source and content credibility; Detecting and combating spamming, trolling, aggression, dog whistles, and toxic online behaviors; Methods for detecting and mitigating low quality and offensive content, bullying and hate speech; Polarization, extremism and radicalization; Echo chambers and filter bubbles.\"\n\n* Foundation Models\n\n\"Use of large language models (LLMs) and other foundation models for web search and data mining, including but not limited to the following tasks: Generative question answering; Indexing and query analysis; Pre-training and self-supervised learning for web-based tasks; Development of new user interfaces and user experiences; Support to information integrity.\"abs\n\nhttp://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=181081&copyownerid=187788","metadata":{}},{"cell_type":"markdown","source":"## Train parquet last five rows","metadata":{}},{"cell_type":"code","source":"#Read One parquet file. Obviously, it's big.\n\ntrain = pd.read_parquet(\"../input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\ntrain.tail(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:10:25.041879Z","iopub.execute_input":"2024-11-18T23:10:25.042689Z","iopub.status.idle":"2024-11-18T23:10:26.837684Z","shell.execute_reply.started":"2024-11-18T23:10:25.042649Z","shell.execute_reply":"2024-11-18T23:10:26.836619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:40:52.164115Z","iopub.execute_input":"2024-11-18T22:40:52.165245Z","iopub.status.idle":"2024-11-18T22:40:52.180078Z","shell.execute_reply.started":"2024-11-18T22:40:52.165198Z","shell.execute_reply":"2024-11-18T22:40:52.179118Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test file: Spanish/English/Portuguese ","metadata":{}},{"cell_type":"code","source":"test = pd.read_parquet(\"../input/wsdm-cup-multilingual-chatbot-arena/test.parquet\")\ntest.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:07:33.635672Z","iopub.execute_input":"2024-11-18T21:07:33.636112Z","iopub.status.idle":"2024-11-18T21:07:33.659808Z","shell.execute_reply.started":"2024-11-18T21:07:33.636076Z","shell.execute_reply":"2024-11-18T21:07:33.658496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt 4th row: Please be Boring","metadata":{}},{"cell_type":"code","source":"train['prompt'][4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:43:04.882334Z","iopub.execute_input":"2024-11-18T22:43:04.882755Z","iopub.status.idle":"2024-11-18T22:43:04.890394Z","shell.execute_reply.started":"2024-11-18T22:43:04.882722Z","shell.execute_reply":"2024-11-18T22:43:04.889104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understood?","metadata":{}},{"cell_type":"code","source":"train['response_b'][4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:42:50.857511Z","iopub.execute_input":"2024-11-18T22:42:50.858022Z","iopub.status.idle":"2024-11-18T22:42:50.866239Z","shell.execute_reply.started":"2024-11-18T22:42:50.85798Z","shell.execute_reply":"2024-11-18T22:42:50.864686Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Alright, I'll be as boring as possible ; )  ","metadata":{}},{"cell_type":"code","source":"train['response_a'][4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:11:01.009151Z","iopub.execute_input":"2024-11-18T23:11:01.009571Z","iopub.status.idle":"2024-11-18T23:11:01.017843Z","shell.execute_reply.started":"2024-11-18T23:11:01.009533Z","shell.execute_reply":"2024-11-18T23:11:01.016728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Two lines Required to Plot Plotly\n\nimport plotly.io as pio\npio.renderers.default = 'iframe'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:25:37.567449Z","iopub.execute_input":"2024-11-18T21:25:37.567826Z","iopub.status.idle":"2024-11-18T21:25:37.766715Z","shell.execute_reply.started":"2024-11-18T21:25:37.567795Z","shell.execute_reply":"2024-11-18T21:25:37.765351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO\n\nfig = px.bar(train[\"winner\"].value_counts(),\n             title=\"Counts of Battle Winner Models\", text_auto=True, height=400, color_discrete_sequence=['crimson'])\nfig.update_layout(xaxis_title=\"Winner Models\", yaxis_title=\"Count\", \n                  showlegend=False)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:45:18.107327Z","iopub.execute_input":"2024-11-18T21:45:18.107836Z","iopub.status.idle":"2024-11-18T21:45:18.19641Z","shell.execute_reply.started":"2024-11-18T21:45:18.107796Z","shell.execute_reply":"2024-11-18T21:45:18.195166Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO\n\nfig = px.bar(train[\"model_a\"].value_counts(),\n             title=\"Counts of Battle Outcomes A\", text_auto=True, height=400, color_discrete_sequence=['Chartreuse'])\nfig.update_layout(xaxis_title=\"Battle Outcome A\", yaxis_title=\"Count\", \n                  showlegend=False)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:38:46.59765Z","iopub.execute_input":"2024-11-18T21:38:46.598104Z","iopub.status.idle":"2024-11-18T21:38:46.686124Z","shell.execute_reply.started":"2024-11-18T21:38:46.598065Z","shell.execute_reply":"2024-11-18T21:38:46.684997Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO\n\nfig = px.bar(train[\"model_b\"].value_counts(),\n             title=\"Counts of Battle Outcomes B\", text_auto=True, height=400, color_discrete_sequence=['coral'])\nfig.update_layout(xaxis_title=\"Battle Outcome B\", yaxis_title=\"Count\",\n                  showlegend=False)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:40:18.868013Z","iopub.execute_input":"2024-11-18T21:40:18.868472Z","iopub.status.idle":"2024-11-18T21:40:18.960184Z","shell.execute_reply.started":"2024-11-18T21:40:18.868427Z","shell.execute_reply":"2024-11-18T21:40:18.958964Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO\n\nfig = px.bar(pd.concat([train[\"model_a\"], train[\"model_b\"]]).value_counts(),\n             title=\"Battle Count for Each Model\", text_auto=True)\nfig.update_layout(xaxis_title=\"model\", yaxis_title=\"Battle Count\", height=400,\n                  showlegend=False)\nfig  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:46:38.311286Z","iopub.execute_input":"2024-11-18T21:46:38.311712Z","iopub.status.idle":"2024-11-18T21:46:38.406155Z","shell.execute_reply.started":"2024-11-18T21:46:38.311673Z","shell.execute_reply":"2024-11-18T21:46:38.405007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO\n\ndef visualize_battle_count(train, title, show_num_models=30):\n    ptbl = pd.pivot_table(train, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\",\n                          fill_value=0)\n\n    battle_counts = ptbl + ptbl.T\n    ordering = battle_counts.sum().sort_values(ascending=False).index\n    ordering = ordering[:show_num_models]\n    fig = px.imshow(battle_counts.loc[ordering, ordering],\n                    title=title, text_auto=True)\n    fig.update_layout(xaxis_title=\"Model B\",\n                      yaxis_title=\"Model A\",\n                      xaxis_side=\"top\", height=800, width=800,\n                      title_y=0.07, title_x=0.5,\n                      font=dict(size=10))\n    fig.update_traces(hovertemplate=\n                      \"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\")\n    return fig\n\nfig = visualize_battle_count(train, title=\"Battle Count of Each Combination of Models\", show_num_models=30)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:47:51.13145Z","iopub.execute_input":"2024-11-18T21:47:51.132005Z","iopub.status.idle":"2024-11-18T21:47:51.273052Z","shell.execute_reply.started":"2024-11-18T21:47:51.131957Z","shell.execute_reply":"2024-11-18T21:47:51.27183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#AttributeError: Can only use .str accessor with string values!\n\nbattles_no_ties = train[~train[\"winner\"].str.contains(\"winner_tie\")]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:49:09.950958Z","iopub.execute_input":"2024-11-18T21:49:09.951356Z","iopub.status.idle":"2024-11-18T21:49:09.979091Z","shell.execute_reply.started":"2024-11-18T21:49:09.951321Z","shell.execute_reply":"2024-11-18T21:49:09.977975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_battle_count(battles_no_ties, \"Battle Count for Each Combination of Models (without Ties)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T21:49:29.592331Z","iopub.execute_input":"2024-11-18T21:49:29.592708Z","iopub.status.idle":"2024-11-18T21:49:29.680163Z","shell.execute_reply.started":"2024-11-18T21:49:29.592678Z","shell.execute_reply":"2024-11-18T21:49:29.678986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter\n\nmodel_train = pd.concat([train.model_a, train.model_b])\ncounts = model_train.value_counts().reset_index()\ncounts.columns = ['LLM', 'Count']\n\n# Create a bar plot with custom styling using Plotly\nfig = px.bar(counts, x='LLM', y='Count',\n             title='Distribution of LLMs',\n             color='Count', color_continuous_scale='turbo')\n\nfig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:00:10.966775Z","iopub.execute_input":"2024-11-18T22:00:10.967198Z","iopub.status.idle":"2024-11-18T22:00:11.070848Z","shell.execute_reply.started":"2024-11-18T22:00:10.967163Z","shell.execute_reply":"2024-11-18T22:00:11.069733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Install Keras","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp datasets\n!pip install -q -U keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:35:20.992315Z","iopub.execute_input":"2024-11-18T23:35:20.993112Z","iopub.status.idle":"2024-11-18T23:35:42.63555Z","shell.execute_reply.started":"2024-11-18T23:35:20.993053Z","shell.execute_reply":"2024-11-18T23:35:42.634224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Solo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\n# Set the backbend before importing Keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n\nimport keras_nlp\nimport keras\n\n# Run at half precision.\n#keras.config.set_floatx(\"bfloat16\")\n\n# Training Configurations\ntoken_limit = 256\nlora_name = \"translator\"\nlora_rank = 4\nlr_value = 1e-4\ntrain_epoch = 5\nmodel_id = \"gemma2_instruct_2b_en\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:35:51.699869Z","iopub.execute_input":"2024-11-18T23:35:51.700327Z","iopub.status.idle":"2024-11-18T23:35:59.939984Z","shell.execute_reply.started":"2024-11-18T23:35:51.700288Z","shell.execute_reply":"2024-11-18T23:35:59.938828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Tokenizing the Dataset","metadata":{}},{"cell_type":"code","source":"#By StackOverflow https://stackoverflow.com/questions/53982871/pandas-reading-first-n-rows-from-parquet-file\n\nfrom pyarrow.parquet import ParquetFile\nimport pyarrow as pa \n\npf = ParquetFile('../input/wsdm-cup-multilingual-chatbot-arena/train.parquet') \nfirst_eight_rows = next(pf.iter_batches(batch_size = 8)) \n#df = pa.Table.from_batches([first_ten_rows]).to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:38:14.243263Z","iopub.execute_input":"2024-11-18T23:38:14.243668Z","iopub.status.idle":"2024-11-18T23:38:14.361631Z","shell.execute_reply.started":"2024-11-18T23:38:14.243638Z","shell.execute_reply":"2024-11-18T23:38:14.360404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Memory Issues:\n\nI reduced the number of batches/rows cause the Notebook allocated too much memory.  With less I couldn't also print even (train[1])\n\nI also reduced to Gemma2_2b instead of the 9b.","metadata":{}},{"cell_type":"code","source":"#By Solo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\ntokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\nimport pandas as pd\ndf = pa.Table.from_batches([first_eight_rows]).to_pandas()\n\ntrain = []\n\nfor i,x in df.iterrows():\n    item = f\"<start_of_turn>user\\n{x['response_b']}<end_of_turn>\\n<start_of_turn>model\\n{x['prompt']}<end_of_turn>\"\n    length = len(tokenizer(item))\n    # skip data if the token length is longer than our limit\n    if length < token_limit:\n        train.append(item)\n\nprint(len(train))\nprint(train[0])\nprint(train[1])\n#print(train[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:39:04.302536Z","iopub.execute_input":"2024-11-18T23:39:04.303527Z","iopub.status.idle":"2024-11-18T23:39:08.288192Z","shell.execute_reply.started":"2024-11-18T23:39:04.303465Z","shell.execute_reply":"2024-11-18T23:39:08.286892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#LoRA fine-tuning","metadata":{}},{"cell_type":"code","source":"#By Solo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\nimport time\n\ngemma = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma.summary()\n\ntick_start = 0\n\n\ndef tick():\n    global tick_start\n    tick_start = time.time()\n\n\ndef tock():\n    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n\n\ndef text_gen(prompt):\n    tick()\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)\n\ntock()\n\n\ntext_gen(\"Please be boring\")\ntext_gen(\n    \"Alright, I'll be as boring as possible.Today, I woke up at 6:30 AM, just like every other weekday.\"\n)\ntext_gen(\n    \"Do you understand? Please be boring.\"\n)\ntext_gen(\"Once I arrived at the office, I settled into my cubicle and started working on the same project I've been working on for the past few weeks. The tasks were monotonous and required no creativity or problem-solving skills.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:39:18.726694Z","iopub.execute_input":"2024-11-18T23:39:18.72754Z","iopub.status.idle":"2024-11-18T23:54:20.152394Z","shell.execute_reply.started":"2024-11-18T23:39:18.727471Z","shell.execute_reply":"2024-11-18T23:54:20.150871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#LoRA fine-tuning","metadata":{}},{"cell_type":"code","source":"#By Solo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\n# Enable LoRA for the model and set the LoRA rank (4, 8 or 16).\ngemma.backbone.enable_lora(rank=lora_rank)\ngemma.summary()\n\n# Limit the input sequence length (to control memory usage).\ngemma.preprocessor.sequence_length = token_limit\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=lr_value,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:57:23.129197Z","iopub.execute_input":"2024-11-18T23:57:23.129769Z","iopub.status.idle":"2024-11-18T23:57:23.752723Z","shell.execute_reply.started":"2024-11-18T23:57:23.129726Z","shell.execute_reply":"2024-11-18T23:57:23.751674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Save LoRA for each epoch","metadata":{}},{"cell_type":"code","source":"#By Solo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\nclass CustomCallback(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    model_name = f\"/kaggle/working/{lora_name}_{lora_rank}_last.lora.h5\"\n    gemma.backbone.save_lora_weights(model_name)\n\n    # Evaluate\n\n    text_gen(\"Please be boring\")\n    text_gen(\n      \"I started working on the same project I've been working on for the past few weeks. The tasks were monotonous and required no creativity.\"\n    )\n    text_gen(\n      \"Be boring. Do you understand?\"\n    )\n    text_gen(\"Indeed, be as boring as possible\")\n\nhistory = gemma.fit(train, epochs=train_epoch, batch_size=1, callbacks=[CustomCallback()])\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:57:33.507424Z","iopub.execute_input":"2024-11-18T23:57:33.508319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![](https://m.media-amazon.com/images/I/31drt0p5OGL.jpg)https://www.amazon.sg/Boring-Notebook-Please-make-boring/dp/1688792139","metadata":{}},{"cell_type":"markdown","source":"#Acknowledgements:\n\nhttps://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaH\n\nAwsaf/Keras Team https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter\n\nSolo https://www.kaggle.com/code/guru001/translator-of-tamil-thirukural-old-literature/notebook\n\n\nWork with few parquet rows:\nmpwolke https://www.kaggle.com/code/mpwolke/us-coast-guard\n\nmpwolke https://www.kaggle.com/code/mpwolke/by-law-against-the-jungle-russian-mlsummarization","metadata":{}}]}