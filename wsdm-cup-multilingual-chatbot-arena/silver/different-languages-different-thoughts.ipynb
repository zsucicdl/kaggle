{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":85984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on November 19, 2024. By Marília Prata, mpwolke","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-20T00:55:39.365706Z","iopub.execute_input":"2024-11-20T00:55:39.366131Z","iopub.status.idle":"2024-11-20T00:55:40.783556Z","shell.execute_reply.started":"2024-11-20T00:55:39.366076Z","shell.execute_reply":"2024-11-20T00:55:40.782283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Does language shape how we think? \n\nLinguistic relativity & linguistic determinism -- Linguistics 101\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Df25r8pcuI8\" title=\"Does language shape how we think? Linguistic relativity &amp; linguistic determinism -- Linguistics 101\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nhttps://www.youtube.com/watch?v=Df25r8pcuI8","metadata":{}},{"cell_type":"code","source":"#Read One parquet file. Obviously, it's big.\n\ntrain = pd.read_parquet(\"../input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\ntrain.tail(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:55:55.181931Z","iopub.execute_input":"2024-11-20T00:55:55.182489Z","iopub.status.idle":"2024-11-20T00:55:59.236518Z","shell.execute_reply.started":"2024-11-20T00:55:55.182446Z","shell.execute_reply":"2024-11-20T00:55:59.23535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Different languages. Different ways of thinking.\n\n\"When we talk about language, we often dig down to universal categories like nouns and verbs, consonants and vowels, phrases and sentences.  We end up with these cross-language concepts that individual languages are built on almost as if the colorful diversity found in the world's languages is just icing on the strong unity of the linguistic cake and language is grounded in our way of\nthinking and processing information which is itself universal among humans. **So languages and cultures are superficial, but language and cognition run deep**. But this isn't the only way to look at language. What if the language we are brought up to speak actually relates to the way we look at reality? From this perspective a language is a particular way of conceptualizing the world, and has close ties to culture.\n\nThe **Sapir-Whorf Hypothesis** posits that language either determines or influences one's thought. In other words, people who speak different languages see the world differently, based on the language they use to describe it.\n\nIn the 1930's, Benjamin Lee Whorf talked about language this way. He argued that **different languages represent different ways of thinking about the world around us**. This view has come to be called linguistic relativity. Exploring the grammar of the Hopi language, he concluded that the Hopi have an entirely different concept of the time than European languages do and that the European concepts of \"time\" and \"matter\" are actually conditioned by language itself. One practical consequence of linguistic relativity: direct translation between languages isn't always possible.\n\n\"Since Hopi (Native American language) and English aren't simply ways of expressing the same thing in different words, you can't actually preserve thoughts or viewpoints when you translate between them. In its strongest expression, linguistic relativity - the idea that viewpoints vary from language to language - relies on linguistic determinism - the idea that language determines thought.\"\n\n\"In other words how people think doesn't just vary depending on their language but is actually grounded in - determined by - the specific language of their community.\"\n\n\"Linguistic relativity has been abandoned and criticized over the decades with critics aiming to show that perception and cognition are universal, not tied to language and culture, but some psychologists and anthropologists continue to argue that differences in a language's structure and words may play a role in determining how we think. Experiments on how speakers of different languages approach non-linguistic tasks continue to spark this debate. Thank you for joining me on this quick tour of linguistic relativity and linguistic determinism.\"\n\nhttps://www.youtube.com/watch?v=Df25r8pcuI8\n\nhttp://www.nativlang.com/linguistics/...","metadata":{}},{"cell_type":"code","source":"zh = train[(train['language']=='Chinese')].reset_index(drop=True)\nzh.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:58:46.87152Z","iopub.execute_input":"2024-11-20T00:58:46.871875Z","iopub.status.idle":"2024-11-20T00:58:46.898377Z","shell.execute_reply.started":"2024-11-20T00:58:46.871842Z","shell.execute_reply":"2024-11-20T00:58:46.897208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Keras installation","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n!pip install -q -U kagglehub --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:06:11.359569Z","iopub.execute_input":"2024-11-20T01:06:11.359954Z","iopub.status.idle":"2024-11-20T01:06:48.597129Z","shell.execute_reply.started":"2024-11-20T01:06:11.359921Z","shell.execute_reply":"2024-11-20T01:06:48.595602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:06:53.271726Z","iopub.execute_input":"2024-11-20T01:06:53.272198Z","iopub.status.idle":"2024-11-20T01:07:08.537594Z","shell.execute_reply.started":"2024-11-20T01:06:53.272158Z","shell.execute_reply":"2024-11-20T01:07:08.536579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Kaggle Secrets","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#Make yours and Add copy to clipboard\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_licorne\")\n\n#Gabriel's line\n#from kaggle_secrets import UserSecretsClient\n#user_secrets = UserSecretsClient()\n#os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n#os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:07:35.233347Z","iopub.execute_input":"2024-11-20T01:07:35.23403Z","iopub.status.idle":"2024-11-20T01:07:36.000315Z","shell.execute_reply.started":"2024-11-20T01:07:35.233989Z","shell.execute_reply":"2024-11-20T01:07:35.99934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass Config:\n    seed = 42\n    dataset_path = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena\"\n    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 5 # Original is 10 number of epochs to train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:08:16.667795Z","iopub.execute_input":"2024-11-20T01:08:16.668468Z","iopub.status.idle":"2024-11-20T01:08:16.67495Z","shell.execute_reply.started":"2024-11-20T01:08:16.668425Z","shell.execute_reply":"2024-11-20T01:08:16.673615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:08:33.845927Z","iopub.execute_input":"2024-11-20T01:08:33.847382Z","iopub.status.idle":"2024-11-20T01:08:33.852495Z","shell.execute_reply.started":"2024-11-20T01:08:33.847334Z","shell.execute_reply":"2024-11-20T01:08:33.851347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Don't change anything on Template line. Just the rows (in blue)\n#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ntemplate = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\nzh[\"prompt\"] = zh.apply(lambda row: template.format(Category=row.response_b,\n                                                             Question=row.prompt,\n                                                             Answer=row.response_a), axis=1)\ndata = zh.prompt.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:11:24.832059Z","iopub.execute_input":"2024-11-20T01:11:24.832525Z","iopub.status.idle":"2024-11-20T01:11:24.973207Z","shell.execute_reply.started":"2024-11-20T01:11:24.832489Z","shell.execute_reply":"2024-11-20T01:11:24.972052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Template utility function","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ndef colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:11:58.135733Z","iopub.execute_input":"2024-11-20T01:11:58.136172Z","iopub.status.idle":"2024-11-20T01:11:58.143198Z","shell.execute_reply.started":"2024-11-20T01:11:58.136137Z","shell.execute_reply":"2024-11-20T01:11:58.141586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma Causal","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ngemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:12:30.619793Z","iopub.execute_input":"2024-11-20T01:12:30.62028Z","iopub.status.idle":"2024-11-20T01:14:06.692319Z","shell.execute_reply.started":"2024-11-20T01:12:30.620241Z","shell.execute_reply":"2024-11-20T01:14:06.691171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Define the specialized class","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, category, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                Category=category,\n                Question=question,\n                Answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:25.783182Z","iopub.execute_input":"2024-11-20T01:14:25.783591Z","iopub.status.idle":"2024-11-20T01:14:25.790293Z","shell.execute_reply.started":"2024-11-20T01:14:25.783553Z","shell.execute_reply":"2024-11-20T01:14:25.788957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:31.278799Z","iopub.execute_input":"2024-11-20T01:14:31.27921Z","iopub.status.idle":"2024-11-20T01:14:31.420256Z","shell.execute_reply.started":"2024-11-20T01:14:31.279175Z","shell.execute_reply":"2024-11-20T01:14:31.419109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:37.636914Z","iopub.execute_input":"2024-11-20T01:14:37.638207Z","iopub.status.idle":"2024-11-20T01:14:37.645015Z","shell.execute_reply.started":"2024-11-20T01:14:37.638154Z","shell.execute_reply":"2024-11-20T01:14:37.643815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:47.860612Z","iopub.execute_input":"2024-11-20T01:14:47.861073Z","iopub.status.idle":"2024-11-20T01:14:48.314888Z","shell.execute_reply.started":"2024-11-20T01:14:47.861019Z","shell.execute_reply":"2024-11-20T01:14:48.313855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma_causal_lm\n\r\nEpochs!","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#set sequence length cf. config (512)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4307\n\n日本人为什么智力高\n\nGoogle Translate:\n\nWhy are Japanese people so intelligent?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[4307]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4309\n\n你好，请你介绍下你自己\n\nGoogle translate:\n\nHello, introduce yourself","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[4309]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 5\n\n任务概述：\\n\\n1. **选择数据集**：我们将选择一个至少包含30个特征的开源分类数据集\n\nGoogle translate:\n\nTask Overview:\\n\\n1. **Select a dataset**: We will select an open source classification dataset with at least 30 features","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[5]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 6\n\n以下是一些提升英语水平的有效方法：\\n\\n1.  **定期学习英语**：每天都进行点滴的英语学习，尤其是在工作或 leisure 时间。在学习英语前，考虑好自己的学习时间规划和强调重要内容。在第二个月上学之后，各日都安排一些学习时间。\\n2.  **阅读外国 Literature**\n\nGoogle translate:\n\nHere are some effective ways to improve your English:\\n\\n1. **Study English regularly**: Study English a little bit every day, especially during work or leisure time. Before learning English, think about your study schedule and emphasize important content. After the second month of school, arrange some study time every day.\\n2. **Read foreign literature**","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[6]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### zh'prompt'2\n\n我们需要解决的问题是：一根12米长的电线被切成两片。如果然后使用较长的部分形成正方形的周长，如果原始电线在任意点切割，正方形面积大于4的概率是多少？\n\nGoogle Translate:\n\nThe problem we need to solve is: A 12-meter-long wire is cut into two pieces. If the longer piece is then used to form the perimeter of a square, what is the probability that the area of ​​the square is greater than 4 if the original wire is cut at any point?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"让我们来分解这个问题\" #response_a row index2 translation: Let’s break this down\nquestion = \"我们需要解决的问题是：一根12米长的电线被切成两片。如果然后使用较长的部分形成正方形的周长，如果原始电线在任意点切割，正方形面积大于4的概率是多少？\"  #How many cover arts do we have?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index6\n\n以下是一些提升英语水平的有效方法：\\n\\n1. 定期学习英语：每天都进行点滴的英语学习  \n\nHere are some effective ways to improve your English:\\n\\n1. Learn English regularly: Learn English a little bit every day  ","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"定期学习英语\" #model_a row index 6 Study english regularly\nquestion = \"以下是一些提升英语水平的有效方法：\\n\\n1. 定期学习英语：每天都进行点滴的英语学习\"  #Here are some effective ways to improve your English:\\n\\n1. Learn English regularly: Learn English a little bit every day\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4309\n\r\n你好，请你介绍下你自 (Hello, please introduce yourself)\n\nGoogle translate:\n\n介绍\\n\\n我是一款人工智能语言模型 (response_b)\n\nIntroduction\\n\\nI am an artificial intelligence language model己","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"介绍\\n\\n我是一款人工智能语言模型\" # Introduction\\n\\nI am an artificial intelligence language model\nquestion = \"你好，请你介绍下你自己\"  #Hello, please introduce yourself Prompt 4309\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 10\n\n\n探索性问题:** 在未来的城市中，如何设计一个智能的交通系统\n\nExploratory question: How to design an intelligent transportation system in the future city?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"让城市的交通更加高效、绿色和可持续\" # Make urban transportation more efficient, green and sustainable\nquestion = \"探索性问题:** 在未来的城市中，如何设计一个智能的交通系统\"  # Prompt 10 Exploratory question: How to design an intelligent transportation system in the future city?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Save the model","metadata":{}},{"cell_type":"code","source":"preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\ngemma_causal_lm.save_to_preset(preset_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Acknowledgements:\n\r\nGabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook","metadata":{}}]}