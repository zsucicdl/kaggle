{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":85984,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on November 19, 2024. By Mar√≠lia Prata, mpwolke","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-20T00:55:39.365706Z","iopub.execute_input":"2024-11-20T00:55:39.366131Z","iopub.status.idle":"2024-11-20T00:55:40.783556Z","shell.execute_reply.started":"2024-11-20T00:55:39.366076Z","shell.execute_reply":"2024-11-20T00:55:40.782283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Does language shape how we think? \n\nLinguistic relativity & linguistic determinism -- Linguistics 101\n\n<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Df25r8pcuI8\" title=\"Does language shape how we think? Linguistic relativity &amp; linguistic determinism -- Linguistics 101\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\nhttps://www.youtube.com/watch?v=Df25r8pcuI8","metadata":{}},{"cell_type":"code","source":"#Read One parquet file. Obviously, it's big.\n\ntrain = pd.read_parquet(\"../input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\ntrain.tail(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:55:55.181931Z","iopub.execute_input":"2024-11-20T00:55:55.182489Z","iopub.status.idle":"2024-11-20T00:55:59.236518Z","shell.execute_reply.started":"2024-11-20T00:55:55.182446Z","shell.execute_reply":"2024-11-20T00:55:59.23535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Different languages. Different ways of thinking.\n\n\"When we talk about language, we often dig down to universal categories like nouns and verbs, consonants and vowels, phrases and sentences.  We end up with these cross-language concepts that individual languages are built on almost as if the colorful diversity found in the world's languages is just icing on the strong unity of the linguistic cake and language is grounded in our way of\nthinking and processing information which is itself universal among humans. **So languages and cultures are superficial, but language and cognition run deep**. But this isn't the only way to look at language. What if the language we are brought up to speak actually relates to the way we look at reality? From this perspective a language is a particular way of conceptualizing the world, and has close ties to culture.\n\nThe **Sapir-Whorf Hypothesis** posits that language either determines or influences one's thought. In other words, people who speak different languages see the world differently, based on the language they use to describe it.\n\nIn the 1930's, Benjamin Lee Whorf talked about language this way. He argued that **different languages represent different ways of thinking about the world around us**. This view has come to be called linguistic relativity. Exploring the grammar of the Hopi language, he concluded that the Hopi have an entirely different concept of the time than European languages do and that the European concepts of \"time\" and \"matter\" are actually conditioned by language itself. One practical consequence of linguistic relativity: direct translation between languages isn't always possible.\n\n\"Since Hopi (Native American language) and English aren't simply ways of expressing the same thing in different words, you can't actually preserve thoughts or viewpoints when you translate between them. In its strongest expression, linguistic relativity - the idea that viewpoints vary from language to language - relies on linguistic determinism - the idea that language determines thought.\"\n\n\"In other words how people think doesn't just vary depending on their language but is actually grounded in - determined by - the specific language of their community.\"\n\n\"Linguistic relativity has been abandoned and criticized over the decades with critics aiming to show that perception and cognition are universal, not tied to language and culture, but some psychologists and anthropologists continue to argue that differences in a language's structure and words may play a role in determining how we think. Experiments on how speakers of different languages approach non-linguistic tasks continue to spark this debate. Thank you for joining me on this quick tour of linguistic relativity and linguistic determinism.\"\n\nhttps://www.youtube.com/watch?v=Df25r8pcuI8\n\nhttp://www.nativlang.com/linguistics/...","metadata":{}},{"cell_type":"code","source":"zh = train[(train['language']=='Chinese')].reset_index(drop=True)\nzh.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T00:58:46.87152Z","iopub.execute_input":"2024-11-20T00:58:46.871875Z","iopub.status.idle":"2024-11-20T00:58:46.898377Z","shell.execute_reply.started":"2024-11-20T00:58:46.871842Z","shell.execute_reply":"2024-11-20T00:58:46.897208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Keras installation","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n!pip install -q -U kagglehub --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:06:11.359569Z","iopub.execute_input":"2024-11-20T01:06:11.359954Z","iopub.status.idle":"2024-11-20T01:06:48.597129Z","shell.execute_reply.started":"2024-11-20T01:06:11.359921Z","shell.execute_reply":"2024-11-20T01:06:48.595602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:06:53.271726Z","iopub.execute_input":"2024-11-20T01:06:53.272198Z","iopub.status.idle":"2024-11-20T01:07:08.537594Z","shell.execute_reply.started":"2024-11-20T01:06:53.272158Z","shell.execute_reply":"2024-11-20T01:07:08.536579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Kaggle Secrets","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#Make yours and Add copy to clipboard\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_licorne\")\n\n#Gabriel's line\n#from kaggle_secrets import UserSecretsClient\n#user_secrets = UserSecretsClient()\n#os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n#os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:07:35.233347Z","iopub.execute_input":"2024-11-20T01:07:35.23403Z","iopub.status.idle":"2024-11-20T01:07:36.000315Z","shell.execute_reply.started":"2024-11-20T01:07:35.233989Z","shell.execute_reply":"2024-11-20T01:07:35.99934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass Config:\n    seed = 42\n    dataset_path = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena\"\n    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 5 # Original is 10 number of epochs to train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:08:16.667795Z","iopub.execute_input":"2024-11-20T01:08:16.668468Z","iopub.status.idle":"2024-11-20T01:08:16.67495Z","shell.execute_reply.started":"2024-11-20T01:08:16.668425Z","shell.execute_reply":"2024-11-20T01:08:16.673615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:08:33.845927Z","iopub.execute_input":"2024-11-20T01:08:33.847382Z","iopub.status.idle":"2024-11-20T01:08:33.852495Z","shell.execute_reply.started":"2024-11-20T01:08:33.847334Z","shell.execute_reply":"2024-11-20T01:08:33.851347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Don't change anything on Template line. Just the rows (in blue)\n#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ntemplate = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\nzh[\"prompt\"] = zh.apply(lambda row: template.format(Category=row.response_b,\n                                                             Question=row.prompt,\n                                                             Answer=row.response_a), axis=1)\ndata = zh.prompt.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:11:24.832059Z","iopub.execute_input":"2024-11-20T01:11:24.832525Z","iopub.status.idle":"2024-11-20T01:11:24.973207Z","shell.execute_reply.started":"2024-11-20T01:11:24.832489Z","shell.execute_reply":"2024-11-20T01:11:24.972052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Template utility function","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ndef colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:11:58.135733Z","iopub.execute_input":"2024-11-20T01:11:58.136172Z","iopub.status.idle":"2024-11-20T01:11:58.143198Z","shell.execute_reply.started":"2024-11-20T01:11:58.136137Z","shell.execute_reply":"2024-11-20T01:11:58.141586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma Causal","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ngemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:12:30.619793Z","iopub.execute_input":"2024-11-20T01:12:30.62028Z","iopub.status.idle":"2024-11-20T01:14:06.692319Z","shell.execute_reply.started":"2024-11-20T01:12:30.620241Z","shell.execute_reply":"2024-11-20T01:14:06.691171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Define the specialized class","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nclass GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, category, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                Category=category,\n                Question=question,\n                Answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:25.783182Z","iopub.execute_input":"2024-11-20T01:14:25.783591Z","iopub.status.idle":"2024-11-20T01:14:25.790293Z","shell.execute_reply.started":"2024-11-20T01:14:25.783553Z","shell.execute_reply":"2024-11-20T01:14:25.788957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:31.278799Z","iopub.execute_input":"2024-11-20T01:14:31.27921Z","iopub.status.idle":"2024-11-20T01:14:31.420256Z","shell.execute_reply.started":"2024-11-20T01:14:31.279175Z","shell.execute_reply":"2024-11-20T01:14:31.419109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:37.636914Z","iopub.execute_input":"2024-11-20T01:14:37.638207Z","iopub.status.idle":"2024-11-20T01:14:37.645015Z","shell.execute_reply.started":"2024-11-20T01:14:37.638154Z","shell.execute_reply":"2024-11-20T01:14:37.643815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T01:14:47.860612Z","iopub.execute_input":"2024-11-20T01:14:47.861073Z","iopub.status.idle":"2024-11-20T01:14:48.314888Z","shell.execute_reply.started":"2024-11-20T01:14:47.861019Z","shell.execute_reply":"2024-11-20T01:14:48.313855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Gemma_causal_lm\n\r\nEpochs!","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\n#set sequence length cf. config (512)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4307\n\nÊó•Êú¨‰∫∫‰∏∫‰ªÄ‰πàÊô∫ÂäõÈ´ò\n\nGoogle Translate:\n\nWhy are Japanese people so intelligent?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[4307]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4309\n\n‰Ω†Â•ΩÔºåËØ∑‰Ω†‰ªãÁªç‰∏ã‰Ω†Ëá™Â∑±\n\nGoogle translate:\n\nHello, introduce yourself","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[4309]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 5\n\n‰ªªÂä°Ê¶ÇËø∞Ôºö\\n\\n1. **ÈÄâÊã©Êï∞ÊçÆÈõÜ**ÔºöÊàë‰ª¨Â∞ÜÈÄâÊã©‰∏Ä‰∏™Ëá≥Â∞ëÂåÖÂê´30‰∏™ÁâπÂæÅÁöÑÂºÄÊ∫êÂàÜÁ±ªÊï∞ÊçÆÈõÜ\n\nGoogle translate:\n\nTask Overview:\\n\\n1. **Select a dataset**: We will select an open source classification dataset with at least 30 features","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[5]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 6\n\n‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊèêÂçáËã±ËØ≠Ê∞¥Âπ≥ÁöÑÊúâÊïàÊñπÊ≥ïÔºö\\n\\n1.  **ÂÆöÊúüÂ≠¶‰π†Ëã±ËØ≠**ÔºöÊØèÂ§©ÈÉΩËøõË°åÁÇπÊª¥ÁöÑËã±ËØ≠Â≠¶‰π†ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∑•‰ΩúÊàñ leisure Êó∂Èó¥„ÄÇÂú®Â≠¶‰π†Ëã±ËØ≠ÂâçÔºåËÄÉËôëÂ•ΩËá™Â∑±ÁöÑÂ≠¶‰π†Êó∂Èó¥ËßÑÂàíÂíåÂº∫Ë∞ÉÈáçË¶ÅÂÜÖÂÆπ„ÄÇÂú®Á¨¨‰∫å‰∏™Êúà‰∏äÂ≠¶‰πãÂêéÔºåÂêÑÊó•ÈÉΩÂÆâÊéí‰∏Ä‰∫õÂ≠¶‰π†Êó∂Èó¥„ÄÇ\\n2.  **ÈòÖËØªÂ§ñÂõΩ Literature**\n\nGoogle translate:\n\nHere are some effective ways to improve your English:\\n\\n1. **Study English regularly**: Study English a little bit every day, especially during work or leisure time. Before learning English, think about your study schedule and emphasize important content. After the second month of school, arrange some study time every day.\\n2. **Read foreign literature**","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\nrow = zh.iloc[6]\ngemma_qa.query(row.response_a,row.prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### zh'prompt'2\n\nÊàë‰ª¨ÈúÄË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊòØÔºö‰∏ÄÊ†π12Á±≥ÈïøÁöÑÁîµÁ∫øË¢´ÂàáÊàê‰∏§Áâá„ÄÇÂ¶ÇÊûúÁÑ∂Âêé‰ΩøÁî®ËæÉÈïøÁöÑÈÉ®ÂàÜÂΩ¢ÊàêÊ≠£ÊñπÂΩ¢ÁöÑÂë®ÈïøÔºåÂ¶ÇÊûúÂéüÂßãÁîµÁ∫øÂú®‰ªªÊÑèÁÇπÂàáÂâ≤ÔºåÊ≠£ÊñπÂΩ¢Èù¢ÁßØÂ§ß‰∫é4ÁöÑÊ¶ÇÁéáÊòØÂ§öÂ∞ëÔºü\n\nGoogle Translate:\n\nThe problem we need to solve is: A 12-meter-long wire is cut into two pieces. If the longer piece is then used to form the perimeter of a square, what is the probability that the area of ‚Äã‚Äãthe square is greater than 4 if the original wire is cut at any point?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"ËÆ©Êàë‰ª¨Êù•ÂàÜËß£Ëøô‰∏™ÈóÆÈ¢ò\" #response_a row index2 translation: Let‚Äôs break this down\nquestion = \"Êàë‰ª¨ÈúÄË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊòØÔºö‰∏ÄÊ†π12Á±≥ÈïøÁöÑÁîµÁ∫øË¢´ÂàáÊàê‰∏§Áâá„ÄÇÂ¶ÇÊûúÁÑ∂Âêé‰ΩøÁî®ËæÉÈïøÁöÑÈÉ®ÂàÜÂΩ¢ÊàêÊ≠£ÊñπÂΩ¢ÁöÑÂë®ÈïøÔºåÂ¶ÇÊûúÂéüÂßãÁîµÁ∫øÂú®‰ªªÊÑèÁÇπÂàáÂâ≤ÔºåÊ≠£ÊñπÂΩ¢Èù¢ÁßØÂ§ß‰∫é4ÁöÑÊ¶ÇÁéáÊòØÂ§öÂ∞ëÔºü\"  #How many cover arts do we have?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index6\n\n‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊèêÂçáËã±ËØ≠Ê∞¥Âπ≥ÁöÑÊúâÊïàÊñπÊ≥ïÔºö\\n\\n1. ÂÆöÊúüÂ≠¶‰π†Ëã±ËØ≠ÔºöÊØèÂ§©ÈÉΩËøõË°åÁÇπÊª¥ÁöÑËã±ËØ≠Â≠¶‰π†  \n\nHere are some effective ways to improve your English:\\n\\n1. Learn English regularly: Learn English a little bit every day  ","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"ÂÆöÊúüÂ≠¶‰π†Ëã±ËØ≠\" #model_a row index 6 Study english regularly\nquestion = \"‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊèêÂçáËã±ËØ≠Ê∞¥Âπ≥ÁöÑÊúâÊïàÊñπÊ≥ïÔºö\\n\\n1. ÂÆöÊúüÂ≠¶‰π†Ëã±ËØ≠ÔºöÊØèÂ§©ÈÉΩËøõË°åÁÇπÊª¥ÁöÑËã±ËØ≠Â≠¶‰π†\"  #Here are some effective ways to improve your English:\\n\\n1. Learn English regularly: Learn English a little bit every day\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 4309\n\r\n‰Ω†Â•ΩÔºåËØ∑‰Ω†‰ªãÁªç‰∏ã‰Ω†Ëá™ (Hello, please introduce yourself)\n\nGoogle translate:\n\n‰ªãÁªç\\n\\nÊàëÊòØ‰∏ÄÊ¨æ‰∫∫Â∑•Êô∫ËÉΩËØ≠Ë®ÄÊ®°Âûã (response_b)\n\nIntroduction\\n\\nI am an artificial intelligence language modelÂ∑±","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"‰ªãÁªç\\n\\nÊàëÊòØ‰∏ÄÊ¨æ‰∫∫Â∑•Êô∫ËÉΩËØ≠Ë®ÄÊ®°Âûã\" # Introduction\\n\\nI am an artificial intelligence language model\nquestion = \"‰Ω†Â•ΩÔºåËØ∑‰Ω†‰ªãÁªç‰∏ã‰Ω†Ëá™Â∑±\"  #Hello, please introduce yourself Prompt 4309\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prompt chinese index 10\n\n\nÊé¢Á¥¢ÊÄßÈóÆÈ¢ò:** Âú®Êú™Êù•ÁöÑÂüéÂ∏Ç‰∏≠ÔºåÂ¶Ç‰ΩïËÆæËÆ°‰∏Ä‰∏™Êô∫ËÉΩÁöÑ‰∫§ÈÄöÁ≥ªÁªü\n\nExploratory question: How to design an intelligent transportation system in the future city?","metadata":{}},{"cell_type":"code","source":"#By Gabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook\n\ncategory = \"ËÆ©ÂüéÂ∏ÇÁöÑ‰∫§ÈÄöÊõ¥Âä†È´òÊïà„ÄÅÁªøËâ≤ÂíåÂèØÊåÅÁª≠\" # Make urban transportation more efficient, green and sustainable\nquestion = \"Êé¢Á¥¢ÊÄßÈóÆÈ¢ò:** Âú®Êú™Êù•ÁöÑÂüéÂ∏Ç‰∏≠ÔºåÂ¶Ç‰ΩïËÆæËÆ°‰∏Ä‰∏™Êô∫ËÉΩÁöÑ‰∫§ÈÄöÁ≥ªÁªü\"  # Prompt 10 Exploratory question: How to design an intelligent transportation system in the future city?\ngemma_qa.query(category,question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Save the model","metadata":{}},{"cell_type":"code","source":"preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\ngemma_causal_lm.save_to_preset(preset_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Acknowledgements:\n\r\nGabriel Preda https://www.kaggle.com/code/gpreda/fine-tuning-gemma-2-model-using-lora-and-keras/notebook","metadata":{}}]}