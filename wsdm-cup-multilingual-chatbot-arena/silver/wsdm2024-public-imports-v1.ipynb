{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **PACKAGE INSTALLATIONS**","metadata":{}},{"cell_type":"code","source":"%%writefile -a req_kaggle.txt\n\nlightgbm==4.5.0\nxgboost==2.1.2\nscikit-learn==1.5.2\nnumpy==1.26.4\nscipy==1.14.1\npolars==1.14.0\ncloudpickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:39.214346Z","iopub.execute_input":"2024-11-19T12:39:39.214759Z","iopub.status.idle":"2024-11-19T12:39:39.256862Z","shell.execute_reply.started":"2024-11-19T12:39:39.214722Z","shell.execute_reply":"2024-11-19T12:39:39.255515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n\nimport os\ntry:\n    os.mkdir(f\"/kaggle/working/packages\")\nexcept:\n    pass\n\n!pip download -q -r req_kaggle.txt -d /kaggle/working/packages\n!pip download polars==1.14.0 -q -d /kaggle/working/packages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:39.258949Z","iopub.execute_input":"2024-11-19T12:39:39.25932Z","iopub.status.idle":"2024-11-19T12:39:55.118127Z","shell.execute_reply.started":"2024-11-19T12:39:39.259285Z","shell.execute_reply":"2024-11-19T12:39:55.116641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(f\"\\n---> Commencing imports-part1\")\n\nfrom gc import collect\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom IPython.display import display_html, clear_output\nclear_output()\nimport os, sys, logging, re, joblib, ctypes, shutil, random\nfrom copy import deepcopy\n\nimport xgboost as xgb, lightgbm as lgb, catboost as cb, sklearn as sk, pandas as pd\nprint(f\"---> XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__} | Catboost = {cb.__version__}\")\nprint(f\"---> Sklearn = {sk.__version__}| Pandas = {pd.__version__}\")\ncollect()\n\n# General library imports:-\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom gc import collect\n\nfrom os import path, walk, getpid\nfrom psutil import Process\nimport re\nfrom collections import Counter\nfrom itertools import product\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom IPython.display import display_html, clear_output\nfrom pprint import pprint\nfrom functools import partial\nfrom copy import deepcopy\nimport pandas as pd, numpy as np, os, joblib\nimport polars as pl\nimport polars.selectors as cs\nimport re\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom colorama import Fore, Style, init\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom tqdm.notebook import tqdm\n\nprint(f\"---> Imports- part 1 done\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.12006Z","iopub.execute_input":"2024-11-19T12:39:55.120445Z","iopub.status.idle":"2024-11-19T12:39:55.128502Z","shell.execute_reply.started":"2024-11-19T12:39:55.120409Z","shell.execute_reply":"2024-11-19T12:39:55.127299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a myimports.py\n\n# Pipeline specifics:-\nfrom sklearn.preprocessing import (RobustScaler,\n                                   MinMaxScaler,\n                                   StandardScaler,\n                                   FunctionTransformer as FT,\n                                   PowerTransformer,\n                                  )\nfrom sklearn.impute import SimpleImputer as SI\nfrom sklearn.model_selection import (RepeatedStratifiedKFold as RSKF,\n                                     StratifiedKFold as SKF,\n                                     StratifiedGroupKFold as SGKF,\n                                     KFold,\n                                     GroupKFold as GKF,\n                                     RepeatedKFold as RKF,\n                                     PredefinedSplit as PDS,\n                                     cross_val_score,\n                                     cross_val_predict,\n                                    )\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.feature_selection import VarianceThreshold as VT\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.compose import ColumnTransformer, make_column_selector\n\n# ML Model training:-\nfrom sklearn.metrics import (\nroc_auc_score, brier_score_loss, accuracy_score, cohen_kappa_score, \nr2_score, root_mean_squared_error as rmse, mean_squared_error as mse,\nmake_scorer,      \n)\n\nfrom xgboost import QuantileDMatrix, XGBClassifier as XGBC, XGBRegressor as XGBR\nfrom lightgbm import log_evaluation, early_stopping, LGBMClassifier as LGBMC, LGBMRegressor as LGBMR\nfrom catboost import CatBoostClassifier as CBC, Pool, CatBoostRegressor as CBR\nfrom sklearn.ensemble import HistGradientBoostingClassifier as HGBC, RandomForestClassifier as RFC\nfrom sklearn.ensemble import HistGradientBoostingRegressor as HGBR, RandomForestRegressor as RFR\nfrom sklearn.linear_model import LogisticRegression as LRC, Ridge, Lasso\n\n# Autoencoder inputs\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Ensemble and tuning:-\nimport optuna\nfrom optuna import Trial, trial, create_study\nfrom optuna.pruners import HyperbandPruner\nfrom optuna.samplers import TPESampler, CmaEsSampler\n\n# Setting rc parameters in seaborn for plots and graphs-\nsns.set({\"axes.facecolor\"       : \"#ffffff\",\n         \"figure.facecolor\"     : \"#ffffff\",\n         \"axes.edgecolor\"       : \"#000000\",\n         \"grid.color\"           : \"#ffffff\",\n         \"font.family\"          : ['Cambria'],\n         \"axes.labelcolor\"      : \"#000000\",\n         \"xtick.color\"          : \"#000000\",\n         \"ytick.color\"          : \"#000000\",\n         \"grid.linewidth\"       : 0.75,\n         \"grid.linestyle\"       : \"--\",\n         \"axes.titlecolor\"      : '#0099e6',\n         'axes.titlesize'       : 8.5,\n         'axes.labelweight'     : \"bold\",\n         'legend.fontsize'      : 7.0,\n         'legend.title_fontsize': 7.0,\n         'font.size'            : 7.5,\n         'xtick.labelsize'      : 12.5,\n         'ytick.labelsize'      : 9.0,\n        }\n       )\n\n# Color printing\ndef PrintColor(text: str, color = Fore.BLUE, style = Style.BRIGHT):\n    \"Prints color outputs using colorama using a text F-string\"\n    print(style + color + text + Style.RESET_ALL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.131768Z","iopub.execute_input":"2024-11-19T12:39:55.132253Z","iopub.status.idle":"2024-11-19T12:39:55.153854Z","shell.execute_reply.started":"2024-11-19T12:39:55.132181Z","shell.execute_reply":"2024-11-19T12:39:55.152555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(f\"---> Commencing imports-part2\")\noptuna.logging.set_verbosity = optuna.logging.ERROR\noptuna.logging.disable_default_handler()\nprint(f\"---> XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__}\")\n\n##################################################################\n# Customizing logging for LGBM\nclass MyLogger:\n    \"\"\"\n    This class helps to suppress logs in lightgbm and Optuna\n    Source - https://github.com/microsoft/LightGBM/issues/6014\n    \"\"\"\n\n    def init(self, logging_lbl: str):\n        self.logger = logging.getLogger(logging_lbl)\n        self.logger.setLevel(logging.ERROR)\n\n    def info(self, message):\n        pass\n\n    def warning(self, message):\n        pass\n\n    def error(self, message):\n        self.logger.error(message)\n\nl = MyLogger()\nl.init(logging_lbl = \"lightgbm_custom\")\nlgb.register_logger(l)\n\n##################################################################\n# Customizing logging for XGBoost\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.ERROR)\nformatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n\nstdout_handler = logging.StreamHandler(sys.stdout)\nstdout_handler.setLevel(logging.INFO)\nstdout_handler.setFormatter(formatter)\n\nfile_handler = logging.FileHandler(f'xgb_optimize.log')\nfile_handler.setLevel(logging.ERROR)\nfile_handler.setFormatter(formatter)\n\nlogger.addHandler(file_handler)\nlogger.addHandler(stdout_handler)\n\nclass XGBLogging(xgb.callback.TrainingCallback):\n    \"\"\"log train logs to file\"\"\"\n\n    def __init__(self, epoch_log_interval=100):\n        self.epoch_log_interval = epoch_log_interval\n\n    def after_iteration(self, model, epoch:int,\n                        evals_log:xgb.callback.TrainingCallback.EvalsLog\n                        ):\n\n        if self.epoch_log_interval <= 0:\n            pass\n\n        elif (epoch %  self.epoch_log_interval == 0):\n            for data, metric in evals_log.items():\n                for metric_name, log in metric.items():\n                    score = log[-1][0] if isinstance(log[-1], tuple) else log[-1]\n                    logger.info(f\"XGBLogging epoch {epoch} dataset {data} {metric_name} {score}\")\n\n        return False\n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 200)\nprint(f\"---> Imports- part 2 done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.155588Z","iopub.execute_input":"2024-11-19T12:39:55.155949Z","iopub.status.idle":"2024-11-19T12:39:55.1783Z","shell.execute_reply.started":"2024-11-19T12:39:55.155916Z","shell.execute_reply":"2024-11-19T12:39:55.176962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a myimports.py\n\nprint(f\"---> Seeding everything\")\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(2024)\nprint(f\"\\n---> Imports done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **TRAINING ELEMENTS**","metadata":{}},{"cell_type":"code","source":"%%writefile -a training.py\n\nclass Utils:\n    \"\"\"\n    This class creates and uses several utility methods to be used across the code\n    \"\"\";\n\n    def __init__(self):\n        pass\n\n    def ScoreMetric(self, ytrue, ypred)-> float:\n        \"\"\"\n        This method calculates the metric for the competition\n        Inputs- ytrue, ypred:- input truth and predictions\n        Output- float:- competition metric\n        \"\"\";\n\n        score = \\\n        accuracy_score(\n            np.uint8(np.round(ytrue, 0)),\n            np.uint8(np.round(ypred, 0)),\n        )\n        return score\n\n    def CleanMemory(self):\n        \"This method cleans the memory off unused objects and displays the cleaned state RAM usage\"\n\n        collect();\n        libc.malloc_trim(0)\n        pid        = getpid()\n        py         = Process(pid)\n        memory_use = py.memory_info()[0] / 2. ** 30\n        return f\"\\nRAM usage = {memory_use :.4} GB\"\n\n    def DisplayAdjTbl(self, *args):\n        \"\"\"\n        This function displays pandas tables in an adjacent manner, sourced from the below link-\n        https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n        \"\"\"\n\n        html_str = ''\n        for df in args:\n            html_str += df.to_html()\n        display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n        collect()\n\n    def DisplayScores(\n        self, Scores: pd.DataFrame, TrainScores: pd.DataFrame, methods: list\n    ):\n        \"This method displays the scores and their means\"\n\n        args = \\\n        [Scores.style.format(precision = 5).\\\n         background_gradient(cmap = \"Blues\", subset = methods + [\"Ensemble\"]).\\\n         set_caption(f\"\\nOOF scores across methods and folds\\n\"),\n\n         TrainScores.style.format(precision = 5).\\\n         background_gradient(cmap = \"Pastel2\", subset = methods).\\\n         set_caption(f\"\\nTrain scores across methods and folds\\n\")\n        ];\n\n        PrintColor(f\"\\n\\n\\n---> OOF score across all methods and folds\\n\",\n                   color = Fore.LIGHTMAGENTA_EX\n                   )\n        self.DisplayAdjTbl(*args)\n\n        print('\\n')\n        display(Scores.mean().to_frame().\\\n                transpose().\\\n                style.format(precision = 5).\\\n                background_gradient(cmap = \"mako\", axis=1,\n                                    subset = Scores.columns\n                                   ).\\\n                set_caption(f\"\\nOOF mean scores across methods and folds\\n\")\n               )\n\n\nutils = Utils()\ncollect()\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.180091Z","iopub.execute_input":"2024-11-19T12:39:55.180435Z","iopub.status.idle":"2024-11-19T12:39:55.201915Z","shell.execute_reply.started":"2024-11-19T12:39:55.180402Z","shell.execute_reply":"2024-11-19T12:39:55.200811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a training.py\n\ndef MakePermImp(\n        method, mdl, X, y, ygrp,\n        myscorer, \n        n_repeats = 2,\n        state = 42,\n        ntop: int = 15,\n        **params,\n):\n    \"\"\"\n    This function makes the permutation importance for the provided model and returns the importance scores for all features\n    \n    Note-\n    myscorer - scikit-learn -> metrics -> make_scorer object with the corresponding eval metric and relevant details\n    \"\"\"\n\n    cv        = PDS(ygrp)\n    n_splits  = ygrp.nunique()\n    drop_cols = [\"Source\", \"id\", \"Id\", \"Label\", \"fold_nb\"]\n\n    for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y))):\n        Xtr  = X.iloc[train_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n        Xdev = X.iloc[dev_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n        ytr  = y.loc[Xtr.index]\n        ydev = y.loc[Xdev.index]\n\n        model = clone(mdl)\n        sel_cols = list(Xdev.columns)\n        model.fit(Xtr, ytr)\n\n        imp_ = permutation_importance(model,\n                                      Xdev, ydev,\n                                      scoring = myscorer,\n                                      n_repeats = n_repeats,\n                                      random_state = state,\n                                      )[\"importances_mean\"]\n        imp_ = pd.Series(index = sel_cols, data = imp_)\n\n        display(\n            imp_.\\\n            sort_values(ascending = False).\\\n            head(ntop).\\\n            to_frame().\\\n            transpose().\\\n            style.\\\n            format(formatter = '{:,.3f}').\\\n            background_gradient(\"icefire\", axis=1).\\\n            set_caption(f\"Top {ntop} features\")\n            )\n\n        return imp_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.203252Z","iopub.execute_input":"2024-11-19T12:39:55.203664Z","iopub.status.idle":"2024-11-19T12:39:55.222325Z","shell.execute_reply.started":"2024-11-19T12:39:55.203624Z","shell.execute_reply":"2024-11-19T12:39:55.22109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a training.py\n\nclass ModelTrainer:\n    \"This class trains the provided model on the train-test data and returns the predictions and fitted models\"\n\n    def __init__(\n        self,\n        problem_type   : str   = \"binary\", \n        es             : int   = 100,\n        target         : str   = \"\",\n        metric_lbl     : str   = \"auc\",\n        orig_req       : bool  = False,\n        orig_all_folds : bool  = False,\n        drop_cols      : list  = [\"Source\", \"id\", \"Id\", \"Label\", \"fold_nb\"],\n    ):\n        \"\"\"\n        Key parameters-\n        es_iter - early stopping rounds for boosted trees\n        \"\"\"\n\n        self.problem_type   = problem_type\n        self.es_iter        = es\n        self.target         = target\n        self.drop_cols      = drop_cols + [self.target]\n        self.metric_lbl     = metric_lbl\n        self.orig_req       = orig_req\n        self.orig_all_folds = orig_all_folds\n\n    def ScoreMetric(self, ytrue, ypred):\n        \"\"\"\n        This is the metric function for the competition scoring\n        \"\"\"\n\n        if self.metric_lbl == \"accuracy\":\n            return accuracy_score(np.uint8(np.round(ytrue, 0)), np.uint8(np.round(ypred)))\n        elif self.metric_lbl == \"auc\":\n            return roc_auc_score(ytrue, ypred)\n        elif self.metric_lbl == \"log_loss\":\n            return log_loss(ytrue, ypred)\n\n    def PlotFtreImp(\n        self, \n        ftreimp: pd.Series, \n        method: str,\n        ntop: int = 50,\n        title_specs: dict = {'fontsize': 9,'fontweight' : 'bold','color': '#992600'},\n        **params,\n    ):\n        \"This function plots the feature importances for the model provided\"\n\n        print()\n        fig, ax = plt.subplots(1, 1, figsize = (25, 7.5))\n\n        ftreimp.sort_values(ascending = False).\\\n        head(ntop).\\\n        plot.bar(ax = ax, color = \"blue\")\n        ax.set_title(f\"Feature Importances - {method}\", **title_specs)\n\n        plt.tight_layout()\n        plt.show()\n        print()\n\n    def PostProcessPreds(self, ypred):\n        \"This method post-processes predictions optionally\"\n        return np.clip(ypred, a_min = 0.0, a_max = 1.0)\n\n    def LoadData(\n            self, X, y, Xtest,\n            train_idx : list = [],\n            dev_idx   : list = [],\n            ):\n        \"This method loads the train and test data for the model fold using/ not using the original data\"\n\n        if self.orig_req == False:\n            Xtr  = X.iloc[train_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ytr  = y.iloc[Xtr.index]\n            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ydev = y.iloc[Xdev.index]\n\n        elif self.orig_req == True and self.orig_all_folds == True:\n            Xtr  = X.iloc[train_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ytr  = y.iloc[Xtr.index]\n            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ydev = y.iloc[Xdev.index]\n\n            orig_x = X.query(\"Source == 'Original'\")[Xtr.columns]\n            orig_y = y.iloc[orig_x.index]\n\n            Xtr = pd.concat([Xtr, orig_x], axis = 0, ignore_index = True)\n            ytr = pd.concat([ytr, orig_y], axis = 0, ignore_index = True)\n\n        elif self.orig_req == True and self.orig_all_folds == False:\n            Xtr  = X.iloc[train_idx].drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ytr  = y.iloc[Xtr.index]\n            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n            ydev = y.iloc[Xdev.index]\n\n        Xt = Xtest[Xdev.columns]\n\n        print(f\"\\n---> Shapes = {Xtr.shape} {ytr.shape} -- {Xdev.shape} {ydev.shape} -- {Xt.shape}\")\n        return (Xtr, ytr, Xdev, ydev, Xt)\n    \n    def MakePreds(self, X, fitted_model):\n        \"This method creates the model predictions based on the model provided, with optional post-processing\"\n\n        if self.problem_type == \"regression\":\n            return self.PostProcessPreds(fitted_model.predict(X))\n        elif self.problem_type == \"binary\":\n            return self.PostProcessPreds(fitted_model.predict_proba(X)[:, 1])\n        elif self.problem_type == \"multiclass\":\n            return self.PostProcessPreds(fitted_model.predict_proba(X))\n\n    def MakeOrigPreds(\n            self, orig: pd.DataFrame, fitted_models: list, n_splits : int, ygrp: pd.Series,\n            ):\n        \"This method creates the original data predictions separately only if required\"\n\n        if self.orig_req == False:\n            orig_preds = 0\n\n        elif self.orig_req == True and self.orig_all_folds == True:\n            orig_preds = 0\n            df = orig.drop(self.drop_cols, axis = 1, errors = \"ignore\")\n\n            for fitted_model in fitted_models:\n                orig_preds = orig_preds + (self.MakePreds(df, fitted_model) / n_splits)\n\n        elif self.orig_req == True and self.orig_all_folds == False:\n            len_orig   = orig.shape[0]\n            orig.index = range(len_orig)\n            orig_ygrp  = ygrp[-1 * len_orig:]\n            orig_ygrp.index = range(len_orig)\n            \n            orig_preds = np.zeros(len_orig)\n            for fold_nb, fitted_model in enumerate(fitted_models):\n                df = \\\n                orig.iloc[orig_ygrp.loc[orig_ygrp == fold_nb].index].\\\n                drop(self.drop_cols, axis=1, errors = \"ignore\")\n                \n                orig_preds[df.index] = self.MakePreds(df, fitted_model)\n                del df\n        return orig_preds\n\n    def MakeOfflineModel(\n        self, X, y, ygrp, Xtest, mdl, method,\n        test_preds_req   : bool = True,\n        ftreimp_plot_req : bool = True,\n        ntop             : int  = 50,\n        **params,\n    ):\n        \"\"\"\n        This function trains the provided model on the dataset and cross-validates appropriately\n\n        Inputs-\n        X, y, ygrp       - training data components (Xtrain, ytrain, fold_nb)\n        Xtest            - test data (optional)\n        model            - model object for training\n        method           - model method label\n        test_preds_req   - boolean flag to extract test set predictions\n        ftreimp_plot_req - boolean flag to plot tree feature importances\n        ntop             - top n features for feature importances plot\n\n        Returns-\n        oof_preds, test_preds - prediction arrays\n        fitted_models         - fitted model list for test set\n        ftreimp               - feature importances across selected features\n        mdl_best_iter         - model average best iteration across folds\n        \"\"\"\n\n        oof_preds     = np.zeros(len(X.loc[X.Source == \"Competition\"]))\n        orig_preds    = np.zeros(len(X.loc[X.Source == \"Original\"]))\n        test_preds    = []\n        mdl_best_iter = []\n        ftreimp       = 0\n\n        scores, tr_scores, fitted_models = [], [], []\n\n        if self.orig_req == True:\n            cv = PDS(ygrp)\n        elif self.orig_req == False:\n            X  = X.loc[X.Source == \"Competition\"]\n            y  = y.iloc[X.index]\n            cv = PDS(ygrp.iloc[0 : len(X)])\n\n        n_splits = ygrp.nunique()\n\n        for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y))):\n            Xtr, ytr, Xdev, ydev, Xt = \\\n            self.LoadData(X, y, Xtest, train_idx, dev_idx)\n\n            model = clone(mdl)\n\n            if \"CB\" in method:\n                model.fit(Xtr, ytr,\n                          eval_set = [(Xdev, ydev)],\n                          verbose = 0,\n                          early_stopping_rounds = self.es_iter,\n                          )\n                best_iter = model.get_best_iteration()\n\n            elif \"LGB\" in method:\n                model.fit(Xtr, ytr,\n                          eval_set = [(Xdev, ydev)],\n                          callbacks = [log_evaluation(0),\n                                       early_stopping(stopping_rounds = self.es_iter, verbose = False,),\n                                       ],\n                          )\n                best_iter = model.best_iteration_\n\n            elif \"XGB\" in method:\n                model.fit(Xtr, ytr,\n                          eval_set = [(Xdev, ydev)],\n                          verbose  = 0,\n                          )\n                best_iter = model.best_iteration\n\n            else:\n                model.fit(Xtr, ytr)\n                best_iter = -1\n\n            fitted_models.append(model)\n\n            try:\n                ftreimp += model.feature_importances_\n            except:\n                pass\n            \n            dev_preds = self.MakePreds(Xdev, model)\n            oof_preds[Xdev.index] = dev_preds\n\n            train_preds  = self.MakePreds(Xtr, model)\n            tr_score     = self.ScoreMetric(ytr.values.flatten(), train_preds)\n            score        = self.ScoreMetric(ydev.values.flatten(), dev_preds)\n\n            scores.append(score)\n            tr_scores.append(tr_score)\n\n            nspace = 15 - len(method) - 2 if fold_nb <= 9 else 15 - len(method) - 1\n            PrintColor(f\"{method} Fold{fold_nb} {' ' * nspace} OOF = {score:.6f} | Train = {tr_score:.6f} | Iter = {best_iter:,.0f} \")\n            mdl_best_iter.append(best_iter)\n\n            if test_preds_req:\n                test_preds.append(self.MakePreds(Xt, model))\n            else:\n                pass\n\n        test_preds    = np.mean(np.stack(test_preds, axis = 1), axis=1)\n        ftreimp       = pd.Series(ftreimp, index = Xdev.columns)\n        mdl_best_iter = np.uint16(np.amax(mdl_best_iter))\n\n        if ftreimp_plot_req :\n            print()\n            self.PlotFtreImp(ftreimp, method = method, ntop = ntop,)\n        else:\n            pass\n\n        PrintColor(f\"\\n---> {np.mean(scores):.6f} +- {np.std(scores):.6f} | OOF\", color = Fore.RED)\n        PrintColor(f\"---> {np.mean(tr_scores):.6f} +- {np.std(tr_scores):.6f} | Train\", color = Fore.RED)\n\n        if mdl_best_iter < 0:\n            pass\n        else:\n            PrintColor(f\"---> Max best iteration = {mdl_best_iter :,.0f}\", color = Fore.RED)\n\n        if self.orig_req:\n            print(f\"---> Collecting original predictions\")\n            orig_preds = self.MakeOrigPreds(X.loc[X.Source == \"Original\"],\n                                            fitted_models,\n                                            n_splits,\n                                            ygrp,\n                                            )\n            oof_preds = np.concatenate([oof_preds, orig_preds], axis= 0)\n        else:\n            pass\n        return (fitted_models, oof_preds, test_preds, ftreimp, mdl_best_iter)\n\n    def MakeOnlineModel(\n        self, X, y, Xtest, model, method,\n        test_preds_req : bool = False,\n    ):\n        \"This method refits the model on the complete train data and returns the model fitted object and predictions\"\n\n        try:\n            model.early_stopping_rounds = None\n        except:\n            pass\n\n        try:\n            model.fit(X, y, verbose = 0)\n        except:\n            model.fit(X, y,)\n\n        oof_preds  = model.predict(X)\n        if test_preds_req:\n            test_preds = model.predict(Xtest[X.columns])\n        else:\n            test_preds = 0\n        return (model, oof_preds, test_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.224078Z","iopub.execute_input":"2024-11-19T12:39:55.224707Z","iopub.status.idle":"2024-11-19T12:39:55.249831Z","shell.execute_reply.started":"2024-11-19T12:39:55.224669Z","shell.execute_reply":"2024-11-19T12:39:55.248427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a training.py\n\nclass OptunaEnsembler:\n    \"\"\"\n    This is the Optuna ensemble class-\n    Source- https://www.kaggle.com/code/arunklenin/ps3e26-cirrhosis-survial-prediction-multiclass\n    \"\"\";\n\n    def __init__(\n        self, state: int = 42, ntrials: int = 300, \n        metric_obj: str = \"minimize\", metric_lbl: str = \"accuracy\",\n        **params\n    ):\n        self.study        = None\n        self.weights      = None\n        self.random_state = state\n        self.n_trials     = ntrials\n        self.direction    = metric_obj\n        self.metric_lbl   = metric_lbl\n\n    def ScoreMetric(self, ytrue, ypred):\n        \"\"\"\n        This is the metric function for the competition\n        \"\"\";\n\n        if self.metric_lbl == \"accuracy\":\n            return accuracy_score(np.uint8(np.round(ytrue, 0)), np.uint8(np.round(ypred)))\n        elif self.metric_lbl == \"auc\":\n            return roc_auc_score(ytrue, ypred)\n        elif self.metric_lbl == \"log_loss\":\n            return log_loss(ytrue, ypred)\n\n    def _objective(\n        self, trial, y_true, y_preds\n    ):\n        \"\"\"\n        This method defines the objective function for the ensemble\n        \"\"\";\n\n        if isinstance(y_preds, pd.DataFrame) or isinstance(y_preds, np.ndarray):\n            weights = [trial.suggest_float(f\"weight{n}\", 0.001, 0.999)\n                       for n in range(y_preds.shape[-1])\n                      ]\n            axis = 1\n\n        elif isinstance(y_preds, list):\n            weights = [trial.suggest_float(f\"weight{n}\", 0.001, 0.999)\n                       for n in range(len(y_preds))\n                      ]\n            axis = 0\n\n        # Calculating the weighted prediction:-\n        weighted_pred  = np.average(np.array(y_preds), axis = axis, weights = weights)\n        score          = self.ScoreMetric(y_true, weighted_pred)\n        return score\n\n    def fit(self, y_true, y_preds):\n        \"This method fits the Optuna objective on the fold level data\";\n\n        optuna.logging.set_verbosity = optuna.logging.ERROR\n\n        self.study = \\\n        optuna.create_study(sampler    = TPESampler(seed = self.random_state),\n                            pruner     = HyperbandPruner(),\n                            study_name = \"Ensemble\",\n                            direction  = self.direction,\n                           )\n\n        obj = partial(self._objective, y_true = y_true, y_preds = y_preds)\n        self.study.optimize(obj, n_trials = self.n_trials)\n\n        if isinstance(y_preds, list):\n            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n        else:\n            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(y_preds.shape[-1])]\n\n    def predict(self, y_preds):\n        \"This method predicts using the fitted Optuna objective\";\n\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict';\n\n        if isinstance(y_preds, list):\n            weighted_pred = np.average(np.array(y_preds), axis=0, weights = self.weights)\n\n        else:\n            weighted_pred = np.average(np.array(y_preds), axis=1, weights = self.weights)\n\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        \"\"\"\n        This method fits the Optuna objective on the fold data, then predicts the test set\n        \"\"\";\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n\n    def weights(self):\n        \"This method returns the non-normalized weights for all models in a fold\"\n        return self.weights\n\nprint()\ncollect();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.252721Z","iopub.execute_input":"2024-11-19T12:39:55.253068Z","iopub.status.idle":"2024-11-19T12:39:55.273553Z","shell.execute_reply.started":"2024-11-19T12:39:55.253035Z","shell.execute_reply":"2024-11-19T12:39:55.272292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile -a training.py\n\ndef NormWeights(weights: dict, methods: list):\n    \"This function normalizes the weights and returns a dataframe of normalized weights across folds and models\"\n\n    weights = pd.DataFrame.from_dict(weights).T\n    weights[\"row_sum\"] = weights.sum(axis=1)\n\n    for col in weights.columns:\n        weights[col] = weights[col] / weights[\"row_sum\"]\n\n    weights.drop(\"row_sum\", axis = 1, inplace = True, errors = \"ignore\")\n    weights.columns    = methods\n    weights.index.name = \"Fold_Nb\"\n    return weights\n\ndef MakeEnsemble(target: str, ntrials: int = 300):\n    \"This function implements the Optuna ensemble on the OOF and test prediction datasets\"\n\n    global OOF_Preds, Mdl_Preds\n\n    PrintColor(f\"\\n{'=' * 20} ENSEMBLE {'=' * 20}\\n\")\n\n    ygrp       = OOF_Preds[\"fold_nb\"]\n    cv         = PDS(ygrp)\n    oof_preds  = np.zeros(len(OOF_Preds))\n    test_preds = []\n    scores     = []\n    weights    = {}\n    drop_cols  = [\"fold_nb\", target, \"Ensemble\"]\n    n_splits   = ygrp.nunique()\n\n    for fold_nb, (_, dev_idx) in tqdm(enumerate(cv.split(OOF_Preds, OOF_Preds[target]))):\n        Xdev = OOF_Preds.iloc[dev_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n        ydev = OOF_Preds.loc[dev_idx, target]\n\n        ens = OptunaEnsembler(ntrials = ntrials)\n        ens.fit(ydev, Xdev,)\n\n        dev_preds = ens.predict(Xdev)\n        score     = ens.ScoreMetric(ydev.values, dev_preds)\n        oof_preds[dev_idx] = dev_preds\n        test_preds.append(\n            ens.predict(Mdl_Preds.drop(drop_cols, axis=1, errors = \"ignore\"))\n        )\n\n        PrintColor(f\"---> {score: .6f} | Fold {fold_nb}\", color = Fore.CYAN)\n        scores.append(score)\n\n        weights[f\"Fold{fold_nb}\"] = ens.weights\n\n    PrintColor(f\"\\n---> OOF = {np.mean(scores): .6f} +- {np.std(scores): .6f} | Ensemble\",\n               color = Fore.RED\n              )\n\n    test_preds = np.mean(np.stack(test_preds, axis=1), axis=1,)\n\n    OOF_Preds[\"Ensemble\"] = oof_preds\n    Mdl_Preds[\"Ensemble\"] = test_preds\n\n    weights = \\\n    NormWeights(\n        weights,\n        methods = Mdl_Preds.drop(drop_cols, axis=1, errors = \"ignore\").columns\n    )\n\n    print(\"\\n\\n\\n\")\n    display(\n        weights.\\\n        style.\\\n        set_caption(\"Normalized weights\").\\\n        format(precision = 6).\\\n        set_properties(\n            props = \"color:red; background-color:white; font-weight: bold; border: maroon dashed 1.6px\"\n        )\n    )\n\n    return weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T12:39:55.275643Z","iopub.execute_input":"2024-11-19T12:39:55.276068Z","iopub.status.idle":"2024-11-19T12:39:55.296685Z","shell.execute_reply.started":"2024-11-19T12:39:55.276021Z","shell.execute_reply":"2024-11-19T12:39:55.295457Z"}},"outputs":[],"execution_count":null}]}