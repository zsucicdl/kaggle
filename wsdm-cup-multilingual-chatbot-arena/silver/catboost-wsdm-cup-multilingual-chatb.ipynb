{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":8478420,"sourceType":"datasetVersion","datasetId":5056610},{"sourceId":9972222,"sourceType":"datasetVersion","datasetId":6135238}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# print(\"\"\"================== Loading Train Features and Testing Features =============\"\"\")\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n# import pickle\n\n# \"\"\"=================== Getting embedding for test file ===============\"\"\"\n# import pandas as pd\n# import numpy as np\n# import re\n# import nltk\n# import torch\n# import pickle\n# from collections import Counter\n# from nltk.corpus import stopwords\n# from transformers import BertTokenizer, BertModel\n# from sklearn.metrics.pairwise import cosine_similarity\n\n# # Ensure NLTK's stopwords are downloaded\n# nltk.download('stopwords')\n\n# # Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# train = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\n\n\n# # Load BERT model and tokenizer, and transfer the model to GPU if available\n# print(\"Loading BERT model...\")\n# bert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_tokenizer')\n# bert_model = BertModel.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_model').to(device)\n\n\n# # Text preprocessing function\n# def preprocess_text(text):\n#     if isinstance(text, str):  # Ensure text is a string before processing\n#         text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n#         words = text.lower().split()  # Lowercase and split the text into words\n#         stop_words = set(stopwords.words('english'))  # Set of stopwords\n#         words = [word for word in words if word not in stop_words]  # Remove stopwords\n#         unique_words = list(Counter(words).keys())  # Get unique words\n#         return ' '.join(unique_words)  # Join them back into a string\n#     return ''  # Return empty string if text is not a valid string\n\n\n# # Apply preprocessing only to specific columns\n# columns_to_preprocess = ['prompt', 'response_a', 'response_b']  # Replace with actual column names\n# for col in columns_to_preprocess:\n#     train[col] = train[col].apply(preprocess_text)\n#     # test[col] = test[col].apply(preprocess_text)\n\n\n# # Function to extract embeddings from BERT\n# def extract_embeddings(text, model, tokenizer, max_length=512):\n#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mean pooling over token embeddings\n#     return embedding\n\n\n# # Function to save embeddings to a pickle file\n# def save_embeddings(data, filename):\n#     with open(filename, 'wb') as f:\n#         pickle.dump(data, f)\n#     print(f\"Saved embeddings to {filename}\")\n\n\n# # Function to generate and save embeddings for each column\n# def generate_and_save_embeddings(df, column_name):\n#     print(f\"Processing column: {column_name}\")\n#     column_data = df[column_name].dropna()  # Remove any NaN values if present\n\n#     # BERT embeddings\n#     embeddings = np.array([extract_embeddings(text, bert_model, bert_tokenizer) for text in column_data])\n#     save_embeddings(embeddings, f'/kaggle/working/{column_name}_bert.pkl')\n#     print(f\"SAVED IS  {column_name}\")\n#     return embeddings\n\n\n# # Generate and save embeddings for each column\n# prompt_embeddings = generate_and_save_embeddings(train, 'prompt')\n# response_a_embeddings = generate_and_save_embeddings(train, 'response_a')\n# response_b_embeddings = generate_and_save_embeddings(train, 'response_b')\n\n\n# # Define function to calculate metrics\n# def calculate_metrics(prompt_emb, response_a_emb, response_b_emb):\n#     metrics = {}\n\n#     # 1. Cosine Similarity\n#     metrics['cosine_similarity_a'] = cosine_similarity([prompt_emb], [response_a_emb])[0][0]\n#     metrics['cosine_similarity_b'] = cosine_similarity([prompt_emb], [response_b_emb])[0][0]\n\n#     # 2. Cosine Distance\n#     metrics['cosine_distance_a'] = 1 - metrics['cosine_similarity_a']\n#     metrics['cosine_distance_b'] = 1 - metrics['cosine_similarity_b']\n\n#     # 3. Mean Euclidean Distance\n#     metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n#     metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n#     # 4. Semantic Overlap (measured by cosine similarity)\n#     metrics['semantic_overlap_a'] = metrics['cosine_similarity_a']\n#     metrics['semantic_overlap_b'] = metrics['cosine_similarity_b']\n\n#     # 5. Topic Coherence (same as semantic overlap for now)\n#     metrics['topic_coherence_a'] = metrics['cosine_similarity_a']\n#     metrics['topic_coherence_b'] = metrics['cosine_similarity_b']\n\n#     # 6. Prompt Adherence\n#     metrics['prompt_adherence_a'] = metrics['cosine_similarity_a']\n#     metrics['prompt_adherence_b'] = metrics['cosine_similarity_b']\n\n#     # 7. Contextual Consistency\n#     metrics['contextual_consistency'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n#     # 8. Diversity of Response\n#     metrics['response_diversity'] = 1 - metrics['contextual_consistency']\n\n#     # 9. Sentiment Similarity Score\n#     metrics['sentiment_similarity_a'] = metrics['cosine_similarity_a']  # Proxy\n#     metrics['sentiment_similarity_b'] = metrics['cosine_similarity_b']  # Proxy\n\n#     # 10. Topic Coverage (same as semantic overlap for simplicity)\n#     metrics['topic_coverage_a'] = metrics['cosine_similarity_a']\n#     metrics['topic_coverage_b'] = metrics['cosine_similarity_b']\n\n\n#     # 12. Depth of Response (Variance of embeddings as proxy)\n#     metrics['depth_of_response_a'] = np.var(response_a_emb)\n#     metrics['depth_of_response_b'] = np.var(response_b_emb)\n\n#     # 13. Focus Score (Inverse of Euclidean Distance)\n#     metrics['focus_score_a'] = 1 / (1 + metrics['euclidean_distance_a'])\n#     metrics['focus_score_b'] = 1 / (1 + metrics['euclidean_distance_b'])\n\n#     # 14. Embedding Vector Magnitude Ratio\n#     metrics['embedding_magnitude_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n#     metrics['embedding_magnitude_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n#     # 15. Off-Topic Score\n#     metrics['off_topic_a'] = metrics['cosine_distance_a']\n#     metrics['off_topic_b'] = metrics['cosine_distance_b']\n\n\n#     # 16. Euclidean Distance\n#     metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n#     metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n#     # 17. Embedding Density\n#     metrics['embedding_density_a'] = np.sum(np.square(response_a_emb))\n#     metrics['embedding_density_b'] = np.sum(np.square(response_b_emb))\n\n#     # 18. Internal Coherence\n#     metrics['internal_coherence_a'] = np.mean(cosine_similarity([response_a_emb]))\n#     metrics['internal_coherence_b'] = np.mean(cosine_similarity([response_b_emb]))\n\n#     # 19. Response Concreteness\n#     metrics['response_concreteness_a'] = np.std(response_a_emb)\n#     metrics['response_concreteness_b'] = np.std(response_b_emb)\n\n#     # 20. Redundancy Score\n#     metrics['redundancy_score_a'] = 1 - np.mean(np.std(response_a_emb))\n#     metrics['redundancy_score_b'] = 1 - np.mean(np.std(response_b_emb))\n\n#     # 21. Semantic Entailment (similar to cosine similarity)\n#     metrics['semantic_entailment_a'] = metrics['cosine_similarity_a']\n#     metrics['semantic_entailment_b'] = metrics['cosine_similarity_b']\n\n#     # 22. Prompt-Response Embedding Ratio\n#     metrics['embedding_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n#     metrics['embedding_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n#     # 23. Informativeness Score\n#     metrics['informativeness_a'] = np.var(response_a_emb)\n#     metrics['informativeness_b'] = np.var(response_b_emb)\n\n#     # 24. Topic Drift Score\n#     metrics['topic_drift_a'] = np.std(response_a_emb - prompt_emb)\n#     metrics['topic_drift_b'] = np.std(response_b_emb - prompt_emb)\n\n#     # 25. Context Alignment\n#     metrics['context_alignment'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n#     #26. Disparity Score\n#     metrics['disparity_score'] = euclidean_distances([response_a_emb], [response_b_emb])[0][0]\n\n\n#     return metrics\n\n\n# def load_data(prompt_path,response_a_path , response_b_path):\n#     # Load precomputed embeddings (assuming they've been generated and saved as instructed)\n#     prompt_embeddings = pickle.load(open(prompt_path, 'rb'))\n#     response_a_embeddings = pickle.load(open(response_a_path, 'rb'))\n#     response_b_embeddings = pickle.load(open(response_b_path , 'rb'))\n\n#     train_features_data = []\n#     for i in range(len(prompt_embeddings)):\n#         metrics = calculate_metrics(prompt_embeddings[i], response_a_embeddings[i], response_b_embeddings[i])\n#         train_features_data.append(metrics)\n#     train_features_data = pd.DataFrame(train_features_data)\n#     return train_features_data\n\n# prompt_path = '/kaggle/input/bert-embedding-of-train-data/prompt_bert.pkl'\n# response_a_path = '/kaggle/input/bert-embedding-of-train-data/response_a_bert.pkl'\n# response_b_path  =  '/kaggle/input/bert-embedding-of-train-data/response_b_bert.pkl'\n# train_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\n# prompt_path = '/kaggle/working/prompt_bert.pkl'\n# response_a_path ='/kaggle/working/response_a_bert.pkl'\n# response_b_path = '/kaggle/working/response_b_bert.pkl'\n# test_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\n# print('======================== TRAIN_FEATURES AND TEST_FEATURES LOADED ===============================')\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\"\"================== Loading Train Features and Testing Features =============\"\"\")\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\nimport pickle\n\n\"\"\"=================== Getting embedding for test file ===============\"\"\"\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport torch\nimport pickle\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Ensure NLTK's stopwords are downloaded\nnltk.download('stopwords')\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n# train = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n\n\n# Load BERT model and tokenizer, and transfer the model to GPU if available\nprint(\"Loading BERT model...\")\nbert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_tokenizer')\nbert_model = BertModel.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_model').to(device)\n\n\n# Text preprocessing function\ndef preprocess_text(text):\n    if isinstance(text, str):  # Ensure text is a string before processing\n        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n        words = text.lower().split()  # Lowercase and split the text into words\n        stop_words = set(stopwords.words('english'))  # Set of stopwords\n        words = [word for word in words if word not in stop_words]  # Remove stopwords\n        unique_words = list(Counter(words).keys())  # Get unique words\n        return ' '.join(unique_words)  # Join them back into a string\n    return ''  # Return empty string if text is not a valid string\n\n\n# Apply preprocessing only to specific columns\ncolumns_to_preprocess = ['prompt', 'response_a', 'response_b']  # Replace with actual column names\nfor col in columns_to_preprocess:\n    # train[col] = train[col].apply(preprocess_text)\n    test[col] = test[col].apply(preprocess_text)\n\n\n# Function to extract embeddings from BERT\ndef extract_embeddings(text, model, tokenizer, max_length=512):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mean pooling over token embeddings\n    return embedding\n\n\n# Function to save embeddings to a pickle file\ndef save_embeddings(data, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(data, f)\n    print(f\"Saved embeddings to {filename}\")\n\n\n# Function to generate and save embeddings for each column\ndef generate_and_save_embeddings(df, column_name):\n    print(f\"Processing column: {column_name}\")\n    column_data = df[column_name].dropna()  # Remove any NaN values if present\n\n    # BERT embeddings\n    embeddings = np.array([extract_embeddings(text, bert_model, bert_tokenizer) for text in column_data])\n    save_embeddings(embeddings, f'/kaggle/working/{column_name}_bert.pkl')\n    print(f\"SAVED IS  {column_name}\")\n    return embeddings\n\n\n# Generate and save embeddings for each column\n# prompt_embeddings = generate_and_save_embeddings(train, 'prompt')\n# response_a_embeddings = generate_and_save_embeddings(train, 'response_a')\n# response_b_embeddings = generate_and_save_embeddings(train, 'response_b')\n\nprompt_embeddings = generate_and_save_embeddings(test, 'prompt')\nresponse_a_embeddings = generate_and_save_embeddings(test, 'response_a')\nresponse_b_embeddings = generate_and_save_embeddings(test, 'response_b')\n\n\n\n# Define function to calculate metrics\ndef calculate_metrics(prompt_emb, response_a_emb, response_b_emb):\n    metrics = {}\n\n    # 1. Cosine Similarity\n    metrics['cosine_similarity_a'] = cosine_similarity([prompt_emb], [response_a_emb])[0][0]\n    metrics['cosine_similarity_b'] = cosine_similarity([prompt_emb], [response_b_emb])[0][0]\n\n    # 2. Cosine Distance\n    metrics['cosine_distance_a'] = 1 - metrics['cosine_similarity_a']\n    metrics['cosine_distance_b'] = 1 - metrics['cosine_similarity_b']\n\n    # 3. Mean Euclidean Distance\n    metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n    metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n    # 4. Semantic Overlap (measured by cosine similarity)\n    metrics['semantic_overlap_a'] = metrics['cosine_similarity_a']\n    metrics['semantic_overlap_b'] = metrics['cosine_similarity_b']\n\n    # 5. Topic Coherence (same as semantic overlap for now)\n    metrics['topic_coherence_a'] = metrics['cosine_similarity_a']\n    metrics['topic_coherence_b'] = metrics['cosine_similarity_b']\n\n    # 6. Prompt Adherence\n    metrics['prompt_adherence_a'] = metrics['cosine_similarity_a']\n    metrics['prompt_adherence_b'] = metrics['cosine_similarity_b']\n\n    # 7. Contextual Consistency\n    metrics['contextual_consistency'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n    # 8. Diversity of Response\n    metrics['response_diversity'] = 1 - metrics['contextual_consistency']\n\n    # 9. Sentiment Similarity Score\n    metrics['sentiment_similarity_a'] = metrics['cosine_similarity_a']  # Proxy\n    metrics['sentiment_similarity_b'] = metrics['cosine_similarity_b']  # Proxy\n\n    # 10. Topic Coverage (same as semantic overlap for simplicity)\n    metrics['topic_coverage_a'] = metrics['cosine_similarity_a']\n    metrics['topic_coverage_b'] = metrics['cosine_similarity_b']\n\n\n    # 12. Depth of Response (Variance of embeddings as proxy)\n    metrics['depth_of_response_a'] = np.var(response_a_emb)\n    metrics['depth_of_response_b'] = np.var(response_b_emb)\n\n    # 13. Focus Score (Inverse of Euclidean Distance)\n    metrics['focus_score_a'] = 1 / (1 + metrics['euclidean_distance_a'])\n    metrics['focus_score_b'] = 1 / (1 + metrics['euclidean_distance_b'])\n\n    # 14. Embedding Vector Magnitude Ratio\n    metrics['embedding_magnitude_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n    metrics['embedding_magnitude_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n    # 15. Off-Topic Score\n    metrics['off_topic_a'] = metrics['cosine_distance_a']\n    metrics['off_topic_b'] = metrics['cosine_distance_b']\n\n\n    # 16. Euclidean Distance\n    metrics['euclidean_distance_a'] = euclidean_distances([prompt_emb], [response_a_emb])[0][0]\n    metrics['euclidean_distance_b'] = euclidean_distances([prompt_emb], [response_b_emb])[0][0]\n\n    # 17. Embedding Density\n    metrics['embedding_density_a'] = np.sum(np.square(response_a_emb))\n    metrics['embedding_density_b'] = np.sum(np.square(response_b_emb))\n\n    # 18. Internal Coherence\n    metrics['internal_coherence_a'] = np.mean(cosine_similarity([response_a_emb]))\n    metrics['internal_coherence_b'] = np.mean(cosine_similarity([response_b_emb]))\n\n    # 19. Response Concreteness\n    metrics['response_concreteness_a'] = np.std(response_a_emb)\n    metrics['response_concreteness_b'] = np.std(response_b_emb)\n\n    # 20. Redundancy Score\n    metrics['redundancy_score_a'] = 1 - np.mean(np.std(response_a_emb))\n    metrics['redundancy_score_b'] = 1 - np.mean(np.std(response_b_emb))\n\n    # 21. Semantic Entailment (similar to cosine similarity)\n    metrics['semantic_entailment_a'] = metrics['cosine_similarity_a']\n    metrics['semantic_entailment_b'] = metrics['cosine_similarity_b']\n\n    # 22. Prompt-Response Embedding Ratio\n    metrics['embedding_ratio_a'] = np.linalg.norm(response_a_emb) / np.linalg.norm(prompt_emb)\n    metrics['embedding_ratio_b'] = np.linalg.norm(response_b_emb) / np.linalg.norm(prompt_emb)\n\n    # 23. Informativeness Score\n    metrics['informativeness_a'] = np.var(response_a_emb)\n    metrics['informativeness_b'] = np.var(response_b_emb)\n\n    # 24. Topic Drift Score\n    metrics['topic_drift_a'] = np.std(response_a_emb - prompt_emb)\n    metrics['topic_drift_b'] = np.std(response_b_emb - prompt_emb)\n\n    # 25. Context Alignment\n    metrics['context_alignment'] = cosine_similarity([response_a_emb], [response_b_emb])[0][0]\n\n    #26. Disparity Score\n    metrics['disparity_score'] = euclidean_distances([response_a_emb], [response_b_emb])[0][0]\n\n\n    return metrics\n\n\ndef load_data(prompt_path,response_a_path , response_b_path):\n    # Load precomputed embeddings (assuming they've been generated and saved as instructed)\n    prompt_embeddings = pickle.load(open(prompt_path, 'rb'))\n    response_a_embeddings = pickle.load(open(response_a_path, 'rb'))\n    response_b_embeddings = pickle.load(open(response_b_path , 'rb'))\n\n    train_features_data = []\n    for i in range(len(prompt_embeddings)):\n        metrics = calculate_metrics(prompt_embeddings[i], response_a_embeddings[i], response_b_embeddings[i])\n        train_features_data.append(metrics)\n    train_features_data = pd.DataFrame(train_features_data)\n    return train_features_data\n\nprompt_path = '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/prompt_bert.pkl'\nresponse_a_path = '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/response_a_bert.pkl'\nresponse_b_path  =  '/kaggle/input/training-llm-prompt-classifciation-bert-embedding/response_b_bert.pkl'\ntrain_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\nprompt_path = '/kaggle/working/prompt_bert.pkl'\nresponse_a_path ='/kaggle/working/response_a_bert.pkl'\nresponse_b_path = '/kaggle/working/response_b_bert.pkl'\ntest_features_from_embedding = load_data(prompt_path,response_a_path , response_b_path)\n\nprint('======================== TRAIN_FEATURES AND TEST_FEATURES LOADED ===============================')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T12:38:17.392171Z","iopub.execute_input":"2024-11-22T12:38:17.392862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('=================== Training and Submission Creation =======')\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Load training and testing data\ntrain_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n\n# Encode the target column into binary format\nlabel_encoder = LabelEncoder()\ntrain_df['target_encoded'] = label_encoder.fit_transform(train_df['winner'])  # Encodes `model_a` as 0, `model_b` as 1\n\n# Define the features and target variable\ny = train_df['target_encoded']\n\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    train_features_from_embedding,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n# Initialize SVC model with a pipeline (scaling + classifier)\nsvc_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Scale data before passing to SVC\n    ('svc', SVC(probability=False, random_state=42))  # `probability=False` for faster training\n])\n\n# Train the SVC model\nprint(\"Training SVC model...\")\nsvc_pipeline.fit(X_train, y_train)\nprint(\"SVC training completed.\")\n\n# Validate the SVC model\nprint(\"Evaluating SVC model on validation set...\")\nsvc_val_predictions = svc_pipeline.predict(X_val)\nprint(\"Classification Report for SVC:\")\nprint(classification_report(y_val, svc_val_predictions, target_names=label_encoder.classes_))\n\n# Make predictions on the test set using SVC\nprint(\"Making predictions on test data with SVC...\")\nsvc_test_predictions = svc_pipeline.predict(test_features_from_embedding)\n\n# Decode predictions back to the original labels\ndecoded_predictions = label_encoder.inverse_transform(svc_test_predictions)\n\n# Create SVC submission DataFrame\nsvc_submission = pd.DataFrame({\n    'id': test_df['id'], \n    'winner': decoded_predictions  # Add decoded predictions\n})\n\n# Save the SVC submission file\nsvc_submission_file = '/kaggle/working/submission.csv'\nsvc_submission.to_csv(svc_submission_file, index=False)\nprint(f\"SVC submission saved to {svc_submission_file}\")\n\n# Optionally, display the first few rows of the SVC submission file\nprint(\"First few rows of the SVC submission file:\")\nprint(svc_submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T17:32:48.723378Z","iopub.execute_input":"2024-11-22T17:32:48.723752Z","iopub.status.idle":"2024-11-22T17:32:50.044155Z","shell.execute_reply.started":"2024-11-22T17:32:48.723717Z","shell.execute_reply":"2024-11-22T17:32:50.04314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"===========Getting Embedding(Vectors Tensor) of ============\")\n\n# import pandas as pd\n# import numpy as np\n# import re\n# import nltk\n# import torch\n# import pickle\n# from collections import Counter\n# from nltk.corpus import stopwords\n# from transformers import BertTokenizer, BertModel, XLNetTokenizer, XLNetModel, GPT2Tokenizer, GPT2Model, T5Tokenizer, T5Model\n# import spacy\n\n# # Ensure NLTK's stopwords are downloaded\n# nltk.download('stopwords')\n\n# # Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Load the data\n# train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv', nrows=5000)\n# test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv', nrows=5000)\n\n# # Load models and transfer them to GPU if available\n# print(\"Loading models...\")\n\n# # BERT\n# bert_tokenizer = BertTokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_tokenizer')\n# bert_model = BertModel.from_pretrained('/kaggle/input/vector-model-embedder/models/bert_model').to(device)\n\n\n# # XLNet\n# xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"/kaggle/input/vector-model-embedder/models/xlnet_tokenizer\")\n# xlnet_model = XLNetModel.from_pretrained(\"/kaggle/input/vector-model-embedder/models/xlnet_model\").to(device)\n\n\n# # GPT-2\n# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/gpt2_tokenizer')\n# gpt2_model = GPT2Model.from_pretrained('/kaggle/input/vector-model-embedder/models/gpt2_model').to(device)\n\n# # T5\n# t5_tokenizer = T5Tokenizer.from_pretrained('/kaggle/input/vector-model-embedder/models/t5_tokenizer')\n# t5_model = T5Model.from_pretrained('/kaggle/input/vector-model-embedder/models/t5_model').to(device)\n\n\n# # SpaCy\n# nlp_local = spacy.load(\"/kaggle/input/vector-model-embedder/models/spacy_model\")\n\n\n# # Text preprocessing function\n# def preprocess_text(text):\n#     text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n#     words = text.lower().split()  # Lowercase and split the text into words\n#     stop_words = set(stopwords.words('english'))  # Set of stopwords\n#     words = [word for word in words if word not in stop_words]  # Remove stopwords\n#     unique_words = list(Counter(words).keys())  # Get unique words\n#     return ' '.join(unique_words)  # Join them back into a string\n\n# # Apply preprocessing to all columns that will be used for embedding extraction\n# train = train.applymap(preprocess_text)\n# test = test.applymap(preprocess_text)\n\n# # Function to extract embeddings from a model\n# def extract_embeddings(text, model, tokenizer, max_length=512):\n#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n#     embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mean pooling over token embeddings\n#     return embedding\n\n# # Function to extract embeddings using SpaCy\n# def text_to_vector(text):\n#     doc = nlp_local(text)\n#     return doc.vector\n\n# # Function to save embeddings to pickle files\n# def save_embeddings(data, filename):\n#     with open(filename, 'wb') as f:\n#         pickle.dump(data, f)\n#     print(f\"Saved embeddings to {filename}\")\n\n# # Function to generate and save embeddings for a list of columns\n# def generate_and_save_embeddings(df, column_names):\n#     for column_name in column_names:\n#         print(f\"Processing column: {column_name}\")\n#         column_data = df[column_name].dropna()  # Remove any NaN values if present\n        \n#         # BERT embeddings\n#         bert_embeddings = np.array([extract_embeddings(text, bert_model, bert_tokenizer) for text in column_data])\n#         save_embeddings(bert_embeddings, f'/kaggle/working/{column_name}_bert.pkl')\n\n#         # XLNet embeddings\n#         xlnet_embeddings = np.array([extract_embeddings(text, xlnet_model, xlnet_tokenizer) for text in column_data])\n#         save_embeddings(xlnet_embeddings, f'/kaggle/working/{column_name}_xlnet.pkl')\n\n#         # GPT-2 embeddings\n#         gpt2_embeddings = np.array([extract_embeddings(text, gpt2_model, gpt2_tokenizer) for text in column_data])\n#         save_embeddings(gpt2_embeddings, f'/kaggle/working/{column_name}_gpt2.pkl')\n\n#         # T5 embeddings\n#         t5_embeddings = np.array([extract_embeddings(text, t5_model, t5_tokenizer) for text in column_data])\n#         save_embeddings(t5_embeddings, f'/kaggle/working/{column_name}_t5.pkl')\n\n#         # SpaCy embeddings\n#         spacy_embeddings = np.array([text_to_vector(text) for text in column_data])\n#         save_embeddings(spacy_embeddings, f'/kaggle/working/{column_name}_spacy.pkl')\n\n# # List of columns for which embeddings need to be generated\n# columns_to_embed = ['prompt', 'response_a', 'response_b']  # Replace with actual column names\n\n# # Generate and save embeddings for the specified columns in both train and test data\n# generate_and_save_embeddings(train, columns_to_embed)\n# generate_and_save_embeddings(test, columns_to_embed)\n\n# print(\"All embeddings generated and saved successfully.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, SGDClassifier, RidgeClassifier\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\n# from sklearn.svm import SVC, LinearSVC, NuSVC\n# from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n# from sklearn.ensemble import IsolationForest\n# import xgboost as xgb\n# from sklearn.metrics import accuracy_score\n\n# path = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/\"\n# train = pd.read_parquet(path + \"train.parquet\")\n# test = pd.read_parquet(path + \"test.parquet\")\n# sub = pd.read_csv(path + \"sample_submission.csv\")\n\n# # 10% as validation split, this percentage could be changed\n# train, valid = train_test_split(train, test_size=0.2, stratify=train[\"winner\"], random_state=161194)\n\n# # Train set can be inverted (and winner too) to get twice the data from the available training dataset\n# train_inv = train.copy()\n# train_inv[\"response_a\"], train_inv[\"response_b\"] = train_inv[\"response_b\"], train_inv[\"response_a\"]\n# train_inv[\"winner\"] = train_inv[\"winner\"].apply(lambda x: \"model_a\" if \"b\" in x else \"model_b\")\n\n# # Compute features\n# def compute_feats(df):\n#     for col in [\"response_a\", \"response_b\", \"prompt\"]:\n#         # response length is a key factor when choosing between two responses\n#         df[f\"{col}_len\"] = df[f\"{col}\"].str.len()\n\n#         # Some characters counting features \n#         df[f\"{col}_spaces\"] = df[f\"{col}\"].str.count(\"\\s\")\n#         df[f\"{col}_punct\"] = df[f\"{col}\"].str.count(\",|\\.|!\")\n#         df[f\"{col}_question_mark\"] = df[f\"{col}\"].str.count(\"\\?\")\n#         df[f\"{col}_quot\"] = df[f\"{col}\"].str.count(\"'|\\\"\")\n#         df[f\"{col}_formatting_chars\"] = df[f\"{col}\"].str.count(\"\\*|\\_\")\n#         df[f\"{col}_math_chars\"] = df[f\"{col}\"].str.count(\"\\-|\\+|\\=\")\n#         df[f\"{col}_curly_open\"] = df[f\"{col}\"].str.count(\"\\{\")\n#         df[f\"{col}_curly_close\"] = df[f\"{col}\"].str.count(\"}\")\n#         df[f\"{col}_round_open\"] = df[f\"{col}\"].str.count(\"\\(\")\n#         df[f\"{col}_round_close\"] = df[f\"{col}\"].str.count(\"\\)\")\n#         df[f\"{col}_special_chars\"] = df[f\"{col}\"].str.count(\"\\W\")\n#         df[f\"{col}_digits\"] = df[f\"{col}\"].str.count(\"\\d\") > 0\n#         df[f\"{col}_letters\"] = df[f\"{col}\"].str.count(\"[a-zA-Z]\") > 0.1 * df[f\"{col}_len\"]\n#         df[f\"{col}_chinese\"] = df[f\"{col}\"].str.count(r'[\\u4e00-\\u9fff]+') > 0.1 * df[f\"{col}_len\"]\n\n#         # Features that show how balanced are curly and round brackets\n#         df[f\"{col}_round_balance\"] = df[f\"{col}_round_open\"] - df[f\"{col}_round_close\"]\n#         df[f\"{col}_curly_balance\"] = df[f\"{col}_curly_open\"] - df[f\"{col}_curly_close\"]\n\n#         # Feature that tells if the string json is present somewhere\n#         df[f\"{col}_json\"] = df[f\"{col}\"].str.lower().str.count(\"json\")\n#     return df\n\n# train = compute_feats(train)\n# train_inv = compute_feats(train_inv)\n# train = pd.concat([train, train_inv])\n# valid = compute_feats(valid)\n# test = compute_feats(test)\n\n# # Feature selection\n# feats = list(train.columns)[8:]\n\n# train[\"winner\"] = (train[\"winner\"] == \"model_a\").astype(\"int\")\n# valid[\"winner\"] = (valid[\"winner\"] == \"model_a\").astype(\"int\")\n\n# X = train[feats]\n# y = train[\"winner\"]\n\n# X_val = valid[feats]\n# y_val = valid[\"winner\"]\n\n# # List of classifiers to use\n# classifiers = {\n#     # \"LogisticRegression\": LogisticRegression(max_iter=1000),\n#     # \"SGDClassifier\": SGDClassifier(),\n#     \"XGBoost\": xgb.XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth=6, objective='binary:logistic'),\n#     # \"MLPClassifier\": MLPClassifier(max_iter=1000),\n#     # \"ExtraTreeClassifier\": ExtraTreeClassifier(),\n#     # \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n#     # \"SVC\": SVC(),\n#     # \"LinearSVC\": LinearSVC(),\n#     # \"NuSVC\": NuSVC(),\n#     # \"ExtraTreesClassifier\": ExtraTreesClassifier(n_estimators=1000),\n#     # \"RandomForestClassifier\": RandomForestClassifier(n_estimators=1000),\n#     # \"IsolationForest\": IsolationForest(),\n#     # \"PassiveAggressiveClassifier\": PassiveAggressiveClassifier(),\n#     # \"RidgeClassifier\": RidgeClassifier()\n# }\n\n# # Train each model and generate predictions\n# predictions = []\n\n# for name, clf in classifiers.items():\n#     print(f\"Training {name}...\")\n#     clf.fit(X, y)\n    \n#     # Evaluate on validation set\n#     y_pred_val = clf.predict(X_val)\n#     print(f\"{name} Accuracy on validation set: {accuracy_score(y_val, y_pred_val):.4f}\")\n    \n#     # Predict on test set\n#     y_pred_test = clf.predict(X_val)\n#     test[\"winner\"] = y_pred_test\n#     test[\"winner\"] = test[\"winner\"].apply(lambda x: \"model_a\" if x == 1 else \"model_b\")\n    \n#     # Prepare submission\n#     sub = test[[\"id\", \"winner\"]]\n#     sub.to_csv(f\"/kaggle/working/submission.csv\", index=False)\n#     predictions.append(sub)\n\n# # ensemble_predictions = np.mean([pred[\"winner\"] == \"model_a\" for pred in predictions], axis=0)\n# # final_submission = test.copy()\n# # final_submission[\"winner\"] = np.where(ensemble_predictions > 0.5, \"model_a\", \"model_b\")\n# # final_submission.to_csv(\"submission_ensemble.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}