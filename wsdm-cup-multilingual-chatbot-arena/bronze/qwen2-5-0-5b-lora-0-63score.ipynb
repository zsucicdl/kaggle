{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":10047183,"sourceType":"datasetVersion","datasetId":6189903},{"sourceId":182754,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155768,"modelId":178224},{"sourceId":182758,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155772,"modelId":178228},{"sourceId":187662,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":159984,"modelId":182363}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n!pip install peft --no-index --find-links=/kaggle/input/peft-pkg/peft_pkg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:08.322997Z","iopub.execute_input":"2024-12-04T03:55:08.323364Z","iopub.status.idle":"2024-12-04T03:55:20.626635Z","shell.execute_reply.started":"2024-12-04T03:55:08.32333Z","shell.execute_reply":"2024-12-04T03:55:20.625429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, TaskType, PeftModel\nfrom tqdm import tqdm\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:20.629022Z","iopub.execute_input":"2024-12-04T03:55:20.629746Z","iopub.status.idle":"2024-12-04T03:55:25.756549Z","shell.execute_reply.started":"2024-12-04T03:55:20.629707Z","shell.execute_reply":"2024-12-04T03:55:25.755478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_transfer(dataframe, instruction):\n    x_cols = ['prompt', 'response_a', 'response_b', 'id']\n    x = dataframe[x_cols]\n\n    messages = []\n    id_list = []\n    for idx in range(len(x)):\n        x_line = x.iloc[idx,:]\n\n        prompt = 'PROMPT: ' + x_line['prompt']\n        model_a = 'MODEL_A: ' + x_line['response_a']\n        model_b = 'MODEL_B: ' + x_line['response_b']\n        text = prompt + model_a + model_b\n\n        message = {\n            'instruction': instruction,\n            'input': text,\n            'output': \"\"\n        }\n\n        messages.append(message)\n        id_list.append(x_line['id'])\n    \n    return messages, id_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:25.757865Z","iopub.execute_input":"2024-12-04T03:55:25.758338Z","iopub.status.idle":"2024-12-04T03:55:25.765167Z","shell.execute_reply.started":"2024-12-04T03:55:25.758304Z","shell.execute_reply":"2024-12-04T03:55:25.764059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(msg, model, tokenizer, repeat=1):\n    device = \"cuda\"\n    \n    model.to(device)\n    messages = [\n        {\"role\": \"system\", \"content\": msg['instruction']},\n        {\"role\": \"user\", \"content\": msg['input']}\n    ]\n\n    repeat_log = []\n    for _ in range(repeat):\n        text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n    \n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=5\n        )\n        generated_ids = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n    \n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        \n        repeat_log.append(response)\n    response = Counter(repeat_log).most_common(1)[0][0]\n\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:25.767547Z","iopub.execute_input":"2024-12-04T03:55:25.767863Z","iopub.status.idle":"2024-12-04T03:55:25.781207Z","shell.execute_reply.started":"2024-12-04T03:55:25.767833Z","shell.execute_reply":"2024-12-04T03:55:25.780305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_pt = \"/kaggle/input/qwen_0.5b_instruct/transformers/default/1\"\nlora_pt = \"/kaggle/input/qwen_0.5b_lorafinetune_5ktoken_62kckpt/transformers/default/1\"\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\ndef load_models(qwen_dir, ckpt):\n    tokenizer = AutoTokenizer.from_pretrained(qwen_dir)\n    model = AutoModelForCausalLM.from_pretrained(qwen_dir)\n    model = PeftModel.from_pretrained(model, model_id=ckpt, config=config)\n    model.eval()\n\n    return tokenizer, model\n\ntokenizer, model = load_models(model_pt, lora_pt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:25.782306Z","iopub.execute_input":"2024-12-04T03:55:25.782618Z","iopub.status.idle":"2024-12-04T03:55:33.544004Z","shell.execute_reply.started":"2024-12-04T03:55:25.782584Z","shell.execute_reply":"2024-12-04T03:55:33.543138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dir = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\"\ntest = pd.read_parquet(test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.545431Z","iopub.execute_input":"2024-12-04T03:55:33.54589Z","iopub.status.idle":"2024-12-04T03:55:33.684035Z","shell.execute_reply.started":"2024-12-04T03:55:33.545842Z","shell.execute_reply":"2024-12-04T03:55:33.683149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instruction = \"\"\"In the text provided below, PROMPT is the question presented; MODEL_A is the response from the first model; MODEL_B is the response from the second model. Please select the best answer from the two responses above. If the first answer is better, return \"model_a\"; if the second answer is better, return \"model_b\".\"\"\"\ntest, id_list = data_transfer(test, instruction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.685176Z","iopub.execute_input":"2024-12-04T03:55:33.685469Z","iopub.status.idle":"2024-12-04T03:55:33.70113Z","shell.execute_reply.started":"2024-12-04T03:55:33.685437Z","shell.execute_reply":"2024-12-04T03:55:33.699973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nfor msg in tqdm(test):\n    try:\n        response = predict(msg, model, tokenizer, 5)\n        predictions.append(response)\n    except:\n        predictions.append(\"model_a\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.702299Z","iopub.execute_input":"2024-12-04T03:55:33.702585Z","iopub.status.idle":"2024-12-04T03:55:33.734648Z","shell.execute_reply.started":"2024-12-04T03:55:33.702555Z","shell.execute_reply":"2024-12-04T03:55:33.733533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.DataFrame({\n    'id': id_list,\n    'winner': predictions\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.735906Z","iopub.execute_input":"2024-12-04T03:55:33.736228Z","iopub.status.idle":"2024-12-04T03:55:33.741311Z","shell.execute_reply.started":"2024-12-04T03:55:33.736197Z","shell.execute_reply":"2024-12-04T03:55:33.740165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.744954Z","iopub.execute_input":"2024-12-04T03:55:33.745437Z","iopub.status.idle":"2024-12-04T03:55:33.758101Z","shell.execute_reply.started":"2024-12-04T03:55:33.745404Z","shell.execute_reply":"2024-12-04T03:55:33.75728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:55:33.759283Z","iopub.execute_input":"2024-12-04T03:55:33.759573Z","iopub.status.idle":"2024-12-04T03:55:33.774432Z","shell.execute_reply.started":"2024-12-04T03:55:33.759544Z","shell.execute_reply":"2024-12-04T03:55:33.773468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}