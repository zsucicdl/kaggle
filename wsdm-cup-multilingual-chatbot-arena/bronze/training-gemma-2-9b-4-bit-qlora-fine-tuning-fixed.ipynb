{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\nThe great [original notebook](https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning/notebook) doesn't work now because of OSS version confliction.  \nTo solve this, the OSS versions following were fixed to run correctly.\n\n- transformers : 4.42.3\n- bitsandbytes : 0.43.1\n- accelerate : 0.32.1\n- peft : 0.11.1","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## What this notebook is\nThis notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).\nI used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.\n\n## Result\n\nI used `id % 5 == 0` as an evaluation set and used all the rest for training.\n\n| subset | log loss |\n| - | - |\n| eval | 0.9371|\n| LB | 0.941 |\n\n## What is QLoRA fine-tuning?\n\nIn the conventional fine-tuning, weight ($\\mathbf{W}$) is updated as follows:\n\n$$\n\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{{\\partial L}}{{\\partial \\mathbf{W}}} = \\mathbf{W} + \\Delta \\mathbf{W}\n$$\n\nwhere $L$ is a loss at this step and $\\eta$ is a learning rate.\n\n[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\\Delta \\mathbf{W} \\in \\mathbb{R}^{\\text{d} \\times \\text{k}}$ by factorizing $\\Delta \\mathbf{W}$ into two (much) smaller matrices, $\\mathbf{B} \\in \\mathbb{R}^{\\text{d} \\times \\text{r}}$ and $\\mathbf{A} \\in \\mathbb{R}^{\\text{r} \\times \\text{k}}$ with $r \\ll \\text{min}(\\text{d}, \\text{k})$.\n\n$$\n\\Delta \\mathbf{W}_{s} \\approx \\mathbf{B} \\mathbf{A}\n$$\n\n<img src=\"https://storage.googleapis.com/pii_data_detection/lora_diagram.png\">\n\nDuring training, only $\\mathbf{A}$ and $\\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.\n\n[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. \nNote that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.\n\n1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.\n\n## Note\nIt takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.\nThis notebook uses only 100 samples for demo purpose, but everything else is same as my setup.","metadata":{}},{"cell_type":"code","source":"# gemma-2 is available from transformers>=4.42.3\n!pip install transformers==4.42.3\n!pip install bitsandbytes==0.43.1\n!pip install accelerate==0.32.1\n!pip install peft==0.11.1","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-08T13:34:05.824341Z","iopub.execute_input":"2024-12-08T13:34:05.825251Z","iopub.status.idle":"2024-12-08T13:34:57.878896Z","shell.execute_reply.started":"2024-12-08T13:34:05.825212Z","shell.execute_reply":"2024-12-08T13:34:57.877762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Define the model name\nmodel_name = 'unsloth/gemma-2-2b-it-bnb-4bit'\n\n# Download the model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Save them locally\nmodel.save_pretrained('./gemma_2b_model')\ntokenizer.save_pretrained('./gemma_2b_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:34:57.881075Z","iopub.execute_input":"2024-12-08T13:34:57.881441Z","iopub.status.idle":"2024-12-08T13:36:05.802402Z","shell.execute_reply.started":"2024-12-08T13:34:57.881407Z","shell.execute_reply":"2024-12-08T13:36:05.80153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the directory containing model and tokenizer\nshutil.make_archive('/kaggle/working/gemma_2b_model', 'zip', '/kaggle/input/gemma_2b_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:05.803312Z","iopub.execute_input":"2024-12-08T13:36:05.803682Z","iopub.status.idle":"2024-12-08T13:36:05.809249Z","shell.execute_reply.started":"2024-12-08T13:36:05.80366Z","shell.execute_reply":"2024-12-08T13:36:05.808549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport copy\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    BitsAndBytesConfig,\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    Gemma2Config,\n    PreTrainedTokenizerBase, \n    EvalPrediction,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom sklearn.metrics import log_loss, accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-02T20:57:06.256164Z","iopub.execute_input":"2024-12-02T20:57:06.256477Z","iopub.status.idle":"2024-12-02T20:57:24.976035Z","shell.execute_reply.started":"2024-12-02T20:57:06.25644Z","shell.execute_reply":"2024-12-02T20:57:24.975109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    output_dir: str = \"output\"\n    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n    max_length: int = 1024\n    n_splits: int = 5\n    fold_idx: int = 0\n    optim_type: str = \"adamw_8bit\"\n    per_device_train_batch_size: int = 2\n    gradient_accumulation_steps: int = 2  # global batch size is 8 \n    per_device_eval_batch_size: int = 8\n    n_epochs: int = 1\n    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n    lr: float = 2e-4\n    warmup_steps: int = 20\n    lora_r: int = 16\n    lora_alpha: float = lora_r * 2\n    lora_dropout: float = 0.05\n    lora_bias: str = \"none\"\n    \nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2024-12-02T20:57:24.977617Z","iopub.execute_input":"2024-12-02T20:57:24.978291Z","iopub.status.idle":"2024-12-02T20:57:24.984656Z","shell.execute_reply.started":"2024-12-02T20:57:24.97824Z","shell.execute_reply":"2024-12-02T20:57:24.983934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Training Arguments","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"output\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"steps\",\n    save_steps=200,\n    optim=config.optim_type,\n    fp16=True,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T20:57:24.98652Z","iopub.execute_input":"2024-12-02T20:57:24.986744Z","iopub.status.idle":"2024-12-02T20:57:25.073203Z","shell.execute_reply.started":"2024-12-02T20:57:24.986725Z","shell.execute_reply":"2024-12-02T20:57:25.072339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"output\",\n    overwrite_output_dir=True,\n    report_to=\"none\",\n    num_train_epochs=config.n_epochs,\n    per_device_train_batch_size=config.per_device_train_batch_size,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    per_device_eval_batch_size=config.per_device_eval_batch_size,\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"steps\",\n    save_steps=200,\n    optim=config.optim_type,\n    fp16=True,\n    learning_rate=config.lr,\n    warmup_steps=config.warmup_steps,\n    ddp_find_unused_parameters=False,\n    gradient_checkpointing=True,\n    logging_dir=\"./logs\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T20:57:25.074221Z","iopub.execute_input":"2024-12-02T20:57:25.074527Z","iopub.status.idle":"2024-12-02T20:57:25.106423Z","shell.execute_reply.started":"2024-12-02T20:57:25.074505Z","shell.execute_reply":"2024-12-02T20:57:25.105687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### LoRA config","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T20:57:25.107431Z","iopub.execute_input":"2024-12-02T20:57:25.107729Z","iopub.status.idle":"2024-12-02T20:57:25.112233Z","shell.execute_reply.started":"2024-12-02T20:57:25.107699Z","shell.execute_reply":"2024-12-02T20:57:25.111422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Instantiate the tokenizer & model","metadata":{}},{"cell_type":"code","source":"tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\ntokenizer.add_eos_token = True  # We'll add <eos> at the end\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-12-02T20:57:25.113391Z","iopub.execute_input":"2024-12-02T20:57:25.113631Z","iopub.status.idle":"2024-12-02T20:57:27.799592Z","shell.execute_reply.started":"2024-12-02T20:57:25.113612Z","shell.execute_reply":"2024-12-02T20:57:27.79865Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Gemma2ForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    num_labels=2,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-12-02T20:57:27.80081Z","iopub.execute_input":"2024-12-02T20:57:27.801452Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Instantiate the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the parquet file\ndf = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\n\n# Save the dataframe as a CSV file with escape characters\ndf.to_csv('/kaggle/working/train.csv', index=False, escapechar='\\\\')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = Dataset.from_csv(\"/kaggle/working/train.csv\")\nprint(len(ds))\nds = ds.select(torch.arange(200))  # We only use the first 100 data for demo purpose","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(\n        self, \n        tokenizer: PreTrainedTokenizerBase, \n        max_length: int\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __call__(self, batch: dict) -> dict:\n        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n        labels=[]\n        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n            if a_win:\n                label = 0\n            elif b_win:\n                label = 1\n            else:\n                label = 2\n            labels.append(label)\n        return {**tokenized, \"labels\": labels}\n        \n    @staticmethod\n    def process_text(text: str) -> str:\n        return text.replace(\"null\", \"\").strip()\n    # def process_text(text: str) -> str:\n    #     return \" \".join(eval(text, {\"null\": \"\"}))\n\n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTokenizer:\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __call__(self, batch: dict) -> dict:\n        # Ensure that the keys exist in the batch before processing\n        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch.get(\"prompt\", [])]\n        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch.get(\"response_a\", [])]\n        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch.get(\"response_b\", [])]\n        \n        # Concatenate all parts into one text field for tokenization\n        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        \n        # Tokenize the texts\n        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding=True)\n        \n        # Handle the winner labels (mapping winner from 'model_a' to 0, 'model_b' to 1)\n        labels = []\n        winners = batch.get(\"winner\", [])\n        \n        for winner in winners:\n            if winner == 'model_a':\n                label = 0\n            elif winner == 'model_b':\n                label = 1\n            # If the winner is neither 'model_a' nor 'model_b', you could choose to skip or handle the error here\n            else:\n                continue  # Or use `label = None` if you want to handle such cases separately\n                \n            labels.append(label)\n        \n        # Return tokenized output with labels\n        return {**tokenized, \"labels\": labels}\n\n    @staticmethod\n    def process_text(text: str) -> str:\n        return text.replace(\"null\", \"\").strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ds[0])  # Check a single example\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encode = CustomTokenizer(tokenizer, max_length=config.max_length)\nds = ds.map(encode, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compute metrics\n\nWe'll compute the log-loss used in LB and accuracy as a auxiliary metric.","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_preds: EvalPrediction) -> dict:\n    preds = eval_preds.predictions\n    labels = eval_preds.label_ids\n    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n    loss = log_loss(y_true=labels, y_pred=probs)\n    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n    return {\"acc\": acc, \"log_loss\": loss}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import EvalPrediction\n\ndef compute_metrics(eval_pred: EvalPrediction) -> dict:\n    # Extract predictions and labels from the EvalPrediction object\n    logits, labels = eval_pred.predictions, eval_pred.label_ids\n\n    # Convert logits to predicted labels (assuming binary classification with logits)\n    pred_labels = logits.argmax(axis=-1)  # For multi-class, use argmax along the correct axis\n\n    # Calculate accuracy and other metrics\n    accuracy = accuracy_score(labels, pred_labels)\n\n    # Return the metrics as a dictionary\n    return {\n        \"accuracy\": accuracy,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split\n\nHere, train and eval is splitted according to their `id % 5`","metadata":{}},{"cell_type":"code","source":"folds = [\n    (\n        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n    ) \n    for fold_idx in range(config.n_splits)\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_idx, eval_idx = folds[config.fold_idx]\n\ntrainer = Trainer(\n    args=training_args, \n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=ds.select(train_idx),\n    eval_dataset=ds.select(eval_idx),\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}