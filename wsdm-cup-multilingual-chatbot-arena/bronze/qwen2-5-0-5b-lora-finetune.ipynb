{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":182754,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155768,"modelId":178224}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<font color=\"red\">If you like this notebook, please give me a upvote.</font>**","metadata":{}},{"cell_type":"markdown","source":"### Note：\n1. My model was trained locally, so if you intend to train it on Kaggle, please pay attention to the GPU usage time and memory usage.\n2. I directly copied the code to Kaggle, so the file paths in the code need to be modified.\n","metadata":{}},{"cell_type":"markdown","source":"# Training Process","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the `data_transfer` function, which is used to convert data into a specific message format.","metadata":{}},{"cell_type":"code","source":"def data_transfer(dataframe, output_pt, instruction):\n    x_cols = ['prompt', 'response_a', 'response_b']\n    x = dataframe[x_cols]\n    y = dataframe['winner']\n\n    messages = []\n    for idx in range(len(x)):\n        x_line = x.iloc[idx,:]\n\n        prompt = 'PROMPT: ' + x_line['prompt']\n        model_a = 'MODEL_A: ' + x_line['response_a']\n        model_b = 'MODEL_B: ' + x_line['response_b']\n        text = prompt + model_a + model_b\n        target = y[idx]\n\n        message = {\n            'instruction': instruction,\n            'input': text,\n            'output': target\n        }\n\n        messages.append(message)\n    \n    with open(output_pt, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the `process_func` function to tokenize the data in advance.","metadata":{}},{"cell_type":"code","source":"def process_func(example):\n\n    MAX_LENGTH = 5000\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(f\"<|im_start|>system\\n{example['instruction']}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", \n                            add_special_tokens=False)\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\n   \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## load and process data","metadata":{}},{"cell_type":"code","source":"original_pt = \"dataset/train.parquet\"\noriginal = pd.read_parquet(original_pt)\n\ntrain, test = train_test_split(original, train_size=0.99)\ntrain.index = [i for i in range(train.shape[0])]\ntest.index = [i for i in range(test.shape[0])]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pt = \"./cooked-dataset/train.jsonl\"\ntest_pt = \"./cooked-dataset/test.jsonl\"\ninstruction = \"\"\"In the text provided below, PROMPT is the question presented; MODEL_A is the response from the first model; MODEL_B is the response from the second model. Please select the best answer from the two responses above. If the first answer is better, return \"model_a\"; if the second answer is better, return \"model_b\".\"\"\"\n\ndata_transfer(train, train_pt, instruction)\ndata_transfer(test, test_pt, instruction)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## load model","metadata":{}},{"cell_type":"code","source":"model_pt = \"/kaggle/input/qwen_0.5b_instruct/transformers/default/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_pt, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_pt, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_json('cooked-dataset/train.jsonl', lines=True)\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_json('cooked-dataset/test.jsonl', lines=True)\ntest_ds = Dataset.from_pandas(test_df)\ntest_dataset = test_ds.map(process_func, remove_columns=test_ds.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('cooked-dataset/tokenized-train.jsonl', \"w\", encoding=\"utf-8\") as file:\n    for message in train_dataset:\n        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('cooked-dataset/tokenized-test.jsonl', \"w\", encoding=\"utf-8\") as file:\n    for message in test_dataset:\n        file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## simple EDA","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib.pyplot as plt\ntrain_len_list = [len(x['input_ids']) for x in tqdm(train_dataset)]\ntest_len_list = [len(x['input_ids']) for x in tqdm(test_dataset)]\n\nplt.figure(figsize=(16,5))\nplt.hist(train_len_list, bins=50, alpha=0.5, label='train',color='red')\nplt.hist(test_len_list, bins=50, alpha=0.5, label='test',color='blue')\nplt.legend(loc='upper right')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## use LoRA finetune","metadata":{}},{"cell_type":"code","source":"config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_peft_model(model, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## difine training args","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"./output/Qwen1.5\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=1,\n    # per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=10,\n    save_steps=1000,\n    learning_rate=5e-5,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",\n    warmup_steps=1000,\n    logging_dir=\"./output/Qwen1.5/logger\",\n    logging_strategy='steps',\n    disable_tqdm=True,\n    dataloader_num_workers=4,\n    do_train=True,\n    do_eval=False, \n    # eval_steps=1,\n    # eval_on_start=True,\n    # eval_strategy='steps',\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluating","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, AutoTokenizer\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom peft import PeftModel\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpts_dir = \"output\\Qwen1.5\"\nckpts = [os.path.join(ckpts_dir, f) for f in os.listdir(ckpts_dir) if 'checkpoint-' in f]\nckpts = sorted(ckpts)\nqwen_dir = \"Qwen\\Qwen2___5-0___5B-Instruct\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_models(qwen_dir, ckpt):\n    tokenizer = AutoTokenizer.from_pretrained(qwen_dir)\n    model = AutoModelForCausalLM.from_pretrained(qwen_dir)\n    model = PeftModel.from_pretrained(model, model_id=ckpt, config=config)\n    model.eval()\n\n    return tokenizer, model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = pd.read_json('cooked-dataset/test.jsonl', lines=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(tokenizer, test_dataset):\n    processed_list = []\n    ground_truths = []\n    iterator = tqdm(range(test_dataset.shape[0]))\n    for line in iterator:\n        line = test_dataset.iloc[line]\n        msg = [\n            {'role':'system','content':line['instruction']},\n            {'role':'user','content':line['input']}\n        ]\n        text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n        model_inputs = tokenizer([text], return_tensors=\"pt\")\n        processed_list.append(model_inputs)\n        ground_truths.append(line['output'])\n    return processed_list, ground_truths","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer, model =  load_models(qwen_dir, ckpts[0])\ntest_dataset, ground_truths = preprocess(tokenizer, test_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef test_accuracy(model, tokenizer, test_dataset, ground_truths):\n    predictions = []\n    truths = []\n    iterator = tqdm(range(len(test_dataset)))\n    for idx in iterator:\n        model_inputs_1 = test_dataset[idx].to('cuda')\n\n        generated_ids_1 = model.generate(\n            model_inputs_1.input_ids,\n            max_new_tokens=512\n        )\n\n        generated_ids_1 = [\n            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs_1.input_ids, generated_ids_1)\n        ]\n\n        response_1 = tokenizer.batch_decode(generated_ids_1, skip_special_tokens=True)[0]\n\n        predictions.append(response_1)\n\n        truths.append(ground_truths[idx])\n        iterator.set_postfix({\n            'Accuracy': accuracy_score(truths, predictions)\n        })\n    return accuracy_score(truths, predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_list = []\nckpt_names = []\nfor ckpt in ckpts[:-1]: # load ckpt path\n    for i in range(5): # ensemble k models\n        tokenizer, model = load_models(qwen_dir, ckpt) # load model\n        model.cuda()\n        acc = test_accuracy(model, tokenizer, test_dataset, ground_truths) # get test accuracy\n        # save acc\n        metric_list.append(acc)\n        ckpt_names.append(int(ckpt.split('-')[-1]))\n        try:\n            # plot\n            # scatter plot\n            plt.figure(figsize=(20, 5))\n            colors = plt.cm.jet(np.linspace(0, 1, len(ckpt_names)))\n            plt.scatter(ckpt_names, metric_list, marker='o', s=50, alpha=0.6, edgecolors='w')\n            plt.xlabel('Steps')\n            plt.ylabel('Accuracy')\n            plt.title('Accuracy over Steps')\n            plt.grid(True)\n            plt.tight_layout()\n            plt.savefig('./scatter_plot_5shot.png')\n            plt.close()\n            # boxplot\n            df = {}\n            for ckpt_name in ckpt_names:\n                df[ckpt_name] = []\n            for ckpt_name, metric in zip(ckpt_names, metric_list):\n                df[ckpt_name].append(metric)\n            df = pd.DataFrame(df)\n            df.columns = ckpt_names\n            plt.figure(figsize=(20, 6))\n            df.boxplot()\n            plt.title('Boxplot of Metrics')\n            plt.xlabel('Metrics')\n            plt.grid(True)\n            plt.tight_layout()\n            plt.savefig('./box_plot_5shot.png')\n            plt.close()\n        except:\n            continue","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 6))\ndf.boxplot()\nplt.title('Boxplot of Metrics')\nplt.xlabel('Metrics')\nplt.grid(True)\nplt.tight_layout()\nplt.xticks(rotation=45)\nplt.savefig('./box_plot_5shot.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}