{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":7074842,"sourceType":"datasetVersion","datasetId":4074593},{"sourceId":7570020,"sourceType":"datasetVersion","datasetId":4021289},{"sourceId":9341631,"sourceType":"datasetVersion","datasetId":5661348}],"dockerImageVersionId":30776,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on single LightGBM baseline LB **0.447**:\n\nhttps://www.kaggle.com/code/snufkin77/mcts-strength-relevant-baseline\n\nDeepTables: Deep-learning Toolkit for Tabular data\n\nhttps://github.com/DataCanvasIO/DeepTables\n\nhttps://deeptables.readthedocs.io/en/latest/model_config.html#parameters\n\n**Version 1**: single DeepTables NN baseline LB **0.462**.\n\n**Version 6**: single DeepTables NN LB **0.448**: `ModelConfig(apply_gbm_features=True)`.\n\n**Version 7**: single DeepTables NN: `ModelConfig(apply_gbm_features=True)`, `ModelConfig(nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'])`.","metadata":{}},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd, polars as pl\nfrom sklearn.model_selection import GroupKFold\nfrom colorama import Fore, Style\n\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom deeptables.models import DeepTable, ModelConfig\nfrom deeptables.models import deepnets\n\nimport kaggle_evaluation.mcts_inference_server\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow version:',tf.__version__+',',\n      'GPU =',tf.test.is_gpu_available())\nprint('DeepTables version:',dt.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"constant_cols = pd.read_csv('/kaggle/input/um-gps-of-mcts-variants-constant-columns/constant_columns.csv').columns.to_list()\ntarget_col = 'utility_agent1'\ngame_col = 'GameRulesetName'\ngame_rule_cols = ['EnglishRules', 'LudRules']\noutput_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\ndropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\nagent_cols = ['agent1', 'agent2']\n\ndef preprocess_data(df): \n    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n    if CFG.split_agent_features:\n        for col in agent_cols:\n            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n    print(f'Data shape: {df.shape}\\n')\n    return df.to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\nLR_START = 1e-3\nLR_MAX = 1e-3\nLR_MIN = 1e-4\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 2\nEPOCHS = 7\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    train_path = '/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv'\n    split_agent_features = True\n    folds = 6\n    epochs = 7\n    batch_size = 128\n    LR_Scheduler = []  # [LR_Scheduler]\n    optimizer = Adam(learning_rate=1e-3)\n    \n    conf = ModelConfig(auto_imputation=False,\n                       auto_discrete=False,\n                       auto_discard_unique=True,\n                       categorical_columns='auto',\n                       apply_gbm_features=True,\n                       fixed_embedding_dim=True,\n                       embeddings_output_dim=4,\n                       embedding_dropout=0.2,\n                       nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n                       dnn_params={\n                           'hidden_units': ((1024, 0.0, True),\n                                            (512, 0.0, True),\n                                            (256, 0.0, True),\n                                            (128, 0.0, True)),\n                           'dnn_activation': 'relu',\n                       },\n                       stacking_op='concat',\n                       output_use_bias=False,\n                       optimizer=optimizer,\n                       task='regression',\n                       loss='auto',\n                       metrics=[\"RootMeanSquaredError\"],\n                       earlystopping_patience=1,\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_nn(data):\n    cv = GroupKFold(n_splits=CFG.folds)\n    groups = data[game_col]\n    X = data.drop([target_col, game_col], axis=1)\n    y = data[target_col]\n    oof = np.zeros(len(data))\n    models = []\n    \n    for fi, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n        print(\"#\"*25)\n        print(f\"### Fold {fi+1}/{CFG.folds} ...\")\n        print(\"#\"*25)   \n        K.clear_session()\n        model = DeepTable(config=CFG.conf)\n        model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                  validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n                  callbacks=CFG.LR_Scheduler,\n                  batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n        models.append(model)\n        \n        # Avoid some errors\n        with K.name_scope(CFG.optimizer.__class__.__name__):\n            for j, var in enumerate(CFG.optimizer.weights):\n                name = 'variable{}'.format(j)\n                CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n        CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n        \n        oof_preds = model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n        rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n        print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n        if rmse>1.0:\n            print(f'{Fore.GREEN}{Style.BRIGHT}Replace Fold {fi+1} oof_preds values with mean of y_valid in Overall CV rmse calculation.\\n')\n            if fi<CFG.folds: oof[valid_idx] = np.mean(y.iloc[valid_idx])\n            else: oof[valid_idx] += np.mean(y.iloc[valid_idx])\n        else:\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n            \n    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n    plot_model(model.get_model().model)\n    return models\n\ndef infer_nn(data, models):\n    return np.mean([model.predict(data, verbose=1, batch_size=512).flatten() for model in models], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrun_i = 0\ndef predict(test_data, submission):\n    global run_i, models\n    if run_i == 0:\n        train_df = pl.read_csv(CFG.train_path)\n        models = train_nn(preprocess_data(train_df))\n    run_i += 1\n    test_data = preprocess_data(test_data).drop(columns=game_col)\n    return submission.with_columns(pl.Series(target_col, infer_nn(test_data, models)))\n\ninference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n         '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}