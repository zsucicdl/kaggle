{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 50px; font-weight: bold;\">Imports</span>","metadata":{}},{"cell_type":"code","source":"# System operations\nimport gc\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preprocessing\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation API\nimport kaggle_evaluation.mcts_inference_server","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show all available columns\npd.options.display.max_columns = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploratory data analysis\nimport plotly.colors as pc\nimport plotly.express as px\nimport plotly.graph_objects as go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model development\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as mse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 50px; font-weight: bold;\">Configuration</span>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    # Paths to competition data\n    train_path = Path('/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv')\n    test_path = Path('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv')\n    subm_path = Path('/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv')\n    \n    # Feature engineering (FE) arguments\n    batch_size = 16384\n    low_memory = True\n    \n    # Color for EDA and MD\n    color = '#E0BFB8'\n    \n    # Model development (MD) arguments\n    early_stop = 50\n    n_splits = 5\n    \n    # LightGBM parameters\n    lgb_p = {\n        'objective': 'regression',\n        'num_iterations': 400,\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 1.0,\n        'num_leaves': 64,\n        'metric': 'rmse',\n        'device': 'cpu',\n        'max_depth': 4,\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 50px; font-weight: bold;\">Feature Engineering</span>","metadata":{}},{"cell_type":"code","source":"class FE:\n    \n    def __init__(self, batch_size, low_memory):\n        self.batch_size = batch_size # Number of lines to read into the buffer at once\n        self.low_memory = low_memory # Reduce memory pressure\n        \n    def clean_data(self, df):\n        \n        # Define columns to drop\n        drop_cols = [\n            'num_wins_agent1',\n            'num_draws_agent1',\n            'num_losses_agent1',\n        ]\n        \n        # Drop columns\n        for col in drop_cols:\n            if col in df.columns:\n                df = df.drop(col)\n        \n        return df\n    \n    def set_datatypes(self, df):\n        \n        # Define categorical columns\n        cat_cols = [\n            'GameRulesetName',\n            'agent1',\n            'agent2', \n            'Behaviour', \n            'StateRepetition', \n            'Duration',\n            'Complexity',\n            'BoardCoverage',\n            'GameOutcome',\n            'StateEvaluation',\n            'Clarity',\n            'Decisiveness',\n            'Drama',\n            'MoveEvaluation',\n            'StateEvaluationDifference',\n            'BoardSitesOccupied',\n            'BranchingFactor',\n            'DecisionFactor',\n            'MoveDistance',\n            'PieceNumber',\n            'ScoreDifference',\n            'EnglishRules',\n            'LudRules'\n        ]\n        \n        # Define numeric columns\n        num_cols = [col for col in df.columns if col not in cat_cols]\n        \n        # Set datatypes for categorical columns\n        df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in cat_cols if col in df.columns])            \n        \n        # Set datatypes for numeric columns\n        df = df.with_columns([pl.col(col).cast(pl.Float32) for col in num_cols if col in df.columns])\n        \n        return df\n    \n    def extract_cat_cols(self, df):\n        \n        # Define a list of categorical columns\n        cat_cols = []\n        \n        # Find categorical columns\n        for col in df.columns:\n            if df[col].dtype == pl.Categorical:\n                cat_cols.append(col)\n        \n        return cat_cols\n    \n    def extract_cat_cols(self, df):\n        \n        # Define a list of categorical columns\n        cat_cols = []\n        \n        # Find categorical columns\n        for col in df.columns:\n            if df[col].dtype == pl.Categorical:\n                cat_cols.append(col)\n        \n        return cat_cols\n    \n    def display_info(self, df, for_eda):\n\n        # Display information for EDA\n        if for_eda:\n\n            # Display the shape of the DataFrame\n            print(f'Shape: {df.shape}')\n\n            # Display the memory usage of the DataFrame\n            mem = df.memory_usage().sum() / 1024**2\n            print('Memory usage: {:.2f} MB\\n'.format(mem))\n\n            # Display first rows of the DataFrame\n            display(df.head())\n\n        # Display basic information for non-EDA processing\n        else:\n\n            # Display the shape of the DataFrame\n            print(f'Shape: {df.shape}')\n\n            # Display the memory usage of the DataFrame\n            mem = df.estimated_size() / 1024**2\n            print('Memory usage: {:.2f} MB\\n'.format(mem))\n\n    def process_data(self, path, for_eda=True): # Determines whether to convert to pandas for EDA or keep as polars for processing\n\n        # Load data as polars DataFrame and drop the Id column\n        df = pl.read_csv(path, low_memory=self.low_memory, batch_size=self.batch_size)\n\n        # Drop redundant columns\n        df = self.clean_data(df)\n\n        # Set datatypes for each column\n        df = self.set_datatypes(df)\n\n        # Extract categorical columns\n        cat_cols = self.extract_cat_cols(df)\n\n        # Convert Polars to Pandas DataFrame\n        if for_eda:\n            df = df.to_pandas()\n\n        # Show the shape and first few rows of the DataFrame\n        self.display_info(df, for_eda)\n\n        return df, cat_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize class for feature engineering\nfe = FE(CFG.batch_size, CFG.low_memory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and process train data\ntrain_data, _ = fe.process_data(CFG.train_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 50px; font-weight: bold;\">Exploratory Data Analysis</span>","metadata":{}},{"cell_type":"code","source":"class EDA:\n    \n    def __init__(self, df, color):\n        self.df = df  \n        self.color = color  \n\n    def template(self, fig, title):\n        \n        # Set plot background and layout to match the user's theme\n        fig.update_layout(\n            title=title,\n            title_x=0.5, \n            plot_bgcolor='rgba(0,0,0,0)', \n            paper_bgcolor='rgba(0,0,0,0)',  \n            font=dict(color='#7f7f7f'),\n            margin=dict(l=90, r=90, t=90, b=90), \n            height=900  \n        )\n        \n        return fig\n    \n    def target_distribution(self):\n        \n        # Calculate the distribution of the target variable (utility_agent1)\n        target_distribution = self.df['utility_agent1'].value_counts().sort_index()\n\n        # Create a histogram for the target distribution\n        fig = px.histogram(\n            self.df,\n            x='utility_agent1',\n            nbins=50,  # Granularity of the histogram\n            title='Distribution of Agent 1 Utility',  \n            color_discrete_sequence=[self.color]  \n        )\n\n        # Customize the histogram layout\n        fig.update_layout(\n            xaxis_title='Utility of Agent 1',\n            yaxis_title='Count', \n            bargap=0.1  \n        )\n\n        # Customize hover text: round numbers to 3 decimal places, format large numbers with commas\n        fig.update_traces(\n            hovertemplate='Utility: %{x:.3f}<br>Count: %{y:,}'\n        )\n\n        # Apply the template to the histogram\n        fig = self.template(fig, 'Distribution of Agent 1 Utility')\n\n        # Display the histogram\n        fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize class for Exploratory Data Analysis (EDA)\neda = EDA(train_data, CFG.color)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda.target_distribution()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete references to train data\ndel train_data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 50px; font-weight: bold;\">Model Development</span>","metadata":{}},{"cell_type":"code","source":"class MD:\n    \n    def __init__(self, \n                 early_stop, \n                 n_splits,\n                 color,\n                 lgb_p):\n        \n        self.early_stop = early_stop\n        self.n_splits = n_splits\n        self.color = color\n        self.lgb_p = lgb_p\n    \n    def plot_cv(self, fold_scores, model_name):\n        \n        # Round the fold scores to 2 decimal places\n        fold_scores = [round(score, 2) for score in fold_scores]\n        mean_score = round(np.mean(fold_scores), 2)\n        std_score = round(np.std(fold_scores), 2)\n\n        # Create a new figure for plotting\n        fig = go.Figure()\n\n        # Add scatter plot for individual fold scores\n        fig.add_trace(go.Scatter(\n            x = list(range(1, len(fold_scores) + 1)),\n            y = fold_scores,\n            mode = 'markers', \n            name = 'Fold Scores',\n            marker = dict(size = 24, color=self.color, symbol='diamond'), # Diamond shape marker\n            text = [f'{score:.2f}' for score in fold_scores],\n            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n            hoverlabel=dict(font=dict(size=16))  # Adjust the font size here\n        ))\n\n        # Add a horizontal line for the mean score\n        fig.add_trace(go.Scatter(\n            x = [1, len(fold_scores)],\n            y = [mean_score, mean_score],\n            mode = 'lines',\n            name = f'Mean: {mean_score:.2f}',\n            line = dict(dash = 'dash', color = '#FFBF00'), # Colored Amber\n            hoverinfo = 'none'\n        ))\n\n        # Update the layout of the plot\n        fig.update_layout(\n            title = f'{model_name} | Cross-Validation RMSE Scores | Variation of CV scores: {mean_score} ± {std_score}',\n            xaxis_title = 'Fold',\n            yaxis_title = 'RMSE Score',\n            plot_bgcolor = 'rgba(0,0,0,0)',\n            paper_bgcolor = 'rgba(0,0,0,0)',\n            xaxis = dict(\n                gridcolor = 'lightgray',\n                tickmode = 'linear',\n                tick0 = 1,\n                dtick = 1,\n                range = [0.5, len(fold_scores) + 0.5]\n            ),\n            yaxis = dict(gridcolor = 'lightgray')\n        )\n\n        # Display the plot\n        fig.show() \n        \n    def train_lgb(self, data, cat_cols, title):\n        \n        # Convert data for pandas for training\n        data = data.to_pandas()\n        \n        # Extract features columns and label\n        X = data.drop(['utility_agent1'], axis=1)\n        y = data['utility_agent1']\n        \n        # Convert categorical columns to category dtype\n        for col in cat_cols:\n            X[col] = X[col].astype('category')\n        \n        # Initialize cross-validation strategy\n        cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        \n        # Initialize lists to store models, CV scores, and OOF predictions\n        models, scores = [], []\n        oof_preds = np.zeros(len(X))\n        \n        # Perform cross-validation\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n            \n            # Split the data into training and validation sets for the current fold\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n            # Train the model\n            model = lgb.LGBMRegressor(**self.lgb_p)\n            model.fit(X_train, y_train,\n                      eval_set=[(X_valid, y_valid)],\n                      eval_metric='rmse',\n                      callbacks=[lgb.early_stopping(self.early_stop, verbose=0), \n                                 lgb.log_evaluation(0)])\n            \n            # Append the trained model to the list\n            models.append(model)\n            \n            # Make predictions on the validation set\n            oof_preds[valid_index] = model.predict(X_valid)\n            \n            # Calculate and store the RMSE score for the current fold\n            score = mse(y_valid, oof_preds[valid_index], squared=False)\n            scores.append(score)\n        \n        # Plot the cross-validation results\n        self.plot_cv(scores, title)\n        \n        return models, oof_preds\n\n    def infer_lgb(self, data, cat_cols, models):\n        \n        # Convert data for pandas for inference\n        data = data.to_pandas()\n\n        # Convert categorical columns to category dtype\n        for col in cat_cols:\n            data[col] = data[col].astype('category')\n\n        # Return the averaged predictions of LightGBM models\n        return np.mean([model.predict(data) for model in models], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize class for model development\nmd = MD(CFG.early_stop,\n        CFG.n_splits,\n        CFG.color, \n        CFG.lgb_p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 30px; font-weight: bold;\">Define the trainer function</span>","metadata":{}},{"cell_type":"code","source":"def train_model():\n    \n    global cat_cols, lgb_models\n    \n    # Load and process train data - extract categorical columns\n    train, cat_cols = fe.process_data(CFG.train_path, for_eda=False)\n    \n    # Train LightGBM models\n    lgb_models, _ = md.train_lgb(train, cat_cols, 'LightGBM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 30px; font-weight: bold;\">Define the predict call</span>","metadata":{}},{"cell_type":"code","source":"# Initialize a counter to keep track of prediction calls\ncounter = 0\n\n# Define the predict function for the API\ndef predict(test, submission):\n    \n    # Use the global counter variable\n    global counter\n    \n    # If this is the first prediction call, train LightGBM models\n    if counter == 0:\n        \n        # Train LightGBM models\n        train_model()\n        \n    # Increment the counter for each prediction call to avoid re-training\n    counter += 1\n    \n    # Generate test predictions and assign them to the submission DataFrame\n    return submission.with_columns(pl.Series('utility_agent1', md.infer_lgb(test, cat_cols, lgb_models)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color:#E0BFB8; font-size: 30px; font-weight: bold;\">Call the gateway server</span>","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'\n        )\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}