{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport polars as pl\nimport pandas as pd#导入csv文件的库\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nimport kaggle_evaluation.mcts_inference_server\nimport numpy as np#对矩阵进行科学计算的库\nimport re#用于正则表达式提取的库\nimport gc#垃圾回收的库\nimport sklearn\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV  #模型参数调优小参数 \nimport optuna   #模型参数调优多参数 \nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.ensemble\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedGroupKFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.feature_selection import SelectKBest, chi2,f_regression\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom pathlib import Path\ncomp_path = Path('/kaggle/input/um-game-playing-strength-of-mcts-variants')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-10T01:52:58.13037Z","iopub.execute_input":"2024-10-10T01:52:58.131319Z","iopub.status.idle":"2024-10-10T01:52:58.140118Z","shell.execute_reply.started":"2024-10-10T01:52:58.131275Z","shell.execute_reply":"2024-10-10T01:52:58.139058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Read Data**","metadata":{}},{"cell_type":"code","source":"train=pl.read_csv(\"/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv\")\n# train=train.to_pandas()\nprint(f\"len(train):{len(train)}\")\ntest=pl.read_csv(\"/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv\")\n# test=test.to_pandas()\nprint(f\"len(test):{len(test)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:52:58.142562Z","iopub.execute_input":"2024-10-10T01:52:58.143353Z","iopub.status.idle":"2024-10-10T01:53:03.176256Z","shell.execute_reply.started":"2024-10-10T01:52:58.143303Z","shell.execute_reply":"2024-10-10T01:53:03.175225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.177469Z","iopub.execute_input":"2024-10-10T01:53:03.177764Z","iopub.status.idle":"2024-10-10T01:53:03.189327Z","shell.execute_reply.started":"2024-10-10T01:53:03.177733Z","shell.execute_reply":"2024-10-10T01:53:03.188362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 异常数据判断处理","metadata":{}},{"cell_type":"code","source":"# 1.先解决数据质量问题：确保数据干净无误，这是任何机器学习项目的基础。\n# 2.接下来重点放在特征工程上：好的特征往往比复杂的模型更能提高性能。\n# 3.然后进行模型选择与调参：找到最适合你数据的模型，并优化其超参数。\n# 4.如果还有改进空间，可以尝试增加数据量或调整目标变量。","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.190669Z","iopub.execute_input":"2024-10-10T01:53:03.191482Z","iopub.status.idle":"2024-10-10T01:53:03.201034Z","shell.execute_reply.started":"2024-10-10T01:53:03.191434Z","shell.execute_reply":"2024-10-10T01:53:03.200184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 检查每列的缺失值数量\n# missing_values = train.isnull().sum()\n# print(missing_values)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.204169Z","iopub.execute_input":"2024-10-10T01:53:03.204611Z","iopub.status.idle":"2024-10-10T01:53:03.215574Z","shell.execute_reply.started":"2024-10-10T01:53:03.204576Z","shell.execute_reply":"2024-10-10T01:53:03.213843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.216838Z","iopub.execute_input":"2024-10-10T01:53:03.217239Z","iopub.status.idle":"2024-10-10T01:53:03.224486Z","shell.execute_reply.started":"2024-10-10T01:53:03.217197Z","shell.execute_reply":"2024-10-10T01:53:03.22331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target = 'utility_agent1'\n\n\n\n\n# df.describe().T","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.225934Z","iopub.execute_input":"2024-10-10T01:53:03.226329Z","iopub.status.idle":"2024-10-10T01:53:03.233343Z","shell.execute_reply.started":"2024-10-10T01:53:03.226283Z","shell.execute_reply":"2024-10-10T01:53:03.232408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # len(df['GameRulesetName'].unique())\n# # df.head()\n# df= train.to_pandas()\n# df.info()\n# #####删除列只有一个数据的无用列\n# dorp_unique_less_col=[]\n# drop_two_frequency_1_col=[]\n# drop_muti_frequency_1_col=[]\n# str_col=[]\n# one_hot_str_col=[]\n# for col in df.drop(['Id','num_draws_agent1','num_losses_agent1',\n#                     'num_wins_agent1','utility_agent1'],axis=1).columns:\n#     col_unique_count = len(df[col].unique())\n# #     print(\"===col\",col_unique_count)\n#     if col_unique_count == 1:\n# #         print(\"========col==\",df[col].unique())\n#         dorp_unique_less_col.append(col)\n#     else :\n# #          col_value_counts=df[col].value_counts().reset_index()\n#          #模型筛选出的无用特征,出现频率少于1%\n#          # 检查列是否为二分\n#          if set(df[col].unique()) == {0, 1}:\n#             # 计算值为1的频率\n#             freq_1 = df[col].mean()\n#             # 如果频率小于1%，加入删除列表\n#             if freq_1 < 0.01:\n#                 drop_two_frequency_1_col.append(col)\n# #                print(\"========col==\",col,col_value_counts)\n#          else :\n#                # 检查列是否为数值类型\n#                 col_type =df[col].dtype\n#                 if pd.api.types.is_numeric_dtype(col_type):\n#                  # 计算每个值的出现频率\n#                     value_frequencies = df[col].value_counts(normalize=True)\n# #                     print(\"=col==\",col,value_frequencies)\n\n#                     # 检查是否有值的频率小于1%\n#                     if any(freq < 0.01 for freq in value_frequencies):\n#                         drop_muti_frequency_1_col.append(col)\n#                 else:\n#                      col_value_counts=df[col].value_counts().reset_index()\n#                      print(col,len(col_value_counts))\n#                      if len(col_value_counts)>50:\n#                                   str_col.append(col)\n#                      else:\n        \n#                                 one_hot_str_col.append(col)\n\n\n# # print(len(dorp_unique_less_col))\n# # print(len(drop_two_frequency_1_col))\n# # print(len(drop_muti_frequency_1_col))\n# print(str_col)\n# print(\"==one_hot_str_col==\",one_hot_str_col)\n# # train.drop()\n\n                        \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.235022Z","iopub.execute_input":"2024-10-10T01:53:03.235693Z","iopub.status.idle":"2024-10-10T01:53:03.242991Z","shell.execute_reply.started":"2024-10-10T01:53:03.235645Z","shell.execute_reply":"2024-10-10T01:53:03.242118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **数据分析**","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:03.244282Z","iopub.execute_input":"2024-10-10T01:53:03.244865Z","iopub.status.idle":"2024-10-10T01:53:04.782021Z","shell.execute_reply.started":"2024-10-10T01:53:03.24482Z","shell.execute_reply":"2024-10-10T01:53:04.780847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 分类变量的性质：\n# 有序变量：使用 LabelEncoder 或 OrdinalEncoder。\n# 无序变量：使用 OneHotEncoder。\n# 数据类型：\n# 文本数据：使用 TfidfVectorizer 或 CountVectorizer。\n# 分类标签：使用 LabelEncoder、OrdinalEncoder 或 OneHotEncoder。\n# 模型需求：\n# 线性模型：通常需要独热编码（OneHotEncoder）来避免顺序关系的误导。\n# 树模型：可以使用 LabelEncoder 或 OrdinalEncoder，因为树模型不会受到顺序关系的影响。\n# 数据规模：\n# 大规模数据：使用 TfidfVectorizer 或 CountVectorizer 生成稀疏矩阵，节省内存。\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.783454Z","iopub.execute_input":"2024-10-10T01:53:04.783855Z","iopub.status.idle":"2024-10-10T01:53:04.789132Z","shell.execute_reply.started":"2024-10-10T01:53:04.783819Z","shell.execute_reply":"2024-10-10T01:53:04.787986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据处理","metadata":{}},{"cell_type":"code","source":"class Preprocessor():\n    def __init__(self,seed=2024,target='utility_agent1',train=None,num_folds=3,\n                 CV_LB_path=\"/kaggle/input/mcts-eda-about-cv-and-lb/0917CV_LB.csv\"):\n        self.seed=seed\n        self.target=target\n        self.train=train\n        self.models=[]#训练和推理的模型\n        self.tfidfs=[]#字符串的tfidf模型\n        self.num_folds=num_folds\n        self.obj_cols =[]  ##字符列\n        self.enc=None\n        self.dorp_unique_less_col=[]\n        self.drop_two_frequency_1_col=[]\n        self.drop_muti_frequency_1_col=[]\n        self.one_hot_str_col=[]  ##用独热编码的字端\n        self.onehot_encoder =None ###one-hot编码\n        self.tfidfvectorizer_dict ={}    ### 词向量\n        self.tfidfvectorizer_select_col_dict={}\n        self.scaler = StandardScaler()\n    \n        #遍历表格df的所有列修改数据类型减少内存使用\n    def reduce_mem_usage(self,df, float16_as32=True):\n        #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n        for col in df.columns:#遍历每列的列名\n            col_type = df[col].dtype#列名的type\n            if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n                c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n                if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n                    #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:#如果是浮点数类型.\n                    #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        if float16_as32:#如果数据需要更高的精度可以选择float32\n                            df[col] = df[col].astype(np.float32)\n                        else:\n                            df[col] = df[col].astype(np.float16)  \n                    #如果数值在float32的取值范围内，对它进行类型转换\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    #如果数值在float64的取值范围内，对它进行类型转换\n                    else:\n                        df[col] = df[col].astype(np.float64)\n        #计算一下结束后的内存\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        #相比一开始的内存减少了百分之多少\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n        return df\n        \n        \n        #清理df的字符串的列\n    def clean(self,df,col):\n        #字符串缺失值填充\n        df[col]=df[col].fillna(\"nan\")\n        #字符串转换成小写\n        df[col]=df[col].apply(lambda x:x.lower())\n        #考虑到这种字符串 ‘MCTS-UCB1-0.6-NST-false‘\n        ps='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n        for p in ps:\n            df[col]=df[col].apply(lambda x:x.replace(p,' '))\n        return df\n    \n#         self.str_col=[]\n        #检查CV和LB情况的CV,LB统计表\n#         self.check=pd.read_csv(CV_LB_path)\n    def make_trian_pro(self,df):\n        for col in df.drop(['Id','GameRulesetName','num_draws_agent1','num_losses_agent1',\n                            'num_wins_agent1','utility_agent1'],axis=1).columns:\n            col_unique_count = len(df[col].unique())\n            if col_unique_count == 1:   ##//只有一列值 删除\n                self.dorp_unique_less_col.append(col)\n            else :\n        #          col_value_counts=df[col].value_counts().reset_index()\n                 #模型筛选出的无用特征,出现频率少于1%\n                 # 检查列是否为二分\n                 if set(df[col].unique()) == {0, 1}:\n                    # 计算值为1的频率\n                    freq_1 = df[col].mean()\n                    # 如果频率小于1%，加入删除列表\n                    if freq_1 < 0.01:\n                         self.drop_two_frequency_1_col.append(col)\n        #                print(\"========col==\",col,col_value_counts)\n                 else :\n                       # 检查列是否为数值类型\n                        col_type =df[col].dtype\n                        if pd.api.types.is_numeric_dtype(col_type):\n                         # 计算每个值的出现频率\n                            value_frequencies = df[col].value_counts(normalize=True)\n        #                     print(\"=col==\",col,value_frequencies)\n\n                            # 检查是否有值的频率小于1%\n                            if any(freq < 0.01 for freq in value_frequencies):\n                                 self.drop_muti_frequency_1_col.append(col)\n                        else:\n                              col_value_counts=df[col].value_counts().reset_index()\n#                               print(col_value_counts)\n                              if len(col_value_counts)>20:\n                                  self.obj_cols.append(col)\n                              else:\n                                  self.one_hot_str_col.append(col)\n\n    \n    def data_deal(self,train,mode='trian'):\n        print(\"agent1 agent2 feature\")\n        cols=['selection','exploration_const','playout','score_bounds']\n        for i in range(len(cols)):\n            for j in range(2):\n                train[f'{cols[i]}{j+1}']=train[f'agent{j+1}'].apply(lambda x:x.split('-')[i+1])\n          #即:rule去掉'(game \"',然后找第一个双引号,并用双引号后面的内容在前面加上'('作为新的rule.\n        def drop_gamename(rule):\n            if rule is None:\n                return None  # 或者返回一个默认值，例如空字符串 ''\n            rule=rule[len('(game \"'):]\n            for i in range(len(rule)):\n                if rule[i]=='\"':\n                    return rule[i+1:]\n        train['LudRules']=train['LudRules'].apply(lambda x:drop_gamename(x))\n        #########\n       \n        \n        \n         #删除\n        train = train.drop(['agent1','agent2'],axis=1)\n        ########\n        \n        if mode ==\"train\":\n            self.make_trian_pro(train)\n            cols_to_drop = [\"Id\",'num_draws_agent1', 'num_losses_agent1', 'num_wins_agent1']\n            train = train.drop(cols_to_drop,axis=1)\n            train = train.drop(self.dorp_unique_less_col,axis=1)\n            train = train.drop(self.drop_two_frequency_1_col,axis=1)\n#             train = train.drop(self.drop_muti_frequency_1_col)\n#             obj_cols =  train.select(pl.col(pl.String)).columns\n            obj_cols = self.obj_cols\n            print(\"=============obj_colslen============\",len(self.obj_cols),obj_cols)\n            if len(self.one_hot_str_col)>0:\n               onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n               self.onehot_encoder =onehot_encoder\n               train_encoded = pd.DataFrame(onehot_encoder.fit_transform(train[self.one_hot_str_col])\n                                            , columns=onehot_encoder.get_feature_names_out(),\n                                         index=train.index)\n#                print(\"==============onehot_encoder============\",train_encoded)\n               train = pd.concat([train, train_encoded ], axis=1)\n               train = train.drop(self.one_hot_str_col,axis=1)\n            \n\n                \n            ##换成词开里\n            if len(obj_cols) >0 :\n                for one_col in obj_cols:\n                        tfidf_matrices = []\n\n                        tfidfvectorizer = TfidfVectorizer(max_features=600)\n                        vector_train = tfidfvectorizer.fit_transform(train[one_col])\n                        feature_names = ['_'.join([one_col, name]) for name in tfidfvectorizer.get_feature_names_out()]\n\n                        # 将 TF-IDF 向量转换为 DataFrame\n                        vector_train_df = pd.DataFrame.sparse.from_spmatrix(vector_train, columns=feature_names, index=train.index)\n\n                        # 存储 TF-IDF 矩阵和特征名称\n                        tfidf_matrices.append(vector_train)\n\n\n                        # 合并所有文本列的 TF-IDF 向量\n                        combined_tfidf_matrix = hstack(tfidf_matrices).tocsr()\n\n                        # 特征选择\n                        selector = SelectKBest(f_regression, k=50)\n                        selected_features = selector.fit_transform(combined_tfidf_matrix, train[self.target])\n\n                        # 获取选中的特征名称\n                        selected_feature_indices = selector.get_support(indices=True)\n                        selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n\n                        # 将选中的特征转换为 DataFrame\n                        selected_features_df = pd.DataFrame.sparse.from_spmatrix(selected_features, columns=selected_feature_names, index=train.index)\n\n                        # 将选中的特征与原始 DataFrame 合并\n                        train = pd.concat([train, selected_features_df], axis=1)\n\n                        # 存储 TfidfVectorizer 和选中的特征名称\n                        self.tfidfvectorizer_dict[one_col] = tfidfvectorizer\n                        self.tfidfvectorizer_select_col_dict[one_col] = selected_feature_names\n        #                     tfidfvectorizer = TfidfVectorizer(max_features=600)\n#                     tfidf_matrices = []\n#                     all_feature_names = []\n#                     vector_train = tfidfvectorizer.fit_transform(train[one_col])\n#                     feature_names = ['_'.join([one_col, name]) for name in \n#                                      tfidfvectorizer.get_feature_names_out()]\n#                     vector_train = pd.DataFrame.sparse.from_spmatrix(vector_train, columns=feature_names, \n#                                                                      index=train.index)\n# #                     train = pd.concat([train, vector_train], axis=1)\n#                     # 存储 TF-IDF 矩阵和特征名称\n#                     tfidf_matrices.append(vector_train)\n#                     all_feature_names.extend(feature_names)\n#                     # 合并所有文本列的 TF-IDF 向量\n#                     combined_tfidf_matrix = hstack(tfidf_matrices).tocsr()\n# #                     combined_tfidf_matrix=tfidf_matrices\n#                     # 特征选择\n#                     selector = SelectKBest(chi2, k=50)\n#                     selected_features = selector.fit_transform(combined_tfidf_matrix, train[self.target])\n\n#                     # 获取选中的特征名称\n#                     selected_feature_indices = selector.get_support(indices=True)\n#                     selected_feature_names = [all_feature_names[i] for i in selected_feature_indices]\n\n#                     # 将选中的特征转换为 DataFrame\n#                     selected_features_df = pd.DataFrame.sparse.from_spmatrix(selected_features, \n#                                                                              columns=selected_feature_names,\n#                                                                              index=vector_train.index)\n#                     train = train.drop(feature_names,axis=1)\n\n#                     # 将选中的特征与原始 DataFrame 合并\n#                     df = pd.concat([train, selected_features_df], axis=1)\n\n#                     self.tfidfvectorizer_dict[one_col] =tfidfvectorizer\n#                     self.tfidfvectorizer_select_col_dict[one_col] = selected_feature_names\n                \n                train = train.drop(obj_cols,axis=1)\n                    \n                    \n                    \n#             enc = OrdinalEncoder(\n#                 handle_unknown='use_encoded_value', unknown_value=-999, encoded_missing_value=-9999)\n#             enc.fit(train[obj_cols])\n#             train_transformed = enc.transform(train[obj_cols])\n#             for e, c in enumerate(obj_cols):\n# #                 train = train.with_columns(pl.Series(c, train_transformed[:, e]))\n#                   train[c] = train_transformed[:, e]\n\n            self.obj_cols = obj_cols\n#             self.enc = enc\n#             return train\n        else :\n            obj_cols = self.obj_cols\n            train = train.drop(self.dorp_unique_less_col,axis=1)\n            train = train.drop(self.drop_two_frequency_1_col,axis=1)\n            if len(self.one_hot_str_col)>0:\n               onehot_encoder=self.onehot_encoder \n               train_encoded = pd.DataFrame(onehot_encoder.transform(train[self.one_hot_str_col])\n                                            , columns=onehot_encoder.get_feature_names_out(),\n                                         index=train.index)\n               train = pd.concat([train, train_encoded ], axis=1)\n               train = train.drop(self.one_hot_str_col,axis=1)\n\n#             test =train\n#             test_transformed = self.enc.transform(test[obj_cols])\n#             for e, c in enumerate(obj_cols):\n# #                 test = test.with_columns(pl.Series(c, test_transformed[:, e]))\n#                  test[c] = test_transformed[:, e]\n            #换成词开里\n            if len(obj_cols) >0 :\n                for one_col in obj_cols:\n                    tfidfvectorizer = self.tfidfvectorizer_dict[one_col]\n                    vector_train = tfidfvectorizer.transform(train[one_col])\n                    feature_names = ['_'.join([one_col, name]) for name in \n                                     tfidfvectorizer.get_feature_names_out()]\n                    vector_train = pd.DataFrame.sparse.from_spmatrix(vector_train, columns=feature_names, \n                                                                     index=train.index)\n                    selected_feature_names = self.tfidfvectorizer_select_col_dict[one_col] \n                    train = pd.concat([train, vector_train[selected_feature_names]], axis=1)\n      \n                train = train.drop(obj_cols,axis=1)\n#             return train\n        train=self.reduce_mem_usage(train)\n        return train\n      \n        \n        \n    def RMSE(self,y_true,y_pred):\n        return np.sqrt(np.mean((y_true-y_pred)**2))\n    def make_model(self):\n        seed = self.seed\n        \n        \n        \n#         cat_model = CatBoostRegressor(\n#         iterations=1500, learning_rate=0.04, depth=9, l2_leaf_reg=0.0005,\n#         min_data_in_leaf=25, bagging_temperature=0.04,\n#         grow_policy='SymmetricTree', verbose=0,\n#             random_state = seed\n#         )\n#     {'iterations': 1480, 'learning_rate': 0.09333122834440728, 'depth': 8, \n#      'l2_leaf_reg': 0.0021736125373129456, 'min_data_in_leaf': 42,\n#      'bagging_temperature': 0.7135945403756551, 'grow_policy': 'SymmetricTree'}.    \n    \n    \n        cat_model = CatBoostRegressor(\n        iterations=1480, learning_rate= 0.09333122834440728, depth=8, l2_leaf_reg=0.0021736125373129456,\n        min_data_in_leaf=42, bagging_temperature= 0.7135945403756551,\n        grow_policy='SymmetricTree', verbose=0,\n            random_state = seed\n        )\n        \n#         {'learning_rate': 0.0304874420014571, 'num_leaves': 176,\n#          'n_estimators': 1724, 'min_child_samples': 12, \n#          'subsample': 0.8996187429841459, 'colsample_bytree': 0.8067548253988669, \n#          'reg_alpha': 0.06388102700399659, 'reg_lambda': 0.07426015085860142, \n#          'scale_pos_weight': 1.0353953519559222, 'min_split_gain': 0.00010745256855372691, \n#          'min_child_weight': 0.0072034568787835975}\n\n#         lgbm_model = LGBMRegressor(\n#             learning_rate=0.02, num_leaves=180, n_estimators=2000, min_child_samples=50,\n#             subsample=0.85, colsample_bytree=0.9, reg_alpha=0.02, reg_lambda=0.001,\n#             scale_pos_weight=1.2, min_split_gain=1e-4, min_child_weight=0.005,\n#             verbose=-1,seed = seed\n#               )\n\n        lgbm_model = LGBMRegressor(\n                learning_rate=0.0304874420014571, num_leaves=176, n_estimators=1724,\n                min_child_samples=12,\n                subsample=0.8996187429841459, colsample_bytree=0.8067548253988669,\n                reg_alpha=0.06388102700399659, reg_lambda=0.07426015085860142,\n                scale_pos_weight=1.0353953519559222, min_split_gain=0.00010745256855372691\n                , min_child_weight=0.0072034568787835975,\n                verbose=-1,seed = seed\n                  )\n          ##模型权重不要\n        voting_regressor = sklearn.ensemble.VotingRegressor(\n            estimators=[('CAT', cat_model), ('Light', lgbm_model)],\n            weights=[0.588196, 0.411804]\n               )\n#         voting_regressor = sklearn.ensemble.VotingRegressor(\n#             estimators=[('CAT', cat_model), ('Light', lgbm_model)]\n#                )\n#         return lgbm_model ##只用lgbm_model测试\n#         return cat_model ##只用cat_model测试\n        return cat_model\n        \n#         lgbm_model = LGBMRegressor(verbose = -100, verbose_eval = -1,              \n#                          n_estimators = 750,\n#                          learning_rate = 0.1,\n#                         reg_alpha = 0.05,\n#                         reg_lambda = 0.05,\n#                         num_leaves = 64,\n#                                  seed = seed\n#                                 )\n\n\n#         lgbm_model2 = LGBMRegressor(verbose = -100, verbose_eval = -1,              \n#                          n_estimators = 750,\n#                          learning_rate = 0.05,\n#                         reg_alpha = 0.1,\n#                         reg_lambda = 0.1,\n#                         num_leaves = 19,\n#                                   seed = seed\n#                                 )\n\n#         hist_model = sklearn.ensemble.HistGradientBoostingRegressor(max_iter = 500,\n#                          learning_rate = 0.1,\n#                         max_leaf_nodes = 15,\n#                         max_depth = None,\n#         categorical_features=None,\n#                        random_state = seed)\n\n#         hist_model2 = sklearn.ensemble.HistGradientBoostingRegressor(max_iter = 500,\n#                          learning_rate = 0.1,\n#                         max_depth = None,\n#         categorical_features=None,\n#                                 random_state = seed)\n\n#         return sklearn.ensemble.VotingRegressor([(\"lgbm\", lgbm_model), (\"hist\", hist_model), \n#                                                  (\"lgbm2\", lgbm_model2), (\"hist2\", hist_model2)])\n\n    def get_trian_data(self,):\n        self.train=self.data_deal(self.train,mode='train')\n        return self.train\n        \n    \n    def train_model(self,):\n        self.train=self.data_deal(self.train,mode='train')\n#         object_columns = self.train.select_dtypes(include=['object']).columns.tolist()\n#         print( self.train.dtypes)\n#         print(self.train.head)\n#         return;\n\n        model = self.make_model()\n        X=self.train.drop([self.target,'GameRulesetName'],axis=1)\n        X_scaled= self.scaler.fit_transform(X)\n        # 将 numpy 数组转换回 pandas DataFrame\n        X = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n        \n        GameRulesetName=self.train['GameRulesetName']\n        y=self.train[self.target]\n    \n#         X=X.to_pandas()\n#         y=y.to_pandas()\n#         X=self.train.drop([],axis=1)\n#         sgkf = KFold(n_splits=self.num_folds,random_state=2024,shuffle=True)\n        sgkf = GroupKFold(n_splits=self.num_folds)\n#         split_list = list(gkf.split(df_train, groups=df_train[groups_col]))\n\n#         for fold, (train_index, valid_index) in (enumerate(sgkf.split(X,y))):\n#         print(\"=======XTYPE========\",type(X))\n        for fold, (train_index, valid_index) in (enumerate(sgkf.split(X,y,GameRulesetName))):\n                print(f\"fold:{fold}\")\n                X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n                y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n#                 print(\"========valid_index====\",train_index,valid_index)\n                model.fit(X_train, y_train)\n                y_val_pred = model.predict(X_valid)\n                print(f\"RMSE:{self.RMSE(y_valid,y_val_pred)}\")\n                self.models.append(model)\n                del X_train,X_valid,y_train,y_valid\n                gc.collect()\n        \n        \n    def predict(self,test):\n        test=self.data_deal(test,mode='test')\n        test=test.drop([\"Id\",'GameRulesetName'],axis=1)\n        X_scaled = self.scaler.transform(test)\n         # 将 numpy 数组转换回 pandas DataFrame\n        test = pd.DataFrame(X_scaled, index=test.index, columns=test.columns)\n#         test=test.drop(['GameRulesetName'],axis=1)\n#         test=test.to_pandas()\n        if len(self.models):#如果有模型\n            return np.mean([np.clip(model.predict(test),-1,1) for model in self.models],axis=0)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.790737Z","iopub.execute_input":"2024-10-10T01:53:04.791114Z","iopub.status.idle":"2024-10-10T01:53:04.845417Z","shell.execute_reply.started":"2024-10-10T01:53:04.791058Z","shell.execute_reply":"2024-10-10T01:53:04.844158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor = Preprocessor(train=train.to_pandas())\n# traindata =preprocessor.data_deal(train=train.to_pandas(),mode='train')\n# preprocessor.train_model()\n# preprocessor.predict(test.to_pandas())\n# print(preprocessor.one_hot_str_col)\n# print(preprocessor.obj_cols)\n# print(preprocessor.preprocessor)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.847016Z","iopub.execute_input":"2024-10-10T01:53:04.847413Z","iopub.status.idle":"2024-10-10T01:53:04.859012Z","shell.execute_reply.started":"2024-10-10T01:53:04.847376Z","shell.execute_reply":"2024-10-10T01:53:04.857926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor = Preprocessor(train=train.to_pandas())\n# train_data = preprocessor.get_trian_data()\n# print(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.863289Z","iopub.execute_input":"2024-10-10T01:53:04.864016Z","iopub.status.idle":"2024-10-10T01:53:04.87301Z","shell.execute_reply.started":"2024-10-10T01:53:04.863968Z","shell.execute_reply":"2024-10-10T01:53:04.871968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# X=train_data.drop([preprocessor.target,\"GameRulesetName\"],axis=1)\n# X_train= preprocessor.scaler.fit_transform(X)\n# #         GameRulesetName=self.train['GameRulesetName']\n# y_train=train_data[preprocessor.target]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.878198Z","iopub.execute_input":"2024-10-10T01:53:04.879409Z","iopub.status.idle":"2024-10-10T01:53:04.885115Z","shell.execute_reply.started":"2024-10-10T01:53:04.879355Z","shell.execute_reply":"2024-10-10T01:53:04.883802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessor = Preprocessor(train=train.to_pandas())\n# preprocessor.train_model()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.886976Z","iopub.execute_input":"2024-10-10T01:53:04.887647Z","iopub.status.idle":"2024-10-10T01:53:04.899486Z","shell.execute_reply.started":"2024-10-10T01:53:04.8876Z","shell.execute_reply":"2024-10-10T01:53:04.898383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GridSearchCV 选择模型参数 ","metadata":{}},{"cell_type":"code","source":"\n\n# #  lgbm_model = LGBMRegressor(\n# #             learning_rate=0.02, num_leaves=180, n_estimators=2000, min_child_samples=50,\n# #             subsample=0.85, colsample_bytree=0.9, reg_alpha=0.02, reg_lambda=0.001,\n# #             scale_pos_weight=1.2, min_split_gain=1e-4, min_child_weight=0.005,\n# #             verbose=-1,seed = seed\n# #               )\n\n# ##############模型参数选择：3*4*4*3*4*3*4*3*3*3*3 次数太多\n# print(\"================start===============\")\n# param_grid_lgbm = {\n#     'learning_rate': [0.01, 0.02, 0.04],\n#     'num_leaves': [64, 128,180, 256],\n#     'n_estimators': [500, 1000, 1500, 2000],\n#     'min_child_samples': [10, 20, 50],\n#     'subsample': [0.8,0.85, 0.9, 1.0],\n#     'colsample_bytree': [0.8, 0.9, 1.0],\n#     'reg_alpha': [0.01,0.02, 0.05, 0.1],\n#     'reg_lambda': [0.01, 0.05, 0.1],\n#     'scale_pos_weight': [1.0, 1.2, 1.5],\n#     'min_split_gain': [0.0, 1e-4, 1e-3],\n#     'min_child_weight': [0.001, 0.005, 0.01],\n#     'verbose': [-1],\n#     'random_state': [preprocessor.seed]\n# }\n\n\n# grid_search_lgbm = GridSearchCV(estimator=LGBMRegressor(), param_grid=param_grid_lgbm, cv=5, \n#                                 scoring='neg_root_mean_squared_error', n_jobs=-1)\n# grid_search_lgbm.fit(X_train, y_train)\n\n# print(f\"LGBM Best Parameters: {grid_search_lgbm.best_params_}\")\n# print(f\"LGBM Best Score: {-grid_search_lgbm.best_score_:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.901366Z","iopub.execute_input":"2024-10-10T01:53:04.901783Z","iopub.status.idle":"2024-10-10T01:53:04.908293Z","shell.execute_reply.started":"2024-10-10T01:53:04.901739Z","shell.execute_reply":"2024-10-10T01:53:04.907096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用optuna选择模型参数","metadata":{}},{"cell_type":"code","source":"# #######33用optu最优。\n# from sklearn.model_selection import cross_val_score\n# cat_model = CatBoostRegressor(\n#         iterations=1500, learning_rate=0.04, depth=9, l2_leaf_reg=0.0005,\n#         min_data_in_leaf=25, bagging_temperature=0.04,\n#         grow_policy='SymmetricTree', verbose=0,\n#             random_state =  preprocessor.seed,\n#         )\n\n# scores = cross_val_score(cat_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n# mean_score = np.mean(scores)\n# print(\"================scores===============\",mean_score)\n# ================scores=============== -0.4600240067815641\n# # import optuna\n# # from lightgbm import LGBMRegressor\n# from sklearn.model_selection import cross_val_score\n# print(\"================start===============\")\n# def objective(trial):\n#     params = {\n#         'learning_rate': trial.suggest_float('learning_rate',0.01, 0.04),\n#         'num_leaves': trial.suggest_int('num_leaves', 64,  256),\n#         'n_estimators': trial.suggest_int('n_estimators', 500,  2000),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n#         'subsample': trial.suggest_float('subsample',0.8, 1.0),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree',0.8,  1.0),\n#         'reg_alpha': trial.suggest_float('reg_alpha',0.01,0.1),\n#         'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 0.1),\n#         'scale_pos_weight': trial.suggest_float('scale_pos_weight',1.0,  1.5),\n#         'min_split_gain': trial.suggest_float('min_split_gain', 0.0,  1e-3),\n#         'min_child_weight': trial.suggest_float('min_child_weight', 0.001,  0.01),\n#         'verbose': -1,\n#         'random_state': preprocessor.seed,\n# #          'device': 'gpu',  # 使用 GPU\n# #         'gpu_platform_id': 0,  # GPU 平台 ID，通常为 0\n# #         'gpu_device_id': 0,  # GPU 设备 ID，通常为 0\n#     }\n    \n#     model = LGBMRegressor(**params, device='gpu')\n#     scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n#     return scores.mean()\n\n# # 创建 Optuna 研究\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=100)\n\n# # 输出最佳参数和最佳得分\n# print(f\"LGBM Best Parameters: {study.best_params}\")\n# print(f\"LGBM Best Score: {-study.best_value:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.909826Z","iopub.execute_input":"2024-10-10T01:53:04.910548Z","iopub.status.idle":"2024-10-10T01:53:04.922297Z","shell.execute_reply.started":"2024-10-10T01:53:04.910502Z","shell.execute_reply":"2024-10-10T01:53:04.921313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##查看模型重要特征\n# 训练模型\n# cat_model.fit(X_train, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.923609Z","iopub.execute_input":"2024-10-10T01:53:04.924Z","iopub.status.idle":"2024-10-10T01:53:04.935704Z","shell.execute_reply.started":"2024-10-10T01:53:04.923958Z","shell.execute_reply":"2024-10-10T01:53:04.934742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.937301Z","iopub.execute_input":"2024-10-10T01:53:04.937686Z","iopub.status.idle":"2024-10-10T01:53:04.946044Z","shell.execute_reply.started":"2024-10-10T01:53:04.937644Z","shell.execute_reply":"2024-10-10T01:53:04.945059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\n# # 获取特征重要性\n# feature_importances = cat_model.feature_importances_\n\n# # 将特征重要性转换为 DataFrame\n# feature_importance_df = pd.DataFrame({\n#     'Feature': X.columns,\n#     'Importance': feature_importances\n# })\n\n# # 按重要性排序\n# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# # 设置图形大小\n# fig, axs = plt.subplots(1, 1, figsize=(10, 100))\n\n# # 设置调色板\n# palette = sns.color_palette(\"RdYlGn_r\", len(feature_importance_df))\n\n# # 绘制特征重要性的条形图\n# sns.barplot(y='Feature', x='Importance', data=feature_importance_df, orient='h', palette=palette)\n\n# # 调整布局\n# plt.tight_layout()\n\n# # 显示图形\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.949438Z","iopub.execute_input":"2024-10-10T01:53:04.949751Z","iopub.status.idle":"2024-10-10T01:53:04.95699Z","shell.execute_reply.started":"2024-10-10T01:53:04.949718Z","shell.execute_reply":"2024-10-10T01:53:04.955984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split, cross_val_score\n\n# # cat_model = CatBoostRegressor(\n# #         iterations=1500, learning_rate=0.04, depth=9, l2_leaf_reg=0.0005,\n# #         min_data_in_leaf=25, bagging_temperature=0.04,\n# #         grow_policy='SymmetricTree', verbose=0,task_type='GPU',\n# #             random_state = seed\n# #         )\n\n# def objective(trial):\n#     # 定义要优化的超参数范围\n#     params = {\n#         'iterations': trial.suggest_int('iterations', 500, 2000),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n#         'depth': trial.suggest_int('depth', 4, 10),\n#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 0.1, log=True),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n#         'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 1.0, log=True),\n#         'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree']),\n#         'random_state': preprocessor.seed,\n    \n#         'verbose': 0\n#     }\n\n#    # 创建模型\n#     model = CatBoostRegressor(**params)\n#     # 使用交叉验证评估模型\n#     scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n#     mean_score = np.mean(scores)\n\n#     retu\n# # 创建 Optuna 研究对象\n# study = optuna.create_study(direction='maximize')  # 注意这里的方向是 maximize\n\n# # 运行优化\n# study.optimize(objective, n_trials=100)\n\n# # 输出最佳参数\n# print(\"Best parameters:\", study.best_params)\n# print(\"Best score:\", -study.best_value)  # 转换回正的均方误差","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.95854Z","iopub.execute_input":"2024-10-10T01:53:04.959338Z","iopub.status.idle":"2024-10-10T01:53:04.969139Z","shell.execute_reply.started":"2024-10-10T01:53:04.95929Z","shell.execute_reply":"2024-10-10T01:53:04.968093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\npreprocessor = Preprocessor(train=train.to_pandas())\ndef predict(test, submission):\n    global counter\n    if counter == 0:\n        # Perform any additional slow steps in the first call to `predict`\n        preprocessor.train_model()\n    counter += 1    \n   \n    return submission.with_columns(pl.Series('utility_agent1', preprocessor.predict(test.to_pandas())))","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:04.970606Z","iopub.execute_input":"2024-10-10T01:53:04.971202Z","iopub.status.idle":"2024-10-10T01:53:08.672265Z","shell.execute_reply.started":"2024-10-10T01:53:04.971155Z","shell.execute_reply":"2024-10-10T01:53:08.671334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 提交结果 ","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-10-10T01:53:08.673355Z","iopub.execute_input":"2024-10-10T01:53:08.673659Z"},"trusted":true},"execution_count":null,"outputs":[]}]}