{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":7074842,"sourceType":"datasetVersion","datasetId":4074593},{"sourceId":7570020,"sourceType":"datasetVersion","datasetId":4021289},{"sourceId":9341631,"sourceType":"datasetVersion","datasetId":5661348}],"dockerImageVersionId":30776,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on single LightGBM baseline with minimal FE, LB **0.447**:\n\nhttps://www.kaggle.com/code/snufkin77/mcts-strength-relevant-baseline\n\nDeepTables: Deep-learning Toolkit for Tabular data\n\nhttps://github.com/DataCanvasIO/DeepTables\n\nhttps://deeptables.readthedocs.io/en/latest/model_config.html#parameters\n\n**Version 1**: single DeepTables NN baseline, LB **0.462**.\n\n**Version 6**: single DeepTables NN, LB **0.448**; `ModelConfig(apply_gbm_features=True)`.\n\n**Version 7**: single DeepTables NN, LB **0.438**; `ModelConfig(apply_gbm_features=True)`, `ModelConfig(nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'])`.\n\n**Version 8**: same as Version 7 + scaling all numerical features to fix issue with divergence in the validation scores. Now overall CV rmse (0.4324) is closer to LB **0.435**. Note that in this version, the `fit_transform` method of scaler was performed on train and test data combined, which is not a good practice, although it does not violate any Kaggle rules.\n\n**Version 9**: same as Version 8, but using scaler with `fit_transform` on the training data alone and then applying `transform` to the test data. CV 0.4319 | LB **0.438**.\n\n**Version 10**: scaling way from Version 9, enable `LearningRateScheduler` with warmup `(LR_START = 1e-4)` on first epoch. CV 0.4343 | LB **0.434**.\n\n**Version 11**: single CatBoost baseline. CV 0.4160 | LB 0.434.\n\n**Version 12**: ensemble NN (CV 0.4343 | LB 0.434) + CatBoost (CV 0.4160 | LB 0.434), `ens_weights = {'nn': 0.50, 'ctb': 0.50}`.","metadata":{}},{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd, polars as pl\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom colorama import Fore, Style\n\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom deeptables.models import DeepTable, ModelConfig\nfrom deeptables.models import deepnets\nfrom catboost import CatBoostRegressor\n\nimport kaggle_evaluation.mcts_inference_server\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow version:',tf.__version__+',',\n      'GPU =',tf.test.is_gpu_available())\nprint('DeepTables version:',dt.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"constant_cols = pd.read_csv('/kaggle/input/um-gps-of-mcts-variants-constant-columns/constant_columns.csv').columns.to_list()\ntarget_col = 'utility_agent1'\ngame_col = 'GameRulesetName'\ngame_rule_cols = ['EnglishRules', 'LudRules']\noutput_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\ndropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\nagent_cols = ['agent1', 'agent2']\n\ndef preprocess_data(df): \n    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n    if CFG.split_agent_features:\n        for col in agent_cols:\n            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n    df = df.to_pandas()\n    print(f'Data shape: {df.shape}\\n')\n    cat_cols = df.select_dtypes(include=['category']).columns.tolist()\n    non_cat_cols = df.select_dtypes(exclude=['category']).columns.tolist()\n    num_cols = [num for num in non_cat_cols if num not in [target_col, game_col]]\n    return df, cat_cols, num_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\nLR_START = 1e-4\nLR_MAX = 1e-3\nLR_MIN = 1e-3\nLR_RAMPUP_EPOCHS = 1\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = 7\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))\nLR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    train_path = '/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv'\n    split_agent_features = True\n    scaler = MinMaxScaler()  # Scaler or None\n    \n    nn = True\n    ctb = True\n    ens_weights = {'nn': 0.50, 'ctb': 0.50}  # While nn = True and ctb = True\n    \n    folds = 6\n    \n    epochs = 7\n    batch_size = 128\n    LR_Scheduler = [LR_Scheduler]\n    optimizer = Adam(learning_rate=1e-3)\n    conf = ModelConfig(auto_imputation=False,\n                       auto_discrete=False,\n                       auto_discard_unique=True,\n                       categorical_columns='auto',\n                       apply_gbm_features=True,\n                       fixed_embedding_dim=True,\n                       embeddings_output_dim=4,\n                       embedding_dropout=0.2,\n                       nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n                       dnn_params={\n                           'hidden_units': ((1024, 0.0, True),\n                                            (512, 0.0, True),\n                                            (256, 0.0, True),\n                                            (128, 0.0, True)),\n                           'dnn_activation': 'relu',\n                       },\n                       stacking_op='concat',\n                       output_use_bias=False,\n                       optimizer=optimizer,\n                       task='regression',\n                       loss='auto',\n                       metrics=[\"RootMeanSquaredError\"],\n                       earlystopping_patience=1,\n                       )\n\n    ctb_params = dict(iterations=5000,\n                      learning_rate=0.03,\n                      depth=9,\n                      l2_leaf_reg=3,\n                      random_strength=0.2,\n                      bagging_temperature=0.3,\n                      loss_function='RMSE',\n                      eval_metric = 'RMSE',\n                      metric_period=500,\n                      od_type='Iter',\n                      od_wait=100,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(data, cat_cols, num_cols, scaler):\n    cv = GroupKFold(n_splits=CFG.folds)\n    groups = data[game_col]\n    X = data.drop([target_col, game_col], axis=1)\n    y = data[target_col]\n    oof = np.zeros(len(data))\n    nn_models = []\n    ctb_models = []\n    \n    print('nn = '+str(CFG.nn))\n    print('ctb = '+str(CFG.ctb),'\\n')\n    \n    for fi, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n        print(\"#\"*25)\n        print(f\"### Fold {fi+1}/{CFG.folds} ...\")\n        print(\"#\"*25)\n\n        if CFG.nn == True and CFG.ctb == False:\n            print('\\n',\"nn only model training.\",'\\n')\n            K.clear_session()\n            nn_model = DeepTable(config=CFG.conf)\n            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n                      callbacks=CFG.LR_Scheduler,\n                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n            nn_models.append(nn_model)\n        \n            # Avoid some errors\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for j, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(j)\n                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n\n            oof_preds = nn_model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        elif CFG.nn == False and CFG.ctb == True:\n            print('\\n',\"ctb only model training.\",'\\n')\n            X_ = X.copy()\n            if CFG.scaler is not None:\n                print(f'Inverse scaling {len(num_cols)} numerical cols.\\n')\n                X_[num_cols] = scaler.inverse_transform(X_[num_cols])\n            ctb_model = CatBoostRegressor(**CFG.ctb_params)\n            ctb_model.fit(X_.iloc[train_idx], y.iloc[train_idx],\n                          eval_set=[(X_.iloc[valid_idx], y.iloc[valid_idx])],\n                          cat_features=cat_cols, use_best_model=True)\n            ctb_models.append(ctb_model)\n\n            oof_preds = ctb_model.predict(X_.iloc[valid_idx])\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        elif CFG.nn == True and CFG.ctb == True:\n            print('\\n',\"nn & ctb model training.\",'\\n')\n            K.clear_session()\n            nn_model = DeepTable(config=CFG.conf)\n            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n                      callbacks=CFG.LR_Scheduler,\n                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n            nn_models.append(nn_model)\n\n            # Avoid some errors\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for j, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(j)\n                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n            \n            X_ = X.copy()\n            if CFG.scaler is not None:\n                print(f'Inverse scaling {len(num_cols)} numerical cols.\\n')\n                X_[num_cols] = scaler.inverse_transform(X_[num_cols])\n            ctb_model = CatBoostRegressor(**CFG.ctb_params)\n            ctb_model.fit(X_.iloc[train_idx], y.iloc[train_idx],\n                          eval_set=[(X_.iloc[valid_idx], y.iloc[valid_idx])],\n                          cat_features=cat_cols, use_best_model=True)\n            ctb_models.append(ctb_model)\n\n            oof_preds = CFG.ens_weights['nn'] * nn_model.predict(X.iloc[valid_idx],\n                                                                 verbose=1, batch_size=512).flatten() + \\\n                        CFG.ens_weights['ctb'] * ctb_model.predict(X_.iloc[valid_idx])\n            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n            if fi<CFG.folds: oof[valid_idx] = oof_preds\n            else: oof[valid_idx] += oof_preds\n                \n        else:\n            raise ValueError(\"No model selected in CFG.\")\n    \n    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n    if CFG.nn==True: plot_model(nn_model.get_model().model)\n    return nn_models, ctb_models\n\n\ndef infer(data, nn_models, ctb_models, num_cols, scaler):\n    if CFG.nn == True and CFG.ctb == False:\n        return np.mean([model.predict(data, verbose=1, batch_size=512).flatten()\n                                            for model in nn_models], axis=0)\n    elif CFG.nn == False and CFG.ctb == True:\n        if CFG.scaler is not None:\n            print(f'Inverse scaling {len(num_cols)} numerical cols.\\n')\n            data[num_cols] = scaler.inverse_transform(data[num_cols])\n        return np.mean([model.predict(data) for model in ctb_models], axis=0)\n    \n    elif CFG.nn == True and CFG.ctb == True:\n        data_ = data.copy()\n        if CFG.scaler is not None:\n            print(f'Inverse scaling {len(num_cols)} numerical cols.\\n')\n            data_[num_cols] = scaler.inverse_transform(data_[num_cols])\n        return CFG.ens_weights['nn'] * np.mean([model.predict(data, verbose=1, batch_size=512).flatten()\n                                                for model in nn_models], axis=0) + \\\n               CFG.ens_weights['ctb'] * np.mean([model.predict(data_) for model in ctb_models], axis=0)\n    else:\n        raise ValueError(\"No model selected in CFG.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nrun_i = 0\nscaler = CFG.scaler\ndef predict(test_data, submission):\n    global run_i, scaler, nn_models, ctb_models\n    if run_i == 0:\n        train_df = pl.read_csv(CFG.train_path)\n        train_df, cat_cols, num_cols = preprocess_data(train_df)\n        if scaler is not None:\n            print(f'Scaling {len(num_cols)} numerical cols.\\n')\n            train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n        nn_models, ctb_models = train(train_df, cat_cols, num_cols, scaler)\n    run_i += 1\n    test_df, cat_cols, num_cols = preprocess_data(test_data)\n    test_df = test_df.drop(columns=game_col)\n    if scaler is not None:\n        print(f'Scaling {len(num_cols)} numerical cols.\\n')\n        test_df[num_cols] = scaler.transform(test_df[num_cols])\n    return submission.with_columns(pl.Series(target_col, infer(test_df, nn_models, ctb_models,\n                                                               num_cols, scaler)))\n\ninference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        ('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n         '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'))","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}