{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 70367,
     "databundleVersionId": 9188054,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "References\n\n1. [Why Did Calibration Lead to a Lower Public Score When Combining Two Kaggle Notebooks?](https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/530472)\n\n2. [Fork of NeurIPS Ariel 2024 - Starter 5be123](https://www.kaggle.com/code/regisvargas/fork-of-neurips-ariel-2024-starter-5be123)\n\n3. [NeurIPS Ariel 2024 - Starter withdifferentparametr](https://www.kaggle.com/code/bingyuniu/neurips-ariel-2024-starter-withdifferentparametr)\n\n4. [[UPDATE]Calibrating and Binning Astronomical Data](https://www.kaggle.com/code/gordonyip/update-calibrating-and-binning-astronomical-data)\n\n5. [[UPDATE]Calibrating and Binning Astronomical Data (copy)](https://www.kaggle.com/code/aaronjday/update-calibrating-and-binning-astronomical-data)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Initialization\n\nThis competition seems requires strong scientific background and I had lot of confusion during EDA process. Therefore, I just build a simple starter for future coding.\n\n## Load Library",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport pickle\nimport time\nimport os\nimport pickle\nimport seaborn as sns\nimport scipy.stats\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T11:26:19.205208Z",
     "iopub.execute_input": "2024-09-05T11:26:19.205642Z",
     "iopub.status.idle": "2024-09-05T11:26:25.926127Z",
     "shell.execute_reply.started": "2024-09-05T11:26:19.205609Z",
     "shell.execute_reply": "2024-09-05T11:26:25.925017Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load Meta-Data\nPATH = \"/kaggle/input/ariel-data-challenge-2024\"\ntrain_adc_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/train_adc_info.csv', \n                             index_col='planet_id')\ntrain_labels = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/train_labels.csv',\n                           index_col='planet_id')\nwavelengths = pd.read_csv(f'{PATH}/wavelengths.csv')\naxis_info = pd.read_parquet(os.path.join(PATH,'axis_info.parquet'))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.115814Z",
     "iopub.execute_input": "2024-09-04T19:21:24.11642Z",
     "iopub.status.idle": "2024-09-04T19:21:24.408664Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.116382Z",
     "shell.execute_reply": "2024-09-04T19:21:24.407378Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_adc_info['AIRS-CH0_adc_gain'].loc[785834]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.414392Z",
     "iopub.execute_input": "2024-09-04T19:21:24.414702Z",
     "iopub.status.idle": "2024-09-04T19:21:24.423036Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.414674Z",
     "shell.execute_reply": "2024-09-04T19:21:24.421762Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Pre-Processing\n## Load Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pywt\ndef wavelet_denoising(data, wavelet = 'db10', sigma=None):\n    \"\"\"Denoises the signal using SURE wavelet shrinkage with the specified wavelet.\"\"\"\n    # Criar uma cÃ³pia do array para evitar o erro de \"read-only\"\n    data = np.array(data, copy=True)\n    \n    # Decompose the signal using discrete wavelet transform\n    coeffs = pywt.wavedec(data, wavelet)\n    \n    # Estimate noise level if sigma is not provided\n    if sigma is None:\n        # Using the Median Absolute Deviation (MAD) estimator for noise level\n        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n    # Apply thresholding (SURE or hard/soft)\n    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n    new_coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]\n    # Reconstruct the signal using the modified coefficients\n    return pywt.waverec(new_coeffs, wavelet)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.424327Z",
     "iopub.execute_input": "2024-09-04T19:21:24.42466Z",
     "iopub.status.idle": "2024-09-04T19:21:24.508202Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.424634Z",
     "shell.execute_reply": "2024-09-04T19:21:24.507172Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile utils.py\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom tqdm import tqdm\nimport pickle\nPATH = \"/kaggle/input/ariel-data-challenge-2024\"\ncut_inf, cut_sup = 39, 321\ndef load_signal_data(planet_id, dataset, instrument, img_size):\n    file_path = f'{PATH}/{dataset}/{planet_id}/{instrument}_signal.parquet'\n    signal = pd.read_parquet(file_path)\n    if instrument == \"AIRS-CH0\":\n        signal = signal.values.astype(np.float64).reshape((signal.shape[0], 32, 356))\n    else:\n        signal = signal.values.astype(np.float64).reshape((signal.shape[0], 32, 32))\n    if dataset == 'train':\n        gain = train_adc_info[instrument+'_adc_gain'].loc[planet_id]\n        offset = train_adc_info[instrument+'_adc_offset'].loc[planet_id]\n        signal = ADC_convert(signal, gain, offset)\n    else:\n        gain = test_adc_info[instrument+'_adc_gain'].loc[planet_id]\n        offset = test_adc_info[instrument+'_adc_offset'].loc[planet_id]\n        signal = ADC_convert(signal, gain, offset)\n    if instrument == \"AIRS-CH0\":\n        dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n        dt_airs[1::2] += 0.1\n        signal = signal[:, :, cut_inf:cut_sup]\n    signal = signal.reshape(signal.shape[0], signal.shape[1] * signal.shape[2])\n    mean_signal = signal.mean(axis=1)\n    #mean_signal = mean_signal / np.linalg.norm(mean_signal)\n    net_signal = mean_signal[1::2] - mean_signal[0::2]\n    #return wavelet_denoising(net_signal)\n    return net_signal\ndef ADC_convert(signal, gain, offset):\n    signal = signal.astype(np.float64)\n    signal /= gain\n    signal += offset\n    return signal\ndef read_and_preprocess(dataset, planet_ids, instrument = \"AIRS-CH0\"):\n    \"\"\"Read the files for all planet_ids and extract the time series.\n    Parameters\n    dataset: 'train' or 'test'\n    planet_ids: list of planet ids\n    instrument: the instrument of observation, 'AIRS-CH0' or 'FGS1', default to 'AIRS-CH0'\n    Returns\n    dataframe with one row per planet_id and 67500 values per row for FGS1 and 5624 for AIRS-CH0\n    \"\"\"\n    img_size = 1024 if instrument == \"FGS1\" else 32*356\n    column_num = 67500 if instrument == 'FGS1' else 5625\n    raw_train = np.full((len(planet_ids), column_num), np.nan, dtype=np.float32)\n    for i, planet_id in tqdm(list(enumerate(planet_ids))):\n        raw_train[i] = load_signal_data(planet_id, dataset, instrument, img_size)\n    return raw_train\ndef feature_engineering(f_raw, a_raw, adc_info, window_size=50, step_size=15):\n    \"\"\"Create a dataframe with combined features from the raw data, including sliding window and time-series statistics.\n    \n    Parameters:\n    f_raw: ndarray of shape (n_planets, 67500)\n    a_raw: ndarray of shape (n_planets, 5625)\n    window_size: int, size of the sliding window for time-series statistics\n    step_size: int, step size for the sliding window\n    \n    Return value:\n    df: DataFrame of shape (n_planets, several features)\n    \"\"\"\n    f_obscured = f_raw[:, 23500:44000].mean(axis=1)\n    f_unobscured = (f_raw[:, :20500].mean(axis=1) + f_raw[:, 47000:].mean(axis=1)) / 2\n    f_relative_reduction = (f_unobscured - f_obscured) / f_unobscured\n    f_std_dev = f_raw.std(axis=1)\n    f_signal_to_noise = f_unobscured / f_std_dev\n    a_obscured = a_raw[:, 1958:3666].mean(axis=1)\n    a_unobscured = (a_raw[:, :1708].mean(axis=1) + a_raw[:, 3916:].mean(axis=1)) / 2\n    a_relative_reduction = (a_unobscured - a_obscured) / a_unobscured\n    a_std_dev = a_raw.std(axis=1)\n    a_signal_to_noise = a_unobscured / a_std_dev\n    f_variance = f_raw.var(axis=1)\n    a_variance = a_raw.var(axis=1)\n    \n    f_skewness = pd.DataFrame(f_raw).skew(axis=1).values\n    a_skewness = pd.DataFrame(a_raw).skew(axis=1).values\n    f_kurtosis = pd.DataFrame(f_raw).kurtosis(axis=1).values\n    a_kurtosis = pd.DataFrame(a_raw).kurtosis(axis=1).values\n    \n    f_half_obscured1 = f_raw[:, 20500:23500].mean(axis=1)\n    f_half_obscured2 = f_raw[:, 44000:47000].mean(axis=1)\n    f_half_reduction1 = (f_unobscured - f_half_obscured1) / f_unobscured\n    f_half_reduction2 = (f_unobscured - f_half_obscured2) / f_unobscured\n    a_half_obscured1 = a_raw[:, 1708:1958].mean(axis=1)\n    a_half_obscured2 = a_raw[:, 3666:3916].mean(axis=1)\n    a_half_reduction1 = (a_unobscured - a_half_obscured1) / a_unobscured\n    a_half_reduction2 = (a_unobscured - a_half_obscured2) / a_unobscured\n    # Sliding window features\n    def sliding_window_features(data, window_size, step_size):\n        features = []\n        max_index = data.shape[1]\n        for start in range(0, max_index - window_size + 1, step_size):\n            end = start + window_size\n            window = data[:, start:end]\n            features.append([\n                np.mean(window, axis=1),\n                np.std(window, axis=1),\n                np.min(window, axis=1),\n                np.max(window, axis=1)\n            ])\n        if features:\n            return np.vstack(features).T  # Stack vertically and transpose to get the correct shape\n        else:\n            return np.empty((data.shape[0], 0))  # Return empty array with correct shape\n    \n    f_sliding_features = sliding_window_features(f_raw, window_size, step_size)\n    a_sliding_features = sliding_window_features(a_raw, window_size, step_size)\n    print(f'f_sliding_features.shape: {f_sliding_features.shape}')\n    print(f'a_sliding_features.shape: {a_sliding_features.shape}')\n    df = pd.DataFrame({\n        'f_relative_reduction': f_relative_reduction,\n        'f_signal_to_noise': f_signal_to_noise,\n        'f_variance': f_variance,\n        'f_skewness': f_skewness,\n        'f_kurtosis': f_kurtosis,\n        'a_relative_reduction': a_relative_reduction,\n        'a_signal_to_noise': a_signal_to_noise,\n        'a_variance': a_variance,\n        'a_skewness': a_skewness,\n        'a_kurtosis': a_kurtosis,\n        'f_half_reduction1': f_half_reduction1,\n        'f_half_reduction2': f_half_reduction2,\n        'a_half_reduction1': a_half_reduction1,\n        'a_half_reduction2': a_half_reduction2\n    })\n    if f_sliding_features.size > 0:\n        f_sliding_df = pd.DataFrame(f_sliding_features, columns=[f'f_slide_{i}' for i in range(f_sliding_features.shape[1])])\n        df = pd.concat([df, f_sliding_df], axis=1)\n    if a_sliding_features.size > 0:\n        a_sliding_df = pd.DataFrame(a_sliding_features, columns=[f'a_slide_{i}' for i in range(a_sliding_features.shape[1])])\n        df = pd.concat([df, a_sliding_df], axis=1)\n    \n    df = pd.concat([df, adc_info.reset_index().iloc[:, 1:6]], axis=1)\n    \n    return df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.509978Z",
     "iopub.execute_input": "2024-09-04T19:21:24.510635Z",
     "iopub.status.idle": "2024-09-04T19:21:24.52168Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.510596Z",
     "shell.execute_reply": "2024-09-04T19:21:24.520641Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile -a utils.py\n\ndef postprocessing(pred_array, index, sigma_pred):\n    \"\"\"Create a submission dataframe from its components\n    \n    Parameters:\n    pred_array: ndarray of shape (n_samples, 283)\n    index: pandas.Index of length n_samples with name 'planet_id'\n    sigma_pred: float\n    \n    Return value:\n    df: DataFrame of shape (n_samples, 566) with planet_id as index\n    \"\"\"\n    return pd.concat([pd.DataFrame(pred_array.clip(0, None), index=index, columns=wavelengths.columns),\n                      pd.DataFrame(sigma_pred, index=index, columns=[f\"sigma_{i}\" for i in range(1, 284)])],\n                     axis=1)\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef competition_score(\n        solution: pd.DataFrame,\n        submission: pd.DataFrame,\n        naive_mean: float,\n        naive_sigma: float,\n        sigma_true: float,\n        row_id_column_name='planet_id',\n    ) -> float:\n    '''\n    This is a Gaussian Log Likelihood based metric. For a submission, which contains the predicted mean (x_hat) and variance (x_hat_std),\n    we calculate the Gaussian Log-likelihood (GLL) value to the provided ground truth (x). We treat each pair of x_hat,\n    x_hat_std as a 1D gaussian, meaning there will be 283 1D gaussian distributions, hence 283 values for each test spectrum,\n    the GLL value for one spectrum is the sum of all of them.\n\n    Inputs:\n        - solution: Ground Truth spectra (from test set)\n            - shape: (nsamples, n_wavelengths)\n        - submission: Predicted spectra and errors (from participants)\n            - shape: (nsamples, n_wavelengths*2)\n        naive_mean: (float) mean from the train set.\n        naive_sigma: (float) standard deviation from the train set.\n        sigma_true: (float) essentially sets the scale of the outputs.\n    '''\n\n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n\n    if submission.min().min() < 0:\n        raise ParticipantVisibleError('Negative values in the submission')\n    for col in submission.columns:\n        if not pd.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n\n    n_wavelengths = len(solution.columns)\n    if len(submission.columns) != n_wavelengths*2:\n        raise ParticipantVisibleError('Wrong number of columns in the submission')\n\n    y_pred = submission.iloc[:, :n_wavelengths].values\n    # Set a non-zero minimum sigma pred to prevent division by zero errors.\n    sigma_pred = np.clip(submission.iloc[:, n_wavelengths:].values, a_min=10**-15, a_max=None)\n    y_true = solution.values\n\n    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred))\n    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true * np.ones_like(y_true)))\n    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=naive_mean * np.ones_like(y_true), scale=naive_sigma * np.ones_like(y_true)))\n\n    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n    return float(np.clip(submit_score, 0.0, 1.0))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.522892Z",
     "iopub.execute_input": "2024-09-04T19:21:24.523241Z",
     "iopub.status.idle": "2024-09-04T19:21:24.537763Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.523201Z",
     "shell.execute_reply": "2024-09-04T19:21:24.536765Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "exec(open('utils.py', 'r').read())",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.539131Z",
     "iopub.execute_input": "2024-09-04T19:21:24.539547Z",
     "iopub.status.idle": "2024-09-04T19:21:24.551586Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.539513Z",
     "shell.execute_reply": "2024-09-04T19:21:24.550405Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\nif os.path.exists(\"/kaggle/input/adc24-intro-training/f_raw_train.pickle\"):\n    f_raw_train = np.load('/kaggle/input/adc24-intro-training/f_raw_train.pickle', allow_pickle=True)\nelse:\n    f_raw_train = read_and_preprocess('train', train_labels.index, 'FGS1')\n    with open('f_raw_train.pickle', 'wb') as f:\n        pickle.dump(f_raw_train, f)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:21:24.553016Z",
     "iopub.execute_input": "2024-09-04T19:21:24.553322Z",
     "iopub.status.idle": "2024-09-04T19:51:20.850862Z",
     "shell.execute_reply.started": "2024-09-04T19:21:24.553298Z",
     "shell.execute_reply": "2024-09-04T19:51:20.848653Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%time\nif os.path.exists(\"/kaggle/input/adc24-intro-training/a_raw_train.pickle\"):\n    a_raw_train = np.load('/kaggle/input/adc24-intro-training/a_raw_train.pickle', allow_pickle=True)\nelse:\n    a_raw_train = read_and_preprocess('train', train_labels.index)\n    with open('a_raw_train.pickle', 'wb') as f:\n        pickle.dump(a_raw_train, f)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T19:51:20.856326Z",
     "iopub.execute_input": "2024-09-04T19:51:20.856765Z",
     "iopub.status.idle": "2024-09-04T20:50:45.767195Z",
     "shell.execute_reply.started": "2024-09-04T19:51:20.856728Z",
     "shell.execute_reply": "2024-09-04T20:50:45.762613Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%time\ntrain = feature_engineering(f_raw_train, a_raw_train, train_adc_info)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:45.771796Z",
     "iopub.execute_input": "2024-09-04T20:50:45.772898Z",
     "iopub.status.idle": "2024-09-04T20:50:49.341214Z",
     "shell.execute_reply.started": "2024-09-04T20:50:45.772847Z",
     "shell.execute_reply": "2024-09-04T20:50:49.340144Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:49.342752Z",
     "iopub.execute_input": "2024-09-04T20:50:49.343093Z",
     "iopub.status.idle": "2024-09-04T20:50:49.387079Z",
     "shell.execute_reply.started": "2024-09-04T20:50:49.343065Z",
     "shell.execute_reply": "2024-09-04T20:50:49.38599Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train = train.iloc[:,:-1]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:49.388514Z",
     "iopub.execute_input": "2024-09-04T20:50:49.389276Z",
     "iopub.status.idle": "2024-09-04T20:50:49.413776Z",
     "shell.execute_reply.started": "2024-09-04T20:50:49.389238Z",
     "shell.execute_reply": "2024-09-04T20:50:49.412817Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Plot",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(6, 2))\nplt.plot(f_raw_train.mean(axis=0))\nfor time_step in [20500, 23500, 44000, 47000]:\n    plt.axvline(time_step, color='gray')\nplt.xlabel('time step')\nplt.title('FGS1: Overall mean')\nplt.show()\n\nplt.figure(figsize=(6, 2))\nplt.plot(a_raw_train.mean(axis=0))\nfor time_step in [20500, 23500, 44000, 47000]:\n    plt.axvline(time_step * 11250 // 135000, color='gray')\nplt.xlabel('time step')\nplt.title('AIRS-CH0: Overall mean')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:49.41532Z",
     "iopub.execute_input": "2024-09-04T20:50:49.415721Z",
     "iopub.status.idle": "2024-09-04T20:50:49.956119Z",
     "shell.execute_reply.started": "2024-09-04T20:50:49.415687Z",
     "shell.execute_reply": "2024-09-04T20:50:49.955006Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "color_array = np.array(plt.rcParams['axes.prop_cycle'].by_key()['color'])\nplt.scatter(train.a_relative_reduction, train_labels.wl_1, s=15, alpha=0.5,\n            c=color_array[train_adc_info.star])\nplt.xlabel('relative signal reduction when planet is in front')\nplt.ylabel('target')\nplt.title('Correlation between relative signal reduction and target')\n# plt.gca().set_aspect('equal')\npoints = [plt.Line2D([0], [0], label=f'star {i}', marker='o', markersize=3,\n         markeredgecolor=color_array[i], markerfacecolor=color_array[i], linestyle='') for i in range(2)]\n\nplt.legend(handles=points)\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:49.957711Z",
     "iopub.execute_input": "2024-09-04T20:50:49.958164Z",
     "iopub.status.idle": "2024-09-04T20:50:50.300199Z",
     "shell.execute_reply.started": "2024-09-04T20:50:49.958127Z",
     "shell.execute_reply": "2024-09-04T20:50:50.299006Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model\n## Rigde Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = Ridge(alpha=1e-12)\n\noof_pred = cross_val_predict(model, train, train_labels)\n\nprint(f\"# R2 score: {r2_score(train_labels, oof_pred):.4f}\")\nsigma_pred = mean_squared_error(train_labels, oof_pred, squared=False)\nprint(f\"# Root mean squared error: {sigma_pred:.7f}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:50.301607Z",
     "iopub.execute_input": "2024-09-04T20:50:50.301964Z",
     "iopub.status.idle": "2024-09-04T20:50:56.295849Z",
     "shell.execute_reply.started": "2024-09-04T20:50:50.301917Z",
     "shell.execute_reply": "2024-09-04T20:50:56.294497Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "oof_df = postprocessing(oof_pred, train_adc_info.index, sigma_pred)\ndisplay(oof_df)\n\ngll_score = competition_score(train_labels.copy().reset_index(),\n                              oof_df.copy().reset_index(),\n                              naive_mean=train_labels.values.mean(),\n                              naive_sigma=train_labels.values.std(),\n                              sigma_true=0.000003)\nprint(f\"# Estimated competition score: {gll_score:.4f}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:56.301816Z",
     "iopub.execute_input": "2024-09-04T20:50:56.303208Z",
     "iopub.status.idle": "2024-09-04T20:50:56.422539Z",
     "shell.execute_reply.started": "2024-09-04T20:50:56.303154Z",
     "shell.execute_reply": "2024-09-04T20:50:56.419194Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model.fit(train, train_labels)\nwith open('model.pickle', 'wb') as f:\n    pickle.dump(model, f)\nwith open('sigma_pred.pickle', 'wb') as f:\n    pickle.dump(sigma_pred, f)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:56.424589Z",
     "iopub.execute_input": "2024-09-04T20:50:56.425078Z",
     "iopub.status.idle": "2024-09-04T20:50:57.448224Z",
     "shell.execute_reply.started": "2024-09-04T20:50:56.425034Z",
     "shell.execute_reply": "2024-09-04T20:50:57.447029Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the data\ntest_adc_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/test_adc_info.csv',\n                           index_col='planet_id')\nsample_submission = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/sample_submission.csv',\n                                index_col='planet_id')\nf_raw_test = read_and_preprocess('test', sample_submission.index, 'FGS1')\na_raw_test = read_and_preprocess('test', sample_submission.index)\ntest = feature_engineering(f_raw_test, a_raw_test, test_adc_info)\ntest = test.iloc[: , :-1]\n# Load the model\nwith open('model.pickle', 'rb') as f:\n    model = pickle.load(f)\nwith open('sigma_pred.pickle', 'rb') as f:\n    sigma_pred = pickle.load(f)\n\n# Predict\ntest_pred = model.predict(test)\n\n# Package into submission file\nsub_df = sub_df = postprocessing(test_pred,\n                        test_adc_info.index,\n                        sigma_pred=np.tile(np.where(test_adc_info[['star']] <= 1, 0.0001555, 0.00085), (1, 283)))\ndisplay(sub_df)\nsub_df.to_csv('submission.csv')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T20:50:57.450323Z",
     "iopub.execute_input": "2024-09-04T20:50:57.451182Z",
     "iopub.status.idle": "2024-09-04T20:51:06.331275Z",
     "shell.execute_reply.started": "2024-09-04T20:50:57.451141Z",
     "shell.execute_reply": "2024-09-04T20:51:06.329771Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
