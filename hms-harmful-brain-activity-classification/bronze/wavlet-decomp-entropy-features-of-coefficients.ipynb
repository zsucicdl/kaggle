{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-18T11:39:50.196359Z","iopub.execute_input":"2024-02-18T11:39:50.196783Z","iopub.status.idle":"2024-02-18T11:39:50.203533Z","shell.execute_reply.started":"2024-02-18T11:39:50.19675Z","shell.execute_reply":"2024-02-18T11:39:50.202038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nfrom numpy import pi\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pywt\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.special import gamma,psi","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:51.758045Z","iopub.execute_input":"2024-02-18T11:39:51.758464Z","iopub.status.idle":"2024-02-18T11:39:51.765195Z","shell.execute_reply.started":"2024-02-18T11:39:51.758431Z","shell.execute_reply":"2024-02-18T11:39:51.76386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyentrp\nfrom pyentrp import entropy as ent","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:52.946753Z","iopub.execute_input":"2024-02-18T11:39:52.948579Z","iopub.status.idle":"2024-02-18T11:39:55.089017Z","shell.execute_reply.started":"2024-02-18T11:39:52.948508Z","shell.execute_reply":"2024-02-18T11:39:55.087343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:55.091956Z","iopub.execute_input":"2024-02-18T11:39:55.093312Z","iopub.status.idle":"2024-02-18T11:39:55.285852Z","shell.execute_reply.started":"2024-02-18T11:39:55.093253Z","shell.execute_reply":"2024-02-18T11:39:55.284328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = os.listdir('/kaggle/input/hms-harmful-brain-activity-classification/train_eegs')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:55.288898Z","iopub.execute_input":"2024-02-18T11:39:55.28953Z","iopub.status.idle":"2024-02-18T11:39:55.307975Z","shell.execute_reply.started":"2024-02-18T11:39:55.289473Z","shell.execute_reply":"2024-02-18T11:39:55.306209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(files)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:55.311565Z","iopub.execute_input":"2024-02-18T11:39:55.312073Z","iopub.status.idle":"2024-02-18T11:39:55.32177Z","shell.execute_reply.started":"2024-02-18T11:39:55.312033Z","shell.execute_reply":"2024-02-18T11:39:55.320292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:55.435177Z","iopub.execute_input":"2024-02-18T11:39:55.435603Z","iopub.status.idle":"2024-02-18T11:39:55.463761Z","shell.execute_reply.started":"2024-02-18T11:39:55.435569Z","shell.execute_reply":"2024-02-18T11:39:55.461944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:39:56.308367Z","iopub.execute_input":"2024-02-18T11:39:56.309703Z","iopub.status.idle":"2024-02-18T11:39:56.334035Z","shell.execute_reply.started":"2024-02-18T11:39:56.309644Z","shell.execute_reply":"2024-02-18T11:39:56.33213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_parquet('/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/1000913311.parquet')\ntest= pd.read_parquet('/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/3911565283.parquet')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:32.848405Z","iopub.execute_input":"2024-02-18T11:40:32.848797Z","iopub.status.idle":"2024-02-18T11:40:32.878116Z","shell.execute_reply.started":"2024-02-18T11:40:32.848766Z","shell.execute_reply":"2024-02-18T11:40:32.876264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train),len(test)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:34.278331Z","iopub.execute_input":"2024-02-18T11:40:34.279156Z","iopub.status.idle":"2024-02-18T11:40:34.287782Z","shell.execute_reply.started":"2024-02-18T11:40:34.279102Z","shell.execute_reply":"2024-02-18T11:40:34.286321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train),len(df_train['label_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:35.322503Z","iopub.execute_input":"2024-02-18T11:40:35.323021Z","iopub.status.idle":"2024-02-18T11:40:35.338859Z","shell.execute_reply.started":"2024-02-18T11:40:35.322908Z","shell.execute_reply":"2024-02-18T11:40:35.337081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['expert_consensus'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:36.314056Z","iopub.execute_input":"2024-02-18T11:40:36.314489Z","iopub.status.idle":"2024-02-18T11:40:36.332446Z","shell.execute_reply.started":"2024-02-18T11:40:36.314452Z","shell.execute_reply":"2024-02-18T11:40:36.330502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"length of dataframe --- \",len(df_train))\nprint(\"length of expert_consensus --- \",df_train['expert_consensus'].count())\n\ndf_train.expert_consensus.value_counts().plot(kind = 'bar')\n### note each class do not have exactly equal no of cases ,\n### so we will try to filter out bias by taking equal amount of sample in every class","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:37.361786Z","iopub.execute_input":"2024-02-18T11:40:37.362232Z","iopub.status.idle":"2024-02-18T11:40:37.699571Z","shell.execute_reply.started":"2024-02-18T11:40:37.362181Z","shell.execute_reply":"2024-02-18T11:40:37.698377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df_train.groupby(['expert_consensus']).size()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:38.488514Z","iopub.execute_input":"2024-02-18T11:40:38.488976Z","iopub.status.idle":"2024-02-18T11:40:38.508643Z","shell.execute_reply.started":"2024-02-18T11:40:38.488939Z","shell.execute_reply":"2024-02-18T11:40:38.507432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:39.215815Z","iopub.execute_input":"2024-02-18T11:40:39.216744Z","iopub.status.idle":"2024-02-18T11:40:39.244334Z","shell.execute_reply.started":"2024-02-18T11:40:39.216688Z","shell.execute_reply":"2024-02-18T11:40:39.242862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## no of parquet files associated with patient id\nx = df_train['patient_id'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:39.846189Z","iopub.execute_input":"2024-02-18T11:40:39.846853Z","iopub.status.idle":"2024-02-18T11:40:39.856298Z","shell.execute_reply.started":"2024-02-18T11:40:39.846804Z","shell.execute_reply":"2024-02-18T11:40:39.855043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trying to find no of samples under each ccategory for every  patient \ndf_train.groupby(['patient_id', 'expert_consensus']).size()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:40.535593Z","iopub.execute_input":"2024-02-18T11:40:40.536028Z","iopub.status.idle":"2024-02-18T11:40:40.565868Z","shell.execute_reply.started":"2024-02-18T11:40:40.535994Z","shell.execute_reply":"2024-02-18T11:40:40.564565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samp_df = df_train.groupby([\"patient_id\",\"expert_consensus\"]).size()\nsamp_df.to_excel(\"patient_expert_consensus.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:41.410255Z","iopub.execute_input":"2024-02-18T11:40:41.410916Z","iopub.status.idle":"2024-02-18T11:40:44.023744Z","shell.execute_reply.started":"2024-02-18T11:40:41.41088Z","shell.execute_reply":"2024-02-18T11:40:44.022147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def renyi_entropy(data, alpha):\n    \n    _, ele_counts = np.unique( np.around(data, decimals = 0 ), return_counts = True )\n    summation = np.sum( np.log( ( ele_counts / len(data)) ** alpha ) ) \n    reny_ent = (1 / (1 - alpha)) * summation\n    return np.array(reny_ent)\n\ndef permutation_ent(data, order):\n    if order < 2:\n        order = 2\n    \n    data = np.around(data, decimals = 0)\n    perm_ent = ent.permutation_entropy(data, order = order, normalize = True)\n    return np.array(perm_ent)\n\ndef tsallis_ent(data, alpha):\n    _, ele_counts = np.unique( np.around(data, decimals = 0 ), return_counts = True )\n    summation = np.sum( np.log( ( ele_counts / len(data)) ** alpha ) ) \n    tsa_ent =  (1 / (alpha - 1)) * ( 1 - summation) \n    return np.array(tsa_ent)\n\ndef kraskov_ent(data, k):\n    #if k < 1:\n        #k = 1\n    \n    #k=int(k)\n    knn = NearestNeighbors(n_neighbors=k)\n    X = np.around(data, decimals = 0 ).reshape(-1,1)\n    knn.fit(X)\n    r, _ = knn.kneighbors(X)\n    n, d = X.shape\n    volume_unit_ball = (pi**(.5*d)) / gamma(.5*d + 1)    \n    kra_ent = (d*np.mean(np.log(r[:,-1] + np.finfo(X.dtype).eps))+ np.log(volume_unit_ball)\n               + psi(n) - psi(k))\n    return np.array(kra_ent)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:44.027059Z","iopub.execute_input":"2024-02-18T11:40:44.027606Z","iopub.status.idle":"2024-02-18T11:40:44.044444Z","shell.execute_reply.started":"2024-02-18T11:40:44.027558Z","shell.execute_reply":"2024-02-18T11:40:44.043333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DWT(data,wavelet_family = 'db10', level = 4):\n\n    #data = np.load(\"../Healthcare_signal_processing/Datasets/Bonn/data_all.npz\")\n    db = pywt.Wavelet(wavelet_family)\n  \n    cA4, cD4, cD3, cD2, cD1 = pywt.wavedec(data, db, level = level)\n    #print(samp.shape,len(cA4),len(cD4),len(d3),len(d2),len(d1))\n\n    return [cA4, cD4, cD3, cD2, cD1]","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:40:44.045756Z","iopub.execute_input":"2024-02-18T11:40:44.046114Z","iopub.status.idle":"2024-02-18T11:40:44.0642Z","shell.execute_reply.started":"2024-02-18T11:40:44.046073Z","shell.execute_reply":"2024-02-18T11:40:44.062935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ngroups = [0, 1, 2, 3,4]\ncolumns=['A - A4','A - D4','A - D3','A - D2','A - D1']\nX_train = pd.read_parquet('/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/1000913311.parquet')\nfor sig in X_train.columns:\n    sig_DWT = DWT(X_train[sig])\n    i = 1 \n    plt.figure(figsize=(20,10))\n    plt.title(sig,loc='center')\n    for group in groups:\n        plt.subplot(len(groups), 1, i)\n        \n        plt.plot(sig_DWT[group])\n        plt.title(sig+' -- '+columns[group], y=0.5, loc='right')\n        i += 1\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:42:34.371565Z","iopub.execute_input":"2024-02-18T11:42:34.372113Z","iopub.status.idle":"2024-02-18T11:42:55.344898Z","shell.execute_reply.started":"2024-02-18T11:42:34.372072Z","shell.execute_reply":"2024-02-18T11:42:55.34335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_enctropy_features (signal,re_alpha =2 ,pe_order=2,tsa_order=3,kras_order=3):\n    decomp_wavelet = DWT(signal) ## DWT will decompose the signal into 5 coefficients \n    features=[]\n    for coeff in decomp_wavelet:\n        features.append(renyi_entropy(coeff,re_alpha))\n        features.append(permutation_ent(coeff,pe_order))\n        features.append(tsallis_ent(coeff,tsa_order))\n        #features.append(kraskov_ent(coeff,kras_order))\n    ## after finiding enrtropies for each coefficeint  we get 20 features - \n    return features","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:42:55.360363Z","iopub.execute_input":"2024-02-18T11:42:55.360794Z","iopub.status.idle":"2024-02-18T11:42:55.375453Z","shell.execute_reply.started":"2024-02-18T11:42:55.36076Z","shell.execute_reply":"2024-02-18T11:42:55.373699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## considering equal number of smaples in each case == 14000\n\"\"\"\nexpert_consensus\nGPD        16702\nGRDA       18861\nLPD        14856\nLRDA       16640\nOther      18808\nSeizure    20933\n\"\"\"\nclasses = ['GPD','GRDA','LPD','LPDA','Other','Seizure']\ninput_X = []\ntarget_values = []\ndf_train_GPD = df_train[df_train['expert_consensus'] == 'GPD']\ndf_train_GRDA = df_train[df_train['expert_consensus'] == 'GRDA']\ndf_train_LPD = df_train[df_train['expert_consensus'] == 'LPD']\ndf_train_LRDA = df_train[df_train['expert_consensus'] == 'LRDA']\ndf_train_Other = df_train[df_train['expert_consensus'] == 'Other']\ndf_train_Seizure = df_train[df_train['expert_consensus'] == 'Seizure']","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:42:55.377903Z","iopub.execute_input":"2024-02-18T11:42:55.378328Z","iopub.status.idle":"2024-02-18T11:42:55.468201Z","shell.execute_reply.started":"2024-02-18T11:42:55.378293Z","shell.execute_reply":"2024-02-18T11:42:55.465739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_GPD","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:42:55.470289Z","iopub.execute_input":"2024-02-18T11:42:55.470896Z","iopub.status.idle":"2024-02-18T11:42:55.505243Z","shell.execute_reply.started":"2024-02-18T11:42:55.470845Z","shell.execute_reply":"2024-02-18T11:42:55.503608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eeg_ids= df_train['eeg_id'].unique()\nd = {}\nfor eeg_id in eeg_ids:\n    lab = df_train[df_train['eeg_id'] == eeg_id]['expert_consensus'].values\n    if all(lab):\n        if lab[0] not in d.keys():\n            d[lab[0]] = [eeg_id]\n        else:\n            d[lab[0]].append(eeg_id)\nfor  key,val in d.items():\n    print('class ->',key ,'no of eegs file which are completely mapped to a single label',str(len(val)))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:42:55.507342Z","iopub.execute_input":"2024-02-18T11:42:55.507974Z","iopub.status.idle":"2024-02-18T11:43:06.175068Z","shell.execute_reply.started":"2024-02-18T11:42:55.507828Z","shell.execute_reply":"2024-02-18T11:43:06.174077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n'''\nfin_labels = labels[np.random.choice(len(labels), size=900, replace=False)]\nfor label_id in fin_labels:\n    temp=[]\n    target_values.append(df_train_gpd[df_train_gpd['eeg_id'] == label_id][['seizure_vote','lpd_vote','gpd_vote','lrda_vote','grda_vote','other_vote']].values)\n    #sig_df= pd.read_parquet(\"/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{label_id}.parquet\".format(label_id = label_id))\n    for col in sig_df.columns:\n        temp.extend(extract_enctropy_features(sig_df[col].values))\nprint(temp)\n'''\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:06.176513Z","iopub.execute_input":"2024-02-18T11:43:06.177108Z","iopub.status.idle":"2024-02-18T11:43:06.186275Z","shell.execute_reply.started":"2024-02-18T11:43:06.17707Z","shell.execute_reply":"2024-02-18T11:43:06.184407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_eeg = []\nvote = []\n","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:06.188269Z","iopub.execute_input":"2024-02-18T11:43:06.189384Z","iopub.status.idle":"2024-02-18T11:43:06.258398Z","shell.execute_reply.started":"2024-02-18T11:43:06.189293Z","shell.execute_reply":"2024-02-18T11:43:06.256577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_Seizure.sort_values('seizure_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n\n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:06.260667Z","iopub.execute_input":"2024-02-18T11:43:06.261085Z","iopub.status.idle":"2024-02-18T11:43:13.305012Z","shell.execute_reply.started":"2024-02-18T11:43:06.261051Z","shell.execute_reply":"2024-02-18T11:43:13.303711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vote),len(input_eeg)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:13.309026Z","iopub.execute_input":"2024-02-18T11:43:13.30953Z","iopub.status.idle":"2024-02-18T11:43:13.317522Z","shell.execute_reply.started":"2024-02-18T11:43:13.309494Z","shell.execute_reply":"2024-02-18T11:43:13.316322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_eeg[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:13.319015Z","iopub.execute_input":"2024-02-18T11:43:13.319935Z","iopub.status.idle":"2024-02-18T11:43:13.350585Z","shell.execute_reply.started":"2024-02-18T11:43:13.319889Z","shell.execute_reply":"2024-02-18T11:43:13.349002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_GPD.sort_values('gpd_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n    \n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:13.353884Z","iopub.execute_input":"2024-02-18T11:43:13.354786Z","iopub.status.idle":"2024-02-18T11:43:20.107508Z","shell.execute_reply.started":"2024-02-18T11:43:13.354735Z","shell.execute_reply":"2024-02-18T11:43:20.106375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_GRDA.sort_values('grda_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n    \n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:20.10909Z","iopub.execute_input":"2024-02-18T11:43:20.110077Z","iopub.status.idle":"2024-02-18T11:43:26.909706Z","shell.execute_reply.started":"2024-02-18T11:43:20.110035Z","shell.execute_reply":"2024-02-18T11:43:26.907868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_LPD.sort_values('lpd_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n    \n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:26.910901Z","iopub.execute_input":"2024-02-18T11:43:26.911282Z","iopub.status.idle":"2024-02-18T11:43:33.512498Z","shell.execute_reply.started":"2024-02-18T11:43:26.911248Z","shell.execute_reply":"2024-02-18T11:43:33.511335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_LRDA.sort_values('lrda_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n\n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:33.514713Z","iopub.execute_input":"2024-02-18T11:43:33.515125Z","iopub.status.idle":"2024-02-18T11:43:40.336725Z","shell.execute_reply.started":"2024-02-18T11:43:33.51509Z","shell.execute_reply":"2024-02-18T11:43:40.335311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train_Other.sort_values('other_vote', ascending=False).reset_index()\ni, count=0, 0\npatient_li=[]\nwhile count<100:\n    pat_id = sample['patient_id'][i]\n    if pat_id in patient_li:\n        i+=1\n        continue\n    patient_li.append(pat_id)\n    eeg_id = sample['eeg_id'][i]\n    offset_idx = int(sample['eeg_label_offset_seconds'][i]*100)\n    eeg = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/{eeg_id}.parquet')\n    eeg = eeg[offset_idx:offset_idx+5000]\n    temp = []\n    for col in eeg.columns:\n        if all(np.isnan(eeg[col])) :\n            print(eeg.eeg_id.unique())\n        temp.extend(extract_enctropy_features(eeg[col].values))\n    input_eeg.append(temp)\n    vote.append(sample[['seizure_vote' ,'lpd_vote' ,'gpd_vote', 'lrda_vote', 'grda_vote' ,'other_vote']].iloc[i].values)\n    \n\n    count+=1\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:40.339343Z","iopub.execute_input":"2024-02-18T11:43:40.34055Z","iopub.status.idle":"2024-02-18T11:43:46.988693Z","shell.execute_reply.started":"2024-02-18T11:43:40.340485Z","shell.execute_reply":"2024-02-18T11:43:46.987073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vote),len(input_eeg)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:46.991134Z","iopub.execute_input":"2024-02-18T11:43:46.991778Z","iopub.status.idle":"2024-02-18T11:43:46.998814Z","shell.execute_reply.started":"2024-02-18T11:43:46.991725Z","shell.execute_reply":"2024-02-18T11:43:46.997569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## the features are extracted and further can be used to train a model\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn import svm\ndef classify_svm(X,Y):\n    #X, Y = extract_best_features()\n    acc = []\n    classify = svm.SVC(gamma='scale', kernel = 'rbf')\n    sensitivity , specificity =0,0\n    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n    classify.fit(X_train, Y_train)\n    Y_pred = classify.predict(X_test)\n    cm = confusion_matrix(Y_test,Y_pred)\n    total=sum(sum(cm))\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n    TP=cm[0,0]\n    TN=cm[1,1]\n    FP=cm[1,0]\n    FN=cm[0,1]\n    #print(np.mean(np.array(acc)))\n    return [sensitivity,specificity,TP,TN,FP,FN]\n\ndef classify_LDA(X,Y):\n    #X, Y = extract_best_features()\n    acc = []\n    classify = LinearDiscriminantAnalysis()\n    sensitivity , specificity =0,0\n    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n    classify.fit(X_train, Y_train)\n    Y_pred = classify.predict(X_test)\n    cm = confusion_matrix(Y_test,Y_pred)\n    total=sum(sum(cm))\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n    TP=cm[0,0]\n    TN=cm[1,1]\n    FP=cm[1,0]\n    FN=cm[0,1]\n    #print(np.mean(np.array(acc)))\n    return [sensitivity,specificity,TP,TN,FP,FN]\n\ndef classify_KNN(X,Y):\n    #X, Y = extract_best_features()\n    acc = []\n    classify = KNeighborsClassifier(n_neighbors=3)\n    sensitivity , specificity =0,0\n    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n    classify.fit(X_train, Y_train)\n    Y_pred = classify.predict(X_test)\n    cm = confusion_matrix(Y_test,Y_pred)\n    total=sum(sum(cm))\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n    TP=cm[0,0]\n    TN=cm[1,1]\n    FP=cm[1,0]\n    FN=cm[0,1]\n    #print(np.mean(np.array(acc)))\n    return [sensitivity,specificity,TP,TN,FP,FN]\n\ndef classify_RF(X,Y):\n    #X, Y = extract_best_features()\n    acc = []\n    classify = RandomForestClassifier(max_depth=2, random_state=0)\n    sensitivity , specificity =0,0\n    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n    classify.fit(X_train, Y_train)\n    Y_pred = classify.predict(X_test)\n    cm = confusion_matrix(Y_test,Y_pred)\n    total=sum(sum(cm))\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n    TP=cm[0,0]\n    TN=cm[1,1]\n    FP=cm[1,0]\n    FN=cm[0,1]\n    #print(np.mean(np.array(acc)))\n    return [sensitivity,specificity,TP,TN,FP,FN]\n\ndef classify_ANN(X,Y):\n    #X, Y = extract_best_features()\n    acc = []\n    sensitivity , specificity =0,0\n    #ten_fold_score = cross_val_score(classify, X, Y, cv = 10)\n    model = Sequential()\n    model.add(Dense(20, input_dim=X.shape[1],kernel_initializer='normal', activation='relu'))\n    model.add(Dense(32,kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16,kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    # fit the keras model on the dataset\n\n    X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n    model.fit(np.array(X_train), np.array(Y_train), epochs=50, batch_size=5, verbose=0)\n    Y_pred = model.predict(np.array(X_test))\n        \n    cm = confusion_matrix(np.array(Y_test), np.round(Y_pred))\n    total=sum(sum(cm))\n    sensitivity = cm[0,0]/(cm[0,0]+cm[0,1])\n    specificity = cm[1,1]/(cm[1,0]+cm[1,1])\n    TP=cm[0,0]\n    TN=cm[1,1]\n    FP=cm[1,0]\n    FN=cm[0,1]\n    #print(np.mean(np.array(acc)))\n    return [sensitivity,specificity,TP,TN,FP,FN]      ","metadata":{"execution":{"iopub.status.busy":"2024-02-18T11:43:47.000706Z","iopub.execute_input":"2024-02-18T11:43:47.00164Z","iopub.status.idle":"2024-02-18T11:43:47.025765Z","shell.execute_reply.started":"2024-02-18T11:43:47.001601Z","shell.execute_reply":"2024-02-18T11:43:47.024394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}