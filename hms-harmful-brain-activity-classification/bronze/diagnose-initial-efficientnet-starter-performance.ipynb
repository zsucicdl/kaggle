{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782},{"sourceId":7447509,"sourceType":"datasetVersion","datasetId":4334995},{"sourceId":7450712,"sourceType":"datasetVersion","datasetId":4336944},{"sourceId":158958765,"sourceType":"kernelVersion"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport gc\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\nimport pytorch_lightning as pl\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport albumentations as albu\nfrom sklearn.model_selection import KFold, GroupKFold\nimport pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras import layers, models, optimizers, losses\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom tensorflow.keras.utils import plot_model, to_categorical\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.signal import butter, lfilter\nimport tensorflow as tf\nfrom keras.saving import load_model\n\nimport pandas as pd, numpy as np, os\nimport matplotlib.pyplot as plt\nimport gc\nfrom tqdm import tqdm\nfrom tensorflow.keras import layers, models, optimizers, losses\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nkl = losses.KLDivergence()\nfrom tensorflow.keras.utils import plot_model, to_categorical\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.signal import butter, lfilter\nimport tensorflow as tf\nfrom keras.saving import load_model\n!pip install --no-index --find-links=/kaggle/input/tf-efficientnet-whl-files /kaggle/input/tf-efficientnet-whl-files/efficientnet-1.1.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-09T21:13:50.549369Z","iopub.execute_input":"2024-02-09T21:13:50.549702Z","iopub.status.idle":"2024-02-09T21:14:24.886408Z","shell.execute_reply.started":"2024-02-09T21:13:50.549668Z","shell.execute_reply":"2024-02-09T21:14:24.885125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intro\n\nFirst, sorry for the mess above, I pulled it from a notebook I have been working on different things to try and improve performance and I didn't clean up.\n\nThe purpose of this notebook is to determine which type of classes the initial network provided in the [EfficientNet](https://www.kaggle.com/code/cdeotte/efficientnetb0-starter-lb-0-43) starter notebook performs well on to guide what type of features and training methodologies would be valuebale for future solutions as part of a possible solution.  To do this, will be evaluating out of fold performance of each fold neural network.  It is noted that out of fold performance was not saved during train time, so I depending on an assumption that [GroupKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) produces repeatable folds, though I did not validate this and only took this as possibly true based on some internet forum discussions, so this could not be actually true.  Due to this, actual performance of solution cannot be garunteed but comparisions of the network to itself may still be useful.","metadata":{}},{"cell_type":"code","source":"# Load Training Data\ndf = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\nTARGETS = df.columns[-6:]\nprint('Train shape:', df.shape )\nprint('Targets', list(TARGETS))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:14:24.888504Z","iopub.execute_input":"2024-02-09T21:14:24.889527Z","iopub.status.idle":"2024-02-09T21:14:25.159148Z","shell.execute_reply.started":"2024-02-09T21:14:24.889489Z","shell.execute_reply":"2024-02-09T21:14:25.158243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Non-overlapping EEG ID Train Data\ntrain = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\ntrain.columns = ['spec_id','min']\n\ntmp = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_label_offset_seconds':'max'})\ntrain['max'] = tmp\n\ntmp = df.groupby('eeg_id')[['patient_id']].agg('first')\ntrain['patient_id'] = tmp\n\ntmp = df.groupby('eeg_id')[TARGETS].agg('sum')\nfor t in TARGETS:\n    train[t] = tmp[t].values\n\ny_data = train[TARGETS].values\ny_data = y_data / y_data.sum(axis=1,keepdims=True)\ntrain[TARGETS] = y_data\n\ntmp = df.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target'] = tmp\n\ntrain = train.reset_index()\nprint('Train non-overlapp eeg_id shape:', train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:14:25.160216Z","iopub.execute_input":"2024-02-09T21:14:25.160535Z","iopub.status.idle":"2024-02-09T21:14:25.256058Z","shell.execute_reply.started":"2024-02-09T21:14:25.160509Z","shell.execute_reply":"2024-02-09T21:14:25.255141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We will use the following subset of raw EEG features:')\nFEATS = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\nFEAT2IDX = {x:y for x,y in zip(FEATS,range(len(FEATS)))}\nprint( list(FEATS) )","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:14:25.258498Z","iopub.execute_input":"2024-02-09T21:14:25.258762Z","iopub.status.idle":"2024-02-09T21:14:25.26435Z","shell.execute_reply.started":"2024-02-09T21:14:25.258739Z","shell.execute_reply":"2024-02-09T21:14:25.263383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read Train Specs\nprint('Reding Spectrograms...')\nspectrograms = np.load('/kaggle/input/brain-spectrograms/specs.npy',allow_pickle=True).item()\nprint('Train spectrograms read!')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:14:25.265444Z","iopub.execute_input":"2024-02-09T21:14:25.265701Z","iopub.status.idle":"2024-02-09T21:15:40.431317Z","shell.execute_reply.started":"2024-02-09T21:14:25.265678Z","shell.execute_reply":"2024-02-09T21:15:40.430324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Reading EEG Spectrograms...')\nall_eegs = np.load('/kaggle/input/brain-eeg-spectrograms/eeg_specs.npy',allow_pickle=True).item()\nprint('EEG generated spectrograms read!')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:15:40.432589Z","iopub.execute_input":"2024-02-09T21:15:40.434958Z","iopub.status.idle":"2024-02-09T21:17:12.184894Z","shell.execute_reply.started":"2024-02-09T21:15:40.434927Z","shell.execute_reply":"2024-02-09T21:17:12.183834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_KAGGLE_SPECTROGRAMS = True\nUSE_EEG_SPECTROGRAMS = True","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.186321Z","iopub.execute_input":"2024-02-09T21:17:12.186689Z","iopub.status.idle":"2024-02-09T21:17:12.191287Z","shell.execute_reply.started":"2024-02-09T21:17:12.186655Z","shell.execute_reply":"2024-02-09T21:17:12.190331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MIXED PRECISION\nMIX = True\nif MIX:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.192723Z","iopub.execute_input":"2024-02-09T21:17:12.193458Z","iopub.status.idle":"2024-02-09T21:17:12.205263Z","shell.execute_reply.started":"2024-02-09T21:17:12.193426Z","shell.execute_reply":"2024-02-09T21:17:12.204323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as albu\n\n\nTARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\nTARS2 = {x:y for y,x in TARS.items()}\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, batch_size=32, shuffle=False, augment=False, mode='train',\n                 specs = spectrograms, eeg_specs = all_eegs):\n\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.mode = mode\n        self.specs = specs\n        self.eeg_specs = eeg_specs\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int( np.ceil( len(self.data) / self.batch_size ) )\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        #if self.augment: X = self.__augment_batch(X)\n        if self.augment: X, y = mixup_batch(X, y)\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.data) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples'\n\n        X = np.zeros((len(indexes),128,256,8),dtype='float32')\n        y = np.zeros((len(indexes),6),dtype='float32')\n        img = np.ones((128,256),dtype='float32')\n\n        for j,i in enumerate(indexes):\n            row = self.data.iloc[i]\n            if self.mode=='test':\n                r = 0\n            else:\n                r = int( (row['min'] + row['max'])//4 )\n\n            for k in range(4):\n                # EXTRACT 300 ROWS OF SPECTROGRAM\n                img = self.specs[row.spec_id][r:r+300,k*100:(k+1)*100].T\n                #img = self.specs[row.spec_id][:,:].T\n                # LOG TRANSFORM SPECTROGRAM\n                img = np.clip(img,np.exp(-4),np.exp(8))\n                img = np.log(img)\n\n                # STANDARDIZE PER IMAGE\n                ep = 1e-6\n                m = np.nanmean(img.flatten())\n                s = np.nanstd(img.flatten())\n                img = (img-m)/(s+ep)\n                img = np.nan_to_num(img, nan=0.0)\n\n                # CROP TO 256 TIME STEPS\n                X[j,14:-14,:,k] = img[:,:256] / 2.0\n\n            # EEG SPECTROGRAMS - THIS IS NEW\n            img = self.eeg_specs[row.eeg_id]\n            # STANDARDIZE PER IMAGE\n            #ep = 1e-6\n            #m = np.nanmean(img.flatten())\n            #s = np.nanstd(img.flatten())\n            #img = (img-m)/(s+ep)\n            #img = np.nan_to_num(img, nan=0.0)\n            X[j,:,:,4:] = img\n\n            if self.mode!='test':\n                y[j,] = row[TARGETS]\n\n        return X,y\n\n    def __random_transform(self, img):\n        composition = albu.Compose([\n            albu.HorizontalFlip(p=0.5),\n            albu.CoarseDropout(max_holes=8,max_height=16,max_width=8,fill_value=0,p=0.5),\n            albu.GaussNoise(var_limit=(0.01, 0.05))\n        ])\n        return composition(image=img)['image']\n\n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ] = self.__random_transform(img_batch[i, ])\n            img_batch[i, ] = augment(img_batch[i, ])\n        return img_batch","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.206587Z","iopub.execute_input":"2024-02-09T21:17:12.207515Z","iopub.status.idle":"2024-02-09T21:17:12.229696Z","shell.execute_reply.started":"2024-02-09T21:17:12.207483Z","shell.execute_reply":"2024-02-09T21:17:12.228815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KLDivergence:\n    def __init__(self, reduction='auto', name='kl_divergence', fn=None):\n        self.reduction = reduction\n        self.name = name\n\n    def __call__(self, y_true, y_pred):\n        # Your Kullback-Leibler Divergence calculation logic here\n        # Make sure to use self.reduction where applicable\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.233059Z","iopub.execute_input":"2024-02-09T21:17:12.233402Z","iopub.status.idle":"2024-02-09T21:17:12.243151Z","shell.execute_reply.started":"2024-02-09T21:17:12.233379Z","shell.execute_reply":"2024-02-09T21:17:12.242321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    \n    inp = tf.keras.Input(shape=(128,256,8))\n    base_model = efn.EfficientNetB0(include_top=False, weights=None, input_shape=None)\n    \n    # RESHAPE INPUT 128x256x8 => 512x512x3 MONOTONE IMAGE\n    # KAGGLE SPECTROGRAMS\n    x1 = [inp[:,:,:,i:i+1] for i in range(4)]\n    x1 = tf.keras.layers.Concatenate(axis=1)(x1)\n    # EEG SPECTROGRAMS\n    x2 = [inp[:,:,:,i+4:i+5] for i in range(4)]\n    x2 = tf.keras.layers.Concatenate(axis=1)(x2)\n    # MAKE 512X512X3\n    if USE_KAGGLE_SPECTROGRAMS & USE_EEG_SPECTROGRAMS:\n        x = tf.keras.layers.Concatenate(axis=2)([x1,x2])\n    elif USE_EEG_SPECTROGRAMS: x = x2\n    else: x = x1\n    x = tf.keras.layers.Concatenate(axis=3)([x,x,x])\n    \n    # OUTPUT\n    x = base_model(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n        \n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n    loss = tf.keras.losses.KLDivergence()\n\n    model.compile(loss=loss, optimizer = opt, metrics=['accuracy'] ) \n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.244333Z","iopub.execute_input":"2024-02-09T21:17:12.245291Z","iopub.status.idle":"2024-02-09T21:17:12.256173Z","shell.execute_reply.started":"2024-02-09T21:17:12.245238Z","shell.execute_reply":"2024-02-09T21:17:12.255159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\nimport tensorflow.keras.backend as K, gc\nimport efficientnet.tfkeras as efn\n\nall_oof = []\nall_true = []\n\ngkf = GroupKFold(n_splits=5)\n\ngc.collect()\n\n\nfor i, (train_index, valid_index) in enumerate(gkf.split(train, train.target, train.patient_id)):\n    print(f'Evaluating Model {i+1}...')\n    model = build_model()\n    valid_gen = DataGenerator(train.iloc[valid_index], shuffle=False, batch_size=8, mode='valid', augment=False)\n    model.load_weights(f'/kaggle/input/brain-efficientnet-models-v3-v4-v5/EffNet_v5_f{i}.h5')\n    oof = model.predict(valid_gen, verbose=1)\n    all_oof.append(oof)\n    all_true.append(train.iloc[valid_index][TARGETS].values)\n    \n    del model, oof\n    gc.collect()\n\nall_oof = np.concatenate(all_oof)\nall_true = np.concatenate(all_true)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:17:12.257365Z","iopub.execute_input":"2024-02-09T21:17:12.258098Z","iopub.status.idle":"2024-02-09T21:19:10.71529Z","shell.execute_reply.started":"2024-02-09T21:17:12.258072Z","shell.execute_reply":"2024-02-09T21:19:10.714318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KL- Div Score\n\nThe KL-Div score is substantially better than shown in the notebook (0.57 CV, this is showing 0.45).  This is likely due to data leakage.  I do not intend to fix this in an attempt to preserve compute resources but in future trains the out of fold performance should be saved off for this analysis to be more useful.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/kaggle-kl-div')\nfrom kaggle_kl_div import score\n\noof = pd.DataFrame(all_oof.copy())\noof['id'] = np.arange(len(oof))\n\ntrue = pd.DataFrame(all_true.copy())\ntrue['id'] = np.arange(len(true))\n\ncv = score(solution=true, submission=oof, row_id_column_name='id')\nprint('CV Score KL-Div for EfficientNetB0 =',cv)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:19:10.717597Z","iopub.execute_input":"2024-02-09T21:19:10.717919Z","iopub.status.idle":"2024-02-09T21:19:10.794453Z","shell.execute_reply.started":"2024-02-09T21:19:10.717886Z","shell.execute_reply":"2024-02-09T21:19:10.793653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix\n\nThis analysis is limited due to there being multiple possible diagnosis in each signal, this is ignored when using the argmax.  As such, the global performance isn't that valubale but comparing between classes can be.  We can see in these confusion matrixies that there is a problem detecting LRDA in the signals.  When this analysis is repeated with the PyTorch [ResNet](https://www.kaggle.com/code/yunsuxiaozi/hms-eegs-resnet34d-512-512-training-5-folds) model, it is seen this is not an issue for that network.  This helps explain why the [ensampling](https://www.kaggle.com/code/cody11null/quick-ensemble) of these two networks is so successful.  [This](https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467000) discussion post is very helpful for understanding what is charactersitic of each type of classificaion.\n\n**Possible Solutions**\n* Class weighting\n* Addition of features that help express information in the delta waves (some disagreement on what they are in literature. Some sort of range between 0.5 and 4 Hz is typical)\n* Addition of features/models that help differentiate Lateral vs General activities","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n\ncm = confusion_matrix(np.argmax(all_true, axis=1), np.argmax(all_oof, axis=1))\ncm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nclass_labels = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']\nplt.figure\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\ndisp.plot(cmap='Blues', values_format='d')\n\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:19:10.795673Z","iopub.execute_input":"2024-02-09T21:19:10.79633Z","iopub.status.idle":"2024-02-09T21:19:11.133396Z","shell.execute_reply.started":"2024-02-09T21:19:10.796294Z","shell.execute_reply":"2024-02-09T21:19:11.132468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=class_labels)\ndisp.plot(cmap='Blues', values_format='.2f')  # Use '.2f' for two decimal places","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:19:11.134655Z","iopub.execute_input":"2024-02-09T21:19:11.135009Z","iopub.status.idle":"2024-02-09T21:19:11.480735Z","shell.execute_reply.started":"2024-02-09T21:19:11.134982Z","shell.execute_reply":"2024-02-09T21:19:11.479778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC Curves\n\nThis analysis shows that the AUC of the KL-Divergence optimized solutions are in agreement.  This suggests that strong classificaion performance can be achieved without paying the price of over-confident wrong answers.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\n# Initialize variables for ROC\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\n# Compute ROC curve for each class\nfor i in range(6):\n    # Use 'multilabel-indicator' to handle multiple labels for each sample\n    fpr[i], tpr[i], _ = roc_curve((np.argmax(all_true, axis=1) == i).astype(int), all_oof[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curve for each class\nplt.figure(figsize=(8, 8))\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\nfor i, color in zip(range(6), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {class_labels[i]} (AUC = {roc_auc[i]:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (One-vs-All)')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-09T21:19:11.481877Z","iopub.execute_input":"2024-02-09T21:19:11.482162Z","iopub.status.idle":"2024-02-09T21:19:11.784725Z","shell.execute_reply.started":"2024-02-09T21:19:11.482137Z","shell.execute_reply":"2024-02-09T21:19:11.783825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Conclusion\n\nThis network is struggling most with LRDA. This is the least frequent class, so adding more LRDA signals could held.  In the absence of more data, we should consider how to bring out important features to these signals. This signal classification is based on Delta-wave characteristics, which are generally definced as between 1-4 Hz.  May want to add specific filters focusing on pulling out the delta wave data.  Using Matlab's [buttord](https://www.mathworks.com/help/signal/ref/buttord.html), the order of the filter should be 4.  \n\nThis network would also benifit from features of General vs Lateralized events and Other events vs all other classificaiton events.\n\nEnsampling with networks that have different weaknesses have been show to agregrate in better scores.","metadata":{}}]}