{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7498245,"sourceType":"datasetVersion","datasetId":4330042}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd, numpy as np, os\nimport torch\nimport torch.nn as nn\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-28T13:39:31.624553Z","iopub.execute_input":"2024-01-28T13:39:31.625091Z","iopub.status.idle":"2024-01-28T13:39:35.482238Z","shell.execute_reply.started":"2024-01-28T13:39:31.625063Z","shell.execute_reply":"2024-01-28T13:39:35.481255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = \"/kaggle/input/hms-harmful-brain-activity-classification/\"\ntrain_csv = pd.read_csv(base_path+\"train.csv\")\ntrain_eeg1 = pd.read_parquet(base_path+\"train_eegs/1628180742.parquet\")\ntrain_spectrogram1 = pd.read_parquet(base_path+\"train_spectrograms/2147388374.parquet\")\ntargets = train_csv.columns[-6:]\ntars = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:35.483747Z","iopub.execute_input":"2024-01-28T13:39:35.484133Z","iopub.status.idle":"2024-01-28T13:39:36.051204Z","shell.execute_reply.started":"2024-01-28T13:39:35.484107Z","shell.execute_reply":"2024-01-28T13:39:36.050271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_csv.groupby('eeg_id')[['spectrogram_id', \n                                     'spectrogram_label_offset_seconds']].agg({\n    'spectrogram_id':'first', 'spectrogram_label_offset_seconds' : 'min'\n})\n\ntrain.columns=['spec_id', 'min_time']\n\ntmp = train_csv.groupby('eeg_id')[['spectrogram_id', \n                                     'spectrogram_label_offset_seconds']].agg({\n    'spectrogram_label_offset_seconds':'max'    \n})\ntrain['max_time']=tmp\n\ntmp = train_csv.groupby('eeg_id')[['patient_id']].agg({\n    'patient_id':'first'\n})\ntrain['patient_id']=tmp\n\ntmp = train_csv.groupby('eeg_id')[targets].agg('sum')\nfor t in targets:\n    train[t]=tmp[t].values\n    \ny_train = train[targets].values\ny_train = y_train/y_train.sum(axis=1, keepdims=True)\n\ntrain[targets]=y_train\ntmp = train_csv.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target']=tmp\ntrain = train.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:36.0526Z","iopub.execute_input":"2024-01-28T13:39:36.053201Z","iopub.status.idle":"2024-01-28T13:39:36.137963Z","shell.execute_reply.started":"2024-01-28T13:39:36.053172Z","shell.execute_reply":"2024-01-28T13:39:36.137021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:36.140381Z","iopub.execute_input":"2024-01-28T13:39:36.140728Z","iopub.status.idle":"2024-01-28T13:39:36.165092Z","shell.execute_reply.started":"2024-01-28T13:39:36.140701Z","shell.execute_reply":"2024-01-28T13:39:36.164248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nGEN_FEATS=False\nfrom tqdm.notebook import trange\nfrom tqdm import tqdm\nimport time\nfrom scipy.stats import kurtosis, skew, entropy\nimport gc\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nPATH = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n\nif GEN_FEATS:\n    cols = train_spectrogram1.columns[1:]\n    feats = [f\"{c}_mean\" for c in cols]\n    feats += [f\"{c}_std\" for c in cols]\n    feats += [f\"{c}_kurtosis\" for c in cols]\n    feats += [f\"{c}_skew\" for c in cols]\n    feats += [f\"{c}_min\" for c in cols]\n    feats += [f\"{c}_max\" for c in cols]\n    feats += [f\"{c}_energy\" for c in cols]\n    feats += [f\"{c}_entropy\" for c in cols]\n    print(f\"Loading {len(feats)} Spectogram Features....\")\n    del cols; gc.collect()\n    data = np.zeros((len(train), len(feats)))\n    for i in tqdm(range(len(train)), desc=\"Generating features\"):\n        spec_id = str(train.iloc[i,:]['spec_id'])\n        spec = pd.read_parquet(PATH+spec_id+\".parquet\")\n        spec = spec.drop(['time'], axis=1).values\n        #mean\n        data[i, :400] = np.nanmean(spec, axis=0, keepdims=True)\n        #std\n        data[i, 400:800] = np.nanstd(spec, axis=0, keepdims=True)\n        #kurtosis\n        data[i, 800:1200] = kurtosis(spec, nan_policy = \"omit\", \n                                     axis=0, keepdims=True)\n        #skew\n        data[i, 1200:1600] = skew(spec, nan_policy = \"omit\",\n                                 axis=0, keepdims=True)\n        #min\n        data[i, 1600:2000] = np.nanmin(spec, axis=0, keepdims=True)\n        \n        #max\n        data[i, 2000:2400] = np.nanmax(spec, axis=0, keepdims=True)\n        \n        #energy\n        data[i, 2400:2800] = np.nansum(spec**2, axis=0, keepdims=True)/len(spec)\n        \n        #entropy\n        spec = np.nan_to_num(spec)\n        data[i, 2800:3200] = entropy(spec, axis=0 )\n    \n    train_data = pd.DataFrame(data)\n    train_data.columns = feats\n    train_data.to_parquet(\"./heavydata.parquet\", compression=\"gzip\")\n    \nLOAD_SPEC = True\nif LOAD_SPEC:\n    print(\"Loading 3200 Spectogram Features....\")\n    data = pd.read_parquet(\"/kaggle/input/spectogram-data/heavydata.parquet\")\n    feats = data.columns.values\n    data = data.values\n    data[:, 800:1200] = np.nan_to_num(data[:, 800:1200])\n    data[:, 1200:1600] = np.nan_to_num(data[:, 1200:1600])\n    data[:, 2800:3200] = np.nan_to_num(data[:, 2800:3200])\n    \n    \n    print(\"Loaded all the features from spectograms\")\n\n#nanstd() scipy.stats.kurtosis() scipy.stats.skew() min() max() scipy.stats.entropy()\n#nan_policy='omit' keepdims=True\n#check axis (=0 most prolly)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:36.166674Z","iopub.execute_input":"2024-01-28T13:39:36.166919Z","iopub.status.idle":"2024-01-28T13:39:36.794858Z","shell.execute_reply.started":"2024-01-28T13:39:36.166898Z","shell.execute_reply":"2024-01-28T13:39:36.793685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_eeg(eeg_id, test=False):\n    if test:\n        PATH = \"/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/\"\n    else:\n        PATH = \"/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/\"\n    eeg = pd.read_parquet(PATH+eeg_id+\".parquet\")\n    start = (eeg.shape[0] - 10_000)//2\n    data = eeg.iloc[start:start+10_000, :-1].values\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:36.795948Z","iopub.execute_input":"2024-01-28T13:39:36.796285Z","iopub.status.idle":"2024-01-28T13:39:36.801856Z","shell.execute_reply.started":"2024-01-28T13:39:36.796259Z","shell.execute_reply":"2024-01-28T13:39:36.801053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nGEN_EEG_FEATS = False\nif GEN_EEG_FEATS:\n    cols = train_eeg1.columns[:-1]\n    feats = [f\"{c}_mean\" for c in cols]\n    feats += [f\"{c}_std\" for c in cols]\n    feats += [f\"{c}_kurtosis\" for c in cols]\n    feats += [f\"{c}_skew\" for c in cols]\n    feats += [f\"{c}_min\" for c in cols]\n    feats += [f\"{c}_max\" for c in cols]\n    feats += [f\"{c}_energy\" for c in cols]\n    feats += [f\"{c}_entropy\" for c in cols]\n    \n    eeg_data = np.zeros((len(train), len(feats)))\n    \n    for i in trange(len(train)):\n        eeg_id = str(train.iloc[i,:]['eeg_id'])\n        data = get_eeg(eeg_id)\n        \n        #mean\n        eeg_data[i, :19] = np.nanmean(data, axis=0, keepdims=True)\n        #std\n        eeg_data[i, 19:38] = np.nanstd(data, axis=0, keepdims=True)\n        #kurtosis\n        eeg_data[i, 38:19*3] = kurtosis(data, axis=0,\n                                       nan_policy='omit', keepdims=True)\n        #skew\n        eeg_data[i, 19*3:19*4] = skew(data, axis=0,\n                                     nan_policy='omit', keepdims=True)\n        #min\n        eeg_data[i, 19*4:19*5] = np.nanmin(data, axis=0, keepdims=True)\n        #max\n        eeg_data[i, 19*5:19*6] = np.nanmax(data, axis=0, keepdims=True)\n        #energy\n        eeg_data[i, 19*6:19*7] = np.nansum(data**2, axis=0, keepdims=True)/len(data)\n        #entropy\n        data = np.nan_to_num(data)\n        eeg_data[i, 19*7:19*8] = entropy(data, axis=0)\n        \n    train_data = pd.DataFrame(eeg_data)\n    train_data.columns = feats\n    train_data.to_parquet(\"./eeg_data.parquet\", compression='gzip')\n    \nLOAD_EEG = False\nif LOAD_EEG:\n    print(f\"Loading {19*8} EEG features....\")\n    data = pd.read_parquet(\"/kaggle/input/spectogram-data/eeg_data.parquet\")\n    feats = data.columns.values\n    #data.replace([np.inf, -np.inf], np.nan, inplace=True)\n    data = data.values\n    data = np.nan_to_num(data)\n    print(\"Loaded all the features\")","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:36.80329Z","iopub.execute_input":"2024-01-28T13:39:36.80355Z","iopub.status.idle":"2024-01-28T13:39:37.049361Z","shell.execute_reply.started":"2024-01-28T13:39:36.803527Z","shell.execute_reply":"2024-01-28T13:39:37.04827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = train['target'].map(tars).values","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:37.050577Z","iopub.execute_input":"2024-01-28T13:39:37.050923Z","iopub.status.idle":"2024-01-28T13:39:37.058327Z","shell.execute_reply.started":"2024-01-28T13:39:37.050897Z","shell.execute_reply":"2024-01-28T13:39:37.057323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:37.059358Z","iopub.execute_input":"2024-01-28T13:39:37.059635Z","iopub.status.idle":"2024-01-28T13:39:37.368765Z","shell.execute_reply.started":"2024-01-28T13:39:37.059611Z","shell.execute_reply":"2024-01-28T13:39:37.367791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KL_loss(p,q):\n    epsilon=10**(-15)\n    p=torch.clip(p,epsilon,1-epsilon)\n    q = nn.functional.log_softmax(q,dim=1)\n    return torch.mean(torch.sum(p*(torch.log(p)-q),dim=1))\n\ndef AOS(total_error):\n    return 1/total_error","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:37.373553Z","iopub.execute_input":"2024-01-28T13:39:37.37402Z","iopub.status.idle":"2024-01-28T13:39:37.37971Z","shell.execute_reply.started":"2024-01-28T13:39:37.373994Z","shell.execute_reply":"2024-01-28T13:39:37.378628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import GroupKFold\ntrain_cat=True\nif train_cat:\n    print(\"Training CatBoost model\")\n    n_splits=5\n    gkf = GroupKFold(n_splits=5)\n    device = 'GPU' if torch.cuda.is_available() else 'CPU'\n    probs=[]\n    true=[]\n    best_score = np.inf\n    print(f\"Running on {device}\")\n    for i, (train_index, valid_index) in enumerate(tqdm(gkf.split(data, Y, train.patient_id))):\n        cat_model = CatBoostClassifier(task_type=device, loss_function=\"MultiClass\")\n        train_pool = Pool(data = data[train_index, :], label = Y[train_index])\n        valid_pool = Pool(data = data[valid_index, :], label = Y[valid_index])\n    \n        cat_model.fit(train_pool,\n                 verbose=100,\n                 eval_set=valid_pool)\n    \n        prob = cat_model.predict_proba(valid_pool)\n        probs.append(prob)\n        true.append(y_train[valid_index,:])\n        \n        score = float(KL_loss(torch.tensor(prob), torch.tensor(y_train[valid_index, :])))\n        print(f\"Score : {score}\")\n        if score<best_score:\n            cat_model.save_model(\"./bestCATboost.cat\")\n            print(f\"saved the model!\")\n            best_score=score\n    \n        del train_pool, valid_pool, prob\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:39:37.380825Z","iopub.execute_input":"2024-01-28T13:39:37.381084Z","iopub.status.idle":"2024-01-28T13:40:38.438803Z","shell.execute_reply.started":"2024-01-28T13:39:37.38106Z","shell.execute_reply":"2024-01-28T13:40:38.437828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nTOP = 25\nif train_cat:\n    feature_importance = cat_model.feature_importances_\n    #print(feature_importance)\n    sorted_idx = np.argsort(feature_importance)\n    fig = plt.figure(figsize=(10, 8))\n    plt.barh(np.arange(len(sorted_idx))[-TOP:], feature_importance[sorted_idx][-TOP:], align='center')\n    plt.yticks(np.arange(len(sorted_idx))[-TOP:], np.array(feats)[sorted_idx][-TOP:])\n    plt.title(f'Feature Importance - Top {TOP}')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:38.440247Z","iopub.execute_input":"2024-01-28T13:40:38.440984Z","iopub.status.idle":"2024-01-28T13:40:38.889202Z","shell.execute_reply.started":"2024-01-28T13:40:38.440949Z","shell.execute_reply":"2024-01-28T13:40:38.888255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_valid_pool(X, y):\n    pool = []\n    for i in range(len(X)):\n        tup = (X[i,:], y[i])\n        pool.append(tup)\n    return np.array(pool)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:38.89045Z","iopub.execute_input":"2024-01-28T13:40:38.890828Z","iopub.status.idle":"2024-01-28T13:40:38.896775Z","shell.execute_reply.started":"2024-01-28T13:40:38.890792Z","shell.execute_reply":"2024-01-28T13:40:38.895798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_xgb=False\nif train_xgb:\n    print(\"Training XGBoost\")\n    probs=[]\n    true=[]\n    n_splits=5\n    gkf = GroupKFold(n_splits=n_splits)\n    tree_method = 'gpu_hist' if torch.cuda.is_available() else 'hist'\n    print(f\"Running on {tree_method}\")\n    for i, (train_index, valid_index) in enumerate(tqdm(gkf.split(data, Y, train.patient_id))):\n        xgb_model = XGBClassifier(tree_method=tree_method)\n        #train_pool = Pool(data = data[train_index, :], label = Y[train_index])\n        #valid_pool = gen_valid_pool(X = data[valid_index, :], y = Y[valid_index])\n        train_X = data[train_index, :]\n        train_Y = Y[train_index]\n        val_X = data[valid_index, :]\n        val_Y = Y[valid_index]\n    \n        xgb_model.fit(X = train_X, y = train_Y,\n                 verbose=100)\n    \n        prob = xgb_model.predict_proba(val_X)\n        probs.append(prob)\n        true.append(y_train[valid_index, :])\n    \n        del train_X, train_Y, val_X, val_Y\n        gc.collect()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:38.898034Z","iopub.execute_input":"2024-01-28T13:40:38.898385Z","iopub.status.idle":"2024-01-28T13:40:38.907725Z","shell.execute_reply.started":"2024-01-28T13:40:38.898352Z","shell.execute_reply":"2024-01-28T13:40:38.906871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport joblib\ntrain_rf = False\nif train_rf:\n    print(\"Training Random Forest\")\n    probs=[]\n    true=[]\n    n_splits=5\n    gkf = GroupKFold(n_splits=n_splits)\n    device = 'GPU' if torch.cuda.is_available() else 'CPU'\n    best_score = np.inf\n    for i, (train_index, valid_index) in enumerate(tqdm(gkf.split(data, Y, train.patient_id))):\n        rfc_model = RandomForestClassifier(n_estimators=200, \n                                     n_jobs=-1, \n                                    verbose=100,\n                                     random_state=17)\n        \n        train_x, val_x, train_y, val_y = data[train_index,:], data[valid_index, :], Y[train_index], Y[valid_index]\n        \n        rfc_model.fit(train_x, train_y)\n        prob = rfc_model.predict_proba(val_x)\n        probs.append(prob)\n        true.append(y_train[valid_index, :])\n        score = float(KL_loss(torch.tensor(prob), torch.tensor(y_train[valid_index, :])))\n        print(f\"Score : {score}\")\n        if score<best_score:\n            joblib.dump(rfc_model, \"./rfc_model.joblib\")\n            print(f\"saved the model!\")\n            best_score=score\n            \n        del train_x, val_x, train_y, val_y\n        gc.collect()\n            ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:38.908679Z","iopub.execute_input":"2024-01-28T13:40:38.909005Z","iopub.status.idle":"2024-01-28T13:40:39.020198Z","shell.execute_reply.started":"2024-01-28T13:40:38.908972Z","shell.execute_reply":"2024-01-28T13:40:39.019368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_cat:\n    s=0\n    for i in range(n_splits):\n        err = KL_loss(torch.tensor(probs[i]), torch.tensor(true[i]))\n        err = float(err)\n        s += err\n    cat_err = s/5\n    \nif train_xgb and train_cat:\n    s=0\n    for i in range(n_splits, len(probs)):\n        err = KL_loss(torch.tensor(probs[i]), torch.tensor(true[i]))\n        err = float(err)\n        s += err\n    \n    xgb_err = s/5\n\n    cat_AOS, xgb_AOS = AOS(cat_err), AOS(xgb_err)\n    \nif train_rf:\n    s=0\n    for i in range(n_splits):\n        s+=float(KL_loss(torch.tensor(probs[i]), torch.tensor(true[i])))\n        \n    rf_error = s/5","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.021321Z","iopub.execute_input":"2024-01-28T13:40:39.021612Z","iopub.status.idle":"2024-01-28T13:40:39.036379Z","shell.execute_reply.started":"2024-01-28T13:40:39.021586Z","shell.execute_reply":"2024-01-28T13:40:39.035446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_err","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.037516Z","iopub.execute_input":"2024-01-28T13:40:39.037876Z","iopub.status.idle":"2024-01-28T13:40:39.044651Z","shell.execute_reply.started":"2024-01-28T13:40:39.037848Z","shell.execute_reply":"2024-01-28T13:40:39.043657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_SPEC:\n    test_path = \"/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/\"\n    test_csv = pd.read_csv(base_path+\"test.csv\")\n    test_data = np.zeros((len(test_csv), len(feats)))\n    for k in range(len(test_csv)):\n        spec_id = str(test_csv.iloc[k]['spectrogram_id'])\n        spec = pd.read_parquet(test_path+spec_id+\".parquet\")\n        spec = spec.drop(['time'], axis=1).values\n        #mean\n        test_data[k,:400] = np.nanmean(spec, axis=0, keepdims=True)\n    \n        #std\n        test_data[k, 400:800] = np.nanstd(spec, axis=0, keepdims=True)\n    \n        #kurtosis\n        test_data[k, 800:1200] = kurtosis(spec, nan_policy = \"omit\", \n                                     axis=0)\n        #skew\n        test_data[k, 1200:1600] = skew(spec, nan_policy = \"omit\", \n                                     axis=0)\n        #min\n        test_data[k, 1600:2000] = np.nanmin(spec, axis=0, keepdims=True)\n        #max\n        test_data[k, 2000:2400] = np.nanmax(spec, axis=0, keepdims=True)\n        #energy\n        test_data[k, 2400:2800] = np.nansum(spec**2, axis=0, keepdims=True)/len(spec)\n        #entropy\n        spec = np.nan_to_num(spec)\n        test_data[k, 2800:3200] = entropy(spec, axis=0)\n    \n    test_data = np.nan_to_num(test_data)\n    \n    sample_sub = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/sample_submission.csv\", index_col=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.045717Z","iopub.execute_input":"2024-01-28T13:40:39.046084Z","iopub.status.idle":"2024-01-28T13:40:39.057255Z","shell.execute_reply.started":"2024-01-28T13:40:39.046056Z","shell.execute_reply":"2024-01-28T13:40:39.056368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_EEG:\n    test_path = \"/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/\"\n    test_csv = pd.read_csv(base_path+\"test.csv\")\n    test_data = np.zeros((len(test_csv), len(feats)))\n    \n    for i in range(len(test_csv)):\n        eeg_id = str(test_csv.iloc[i]['eeg_id'])\n        data = get_eeg(eeg_id, test=True)\n        \n        \n        #mean\n        test_data[i, :19] = np.nanmean(data, axis=0, keepdims=True)\n        #std\n        test_data[i, 19:38] = np.nanstd(data, axis=0, keepdims=True)\n        #kurtosis\n        test_data[i, 38:19*3] = kurtosis(data, axis=0,\n                                       nan_policy='omit')\n        #skew\n        test_data[i, 19*3:19*4] = skew(data, axis=0,\n                                     nan_policy='omit')\n        #min\n        test_data[i, 19*4:19*5] = np.nanmin(data, axis=0, keepdims=True)\n        #max\n        test_data[i, 19*5:19*6] = np.nanmax(data, axis=0, keepdims=True)\n        #energy\n        test_data[i, 19*6:19*7] = np.nansum(data**2, axis=0, keepdims=True)/len(data)\n        #entropy\n        data = np.nan_to_num(data)\n        test_data[i, 19*7:19*8] = entropy(data, axis=0)\n        \n    test_data = np.nan_to_num(test_data)\n    \n    sample_sub = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/sample_submission.csv\", index_col=False)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.058423Z","iopub.execute_input":"2024-01-28T13:40:39.058764Z","iopub.status.idle":"2024-01-28T13:40:39.145713Z","shell.execute_reply.started":"2024-01-28T13:40:39.058733Z","shell.execute_reply":"2024-01-28T13:40:39.144895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.147739Z","iopub.execute_input":"2024-01-28T13:40:39.148002Z","iopub.status.idle":"2024-01-28T13:40:39.154111Z","shell.execute_reply.started":"2024-01-28T13:40:39.147978Z","shell.execute_reply":"2024-01-28T13:40:39.15313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_cat:\n    cat_preds=[]\n    for i in trange(n_splits):\n        model = CatBoostClassifier(task_type=device)\n        model.load_model(\"./bestCATboost.cat\")\n    \n        test_pool = Pool(data = test_data)\n    \n        cat_pred = model.predict_proba(test_pool)\n        cat_preds.append(cat_pred)\n    \n    cat_pred = np.mean(np.array(cat_preds), axis=0)\n\n    print(cat_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.155431Z","iopub.execute_input":"2024-01-28T13:40:39.155979Z","iopub.status.idle":"2024-01-28T13:40:39.203184Z","shell.execute_reply.started":"2024-01-28T13:40:39.155944Z","shell.execute_reply":"2024-01-28T13:40:39.202252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_xgb:\n    xgb_preds=[]\n    for i in trange(n_splits):\n        xgb_pred = xgb_model.predict_proba(test_data)\n        xgb_preds.append(xgb_pred)\n    \n    xgb_pred = np.mean(np.array(xgb_preds), axis=0)\n    print(xgb_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.204417Z","iopub.execute_input":"2024-01-28T13:40:39.204705Z","iopub.status.idle":"2024-01-28T13:40:39.210374Z","shell.execute_reply.started":"2024-01-28T13:40:39.204679Z","shell.execute_reply":"2024-01-28T13:40:39.209253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_rf:\n    rf_preds=[]\n    for i in trange(n_splits):\n        best_rf_model = joblib.load(\"./rfc_model.joblib\")\n        test_data[np.isneginf(test_data)]=np.nan\n        test_data[:, 19*7:19*8] = 0\n        print(test_data)\n        \n        pred = best_rf_model.predict_proba(test_data)\n        rf_preds.append(pred)\n        \n    rf_pred = np.mean(np.array(rf_preds), axis=0)\n    print(rf_pred)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.211696Z","iopub.execute_input":"2024-01-28T13:40:39.21198Z","iopub.status.idle":"2024-01-28T13:40:39.21846Z","shell.execute_reply.started":"2024-01-28T13:40:39.211954Z","shell.execute_reply":"2024-01-28T13:40:39.21766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_cat:\n    final_pred = cat_pred\n    \nif train_xgb and train_cat:\n    final_pred = (cat_pred + xgb_pred)/2\n    print(final_pred.sum())\n    eps = final_pred.sum() - 1.\n    print(final_pred[0,5])\n    final_pred[0,5] -= eps\n    print(final_pred[0,5])\n    final_pred.sum()\n    \nif train_rf:\n    final_pred = rf_pred\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.219769Z","iopub.execute_input":"2024-01-28T13:40:39.220083Z","iopub.status.idle":"2024-01-28T13:40:39.229994Z","shell.execute_reply.started":"2024-01-28T13:40:39.220046Z","shell.execute_reply":"2024-01-28T13:40:39.22918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nif train_cat:\n    shutil.rmtree('/kaggle/working/catboost_info')\n    #os.remove(\"./bestCATboost.cat\")\n    \nif train_rf:\n    os.remove(\"./rfc_model.joblib\")\n    \nsample_sub[targets]=final_pred\nprint(final_pred.sum())\nsample_sub.to_csv(\"./submission.csv\", index=False)\nprint(\"Shape: \", sample_sub.shape)\nsample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T13:40:39.231325Z","iopub.execute_input":"2024-01-28T13:40:39.231632Z","iopub.status.idle":"2024-01-28T13:40:39.252034Z","shell.execute_reply.started":"2024-01-28T13:40:39.231605Z","shell.execute_reply":"2024-01-28T13:40:39.251085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}