{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7527876,"sourceType":"datasetVersion","datasetId":4384638}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UPDATE\n\nV10:Data preprocessing has been added, resolving the issue of loss convergence.\n\nV12:Added L2 regularization and filtering","metadata":{}},{"cell_type":"code","source":"import os,gc\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nimport pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport glob\nimport torch\nimport warnings\n\n# 禁用所有警告\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:36.909023Z","iopub.execute_input":"2024-02-12T04:17:36.909429Z","iopub.status.idle":"2024-02-12T04:17:36.915636Z","shell.execute_reply.started":"2024-02-12T04:17:36.909399Z","shell.execute_reply":"2024-02-12T04:17:36.91466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train data","metadata":{}},{"cell_type":"code","source":"PATH = '/kaggle/input/hms-harmful-brain-activity-classification/'\ndf = pd.read_csv(PATH + 'train.csv')\nTARGETS = df.columns[-6:]\nprint('Train shape:', df.shape )\nprint('Targets', list(TARGETS))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:38.732021Z","iopub.execute_input":"2024-02-12T04:17:38.732616Z","iopub.status.idle":"2024-02-12T04:17:38.930769Z","shell.execute_reply.started":"2024-02-12T04:17:38.732584Z","shell.execute_reply":"2024-02-12T04:17:38.929864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Non-Overlapping Eeg Id Train Data","metadata":{}},{"cell_type":"code","source":"train = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\ntrain.columns = ['spectrogram_id','min']\n\ntmp = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_label_offset_seconds':'max'})\ntrain['max'] = tmp\n\ntmp = df.groupby('eeg_id')[['patient_id']].agg('first')\ntrain['patient_id'] = tmp\n\ntmp = df.groupby('eeg_id')[TARGETS].agg('sum')\nfor t in TARGETS:\n    train[t] = tmp[t].values\n    \ny_data = train[TARGETS].values\ny_data = y_data / y_data.sum(axis=1,keepdims=True)\ntrain[TARGETS] = y_data\n\ntmp = df.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target'] = tmp\n\ntrain = train.reset_index()\nprint('Train non-overlapp eeg_id shape:', train.shape )\n\ntrain.head(20)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:40.261556Z","iopub.execute_input":"2024-02-12T04:17:40.262271Z","iopub.status.idle":"2024-02-12T04:17:40.353912Z","shell.execute_reply.started":"2024-02-12T04:17:40.262236Z","shell.execute_reply":"2024-02-12T04:17:40.352949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nycol = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\ncd = {'Seizure': 'seizure_vote', 'GPD': 'gpd_vote', 'LRDA': 'lrda_vote', 'Other': 'other_vote', 'GRDA': 'grda_vote', 'LPD': 'lpd_vote'}\n\n# 提取概率列和标签列\neeg_id_col = train.iloc[:, 0]  # 第一列是 eeg_id 列\nprob_cols = train.iloc[:, -7:-1]  # 倒数第七到倒数第二列是概率列\nlabel_col = train.iloc[:, -1]  # 最后一列是标签列\n\n# 将概率列转换为 float32 类型\nprob_cols = prob_cols.astype(\"float32\")\n\n# 归一化概率列\nprob_cols_normalized = prob_cols.div(prob_cols.sum(axis=1), axis=0)\n\n# 重新组合成 DataFrame\nnormalized_data = pd.concat([eeg_id_col, prob_cols_normalized, label_col], axis=1)\n\nnormalized_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:41.007352Z","iopub.execute_input":"2024-02-12T04:17:41.007729Z","iopub.status.idle":"2024-02-12T04:17:41.037457Z","shell.execute_reply.started":"2024-02-12T04:17:41.007697Z","shell.execute_reply":"2024-02-12T04:17:41.036511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normalized_data.to_csv(\"normalized_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:41.867953Z","iopub.execute_input":"2024-02-12T04:17:41.868334Z","iopub.status.idle":"2024-02-12T04:17:41.989064Z","shell.execute_reply.started":"2024-02-12T04:17:41.86828Z","shell.execute_reply":"2024-02-12T04:17:41.988237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EEG_PATH = '/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/'\ntrain_path = '/kaggle/input/processed/normalized_data.csv'","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:42.556153Z","iopub.execute_input":"2024-02-12T04:17:42.557067Z","iopub.status.idle":"2024-02-12T04:17:42.561173Z","shell.execute_reply.started":"2024-02-12T04:17:42.557029Z","shell.execute_reply":"2024-02-12T04:17:42.560221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom scipy.signal import butter, sosfilt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nimport pandas as pd\n\nclass EEGDataset(Dataset):\n    def __init__(self, csv_file, eeg_path):\n        self.csv = pd.read_csv(csv_file)  # 加载 CSV 文件\n        self.eeg_path = eeg_path\n        self.sos = self.butter_bandpass_filter_init()\n        self.FEATS = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']  # 指定需要使用的特征列\n\n    def butter_bandpass_filter_init(self):\n        lowcut = 0.5  # 设置带通滤波的低频截止频率\n        highcut = 40.0  # 设置带通滤波的高频截止频率\n        fs = 200.0  # 采样频率\n        order = 5  # 滤波器阶数\n\n        nyq = 0.5 * fs\n        low = lowcut / nyq\n        high = highcut / nyq\n        sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n        return sos\n\n    def butter_bandpass_filter(self, data):\n        y = sosfilt(self.sos, data)\n        return y\n    \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, idx):\n        eeg_id = self.csv.loc[idx, 'eeg_id']\n        eeg_file_path = f\"{self.eeg_path}{eeg_id}.parquet\"  # 构建 EEG 数据文件的路径\n        eeg_data = pd.read_parquet(eeg_file_path)[self.FEATS].values  # 从 Parquet 文件中加载指定的特征列\n        # 检查是否存在 NaN 值\n        if np.isnan(eeg_data).any():\n            # 如果存在 NaN 值，你可以选择将其填充为特定的值或者进行插值处理\n            # 这里我们使用 SimpleImputer 进行简单的填充处理，将 NaN 值替换为均值\n            imputer = SimpleImputer(strategy='mean')\n            eeg_data = imputer.fit_transform(eeg_data)\n        eeg_data = self.butter_bandpass_filter(eeg_data)  # 对数据进行滤波\n        eeg_data = torch.tensor(eeg_data, dtype=torch.float32)  # 转换为 PyTorch 张量\n                    \n        # 选择中间 10000 个时间点的数据\n        mid_index = eeg_data.shape[0] // 2\n        start_index = mid_index - 5000  # 从中间向左偏移5000个时间点\n        end_index = mid_index + 5000  # 到中间向右偏移5000个时间点\n        eeg_data = eeg_data[start_index:end_index]\n        # 交换维度位置\n        eeg_data = torch.transpose(eeg_data, 0, 1)\n        \n        labels = torch.tensor(self.csv.loc[idx, ycol].values.astype(np.float32), dtype=torch.float32)  # 加载对应的标签\n        #labels = labels.unsqueeze(0).expand(eeg_data.size(0), -1)  # 调整标签的尺寸与输出相匹配\n        \n        return eeg_data, labels\n\n# 创建 EEG 数据集实例\ndataset = EEGDataset(train_path, EEG_PATH)\n\n\n# 创建数据加载器\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:17:43.138148Z","iopub.execute_input":"2024-02-12T04:17:43.139034Z","iopub.status.idle":"2024-02-12T04:17:43.176099Z","shell.execute_reply.started":"2024-02-12T04:17:43.138999Z","shell.execute_reply":"2024-02-12T04:17:43.175332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = dataset[0]\nprint(f\"Sample {0 + 1}: X shape {X.shape}, y shape {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:18:12.102534Z","iopub.execute_input":"2024-02-12T04:18:12.102927Z","iopub.status.idle":"2024-02-12T04:18:12.121423Z","shell.execute_reply.started":"2024-02-12T04:18:12.102894Z","shell.execute_reply":"2024-02-12T04:18:12.120353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN+LSTM Model","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CNNLSTM(nn.Module):\n\n    def __init__(self, in_channels=8, num_classes=6):\n        super(CNNLSTM, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.dropout2 = nn.Dropout(p=0.75)\n        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        self.lstm1 = nn.LSTM(input_size=64, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n\n        self.attention = nn.Sequential(\n            nn.Linear(128 * 2, 64),\n            nn.Tanh(),\n            nn.Linear(64, 1)\n        )\n\n        self.fc = nn.Linear(128 * 2, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.dropout1(x)\n        x = self.pool1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.dropout2(x)\n        x = self.pool2(x)\n\n        # reshape for LSTM\n        batch_size, channels, seq_length = x.size()\n        x = x.permute(0, 2, 1)\n\n        # First LSTM layer\n        x, _ = self.lstm1(x)\n\n        # Second LSTM layer\n        x, _ = self.lstm2(x)\n        \n        # Attention layer\n        att_weights = F.softmax(self.attention(x), dim=1)\n        x = torch.sum(att_weights * x, dim=1)\n\n        # Fully connected layer\n        x = self.fc(x)\n\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:20:23.700181Z","iopub.execute_input":"2024-02-12T04:20:23.701069Z","iopub.status.idle":"2024-02-12T04:20:23.714104Z","shell.execute_reply.started":"2024-02-12T04:20:23.701036Z","shell.execute_reply":"2024-02-12T04:20:23.713152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义一些超参数\ninput_channels = 8\nnum_classes = 6  # 类别数\n\n# 创建EEGNet实例\nmodel = CNNLSTM(in_channels=input_channels, num_classes=num_classes)\n\n# 将模型放在GPU上\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 训练模型\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:20:25.359264Z","iopub.execute_input":"2024-02-12T04:20:25.359613Z","iopub.status.idle":"2024-02-12T04:29:17.135879Z","shell.execute_reply.started":"2024-02-12T04:20:25.359588Z","shell.execute_reply":"2024-02-12T04:29:17.134667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict test data","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/hms-harmful-brain-activity-classification/test.csv'\nTEST_EEG_PATH = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\n\nclass TestEEGDataset(Dataset):\n    def __init__(self, csv_file, eeg_path):\n        self.csv = pd.read_csv(csv_file)  # Load the CSV file\n        self.eeg_path = eeg_path\n        self.sos = self.butter_bandpass_filter_init()  # Initialize the Butterworth filter parameters\n        self.FEATS = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2']\n        \n    def __len__(self):\n        return len(self.csv)\n    \n    def butter_bandpass_filter_init(self):\n        lowcut = 0.5  # Set the low-frequency cutoff for bandpass filtering\n        highcut = 45.0  # Set the high-frequency cutoff for bandpass filtering\n        fs = 200.0  # Sampling frequency\n        order = 5  # Filter order\n\n        nyq = 0.5 * fs\n        low = lowcut / nyq\n        high = highcut / nyq\n        sos = butter(order, [low, high], analog=False, btype='band', output='sos')  # Create second-order sections for the Butterworth filter\n        return sos\n\n    def butter_bandpass_filter(self, data):\n        y = sosfilt(self.sos, data)\n        return y\n\n    def __getitem__(self, idx):\n        eeg_id = self.csv.loc[idx, 'eeg_id']\n        eeg_file_path = f\"{self.eeg_path}{eeg_id}.parquet\"  # Build the EEG data file path\n        eeg_data = pd.read_parquet(eeg_file_path)[self.FEATS].values  # Load EEG data from Parquet file\n\n        eeg_data = self.butter_bandpass_filter(eeg_data)  # Apply filtering to the data\n        eeg_data = torch.tensor(eeg_data, dtype=torch.float32)  # Convert to PyTorch tensor\n         \n        # Select 10,000 data points from the middle\n        mid_index = eeg_data.shape[0] // 2\n        start_index = mid_index - 5000  # Offset 5000 data points to the left from the middle\n        end_index = mid_index + 5000  # Offset 5000 data points to the right from the middle\n        eeg_data = eeg_data[start_index:end_index]\n        # Swap dimensions\n        eeg_data = torch.transpose(eeg_data, 0, 1)\n        \n        return eeg_data\n\n# 创建 EEG 数据集实例\ntestdataset = TestEEGDataset(test_path, TEST_EEG_PATH)\n\n# 创建数据加载器\ntest_dataloader = DataLoader(testdataset, batch_size=32, shuffle=True, num_workers=4)\n\n# 2. 将模型设置为评估模式\nmodel.eval()\n\npredictions = []\n# 执行推理\nwith torch.no_grad():\n    for inputs in test_dataloader:  # 注意这里不需要 labels\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        # 处理输出，得到类别概率值\n        probabilities = torch.softmax(outputs, dim=1)\n        predictions.append(probabilities.cpu().numpy())\n\n# 4. 对预测结果进行处理，如转换为概率\npredictions = np.concatenate(predictions, axis=0)\n\n# 5. 输出预测结果\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:31:26.356735Z","iopub.execute_input":"2024-02-12T04:31:26.357752Z","iopub.status.idle":"2024-02-12T04:31:26.586931Z","shell.execute_reply.started":"2024-02-12T04:31:26.357713Z","shell.execute_reply":"2024-02-12T04:31:26.585744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建 DataFrame\ncolumns = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\nresults_df = pd.DataFrame(predictions, columns=columns)\n\n# 打印 DataFrame\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:31:34.702909Z","iopub.execute_input":"2024-02-12T04:31:34.703327Z","iopub.status.idle":"2024-02-12T04:31:34.714299Z","shell.execute_reply.started":"2024-02-12T04:31:34.703272Z","shell.execute_reply":"2024-02-12T04:31:34.713336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(test_path)\nsub = pd.DataFrame({'eeg_id':test_data.eeg_id.values})\nsub[TARGETS] = results_df\nsub.to_csv('submission.csv',index=False)\nprint('Submissionn shape',sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:31:36.756704Z","iopub.execute_input":"2024-02-12T04:31:36.757673Z","iopub.status.idle":"2024-02-12T04:31:36.779997Z","shell.execute_reply.started":"2024-02-12T04:31:36.757639Z","shell.execute_reply":"2024-02-12T04:31:36.779043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.iloc[:,-6:].sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T04:31:41.467215Z","iopub.execute_input":"2024-02-12T04:31:41.468068Z","iopub.status.idle":"2024-02-12T04:31:41.477132Z","shell.execute_reply.started":"2024-02-12T04:31:41.468036Z","shell.execute_reply":"2024-02-12T04:31:41.476062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}