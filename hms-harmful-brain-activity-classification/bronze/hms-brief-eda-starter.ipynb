{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Hello. Thank you reading my notebook. I am planning to add PyTorch Code in this notebook.","metadata":{}},{"cell_type":"markdown","source":"# 01 Pathing and EDA","metadata":{}},{"cell_type":"code","source":"import os\ndirectory_path = '/kaggle/input'\ncsv_file_paths = []\n\nfor root, dirs, files in os.walk(directory_path):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_file_path = os.path.join(root, file)\n            csv_file_paths.append(csv_file_path)\n\nfor csv_path in csv_file_paths:\n    print(csv_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:35:41.259966Z","iopub.execute_input":"2024-02-07T23:35:41.260264Z","iopub.status.idle":"2024-02-07T23:35:50.266514Z","shell.execute_reply.started":"2024-02-07T23:35:41.260237Z","shell.execute_reply":"2024-02-07T23:35:50.265586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\ntest_df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/sample_submission.csv')\n                       \ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:35:51.425914Z","iopub.execute_input":"2024-02-07T23:35:51.426256Z","iopub.status.idle":"2024-02-07T23:35:52.508198Z","shell.execute_reply.started":"2024-02-07T23:35:51.426228Z","shell.execute_reply":"2024-02-07T23:35:52.50723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 01.1: Pie Chart for Data Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\ntrain_df['expert_consensus'].value_counts().plot(kind='pie', autopct='%1.1f%%')\nplt.title('Distribution of expert_consensus')\nplt.xlabel('Expert Consensus')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\nunique_expert_consensus = train_df['expert_consensus'].unique()\nprint(\"Unique values in 'expert_consensus' column:\", unique_expert_consensus)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:35:55.528166Z","iopub.execute_input":"2024-02-07T23:35:55.528797Z","iopub.status.idle":"2024-02-07T23:35:55.74846Z","shell.execute_reply.started":"2024-02-07T23:35:55.528765Z","shell.execute_reply":"2024-02-07T23:35:55.74746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Check the percentage of each consensus in training data. They are distributed equally in approximation.","metadata":{}},{"cell_type":"markdown","source":"## 01.2: Correlation Heat Map for Consensus","metadata":{}},{"cell_type":"code","source":"train_df_encoded = pd.get_dummies(train_df, columns=['expert_consensus'], drop_first=True)\ncorrelation_matrix = train_df_encoded[['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']].corr()\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap between vote columns and expert_consensus')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:35:58.163638Z","iopub.execute_input":"2024-02-07T23:35:58.163991Z","iopub.status.idle":"2024-02-07T23:35:58.690479Z","shell.execute_reply.started":"2024-02-07T23:35:58.163962Z","shell.execute_reply":"2024-02-07T23:35:58.689594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 01.3 Kernel Distribution Plot for Label Second ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.kdeplot(train_df['spectrogram_label_offset_seconds'], shade=True)\nplt.title('Distribution of spectrogram_label_offset_seconds (KDE Plot)')\nplt.xlabel('Offset Seconds')\nplt.ylabel('Density')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:36:03.916378Z","iopub.execute_input":"2024-02-07T23:36:03.917173Z","iopub.status.idle":"2024-02-07T23:36:04.796332Z","shell.execute_reply.started":"2024-02-07T23:36:03.91713Z","shell.execute_reply":"2024-02-07T23:36:04.795442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vote_columns = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\nvote_stats = train_df[vote_columns].describe()\nprint(vote_stats)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:36:09.294108Z","iopub.execute_input":"2024-02-07T23:36:09.294458Z","iopub.status.idle":"2024-02-07T23:36:09.328359Z","shell.execute_reply.started":"2024-02-07T23:36:09.294432Z","shell.execute_reply":"2024-02-07T23:36:09.327373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 01.4 Box Plot","metadata":{}},{"cell_type":"code","source":"for column in vote_columns:\n    plt.figure(figsize=(8, 5))\n    sns.boxplot(x='expert_consensus', y=column, data=train_df)\n    plt.title(f'Distribution of {column} by expert_consensus')\n    plt.xlabel('expert_consensus')\n    plt.ylabel(column)\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:36:25.075197Z","iopub.execute_input":"2024-02-07T23:36:25.075952Z","iopub.status.idle":"2024-02-07T23:36:27.404661Z","shell.execute_reply.started":"2024-02-07T23:36:25.075916Z","shell.execute_reply":"2024-02-07T23:36:27.403772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [On Progress] 02 PyTorch Modeling","metadata":{}},{"cell_type":"code","source":"import os\ndirectory_path = '/kaggle/input'\nnon_csv_file_paths = []\n\nfor root, dirs, files in os.walk(directory_path):\n    for file in files:\n        if not file.endswith(\".csv\"): \n            file_path = os.path.join(root, file)\n            non_csv_file_paths.append(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:36:56.285626Z","iopub.execute_input":"2024-02-07T23:36:56.285972Z","iopub.status.idle":"2024-02-07T23:37:00.512714Z","shell.execute_reply.started":"2024-02-07T23:36:56.285946Z","shell.execute_reply":"2024-02-07T23:37:00.511925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Length of non_csv_file_paths: {len(non_csv_file_paths)}\")\nprint(f\"Length of train_df: {len(train_df)}\")\nprint(f\"Length of test_df: {len(test_df)}\")\nprint(f\"Length of sample_submission: {len(sample_submission)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:37:00.514169Z","iopub.execute_input":"2024-02-07T23:37:00.51446Z","iopub.status.idle":"2024-02-07T23:37:00.519673Z","shell.execute_reply.started":"2024-02-07T23:37:00.514434Z","shell.execute_reply":"2024-02-07T23:37:00.518847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 02.1 Data leakage or Data Overlap","metadata":{}},{"cell_type":"code","source":"overlap_ids = train_df[train_df['patient_id'].isin(test_df['patient_id'])]['patient_id']\ntrain = train_df[~train_df['patient_id'].isin(overlap_ids)]\n\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:45:29.703491Z","iopub.execute_input":"2024-02-07T23:45:29.704353Z","iopub.status.idle":"2024-02-07T23:45:29.732059Z","shell.execute_reply.started":"2024-02-07T23:45:29.704318Z","shell.execute_reply":"2024-02-07T23:45:29.731081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> No data leakage or overlap has been found","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:40:49.492772Z","iopub.execute_input":"2024-02-07T23:40:49.493614Z","iopub.status.idle":"2024-02-07T23:40:49.511788Z","shell.execute_reply.started":"2024-02-07T23:40:49.493583Z","shell.execute_reply":"2024-02-07T23:40:49.51092Z"}}},{"cell_type":"markdown","source":"## 02.2 Data Split","metadata":{}},{"cell_type":"markdown","source":"> Audio or image data, such as spectrograms, typically take up a large amount of capacity, and it is inefficient to load and process the entire data into memory at once.\n\n> Saving spectrogram data by dividing it into small pieces is a way to efficiently load and process data into memory. This gives you faster performance when processing the entire data by dividing it into small pieces than by processing it all at once.","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport gc\nimport copy\nimport yaml\nimport random\nimport shutil\nfrom time import time\nimport typing as tp\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.cuda import amp\n\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nOUTPUT = ROOT / \"output\"\nSRC = ROOT / \"src\"\n\nDATA = INPUT / \"hms-harmful-brain-activity-classification\"\nTRAIN_SPEC = DATA / \"train_spectrograms\"\nTEST_SPEC = DATA / \"test_spectrograms\"\n\nTMP = ROOT / \"tmp\"\nTRAIN_SPEC_SPLIT = TMP / \"train_spectrograms_split\"\nTEST_SPEC_SPLIT = TMP / \"test_spectrograms_split\"\nTMP.mkdir(exist_ok=True)\nTRAIN_SPEC_SPLIT.mkdir(exist_ok=True)\nTEST_SPEC_SPLIT.mkdir(exist_ok=True)\n\n\nRANDAM_SEED = 1086\nCLASSES = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\nN_CLASSES = len(CLASSES)\nFOLDS = [0, 1, 2, 3, 4]\nN_FOLDS = len(FOLDS)\n\ntrain = pd.read_csv(DATA / \"train.csv\")\n\n# convert vote to probability\ntrain[CLASSES] /= train[CLASSES].sum(axis=1).values[:, None]\ntrain = train.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\nsgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDAM_SEED)\n\ntrain[\"fold\"] = -1\n\nfor fold_id, (_, val_idx) in enumerate(\n    sgkf.split(train, y=train[\"expert_consensus\"], groups=train[\"patient_id\"])\n):\n    train.loc[val_idx, \"fold\"] = fold_id\ntrain.groupby(\"fold\")[CLASSES].sum()\nfor spec_id, df in tqdm(train.groupby(\"spectrogram_id\")):\n    spec = pd.read_parquet(TRAIN_SPEC / f\"{spec_id}.parquet\")\n    \n    spec_arr = spec.fillna(0).values[:, 1:].T.astype(\"float32\")  # (Hz, Time) = (400, 300)\n    \n    for spec_offset, label_id in df[\n        [\"spectrogram_label_offset_seconds\", \"label_id\"]\n    ].astype(int).values:\n        spec_offset = spec_offset // 2\n        split_spec_arr = spec_arr[:, spec_offset: spec_offset + 300]\n        np.save(TRAIN_SPEC_SPLIT / f\"{label_id}.npy\" , split_spec_arr)\n\nFilePath = tp.Union[str, Path]\nLabel = tp.Union[int, float, np.ndarray]","metadata":{"execution":{"iopub.status.busy":"2024-02-08T00:29:29.574748Z","iopub.execute_input":"2024-02-08T00:29:29.575476Z","iopub.status.idle":"2024-02-08T00:35:43.748298Z","shell.execute_reply.started":"2024-02-08T00:29:29.575439Z","shell.execute_reply":"2024-02-08T00:35:43.747408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 02.3  Dataset Load","metadata":{}},{"cell_type":"code","source":"class HMSHBACSpecModel(nn.Module):\n\n    def __init__(\n            self,\n            model_name: str,\n            pretrained: bool,\n            in_channels: int,\n            num_classes: int,\n        ):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name=model_name, pretrained=pretrained,\n            num_classes=num_classes, in_chans=in_channels)\n\n    def forward(self, x):\n        h = self.model(x)      \n\n        return h\n\nclass HMSHBACSpecDataset(torch.utils.data.Dataset):\n\n    def __init__(\n        self,\n        image_paths: tp.Sequence[FilePath],\n        labels: tp.Sequence[Label],\n        transform: A.Compose,\n    ):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index: int):\n        img_path = self.image_paths[index]\n        label = self.labels[index]\n\n        img = np.load(img_path)  # shape: (Hz, Time) = (400, 300)\n        \n        # log transform\n        img = np.clip(img,np.exp(-4), np.exp(8))\n        img = np.log(img)\n        \n        # normalize per image\n        eps = 1e-6\n        img_mean = img.mean(axis=(0, 1))\n        img = img - img_mean\n        img_std = img.std(axis=(0, 1))\n        img = img / (img_std + eps)\n\n        img = img[..., None] # shape: (Hz, Time) -> (Hz, Time, Channel)\n        img = self._apply_transform(img)\n\n        return {\"data\": img, \"target\": label}\n\n    def _apply_transform(self, img: np.ndarray):\n        \"\"\"apply transform to image and mask\"\"\"\n        transformed = self.transform(image=img)\n        img = transformed[\"image\"]\n        return img\n\nclass CrossEntropyLossWithLogits(nn.CrossEntropyLoss):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y, t):\n        loss = super().forward(y, torch.argmax(t, dim=1))\n        return loss\n\nclass CrossEntropyLossWithLogitsForVal(nn.CrossEntropyLoss):\n    def __init__(self):\n        super().__init__()\n        self.logits_list = []\n        self.label_list = []\n\n    def forward(self, y, t):\n        self.logits_list.append(y.cpu().numpy())\n        self.label_list.append(torch.argmax(t, dim=1).cpu().numpy())\n\n    def compute(self):\n        logits = np.concatenate(self.logits_list, axis=0)\n        label = np.concatenate(self.label_list, axis=0)\n        final_metric = super().forward(\n            torch.from_numpy(logits),\n            torch.from_numpy(label)\n        ).item()\n        self.logits_list = []\n        self.label_list = []\n\n        return final_metric\n\nclass CFG:\n    model_name = \"efficientnet_b0\"\n    img_size = 512\n    max_epoch = 9\n    batch_size = 32\n    lr = 1.0e-03\n    weight_decay = 1.0e-02\n    es_patience =  5\n    seed = 1086\n    deterministic = True\n    enable_amp = True\n    device = \"cuda\"\n\ndef set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n    \ndef to_device(\n    tensors: tp.Union[tp.Tuple[torch.Tensor], tp.Dict[str, torch.Tensor]],\n    device: torch.device, *args, **kwargs\n):\n    if isinstance(tensors, tuple):\n        return (t.to(device, *args, **kwargs) for t in tensors)\n    elif isinstance(tensors, dict):\n        return {\n            k: t.to(device, *args, **kwargs) for k, t in tensors.items()}\n    else:\n        return tensors.to(device, *args, **kwargs)\n\ndef get_path_label(train_all):\n    train_size = int(len(train_all) * 0.8)\n    val_size = len(train_all) - train_size\n    \n    # 데이터셋을 랜덤하게 분할하여 train_dataset과 val_dataset을 생성합니다.\n    train_dataset, val_dataset = torch.utils.data.random_split(range(len(train_all)), [train_size, val_size])\n    \n    # train_dataset에서 이미지 경로와 레이블을 가져옵니다.\n    train_data = {\n        \"image_paths\": [TRAIN_SPEC_SPLIT / f\"{train_all.loc[idx, 'label_id']}.npy\" for idx in train_dataset],\n        \"labels\": train_all.loc[train_dataset, CLASSES].values.astype(\"float32\")\n    }\n    \n    # val_dataset에서 이미지 경로와 레이블을 가져옵니다.\n    val_data = {\n        \"image_paths\": [TRAIN_SPEC_SPLIT / f\"{train_all.loc[idx, 'label_id']}.npy\" for idx in val_dataset],\n        \"labels\": train_all.loc[val_dataset, CLASSES].values.astype(\"float32\")\n    }\n    \n    return train_data, val_data\n\n\n\ndef get_transforms(CFG):\n    train_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size),\n        ToTensorV2(p=1.0)\n    ])\n    val_transform = A.Compose([\n        A.Resize(p=1.0, height=CFG.img_size, width=CFG.img_size),\n        ToTensorV2(p=1.0)\n    ])\n    return train_transform, val_transform\n\ndef train_one_fold(CFG, train_all, output_path):\n    \"\"\"Main\"\"\"\n    torch.backends.cudnn.benchmark = True\n    set_random_seed(CFG.seed, deterministic=CFG.deterministic)\n    device = torch.device(CFG.device)\n    \n    train_path_label, val_path_label = get_path_label(train_all)\n    train_transform, val_transform = get_transforms(CFG)\n    \n    train_dataset = HMSHBACSpecDataset(**train_path_label, transform=train_transform)\n    val_dataset = HMSHBACSpecDataset(**val_path_label, transform=val_transform)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=True, drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False, drop_last=False)\n    \n    model = HMSHBACSpecModel(\n        model_name=CFG.model_name, pretrained=True, num_classes=6, in_channels=1)\n    model.to(device)\n    \n    optimizer = optim.AdamW(params=model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    scheduler = lr_scheduler.OneCycleLR(\n        optimizer=optimizer, epochs=CFG.max_epoch,\n        pct_start=0.0, steps_per_epoch=len(train_loader),\n        max_lr=CFG.lr, div_factor=25, final_div_factor=4.0e-01\n    )\n    \n    loss_func = nn.CrossEntropyLoss()\n    loss_func.to(device)\n    loss_func_val = CrossEntropyLossWithLogitsForVal()\n    \n    use_amp = CFG.enable_amp\n    scaler = amp.GradScaler(enabled=use_amp)\n    \n    best_val_loss = 1.0e+09\n    best_epoch = 0\n    train_loss = 0\n    \n    for epoch in range(1, CFG.max_epoch + 1):\n        epoch_start = time()\n        model.train()\n        for batch in train_loader:\n            batch = to_device(batch, device)\n            x, t = batch[\"data\"], batch[\"target\"]\n                \n            optimizer.zero_grad()\n            with amp.autocast(use_amp):\n                y = model(x)\n                loss = loss_func(y, torch.argmax(t, dim=1))\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            train_loss += loss.item()\n            \n        train_loss /= len(train_loader)\n            \n        model.eval()\n        for batch in val_loader:\n            x, t = batch[\"data\"], batch[\"target\"]\n            x = to_device(x, device)\n            with torch.no_grad(), amp.autocast(use_amp):\n                y = model(x)\n            y = y.detach().cpu().to(torch.float32)\n            loss_func_val(y, t)\n        val_loss = loss_func_val.compute()        \n        if val_loss < best_val_loss:\n            best_epoch = epoch\n            best_val_loss = val_loss\n            # print(\"save model\")\n            torch.save(model.state_dict(), str(output_path / f'snapshot_epoch_{epoch}.pth'))\n        \n        elapsed_time = time() - epoch_start\n        print(\n            f\"[epoch {epoch}] train loss: {train_loss: .6f}, val loss: {val_loss: .6f}, elapsed_time: {elapsed_time: .3f}\")\n        \n        if epoch - best_epoch > CFG.es_patience:\n            print(\"Early Stopping!\")\n            break\n            \n        train_loss = 0\n            \n    return best_epoch, best_val_loss","metadata":{"execution":{"iopub.status.busy":"2024-02-08T00:37:49.604447Z","iopub.execute_input":"2024-02-08T00:37:49.604815Z","iopub.status.idle":"2024-02-08T00:37:49.643353Z","shell.execute_reply.started":"2024-02-08T00:37:49.604782Z","shell.execute_reply":"2024-02-08T00:37:49.642489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 02.4 Train","metadata":{}},{"cell_type":"code","source":"\nscore_list = []\nfor fold_id in FOLDS:\n    output_path = Path(f\"fold{fold_id}\")\n    output_path.mkdir(exist_ok=True)\n    print(f\"[fold{fold_id}]\")\n    train_data, val_data = get_path_label(train[train[\"fold\"] != fold_id])\n    score_list.append(train_one_fold(CFG, train_data, output_path))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T00:37:50.168897Z","iopub.execute_input":"2024-02-08T00:37:50.16969Z","iopub.status.idle":"2024-02-08T00:37:50.451694Z","shell.execute_reply.started":"2024-02-08T00:37:50.169659Z","shell.execute_reply":"2024-02-08T00:37:50.450407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}