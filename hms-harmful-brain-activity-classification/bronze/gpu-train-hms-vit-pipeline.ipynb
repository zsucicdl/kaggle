{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":158833581,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Acknowledgment\nThis notebook uses the preprocessed dataset created by [MUHAMMAD AHMED](https://www.kaggle.com/muhammad4hmed) and their notebook **titled [HMS] - Data Prepare - Separate Spectogram**. I would like to express my gratitude for their valuable contribution to the Kaggle community.\n\nLink to the original notebook: [Original Notebook Link](https://www.kaggle.com/code/muhammad4hmed/hms-data-prepare-separate-spectogram)","metadata":{}},{"cell_type":"markdown","source":"# Experiments Details\n* Model: ViT, Epoch: 1, BS: 32 , CV: , LB: 0.96\n* Model: ViT, Epoch: 15, BS: 32 , CV: , LB: \n\n[Inference Notebook](https://www.kaggle.com/dky7376/gpu-infer-hms-vit-pipeline)","metadata":{}},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\ntest = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\nclasses = train['expert_consensus'].unique()\nmapping = {\n    c:i for i, c in enumerate(classes)\n}\nnum_classes = classes.shape[0]\n\nprint(mapping)\nprint(num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:09:29.777853Z","iopub.execute_input":"2024-01-24T10:09:29.778164Z","iopub.status.idle":"2024-01-24T10:09:31.055443Z","shell.execute_reply.started":"2024-01-24T10:09:29.778132Z","shell.execute_reply":"2024-01-24T10:09:31.054257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:09:31.057494Z","iopub.execute_input":"2024-01-24T10:09:31.058536Z","iopub.status.idle":"2024-01-24T10:09:31.090135Z","shell.execute_reply.started":"2024-01-24T10:09:31.058497Z","shell.execute_reply":"2024-01-24T10:09:31.089066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:09:31.093419Z","iopub.execute_input":"2024-01-24T10:09:31.093889Z","iopub.status.idle":"2024-01-24T10:09:31.108774Z","shell.execute_reply.started":"2024-01-24T10:09:31.09384Z","shell.execute_reply":"2024-01-24T10:09:31.107586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"%%writefile train.py\n\nimport os\nimport argparse\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nfrom transformers import ViTModel\nimport matplotlib as cm\nfrom torch.nn.functional import softmax, one_hot\ncmap = cm.colormaps[\"viridis\"]\n\n# Define constants\nnum_classes = 6\nnum_epochs = 15\nbatch_size = 32\n\n# Define dataset class\nclass SpectrogramDataset(Dataset):\n    def __init__(self, file_paths, mapping, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n        self.mapping = mapping\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        label = os.path.basename(file_path).split(\"_\")[1].split(\".\")[0]  # Extract label from filename\n        spectrogram = pd.read_parquet(file_path).drop('time', axis=1).values  # Load parquet file\n        spectrogram = Image.fromarray((cmap(spectrogram) * 255).astype(np.uint8))\n        if self.transform:\n            spectrogram = self.transform(spectrogram)[:3, :, :]\n        return spectrogram, self.mapping[label]\n\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to 224x224\n    transforms.ToTensor(),  # Convert to PyTorch tensor\n])\n\n# Define model class\nclass ViTClassifier(torch.nn.Module):\n    def __init__(self, num_classes=1000):\n        super().__init__()\n        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n        self.classifier = torch.nn.Linear(self.vit.config.hidden_size, num_classes)\n\n    def forward(self, images):\n        output = self.vit(images)\n        output = self.classifier(output.last_hidden_state[:, 0]) \n        output = softmax(output, dim = 1)\n        return output\n\n# Training function\ndef train(model, train_loader, optimizer, scheduler, loss_fn, device):\n    model.train()\n    total_loss = 0.0\n\n    for spectrograms, labels in tqdm(train_loader, desc=\"Training\"):\n        labels_onehot = one_hot(labels, num_classes=num_classes).float().to(device)\n        spectrograms = spectrograms.to(device)\n        optimizer.zero_grad()\n        outputs = model(spectrograms)\n        loss = loss_fn(outputs.log(), labels_onehot)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\n# Validation function\ndef validate(model, val_loader, loss_fn, device):\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for spectrograms, labels in tqdm(val_loader, desc=\"Validation\"):\n            labels = labels.to(device)\n            labels_onehot = one_hot(labels, num_classes=num_classes).float()\n            spectrograms = spectrograms.to(device)\n            outputs = model(spectrograms)\n            _, predicted = torch.max(outputs.data, 1)    \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            loss = loss_fn(outputs.log(), labels_onehot).item()\n            val_loss += loss\n    accuracy = 100 * correct / total\n    kl_divergence = val_loss / len(val_loader)\n    return accuracy, kl_divergence\n\n# Define main function for training\ndef main():\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\"--train_path\", type=str)\n    parser.add_argument(\"--data_folder\", type=str)\n    \n    args = parser.parse_args()\n    \n    # Load the dataset\n    train_data = pd.read_csv(args.train_path)\n    classes = train_data['expert_consensus'].unique()\n    mapping = {\n        c:i for i, c in enumerate(classes)\n    }\n    num_classes = classes.shape[0]\n\n    # Check if GPU is available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Load dataset\n    file_paths = [os.path.join(args.data_folder, f) for f in os.listdir(args.data_folder)]\n    dataset = SpectrogramDataset(file_paths, mapping, transform=transform)\n\n    # Split dataset into training and validation sets\n    train_length = int(0.8 * len(dataset))\n    val_length = len(dataset) - train_length\n    train_set, val_set = torch.utils.data.random_split(dataset, [train_length, val_length])\n\n    # Create data loaders\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Instantiate model, loss function, and optimizer\n    model = ViTClassifier(num_classes).to(device)\n\n    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n    \n    # Set up the learning rate scheduler\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        train_loss = train(model, train_loader, optimizer, scheduler, loss_fn, device)\n        val_accuracy, val_kl_divergence = validate(model, val_loader, loss_fn, device)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} => \"\n              f\"Train Loss: {train_loss:.4f}, \"\n              f\"Validation Accuracy: {val_accuracy:.2f}%, \"\n              f\"Validation KL Divergence: {val_kl_divergence:.4f}\")\n\n    # Save the trained model\n    torch.save(model.state_dict(), \"trained_hms_vit_model_v4.pt\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:21:49.200735Z","iopub.execute_input":"2024-01-24T10:21:49.201118Z","iopub.status.idle":"2024-01-24T10:21:49.211219Z","shell.execute_reply.started":"2024-01-24T10:21:49.201088Z","shell.execute_reply":"2024-01-24T10:21:49.210328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_FOLDER = \"/kaggle/input/hms-data-prepare-separate-spectogram/separate_spectogram/\"\nTRAIN_DATASET = \"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:21:53.064123Z","iopub.execute_input":"2024-01-24T10:21:53.064523Z","iopub.status.idle":"2024-01-24T10:21:53.069244Z","shell.execute_reply.started":"2024-01-24T10:21:53.06449Z","shell.execute_reply":"2024-01-24T10:21:53.068227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate launch --num_processes 2  train.py \\\n  --train_path $TRAIN_DATASET \\\n  --data_folder $DATA_FOLDER","metadata":{"execution":{"iopub.status.busy":"2024-01-24T10:21:58.246376Z","iopub.execute_input":"2024-01-24T10:21:58.247173Z","iopub.status.idle":"2024-01-24T10:22:23.720442Z","shell.execute_reply.started":"2024-01-24T10:21:58.247133Z","shell.execute_reply":"2024-01-24T10:22:23.719189Z"},"trusted":true},"execution_count":null,"outputs":[]}]}