{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7422302,"sourceType":"datasetVersion","datasetId":4318509},{"sourceId":7422421,"sourceType":"datasetVersion","datasetId":4318585}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \\\n   --requirement /kaggle/input/hms-hbac-offline-libs/requirements.txt \\\n   --no-index \\\n   --find-links file:///kaggle/input/hms-hbac-offline-libs/wheels","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-22T15:42:59.629806Z","iopub.execute_input":"2024-01-22T15:42:59.630264Z","iopub.status.idle":"2024-01-22T15:43:10.359569Z","shell.execute_reply.started":"2024-01-22T15:42:59.630227Z","shell.execute_reply":"2024-01-22T15:43:10.357796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All imports in this code block\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport webcolors as wc\nimport math\n\nfrom koilerplate import INPUT_ROOT, WORKING_ROOT, TEMP_ROOT\nfrom pathlib import Path\nfrom enum import Enum\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\n\nfrom eeglib.helpers import Helper\nfrom eeglib.eeg import EEG\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.362506Z","iopub.execute_input":"2024-01-22T15:43:10.362874Z","iopub.status.idle":"2024-01-22T15:43:10.370534Z","shell.execute_reply.started":"2024-01-22T15:43:10.362844Z","shell.execute_reply":"2024-01-22T15:43:10.369077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up some basic file paths\nINPUT_PATH = Path(INPUT_ROOT)\nCOMPETITION_DATA_PATH = INPUT_PATH / \"hms-harmful-brain-activity-classification\"\nCOMPETITION_DATA_PATH","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.371878Z","iopub.execute_input":"2024-01-22T15:43:10.372479Z","iopub.status.idle":"2024-01-22T15:43:10.390847Z","shell.execute_reply.started":"2024-01-22T15:43:10.37245Z","shell.execute_reply":"2024-01-22T15:43:10.389354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the training set CSV\ntrain_info = pd.read_csv(COMPETITION_DATA_PATH/\"train.csv\")\ntrain_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.393765Z","iopub.execute_input":"2024-01-22T15:43:10.394183Z","iopub.status.idle":"2024-01-22T15:43:10.529245Z","shell.execute_reply.started":"2024-01-22T15:43:10.394151Z","shell.execute_reply":"2024-01-22T15:43:10.527601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic data visualisation\n\nThere's not a lot to look at in the basic training csv file. Just look at the expert categories to see if they are roughly the same order of magnitude each.","metadata":{}},{"cell_type":"markdown","source":"### Expert consensus\n\nJust look at the raw data from the training file.","metadata":{}},{"cell_type":"code","source":"# Visualise the expert consensus category\nsns.histplot(x=\"expert_consensus\", data=train_info)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.531497Z","iopub.execute_input":"2024-01-22T15:43:10.531961Z","iopub.status.idle":"2024-01-22T15:43:10.861947Z","shell.execute_reply.started":"2024-01-22T15:43:10.531921Z","shell.execute_reply":"2024-01-22T15:43:10.860914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Votes cast\n\nWe only know that there is a panel of experts, some presumably will not have voted and some may\nvote more based on either the spectrogram or the raw eeg.","metadata":{}},{"cell_type":"code","source":"# Do we know how many experts looked at each eeg trace?\n# Start by simply counting the votes for each entry \ntrain_info[\"vote_count\"] = (\n    train_info[\"seizure_vote\"] +\n    train_info[\"lpd_vote\"] +\n    train_info[\"gpd_vote\"] +\n    train_info[\"lrda_vote\"] +\n    train_info[\"grda_vote\"] +\n    train_info[\"other_vote\"]\n)\ntrain_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.863402Z","iopub.execute_input":"2024-01-22T15:43:10.86406Z","iopub.status.idle":"2024-01-22T15:43:10.892596Z","shell.execute_reply.started":"2024-01-22T15:43:10.864022Z","shell.execute_reply":"2024-01-22T15:43:10.890671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now get the maximum vote count based on EEG ID and Spectrogram ID\ntrain_info[\"max_vote_count_eeg\"] = train_info.groupby(\"eeg_id\")[\"vote_count\"].transform(\"max\")\ntrain_info[\"max_vote_count_spectrogram\"] = train_info.groupby(\"spectrogram_id\")[\"vote_count\"].transform(\"max\")\ntrain_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.894694Z","iopub.execute_input":"2024-01-22T15:43:10.895191Z","iopub.status.idle":"2024-01-22T15:43:10.937165Z","shell.execute_reply.started":"2024-01-22T15:43:10.895149Z","shell.execute_reply":"2024-01-22T15:43:10.935108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now test how many entries there are where the raw count is different from the max\ntotal_rows = train_info.shape[0]\nfewer_eeg_votes = train_info[train_info[\"vote_count\"] < train_info[\"max_vote_count_eeg\"]]\nfewer_eeg_votes_rows = fewer_eeg_votes.shape[0]\nfewer_spectrogram_votes = train_info[train_info[\"vote_count\"] < train_info[\"max_vote_count_spectrogram\"]]\nfewer_spectrogram_votes_rows = fewer_spectrogram_votes.shape[0]\ninconsistent_max_votes = train_info[train_info[\"max_vote_count_eeg\"] != train_info[\"max_vote_count_spectrogram\"]]\ninconsistent_max_votes_rows = inconsistent_max_votes.shape[0]\nprint(f\"{fewer_eeg_votes_rows=}/{total_rows}; {fewer_eeg_votes_rows/total_rows : .2%}\")\nprint(f\"{fewer_spectrogram_votes_rows=}/{total_rows}; {fewer_spectrogram_votes_rows/total_rows : .2%}\")\nprint(f\"{inconsistent_max_votes_rows=}/{total_rows}; {inconsistent_max_votes_rows/total_rows : .2%}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.938929Z","iopub.execute_input":"2024-01-22T15:43:10.93933Z","iopub.status.idle":"2024-01-22T15:43:10.966431Z","shell.execute_reply.started":"2024-01-22T15:43:10.939298Z","shell.execute_reply":"2024-01-22T15:43:10.964599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore further the inconsistent max votes\ninconsistent_fewer_eeg_votes = inconsistent_max_votes[\n    inconsistent_max_votes[\"max_vote_count_eeg\"] < inconsistent_max_votes[\"max_vote_count_spectrogram\"]\n]\ninconsistent_fewer_eeg_votes_rows = inconsistent_fewer_eeg_votes.shape[0]\ninconsistent_fewer_spectrogram_votes = inconsistent_max_votes[\n    inconsistent_max_votes[\"max_vote_count_spectrogram\"] < inconsistent_max_votes[\"max_vote_count_eeg\"]\n]\ninconsistent_fewer_spectrogram_votes_rows = inconsistent_fewer_spectrogram_votes.shape[0]\nprint(\n    f\"{inconsistent_fewer_eeg_votes_rows=}/{inconsistent_max_votes_rows}; \"\n    f\"{inconsistent_fewer_eeg_votes_rows/inconsistent_max_votes_rows : .2%}\"\n)\nprint(\n    f\"{inconsistent_fewer_spectrogram_votes_rows=}/{inconsistent_max_votes_rows}; \"\n    f\"{inconsistent_fewer_spectrogram_votes_rows/inconsistent_max_votes_rows : .2%}\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.968272Z","iopub.execute_input":"2024-01-22T15:43:10.968622Z","iopub.status.idle":"2024-01-22T15:43:10.98297Z","shell.execute_reply.started":"2024-01-22T15:43:10.968593Z","shell.execute_reply":"2024-01-22T15:43:10.981543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Discussion\n\nSo, what do we know:\n1. Sometimes experts will not agree on a label and will not cast a vote.\n2. The spectrogram IDs have more votes associated with them than the eeg ids.\n   this is probably consistent as the spectrograms cover a longer period of time\n   than the EEG traces.\n3. This probably means that the size of the panel could be derived from the max\n   number of votes associated with the spectrogram ID rather than the eeg ID.\n   \nTherefore, to convert our votes to probabilities we use the max count of spectrogram ID votes.","metadata":{}},{"cell_type":"markdown","source":"### Probabilities\n\nWe'll divide the vote numbers by the maximum number of votes grouped by spectrogram IDs","metadata":{}},{"cell_type":"code","source":"train_info[\"P_sz\"] = train_info[\"seizure_vote\"] / train_info[\"max_vote_count_spectrogram\"]\ntrain_info[\"P_lpd\"] = train_info[\"lpd_vote\"] / train_info[\"max_vote_count_spectrogram\"]\ntrain_info[\"P_gpd\"] = train_info[\"gpd_vote\"] / train_info[\"max_vote_count_spectrogram\"]\ntrain_info[\"P_lrda\"] = train_info[\"lrda_vote\"] / train_info[\"max_vote_count_spectrogram\"]\ntrain_info[\"P_grda\"] = train_info[\"grda_vote\"] / train_info[\"max_vote_count_spectrogram\"]\ntrain_info[\"P_other\"] = train_info[\"other_vote\"] / train_info[\"max_vote_count_spectrogram\"]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:10.989923Z","iopub.execute_input":"2024-01-22T15:43:10.990776Z","iopub.status.idle":"2024-01-22T15:43:11.009619Z","shell.execute_reply.started":"2024-01-22T15:43:10.990738Z","shell.execute_reply":"2024-01-22T15:43:11.007322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Work out probabilities across the whole training set\nP_all = np.asarray([\n    train_info[\"P_sz\"].sum(),\n    train_info[\"P_lpd\"].sum(),\n    train_info[\"P_gpd\"].sum(),\n    train_info[\"P_lrda\"].sum(),\n    train_info[\"P_grda\"].sum(),\n    train_info[\"P_other\"].sum()\n]) / total_rows\nP_all","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:11.011643Z","iopub.execute_input":"2024-01-22T15:43:11.012058Z","iopub.status.idle":"2024-01-22T15:43:11.022851Z","shell.execute_reply.started":"2024-01-22T15:43:11.012025Z","shell.execute_reply":"2024-01-22T15:43:11.022207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise as a table\nP_df = pd.DataFrame(data=P_all.reshape((1,6)), columns=[\"P_sz\", \"P_lpd\", \"P_gpd\", \"P_lrda\", \"P_grda\", \"P_other\"])\nP_df","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:11.024094Z","iopub.execute_input":"2024-01-22T15:43:11.024363Z","iopub.status.idle":"2024-01-22T15:43:11.041376Z","shell.execute_reply.started":"2024-01-22T15:43:11.02434Z","shell.execute_reply":"2024-01-22T15:43:11.039448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What's the probability sum across everything, don't expect this to be 1.0\nP_all.sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:11.043338Z","iopub.execute_input":"2024-01-22T15:43:11.04393Z","iopub.status.idle":"2024-01-22T15:43:11.052773Z","shell.execute_reply.started":"2024-01-22T15:43:11.043894Z","shell.execute_reply":"2024-01-22T15:43:11.051725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Level of agreement\n\nFrom the competition overview:\n\n- 'idealized': High level of expert agreement\n- 'proto patterns':  Cases where ~1/2 of experts give a label as “other” and ~1/2\n   give one of the remaining five labels.\n- 'edge cases': Where experts are approximately split between 2 of the 5 named patterns\n\n> Not that easy to program!","metadata":{}},{"cell_type":"code","source":"# My stab at categorizing the agreement based on the vague description\ndef compute_agreement(P_sz:float, P_lpd:float, P_gpd:float, P_lrda:float, P_grda:float, P_other:float) -> str:\n    agreement = \"none\"\n    # Because of the way we have computed the probabilities for a row,\n    # the parameters passed may not add to 1.0. Lets fix this first.\n    P_tot = P_sz + P_lpd + P_gpd + P_lrda + P_grda + P_other\n    P_sz = P_sz / P_tot\n    P_lpd = P_lpd / P_tot\n    P_gpd = P_gpd / P_tot\n    P_lrda = P_lrda / P_tot\n    P_grda = P_grda / P_tot\n    P_other = P_other / P_tot\n    # Now rank them by probability value\n    p_dict = { \"sz\": P_sz, \"lpd\": P_lpd, \"gpd\": P_gpd, \"lrda\": P_lrda, \"grda\": P_grda, \"other\": P_other}\n    p_list = list(p_dict.items())\n    p_list.sort(key=lambda kv: 1-kv[1])\n    # Ignore all but the three highest probabilities, get the top ranking categories\n    first = p_list[0][0]\n    second = p_list[1][0]\n    third = p_list[2][0]\n    # Re-scale by the ignored probabilities of the lower 3 categories\n    p_first = p_dict[first]\n    p_second = p_dict[second]\n    p_third = p_dict[third]\n    p_top_3 = p_first + p_second + p_third\n    p_first /= p_top_3\n    p_second /= p_top_3\n    p_third /= p_top_3\n    if p_first > 0.75:\n        # If not other then there is strong agreement, if is other then no agreement\n        if first != \"other\":\n            agreement = \"idealized\"\n    elif (p_first + p_second) > 0.75:\n        # First two categories combined are significant\n        if first == \"other\" or second == \"other\":\n            # We'll assume that this is a proto-pattern\n            agreement = \"proto-pattern\"\n        else:\n            # We have two of equal-ish ranking\n            agreement = \"edge-case\"\n    else:\n        # No agreement (already set)\n        ...\n    return agreement","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:11.05416Z","iopub.execute_input":"2024-01-22T15:43:11.054459Z","iopub.status.idle":"2024-01-22T15:43:11.064702Z","shell.execute_reply.started":"2024-01-22T15:43:11.054434Z","shell.execute_reply":"2024-01-22T15:43:11.063204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_info[\"agreement\"] = train_info.apply(\n    lambda row : compute_agreement(\n        row[\"P_sz\"], row[\"P_lpd\"], row[\"P_gpd\"], row[\"P_lrda\"], row[\"P_grda\"], row[\"P_other\"]\n    ),\n    axis=1\n)\ntrain_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:11.066325Z","iopub.execute_input":"2024-01-22T15:43:11.066782Z","iopub.status.idle":"2024-01-22T15:43:13.668335Z","shell.execute_reply.started":"2024-01-22T15:43:11.066748Z","shell.execute_reply":"2024-01-22T15:43:13.666855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise the expert consensus category\nsns.histplot(x=\"agreement\", data=train_info)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:13.670399Z","iopub.execute_input":"2024-01-22T15:43:13.670777Z","iopub.status.idle":"2024-01-22T15:43:13.930786Z","shell.execute_reply.started":"2024-01-22T15:43:13.670749Z","shell.execute_reply":"2024-01-22T15:43:13.929367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EEG Data exploration\n\nLoad in an arbitrary sample and have a look at the data","metadata":{}},{"cell_type":"code","source":"# Just choose a arbitrary sample to look at\nSAMPLE = 7_777\nsample_info = train_info.iloc[SAMPLE]\nsample_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:13.932395Z","iopub.execute_input":"2024-01-22T15:43:13.934912Z","iopub.status.idle":"2024-01-22T15:43:13.942808Z","shell.execute_reply.started":"2024-01-22T15:43:13.934869Z","shell.execute_reply":"2024-01-22T15:43:13.941708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the EEG data\nsample_eeg = pd.read_parquet(COMPETITION_DATA_PATH/\"train_eegs\"/f\"{sample_info['eeg_id']}.parquet\")\nsample_eeg","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:13.944215Z","iopub.execute_input":"2024-01-22T15:43:13.94457Z","iopub.status.idle":"2024-01-22T15:43:13.988646Z","shell.execute_reply.started":"2024-01-22T15:43:13.944539Z","shell.execute_reply":"2024-01-22T15:43:13.986784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a time channel to the data. The sampling rate is given in the competition data set info\nEEG_SAMPLING_RATE = 200\nsample_eeg[\"time_offset\"] = sample_eeg.index * (1/EEG_SAMPLING_RATE)\nsample_eeg","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:13.990379Z","iopub.execute_input":"2024-01-22T15:43:13.990771Z","iopub.status.idle":"2024-01-22T15:43:14.038184Z","shell.execute_reply.started":"2024-01-22T15:43:13.990736Z","shell.execute_reply":"2024-01-22T15:43:14.035521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract 10 seconds of data around the event location by slicing the dataframe\nEXTRACT_TIME = 10\neeg_label_begin = sample_info[\"spectrogram_label_offset_seconds\"] - (EXTRACT_TIME / 2)\neeg_label_end = eeg_label_begin + EXTRACT_TIME\nprint(f\"Extracting sample: {eeg_label_begin} <= time_offset < {eeg_label_end}.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.040502Z","iopub.execute_input":"2024-01-22T15:43:14.041553Z","iopub.status.idle":"2024-01-22T15:43:14.050965Z","shell.execute_reply.started":"2024-01-22T15:43:14.041491Z","shell.execute_reply":"2024-01-22T15:43:14.049124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extracted_eeg = sample_eeg[(sample_eeg[\"time_offset\"] >= eeg_label_begin) & (sample_eeg[\"time_offset\"] < eeg_label_end)].copy()\nextracted_eeg[\"time_offset\"] -= sample_info[\"spectrogram_label_offset_seconds\"]\nextracted_eeg","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.05324Z","iopub.execute_input":"2024-01-22T15:43:14.053697Z","iopub.status.idle":"2024-01-22T15:43:14.094854Z","shell.execute_reply.started":"2024-01-22T15:43:14.053655Z","shell.execute_reply":"2024-01-22T15:43:14.0938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single ended vs differential\n\nLooks like the data is single-ended (which is good) but we probably need to make it differential to produce the same type of plots in the sample data.","metadata":{}},{"cell_type":"code","source":"# What are the column names?\nfor col in sample_eeg.columns:\n    print(col)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.096934Z","iopub.execute_input":"2024-01-22T15:43:14.097974Z","iopub.status.idle":"2024-01-22T15:43:14.106167Z","shell.execute_reply.started":"2024-01-22T15:43:14.097923Z","shell.execute_reply":"2024-01-22T15:43:14.103842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Left lateral (LL)\nLL_EEG_CHANNELS = [\"Fp1-F7\", \"F7-T3\", \"T3-T5\", \"T5-O1\"]\n# Left parasagittal (LP)\nLP_EEG_CHANNELS = [\"Fp1-F3\", \"F3-C3\", \"C3-P3\", \"P3-O1\"]\n# Central\nCC_EEG_CHANNELS = [\"Fz-Cz\", \"Cz-Pz\"]\n# Right parasagittal (RP)\nRP_EEG_CHANNELS = [\"Fp2-F4\", \"F4-C4\", \"C4-P4\", \"P4-O2\"]\n# Right lateral (RL)\nRL_EEG_CHANNELS = [\"Fp2-F8\", \"F8-T4\", \"T4-T6\", \"T6-O2\"]\n# Auxiliary information columns\nAUX_EEG_COLUMNS = [ \"EKG\", \"time_offset\" ]\n\n# Define how we want our EEG channels to be constructed\nDIFF_EEG_COLUMNS = \\\n    LL_EEG_CHANNELS + \\\n    LP_EEG_CHANNELS + \\\n    CC_EEG_CHANNELS + \\\n    RP_EEG_CHANNELS + \\\n    RL_EEG_CHANNELS + \\\n    AUX_EEG_COLUMNS\nDIFF_EEG_COLUMNS","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.108204Z","iopub.execute_input":"2024-01-22T15:43:14.108612Z","iopub.status.idle":"2024-01-22T15:43:14.129023Z","shell.execute_reply.started":"2024-01-22T15:43:14.108577Z","shell.execute_reply":"2024-01-22T15:43:14.127695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a new dataframe with differential channels, rather than single ended\ndef make_differential(df:pd.DataFrame, columns:List[str] = DIFF_EEG_COLUMNS):\n    # Take copy as df is potentially a slice\n    df = df.copy();\n    to_drop: List[str] = []\n    for column in columns:\n        single_ended_channels = column.split(\"-\")\n        if len(single_ended_channels) == 2:\n            # We need to make the differential channel\n            df[column] = df[single_ended_channels[0]] - df[single_ended_channels[1]]\n            if single_ended_channels[0] not in to_drop:\n                to_drop.append(single_ended_channels[0])\n            if single_ended_channels[1] not in to_drop:\n                to_drop.append(single_ended_channels[1])\n    # Drop non-differential columns\n    df.drop(to_drop, axis=1, inplace=True)\n    # Now return the dataframe in the correwct column order\n    return df[columns]    ","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.129885Z","iopub.execute_input":"2024-01-22T15:43:14.130251Z","iopub.status.idle":"2024-01-22T15:43:14.139932Z","shell.execute_reply.started":"2024-01-22T15:43:14.130224Z","shell.execute_reply":"2024-01-22T15:43:14.138396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the differential function\nextracted_diff_eeg = make_differential(extracted_eeg)\nextracted_diff_eeg","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.141334Z","iopub.execute_input":"2024-01-22T15:43:14.141908Z","iopub.status.idle":"2024-01-22T15:43:14.194644Z","shell.execute_reply.started":"2024-01-22T15:43:14.141878Z","shell.execute_reply":"2024-01-22T15:43:14.193129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting\n\nWe're going to define a plotting process that can plot the dataframe columns as a grouped, stacked line plot using a combination of seaborn and matplot libraries.","metadata":{}},{"cell_type":"code","source":"# Define s dataclass that defines a group of channels to be plotted\n@dataclass\nclass PlotGroup:\n    channels: List[str]\n    fg_color: str\n    bg_color: str","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.197899Z","iopub.execute_input":"2024-01-22T15:43:14.198453Z","iopub.status.idle":"2024-01-22T15:43:14.205929Z","shell.execute_reply.started":"2024-01-22T15:43:14.198412Z","shell.execute_reply":"2024-01-22T15:43:14.203928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function will plot a group of channels from the dataframe\ndef eeg_group_plot(df: pd.DataFrame, x: str, plot_group: PlotGroup, axes:List[plt.axis], xlim:Tuple[float, float] = (-5.0, +5.0)):\n    for channel, ax in zip(plot_group.channels, axes):\n        sns.lineplot(data=df, x=x, y=channel, ax=ax, color=plot_group.fg_color)\n        ax.set_facecolor(plot_group.bg_color)\n        ax.set_xlim(xlim)\n        ax.set_ylabel(channel, rotation=0, fontsize=12, horizontalalignment='right', verticalalignment='center')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.208122Z","iopub.execute_input":"2024-01-22T15:43:14.208477Z","iopub.status.idle":"2024-01-22T15:43:14.222206Z","shell.execute_reply.started":"2024-01-22T15:43:14.208452Z","shell.execute_reply":"2024-01-22T15:43:14.219975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function will plot and stack multiple groups together\ndef eeg_plot(df: pd.DataFrame, x: str, plot_groups: List[plt.axis], fig_height=12, fig_width=12):\n    num_channels = 0;\n    for plot_group in plot_groups:\n        num_channels += len(plot_group.channels)\n    figure, axes = plt.subplots(num_channels, 1)\n    figure.subplots_adjust(hspace=0)\n    figure.set_figheight(fig_height)\n    figure.set_figwidth(fig_width)\n    axis_num = 0\n    for plot_group in plot_groups:\n        num_group_chans = len(plot_group.channels)\n        group_axes = axes[axis_num:axis_num+num_group_chans]\n        eeg_group_plot(df, x, plot_group, group_axes)\n        axis_num += num_group_chans\n    figure.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.227854Z","iopub.execute_input":"2024-01-22T15:43:14.228296Z","iopub.status.idle":"2024-01-22T15:43:14.237394Z","shell.execute_reply.started":"2024-01-22T15:43:14.228262Z","shell.execute_reply":"2024-01-22T15:43:14.235383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now define what the plot groupings are and what the colours are to use.\n# We'll plot from the left side of the head to the right side, front to back.\n# We'll use red for left size and green for right (as per international navigation lights!)\n# EKG will be last channel\nDIFF_PLOT_GROUPS = [\n    PlotGroup([\"Fp1-F7\", \"F7-T3\", \"T3-T5\", \"T5-O1\"], 'red', wc.CSS3_NAMES_TO_HEX[\"lightpink\"]),\n    PlotGroup([\"Fp1-F3\", \"F3-C3\", \"C3-P3\", \"P3-O1\"], 'red', wc.CSS3_NAMES_TO_HEX[\"lightsalmon\"]),\n    PlotGroup([\"Fz-Cz\", \"Cz-Pz\"], 'black', wc.CSS3_NAMES_TO_HEX[\"gainsboro\"]),\n    PlotGroup([\"Fp2-F4\", \"F4-C4\", \"C4-P4\", \"P4-O2\"], 'green', wc.CSS3_NAMES_TO_HEX[\"darkseagreen\"]),\n    PlotGroup([\"Fp2-F8\", \"F8-T4\", \"T4-T6\", \"T6-O2\"], 'green', wc.CSS3_NAMES_TO_HEX[\"lightseagreen\"]),\n    PlotGroup([\"EKG\"], 'blue', wc.CSS3_NAMES_TO_HEX[\"powderblue\"]),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.238746Z","iopub.execute_input":"2024-01-22T15:43:14.239142Z","iopub.status.idle":"2024-01-22T15:43:14.254227Z","shell.execute_reply.started":"2024-01-22T15:43:14.239111Z","shell.execute_reply":"2024-01-22T15:43:14.252853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now plot the unprocessed data\neeg_plot(extracted_diff_eeg, 'time_offset', DIFF_PLOT_GROUPS)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:14.25548Z","iopub.execute_input":"2024-01-22T15:43:14.255823Z","iopub.status.idle":"2024-01-22T15:43:17.162106Z","shell.execute_reply.started":"2024-01-22T15:43:14.255794Z","shell.execute_reply":"2024-01-22T15:43:17.16106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EEGLIB preprocessing\n\neeglib has sparse documentation and may/may not be useful. Let's just explore what can be done with the preprocessing functions it provides.","metadata":{}},{"cell_type":"markdown","source":"### eeglib Helper class\n\nLooks like data has to start in a `Helper` object and we need to create this from a \nnumpy array (as there's no way to directly import a parquet file).\n\n**However:** If you simply create a `Helper` object from a numpy array the `Helper` \nonly saves the reference to the data and then, potentially, modifies it during \npre-processing. This is not a problem in a forward-running pipeline but when messing\naround in the Notebook this can have unintended side-effects.\n\nTherefore we create a helper function that safely creates the helper from the\nDataFrame without danger of modifying the input dataframe.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass BandPass:\n    low_cutoff: float|None\n    high_cutoff: float|None","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:17.163532Z","iopub.execute_input":"2024-01-22T15:43:17.163859Z","iopub.status.idle":"2024-01-22T15:43:17.169488Z","shell.execute_reply.started":"2024-01-22T15:43:17.163828Z","shell.execute_reply":"2024-01-22T15:43:17.168057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Makes an eeglib helper without potential side effects on the input dataframe\n# Returns the helper and the data dropped when we made the helper\ndef df_to_eeglib_helper(\n    df: pd.DataFrame,\n    columns: List[str]=DIFF_EEG_COLUMNS,\n    drop_columns: List[str]=AUX_EEG_COLUMNS,\n    sample_rate:int=EEG_SAMPLING_RATE, \n    window_size:int|None=None,\n    band_pass:BandPass|None=None,\n    normalize:bool=False,\n    ica:bool=False\n) -> Helper:\n    required_cols = [col for col in columns if col not in drop_columns]\n    # Here is all important copy, gives us a new data array to be mutated by eeglib\n    copy_df = df[required_cols].copy()\n    dropped_df = df[drop_columns].copy()\n    data = copy_df.to_numpy().transpose()\n    helper = Helper(\n        data, \n        sampleRate=sample_rate, \n        names=required_cols, \n        windowSize=window_size,\n        highpass=band_pass.low_cutoff if (band_pass and band_pass.low_cutoff) else None,\n        lowpass=band_pass.high_cutoff if (band_pass and band_pass.high_cutoff) else None,\n        normalize=normalize,\n        ICA=ica,\n    )\n    return helper, dropped_df;","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:17.171119Z","iopub.execute_input":"2024-01-22T15:43:17.171486Z","iopub.status.idle":"2024-01-22T15:43:17.185451Z","shell.execute_reply.started":"2024-01-22T15:43:17.171432Z","shell.execute_reply":"2024-01-22T15:43:17.183565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert a eeglib EEG object back into a standard pandas dataframe\ndef eeg_to_df(\n    eeg: EEG, \n    eeg_channels:List[str], \n    restore_df:pd.DataFrame|None, \n    columns:List[str]=DIFF_EEG_COLUMNS\n):\n    df = pd.DataFrame(eeg.window.window.transpose(), columns=eeg_channels, copy=True)\n    if (restore_df is not None):\n        df = df.join(restore_df.reset_index())\n    return df[columns]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:17.187316Z","iopub.execute_input":"2024-01-22T15:43:17.187751Z","iopub.status.idle":"2024-01-22T15:43:17.204606Z","shell.execute_reply.started":"2024-01-22T15:43:17.18771Z","shell.execute_reply":"2024-01-22T15:43:17.202751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"helper, aux_df = df_to_eeglib_helper(\n    extracted_diff_eeg, \n    window_size=EEG_SAMPLING_RATE*EXTRACT_TIME,\n    band_pass=BandPass(0.8, 40.0),\n    normalize=True,\n)\n# Now we need to get an EEG object\n# Note, looks like a bug in eeglib, can only iterate once so we'll collect the eegs in a list.\n# There is only one eeg as we've told it the window size is the entire eeg\neegs = [eeg for eeg in helper]\neeg = eegs[0]\neeglib_modified_df = eeg_to_df(eeg, helper.names, aux_df)\neeg_plot(eeglib_modified_df, 'time_offset', DIFF_PLOT_GROUPS)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:17.206525Z","iopub.execute_input":"2024-01-22T15:43:17.207057Z","iopub.status.idle":"2024-01-22T15:43:20.003874Z","shell.execute_reply.started":"2024-01-22T15:43:17.206974Z","shell.execute_reply":"2024-01-22T15:43:20.002113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### eeglib data input summary\n\nSo, we've ingressed some of the spectrogram data and it's been pre-processed (basically filtered and normalized).\n\n> Note that I can't get the ICA option to work, ignoring this!\n\nIt looks reasonably sensible although there are definite edge effects (looking at the training edge caused by the\nband-pass filter.","metadata":{}},{"cell_type":"markdown","source":"## eeglib features\n\nSo, why might we use eeglib?? The main reason appears to be that it can decompose\nthe time sequence eeg into a set of features. \nThere is a list in the [eeglib features documentation](https://eeglib.readthedocs.io/en/latest/features.html)\n\nThere _may_ be some visual correlations discernable between the features it produces\nand the expert votes in the training data.\n\nSo how to do this:\n\n1. Read in an EEG completeley into an eeglib helper.\n2. Decompose this into windows, by default eeglib will make 1 second windows.\n3. Generate a feature set, per channel, for each of the 1 second segments.\n4. Do some data exploration for correlation between expert votes and eeglib features.\n","metadata":{}},{"cell_type":"code","source":"# We still have the saw specrogram data in sample_eeg\nsample_eeg_diff = make_differential(sample_eeg)\nsample_eeg_diff","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:20.006364Z","iopub.execute_input":"2024-01-22T15:43:20.006758Z","iopub.status.idle":"2024-01-22T15:43:20.058024Z","shell.execute_reply.started":"2024-01-22T15:43:20.006724Z","shell.execute_reply":"2024-01-22T15:43:20.056542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature generation\n\nDon't really know much about any of these feature types but we'll generate the single channel\nfeatures available in eeglib for the entire sample sliced into one second chunks","metadata":{}},{"cell_type":"code","source":"# For a sample set generate lists of all the channel features\nfeature_helper, _ = df_to_eeglib_helper(\n    sample_eeg_diff, \n    band_pass=BandPass(0.8, 40.0),\n    normalize=True,\n)\nbp_alpha = []\nbp_beta = []\nbp_delta = []\nbp_theta = []\npfd = []\nhfd = []\nhjorth_activity = []\nhjorth_mobility = []\nhjorth_complexity = []\nsamp_en = []\nlzc = []\ndfa = []\nfor eeg in feature_helper:\n    bp = eeg.bandPower()\n    # Split band power and convert to db\n    bp_alpha.append([10.0 * math.log10(ch[\"alpha\"]) for ch in bp])\n    bp_beta.append([10.0 * math.log10(ch[\"beta\"]) for ch in bp])\n    bp_delta.append([10.0 * math.log10(ch[\"delta\"]) for ch in bp])\n    bp_theta.append([10.0 * math.log10(ch[\"theta\"]) for ch in bp])\n    pfd.append(eeg.PFD())\n    hfd.append(eeg.HFD())\n    hjorth_activity.append(eeg.hjorthActivity())\n    hjorth_mobility.append(eeg.hjorthMobility())\n    hjorth_complexity.append(eeg.hjorthComplexity())\n    samp_en.append(eeg.sampEn())\n    lzc.append(eeg.LZC())\n    dfa.append(eeg.DFA())\nprint(\"Done generating features\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:20.059729Z","iopub.execute_input":"2024-01-22T15:43:20.06014Z","iopub.status.idle":"2024-01-22T15:43:27.297464Z","shell.execute_reply.started":"2024-01-22T15:43:20.0601Z","shell.execute_reply":"2024-01-22T15:43:27.296053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now make this into a dataframe\ndf_as_dict = {}\nfor chan_idx, col_name in enumerate(feature_helper.names):\n    df_as_dict[f\"{col_name}.bp_alpha\"] = [feature_values[chan_idx] for feature_values in bp_alpha]\n    df_as_dict[f\"{col_name}.bp_beta\"] = [feature_values[chan_idx] for feature_values in bp_beta]\n    df_as_dict[f\"{col_name}.bp_delta\"] = [feature_values[chan_idx] for feature_values in bp_delta]\n    df_as_dict[f\"{col_name}.bp_theta\"] = [feature_values[chan_idx] for feature_values in bp_theta]\n    df_as_dict[f\"{col_name}.pfd\"] = [feature_values[chan_idx] for feature_values in pfd]\n    df_as_dict[f\"{col_name}.hfd\"] = [feature_values[chan_idx] for feature_values in hfd]\n    df_as_dict[f\"{col_name}.hjorth_activity\"] = [feature_values[chan_idx] for feature_values in hjorth_activity]\n    df_as_dict[f\"{col_name}.hjorth_mobility\"] = [feature_values[chan_idx] for feature_values in hjorth_mobility]\n    df_as_dict[f\"{col_name}.hjorth_complexity\"] = [feature_values[chan_idx] for feature_values in hjorth_complexity]\n    df_as_dict[f\"{col_name}.samp_en\"] = [feature_values[chan_idx] for feature_values in samp_en]\n    df_as_dict[f\"{col_name}.lzc\"] = [feature_values[chan_idx] for feature_values in lzc]\n    df_as_dict[f\"{col_name}.dfa\"] = [feature_values[chan_idx] for feature_values in dfa]\nfeature_df = pd.DataFrame.from_dict(df_as_dict)\nfeature_df\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.29888Z","iopub.execute_input":"2024-01-22T15:43:27.299292Z","iopub.status.idle":"2024-01-22T15:43:27.364655Z","shell.execute_reply.started":"2024-01-22T15:43:27.29926Z","shell.execute_reply":"2024-01-22T15:43:27.363127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now normalize each column using min-max normalization (feature scaling)\n# as better approriate for db scale values and feature values than zscoring\nfeature_df = (feature_df - feature_df.min()) / (feature_df.max() - feature_df.min())\nfeature_df","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.367136Z","iopub.execute_input":"2024-01-22T15:43:27.36757Z","iopub.status.idle":"2024-01-22T15:43:27.422019Z","shell.execute_reply.started":"2024-01-22T15:43:27.367517Z","shell.execute_reply":"2024-01-22T15:43:27.419848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add time channel\nfeature_df = feature_df.copy() # This is because pandas warns the dataframe is fragmented!\nfeature_df[\"time_offset\"] = feature_df.index * 1.0\nfeature_df","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.424726Z","iopub.execute_input":"2024-01-22T15:43:27.42526Z","iopub.status.idle":"2024-01-22T15:43:27.463835Z","shell.execute_reply.started":"2024-01-22T15:43:27.425216Z","shell.execute_reply":"2024-01-22T15:43:27.462609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There's a lot of numbers here! Try boiling some of it down into mean values per region...\nEEG_REGIONS = [\n    (\"LL\", LL_EEG_CHANNELS),\n    (\"LP\", LP_EEG_CHANNELS),\n    (\"CC\", CC_EEG_CHANNELS),\n    (\"RP\", RP_EEG_CHANNELS),\n    (\"RL\", RL_EEG_CHANNELS),\n]\nFEATURE_NAMES = [\n    \"bp_alpha\", \n    \"bp_beta\", \n    \"bp_delta\", \n    \"bp_theta\", \n    \"pfd\", \n    \"hfd\", \n    \"hjorth_activity\",\n    \"hjorth_mobility\", \n    \"hjorth_complexity\", \n    \"samp_en\", \n    \"lzc\", \n    \"dfa\"\n]\nfor region_name, region_chans in EEG_REGIONS:\n    for feature_name in FEATURE_NAMES:\n        region_feature_chans = [f\"{chan}.{feature_name}\" for chan in region_chans]\n        feature_df[f\"{region_name}.{feature_name}\"] = feature_df[region_feature_chans].mean(axis=1)\nfeature_df","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.465604Z","iopub.execute_input":"2024-01-22T15:43:27.465974Z","iopub.status.idle":"2024-01-22T15:43:27.609647Z","shell.execute_reply.started":"2024-01-22T15:43:27.465946Z","shell.execute_reply":"2024-01-22T15:43:27.607589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pivot the training data on time axis\n\nIn order to do some visualisation of the features we need to map the training\ndata into a time series.","metadata":{}},{"cell_type":"code","source":"required_columns = [\n    \"eeg_label_offset_seconds\", \n    \"expert_consensus\", \n    \"P_sz\", \"P_lpd\", \"P_gpd\", \"P_lrda\", \"P_grda\", \"P_other\",\n    \"agreement\"\n]\nsample_train_info = train_info[\n    train_info[\"eeg_id\"] == sample_info.eeg_id\n][required_columns].copy()\nsample_train_info","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.611473Z","iopub.execute_input":"2024-01-22T15:43:27.611903Z","iopub.status.idle":"2024-01-22T15:43:27.641621Z","shell.execute_reply.started":"2024-01-22T15:43:27.611868Z","shell.execute_reply":"2024-01-22T15:43:27.640273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_features = feature_df.copy()\nlabelled_features = labelled_features.join(\n    sample_train_info.set_index(\"eeg_label_offset_seconds\"), \n    on=\"time_offset\", \n    rsuffix=\"_label\"\n)\nlabelled_features","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.643345Z","iopub.execute_input":"2024-01-22T15:43:27.644241Z","iopub.status.idle":"2024-01-22T15:43:27.68375Z","shell.execute_reply.started":"2024-01-22T15:43:27.644196Z","shell.execute_reply":"2024-01-22T15:43:27.682294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix labels and probabilities\nprobabilities = [\"P_sz\", \"P_lpd\", \"P_gpd\", \"P_lrda\", \"P_grda\", \"P_other\"]\ncategories = [\"expert_consensus\", \"agreement\"]\nlabelled_features[probabilities] = labelled_features[probabilities].replace(np.nan, 0)\nlabelled_features[categories] = labelled_features[categories].replace(np.nan, \"na\")\nlabelled_features","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.685258Z","iopub.execute_input":"2024-01-22T15:43:27.68559Z","iopub.status.idle":"2024-01-22T15:43:27.72883Z","shell.execute_reply.started":"2024-01-22T15:43:27.685561Z","shell.execute_reply":"2024-01-22T15:43:27.727652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"REGION_MNEMONICS = [mne for mne, _ in EEG_REGIONS]\nREGION_MNEMONICS","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.732245Z","iopub.execute_input":"2024-01-22T15:43:27.732789Z","iopub.status.idle":"2024-01-22T15:43:27.742696Z","shell.execute_reply.started":"2024-01-22T15:43:27.732749Z","shell.execute_reply":"2024-01-22T15:43:27.740805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_violinplot(\n    data: pd.DataFrame, \n    feature_name:str,\n    x:str=\"expert_consensus\",\n    regions:List[str]=REGION_MNEMONICS,\n    fig_height:float=4,\n    fig_width:float=12\n) -> None:\n    figure, axes = plt.subplots(1, len(regions))\n    figure.subplots_adjust(wspace=0)\n    figure.set_figheight(fig_height)\n    figure.set_figwidth(fig_width)\n    visible_yaxis = True\n    for region, ax in zip(regions, axes):\n        sns.violinplot(data=data, x=x, y=f\"{region}.{feature_name}\", ax=ax)\n        ax.set_ylim((0.0, 1.0))\n        ax.title.set_text(region)\n        ax.yaxis.set_visible(visible_yaxis)\n        ax.set_xlabel(None)\n        ax.set_ylabel(None)\n        visible_yaxis = False\n    figure.supylabel(f\"{feature_name.upper()}\\nby region\")\n    figure.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.744292Z","iopub.execute_input":"2024-01-22T15:43:27.744653Z","iopub.status.idle":"2024-01-22T15:43:27.756463Z","shell.execute_reply.started":"2024-01-22T15:43:27.744623Z","shell.execute_reply":"2024-01-22T15:43:27.753753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature_name in FEATURE_NAMES:\n    feature_violinplot(labelled_features, feature_name, fig_height=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:43:27.758616Z","iopub.execute_input":"2024-01-22T15:43:27.759016Z","iopub.status.idle":"2024-01-22T15:43:35.510831Z","shell.execute_reply.started":"2024-01-22T15:43:27.758964Z","shell.execute_reply":"2024-01-22T15:43:35.509414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Discussion\n\nSo can we see any correlation, I'm not sure however what we are looking at is\nstatistically insignificant - we're only looking at a single EEG sequence from\na single patient.\n\nThe next step would be to feature-ise the entire training data and build a\ndataset containing all of the features across all of the traces.\n\nThen, maybe, we'll see some sort of correlation...","metadata":{}}]}