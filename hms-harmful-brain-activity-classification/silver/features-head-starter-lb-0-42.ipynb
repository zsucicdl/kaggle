{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782},{"sourceId":7523911,"sourceType":"datasetVersion","datasetId":4382744},{"sourceId":7570342,"sourceType":"datasetVersion","datasetId":4407194},{"sourceId":7589178,"sourceType":"datasetVersion","datasetId":4417235}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":279.932947,"end_time":"2024-02-07T17:27:46.449789","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-07T17:23:06.516842","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Features+Head Starter for HMS Brain Comp\nThis is Features+Head Starter notebook for Kaggle's HMS brain comp. This model uses Kaggle's spectrograms and Chris's EEG spectrograms(modified version). This Features+Head Starter achieves CV 0.58 and LB 0.42!\n\nFeatures+Head Starter uses Chris Deotte's Kaggle dataset [here][1]. This dataset is a single file which contains all of Kaggle's 11,138 spectrogram parquets. Reading this single file is much faster than reading 11k separate files. Don't forget to upvote his Kaggle [dataset][1] too! \n\nAlso Uses Chris's EEG spectrograms [here][3] (modified version) \n\n### Train and Infer Tips\n\nThis notebook can be used both to train and submit (infer) to Kaggle LB. When training, you can set variable `submission = False` , you can also set `TEST_MODE = TRUE` to upload 500 samples queckly instead of the whole dataset for testing. \n\nFor submission after training models, you should save them in the LOAD_MODELS_FROM dataset, then run this notebook with `submission = True`.\n\nThis notebook is made as generic as possible to expand and try different experiments.\n\nWhat you could do:\n- Resize images with `IMG_SIZE`\n- Change EfficientNetB(0-7) with `LOAD_BACKBONE_FROM`\n- Data augmentation by setting DataGenerator's parameter to `augment = True`\n\nMany other experiments could be done by modifying code, such as:\n- Input augmentation, the data generator outputs (400x600x3), but it is using a single channel randomly, the other two channels are empty, you could utilize those slot.\n- Custom loss fucntions.\n- Learning Rate scheduler.\n\nThis notebook is a direct descendent of Chris's notebook [here][2]\n\n[1]: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n[2]: https://www.kaggle.com/code/cdeotte/efficientnetb2-starter-lb-0-57\n[3]: https://www.kaggle.com/datasets/nartaa/eeg-spectrograms","metadata":{"papermill":{"duration":0.006778,"end_time":"2024-02-07T17:23:10.119762","exception":false,"start_time":"2024-02-07T17:23:10.112984","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport tensorflow\nimport tensorflow.keras.backend as K\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\n\nIMG_SIZE = (400,600)\nVER = 14\nLOAD_BACKBONE_FROM = '/kaggle/input/efficientnetb-tf-keras/EfficientNetB2.h5'\nLOAD_MODELS_FROM = '/kaggle/input/features-head-starter-models/'\nTEST_MODE = False\nsubmission = True\nnp.random.seed(42)\n\n# USE SINGLE GPU, MULTIPLE GPUS \ngpus = tf.config.list_physical_devices('GPU')\n# WE USE MIXED PRECISION\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nif len(gpus)>1:\n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')\nelse:\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')","metadata":{"papermill":{"duration":13.702195,"end_time":"2024-02-07T17:23:23.827764","exception":false,"start_time":"2024-02-07T17:23:10.125569","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:27.820146Z","iopub.execute_input":"2024-02-08T14:16:27.820603Z","iopub.status.idle":"2024-02-08T14:16:32.141087Z","shell.execute_reply.started":"2024-02-08T14:16:27.82056Z","shell.execute_reply":"2024-02-08T14:16:32.140071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and create Non-Overlapping Eeg Id Train Data\nThe competition data description says that test data does not have multiple crops from the same `eeg_id`. Therefore we will train and validate using only 1 crop per `eeg_id`. There is a discussion about this [here][1].\n\nEEGs with many NANs is removed. They can be kept if we comment out the code.\n\n[1]: https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021","metadata":{"papermill":{"duration":0.005556,"end_time":"2024-02-07T17:23:23.84064","exception":false,"start_time":"2024-02-07T17:23:23.835084","status":"completed"},"tags":[]}},{"cell_type":"code","source":"TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\nif not submission:\n    train = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n    TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n    META = ['spectrogram_id','spectrogram_label_offset_seconds','patient_id','expert_consensus']\n    train = train.groupby('eeg_id')[META+TARGETS\n                           ].agg({**{m:'first' for m in META},**{t:'sum' for t in TARGETS}}).reset_index()\n    train[TARGETS] = train[TARGETS]/train[TARGETS].values.sum(axis=1,keepdims=True)\n    train.columns = ['eeg_id','spec_id','offset','patient_id','target'] + TARGETS\n    train.head(1)\n\n    # REMOVE EEGs WITH MORE THAN 150 NANs\n    eeg_nans = np.load('/kaggle/input/eeg-spectrograms/eeg_nans.npy', allow_pickle=True).item()\n    df = pd.DataFrame.from_dict(eeg_nans, orient='index',columns=['nans']).reset_index(names='eeg_id')\n    df  = df[df['nans'] > 150]\n    df['eeg_id'] = df['eeg_id'].map(lambda x: x.split('.')[0]).astype('int')\n    train = train[~train['eeg_id'].isin(df['eeg_id'])]\n    train.head(1)","metadata":{"papermill":{"duration":0.347751,"end_time":"2024-02-07T17:23:24.193927","exception":false,"start_time":"2024-02-07T17:23:23.846176","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:32.14759Z","iopub.execute_input":"2024-02-08T14:16:32.147939Z","iopub.status.idle":"2024-02-08T14:16:32.159151Z","shell.execute_reply.started":"2024-02-08T14:16:32.147903Z","shell.execute_reply":"2024-02-08T14:16:32.156925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Train Spectrograms and EEGs\n\nWe can read 1 file from Chris's [Kaggle dataset here][1] which contains all the 11k spectrograms in less than 1 minute! Don't forget to upvote this helpful [dataset][1]\n\n[1]: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms","metadata":{"papermill":{"duration":0.005271,"end_time":"2024-02-07T17:23:24.20513","exception":false,"start_time":"2024-02-07T17:23:24.199859","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nif not submission:\n    # FOR TESTING SET READ_FILES TO TRUE\n    if TEST_MODE:\n        train = train.sample(500,random_state=42).reset_index(drop=True)\n        spectrograms = {}\n        for i,e in enumerate(train.spec_id.values):\n            if i%100==0: print(i,', ',end='')\n            x = pd.read_parquet(f'/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/{e}.parquet')\n            spectrograms[e] = x.values\n        all_eegs = {}\n        for i,e in enumerate(train.eeg_id.values):\n            if i%100==0: print(i,', ',end='')\n            x = np.load(f'/kaggle/input/eeg-spectrograms/EEG_Spectrograms/{e}.npy')\n            all_eegs[e] = x\n    else:\n        spectrograms = np.load('/kaggle/input/brain-spectrograms/specs.npy',allow_pickle=True).item()\n        all_eegs = np.load('/kaggle/input/eeg-spectrograms/eeg_specs.npy',allow_pickle=True).item()","metadata":{"papermill":{"duration":222.18751,"end_time":"2024-02-07T17:27:06.399257","exception":false,"start_time":"2024-02-07T17:23:24.211747","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:32.16084Z","iopub.execute_input":"2024-02-08T14:16:32.162026Z","iopub.status.idle":"2024-02-08T14:16:32.176522Z","shell.execute_reply.started":"2024-02-08T14:16:32.161977Z","shell.execute_reply":"2024-02-08T14:16:32.175291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA GENERATOR\nThis data generator outputs 400x600x3, the spectrogram and eeg images are concatenated all togother in a single image, then copied to a single channel of the 3 channels. For using data augmention you can set `augment = True` when creating the train data generator. You can also resize by setting `img_size`","metadata":{"papermill":{"duration":0.00561,"end_time":"2024-02-07T17:27:06.410813","exception":false,"start_time":"2024-02-07T17:27:06.405203","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import albumentations as albu\n\nclass DataGenerator():\n    'Generates data for Keras'\n    def __init__(self, data, specs, eeg_specs, augment=False, mode='train',img_size=IMG_SIZE): \n        self.data = data\n        self.augment = augment\n        self.mode = mode\n        self.img_size = img_size\n        self.specs = specs\n        self.eeg_specs = eeg_specs\n        self.on_epoch_end()\n        \n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, index):\n        X, y = self.data_generation(index)\n        if self.img_size != (400,600): X = self.resize(X)\n        if self.augment: X = self.augmentation(X)\n        return X, y\n    \n    def __call__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n            \n            if i == self.__len__()-1:\n                self.on_epoch_end()\n                \n    def on_epoch_end(self):\n        if self.mode=='train': \n            self.data = self.data.sample(frac=1).reset_index(drop=True)\n                        \n    def data_generation(self, index):\n        X = np.zeros((400,600,3),dtype='float32')\n        y = np.zeros((6,),dtype='float32')\n        \n        row = self.data.iloc[index]\n        if self.mode=='test': \n            offset = 0\n        else:\n            offset = int(row.offset/2)\n            \n        cnl = np.random.choice([0,1,2])\n        spec = self.specs[row.spec_id]\n        eeg = self.eeg_specs[row.eeg_id]\n        for k in range(4):\n            # EXTRACT 300 ROWS OF SPECTROGRAM\n            img = spec[offset:offset+300,k*100:(k+1)*100].T\n            \n            # LOG TRANSFORM SPECTROGRAM\n            img = np.clip(img,np.exp(-4),np.exp(8))\n            img = np.log(img)\n            \n            # STANDARDIZE PER IMAGE\n            img = np.nan_to_num(img, nan=0.0)                \n\n            mn = img.flatten().min()\n            mx = img.flatten().max()\n            ep = 1e-5\n            img = 255 * (img - mn) / (mx - mn + ep)\n            X[k*100:(k+1)*100,:300,cnl] = img\n            \n            # EEG SPECTROGRAMS, ADD FROM 300 WIDTH AND MAKE A 3 CHANNEL IMAGE\n            # STANDARDIZE PER IMAGE\n            img = eeg[:,:,k]\n            mn = img.flatten().min()\n            mx = img.flatten().max()\n            ep = 1e-5\n            img = 255 * (img - mn) / (mx - mn + ep)\n            X[k*100:(k+1)*100:,300:,cnl] = img\n\n\n        if self.mode!='test':\n            y[:] = row[TARGETS]\n            \n        return X,y\n    \n    def resize(self, img):\n        composition = albu.Compose([\n                albu.Resize(IMG_SIZE[0],IMG_SIZE[1])\n            ])\n        return composition(image=img)['image']\n            \n    def augmentation(self, img):\n        composition = albu.Compose([\n                albu.HorizontalFlip(p=0.4),\n                albu.VerticalFlip(p=0.4),\n                albu.RandomRotate90(p=0.4),\n                albu.Rotate(p=0.5,limit=10),\n            ])\n        return composition(image=img)['image']","metadata":{"papermill":{"duration":2.589361,"end_time":"2024-02-07T17:27:09.006112","exception":false,"start_time":"2024-02-07T17:27:06.416751","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:32.179758Z","iopub.execute_input":"2024-02-08T14:16:32.180338Z","iopub.status.idle":"2024-02-08T14:16:33.215382Z","shell.execute_reply.started":"2024-02-08T14:16:32.1803Z","shell.execute_reply":"2024-02-08T14:16:33.213857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DISPLAY DATA GENERATOR\nBelow we display example data generator spectrogram images.","metadata":{"papermill":{"duration":0.005596,"end_time":"2024-02-07T17:27:09.017756","exception":false,"start_time":"2024-02-07T17:27:09.01216","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if not submission:\n    gen = DataGenerator(train, augment=False, specs=spectrograms, eeg_specs=all_eegs)\n    for x,y in gen:\n        break\n    plt.imshow(x[:,:,2])\n    plt.title(f'Target = {y.round(1)}',size=12)\n    plt.yticks([])\n    plt.ylabel('Frequencies (Hz)',size=12)\n    plt.xlabel('Time (sec)',size=12)\n    plt.show()","metadata":{"papermill":{"duration":0.353959,"end_time":"2024-02-07T17:27:09.378243","exception":false,"start_time":"2024-02-07T17:27:09.024284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.217075Z","iopub.execute_input":"2024-02-08T14:16:33.217845Z","iopub.status.idle":"2024-02-08T14:16:33.2256Z","shell.execute_reply.started":"2024-02-08T14:16:33.217804Z","shell.execute_reply":"2024-02-08T14:16:33.224013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{"papermill":{"duration":0.007604,"end_time":"2024-02-07T17:27:09.395313","exception":false,"start_time":"2024-02-07T17:27:09.387709","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## LEARNING RATE","metadata":{"papermill":{"duration":0.007689,"end_time":"2024-02-07T17:27:09.410982","exception":false,"start_time":"2024-02-07T17:27:09.403293","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nif not submission:\n    LR_START = 1e-4\n    LR_MAX = 1e-3\n    LR_RAMPUP_EPOCHS = 0\n    LR_SUSTAIN_EPOCHS = 0\n    LR_STEP_DECAY = 0.1\n    EVERY = 2\n    EPOCHS = 4\n\n    def lrfn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//EVERY)\n        return lr\n\n    rng = [i for i in range(EPOCHS)]\n    y = [lrfn(x) for x in rng]\n    plt.figure(figsize=(6, 2))\n    plt.plot(rng, y, 'o-'); \n    plt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\n    plt.title('Step Training Schedule',size=16); plt.show()\n\n    LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    Constant_LR = tf.keras.callbacks.LearningRateScheduler(lambda x: 0.001, verbose = True)","metadata":{"papermill":{"duration":0.203592,"end_time":"2024-02-07T17:27:09.62282","exception":false,"start_time":"2024-02-07T17:27:09.419228","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.227376Z","iopub.execute_input":"2024-02-08T14:16:33.227855Z","iopub.status.idle":"2024-02-08T14:16:33.243269Z","shell.execute_reply.started":"2024-02-08T14:16:33.227818Z","shell.execute_reply":"2024-02-08T14:16:33.242001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODEL AND UTILITY FUNCTIONS","metadata":{"papermill":{"duration":0.007926,"end_time":"2024-02-07T17:27:09.639381","exception":false,"start_time":"2024-02-07T17:27:09.631455","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_model():  \n    inp = tf.keras.layers.Input((IMG_SIZE[0],IMG_SIZE[1],3))\n    base_model = load_model(f'{LOAD_BACKBONE_FROM}')    \n    x = base_model(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    output = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n    model = tf.keras.Model(inputs=inp, outputs=output)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n    kl = tf.keras.metrics.KLDivergence(name='kl')\n    model.compile(loss=loss_fn, optimizer=opt, metrics=[kl])  \n    return model\n\ndef loss_fn(y_true, y_pred):\n    kl = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)\n    cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n    return tf.nn.compute_average_loss(kl(y_true, y_pred)) + tf.nn.compute_average_loss(cce(y_true, y_pred))\n\ndef score(y_true, y_pred):\n    kl = tf.keras.metrics.KLDivergence()\n    return kl(y_true, y_pred)\n\ndef plot_hist(hist):\n    metrics = ['loss','kl']\n    for i,metric in enumerate(metrics):\n        plt.figure(figsize=(10,4))\n        plt.subplot(1,2,i+1)\n        plt.plot(hist[metric])\n        plt.plot(hist[f'val_{metric}'])\n        plt.title(f'{metric}',size=12)\n        plt.ylabel(f'{metric}',size=12)\n        plt.xlabel('epoch',size=12)\n        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n        plt.show()","metadata":{"papermill":{"duration":0.023615,"end_time":"2024-02-07T17:27:09.671265","exception":false,"start_time":"2024-02-07T17:27:09.64765","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.245666Z","iopub.execute_input":"2024-02-08T14:16:33.246358Z","iopub.status.idle":"2024-02-08T14:16:33.260897Z","shell.execute_reply.started":"2024-02-08T14:16:33.246314Z","shell.execute_reply":"2024-02-08T14:16:33.259496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRANSFER LEARNING","metadata":{"papermill":{"duration":0.008696,"end_time":"2024-02-07T17:27:09.688228","exception":false,"start_time":"2024-02-07T17:27:09.679532","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\nimport tensorflow.keras.backend as K, gc\n\nif not submission:\n    all_oof = []\n    all_true = []\n    losses = []\n    val_losses = []\n    kls = []\n    val_kls = []\n    total_hist = {}\n\n    gkf = GroupKFold(n_splits=5)\n    for i, (train_index, valid_index) in enumerate(gkf.split(train, train.target, train.patient_id)):   \n        \n        print('#'*25)\n        print(f'### Fold {i+1}')\n        \n        train_gen = DataGenerator(train.iloc[train_index], augment=False, specs=spectrograms, eeg_specs=all_eegs)\n        valid_gen = DataGenerator(train.iloc[valid_index], mode='valid', specs=spectrograms, eeg_specs=all_eegs)\n        EPOCHS = 4\n        BATCH_SIZE_PER_REPLICA = 32\n        BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n\n        train_dataset = tf.data.Dataset.from_generator(generator=train_gen, \n                                                   output_signature=(tf.TensorSpec(shape=(IMG_SIZE[0],IMG_SIZE[1],3), dtype=tf.float32),\n                                                                     tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n        val_dataset = tf.data.Dataset.from_generator(generator=valid_gen, \n                                                   output_signature=(tf.TensorSpec(shape=(IMG_SIZE[0],IMG_SIZE[1],3), dtype=tf.float32),\n                                                                     tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n        save_best = tf.keras.callbacks.ModelCheckpoint(f'model_{VER}_{i}.weights.h5', \n                                                   monitor='val_loss', save_best_only=True, save_weights_only=True)\n        \n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n        \n        K.clear_session()\n        with strategy.scope():\n            model = build_model()\n        \n        hist = model.fit(train_dataset, verbose=1, validation_data = val_dataset, \n                         epochs=EPOCHS, callbacks=[save_best,LR])\n        losses.append(hist.history['loss'])\n        val_losses.append(hist.history['val_loss'])\n        kls.append(hist.history['kl'])\n        val_kls.append(hist.history['val_kl'])\n        oof = model.predict(val_dataset, verbose=1)\n        all_oof.append(oof)\n        all_true.append(train.iloc[valid_index][TARGETS].values)    \n        del model, oof\n        gc.collect()\n        \n    total_hist['loss'] = np.mean(losses,axis=0)\n    total_hist['val_loss'] = np.mean(val_losses,axis=0)\n    total_hist['kl'] = np.mean(kls,axis=0)\n    total_hist['val_kl'] = np.mean(val_kls,axis=0)\n    all_oof = np.concatenate(all_oof)\n    all_true = np.concatenate(all_true)\n    plot_hist(total_hist)\n    print('#'*25)\n    print(f'CV KL SCORE: {score(all_true,all_oof)}')","metadata":{"papermill":{"duration":0.029503,"end_time":"2024-02-07T17:27:09.725805","exception":false,"start_time":"2024-02-07T17:27:09.696302","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.262665Z","iopub.execute_input":"2024-02-08T14:16:33.263033Z","iopub.status.idle":"2024-02-08T14:16:33.281225Z","shell.execute_reply.started":"2024-02-08T14:16:33.263001Z","shell.execute_reply":"2024-02-08T14:16:33.28019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test and Create Submission CSV\nInfer the test data and create a `submission.csv` file.","metadata":{"papermill":{"duration":0.007873,"end_time":"2024-02-07T17:27:09.742118","exception":false,"start_time":"2024-02-07T17:27:09.734245","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pywt, librosa\n\nUSE_WAVELET = None \n\nNAMES = ['LL','LP','RP','RR']\n\nFEATS = [['Fp1','F7','T3','T5','O1'],\n         ['Fp1','F3','C3','P3','O1'],\n         ['Fp2','F8','T4','T6','O2'],\n         ['Fp2','F4','C4','P4','O2']]\n\n# DENOISE FUNCTION\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise(x, wavelet='haar', level=1):    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    ret=pywt.waverec(coeff, wavelet, mode='per')\n    \n    return ret\n\nimport librosa\n\ndef spectrogram_from_eeg(parquet_path, display=False):\n    \n    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n    eeg = pd.read_parquet(parquet_path)\n    middle = (len(eeg)-10_000)//2\n    eeg = eeg.iloc[middle:middle+10_000]\n    \n    # VARIABLE TO HOLD SPECTROGRAM\n    img = np.zeros((100,300,4),dtype='float32')\n    \n    if display: plt.figure(figsize=(10,7))\n    signals = []\n    for k in range(4):\n        COLS = FEATS[k]\n        \n        for kk in range(4):\n            # FILL NANS\n            x1 = eeg[COLS[kk]].values\n            x2 = eeg[COLS[kk+1]].values\n            m = np.nanmean(x1)\n            if np.isnan(x1).mean()<1: x1 = np.nan_to_num(x1,nan=m)\n            else: x1[:] = 0\n            m = np.nanmean(x2)\n            if np.isnan(x2).mean()<1: x2 = np.nan_to_num(x2,nan=m)\n            else: x2[:] = 0\n                \n            # COMPUTE PAIR DIFFERENCES\n            x = x1 - x2\n\n            # DENOISE\n            if USE_WAVELET:\n                x = denoise(x, wavelet=USE_WAVELET)\n            signals.append(x)\n\n            # RAW SPECTROGRAM\n            mel_spec = librosa.feature.melspectrogram(y=x, sr=200, hop_length=len(x)//300, \n                  n_fft=1024, n_mels=100, fmin=0, fmax=20, win_length=128)\n            \n            # LOG TRANSFORM\n            width = (mel_spec.shape[1]//30)*30\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[:,:width]\n            img[:,:,k] += mel_spec_db\n                \n        # AVERAGE THE 4 MONTAGE DIFFERENCES\n        img[:,:,k] /= 4.0\n        \n        if display:\n            plt.subplot(2,2,k+1)\n            plt.imshow(img[:,:,k],aspect='auto',origin='lower')\n            \n    if display: \n        plt.show()\n        plt.figure(figsize=(10,5))\n        offset = 0\n        for k in range(4):\n            if k>0: offset -= signals[3-k].min()\n            plt.plot(range(10_000),signals[k]+offset,label=NAMES[3-k])\n            offset += signals[3-k].max()\n        plt.legend()\n        plt.show()\n        \n    return img","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.034306,"end_time":"2024-02-07T17:27:09.784426","exception":false,"start_time":"2024-02-07T17:27:09.75012","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.282509Z","iopub.execute_input":"2024-02-08T14:16:33.283219Z","iopub.status.idle":"2024-02-08T14:16:33.304808Z","shell.execute_reply.started":"2024-02-08T14:16:33.283172Z","shell.execute_reply":"2024-02-08T14:16:33.303346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission:\n    test = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\n    print('Test shape',test.shape)\n    test.head()","metadata":{"papermill":{"duration":0.029223,"end_time":"2024-02-07T17:27:09.821959","exception":false,"start_time":"2024-02-07T17:27:09.792736","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.30674Z","iopub.execute_input":"2024-02-08T14:16:33.307126Z","iopub.status.idle":"2024-02-08T14:16:33.327802Z","shell.execute_reply.started":"2024-02-08T14:16:33.307092Z","shell.execute_reply":"2024-02-08T14:16:33.326553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL SPECTROGRAMS\nif submission:\n    PATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/'\n    files2 = os.listdir(PATH2)\n    print(f'There are {len(files2)} test spectrogram parquets')\n    \n    spectrograms2 = {}\n    for i,f in enumerate(files2):\n        if i%100==0: print(i,', ',end='')\n        tmp = pd.read_parquet(f'{PATH2}{f}')\n        name = int(f.split('.')[0])\n        spectrograms2[name] = tmp.iloc[:,1:].values\n    \n    # RENAME FOR DATA GENERATOR\n    test = test.rename({'spectrogram_id':'spec_id'},axis=1)","metadata":{"papermill":{"duration":0.239453,"end_time":"2024-02-07T17:27:10.070021","exception":false,"start_time":"2024-02-07T17:27:09.830568","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.329299Z","iopub.execute_input":"2024-02-08T14:16:33.330147Z","iopub.status.idle":"2024-02-08T14:16:33.401324Z","shell.execute_reply.started":"2024-02-08T14:16:33.33011Z","shell.execute_reply":"2024-02-08T14:16:33.400503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL EEG SPECTROGRAMS\nif submission:\n    PATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\n    DISPLAY = 0\n    EEG_IDS2 = test.eeg_id.unique()\n    all_eegs2 = {}\n\n    print('Converting Test EEG to Spectrograms...'); print()\n    for i,eeg_id in enumerate(EEG_IDS2):\n        \n        # CREATE SPECTROGRAM FROM EEG PARQUET\n        img = spectrogram_from_eeg(f'{PATH2}{eeg_id}.parquet', i<DISPLAY)\n        all_eegs2[eeg_id] = img","metadata":{"papermill":{"duration":11.019,"end_time":"2024-02-07T17:27:21.097614","exception":false,"start_time":"2024-02-07T17:27:10.078614","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:33.402625Z","iopub.execute_input":"2024-02-08T14:16:33.403132Z","iopub.status.idle":"2024-02-08T14:16:35.687319Z","shell.execute_reply.started":"2024-02-08T14:16:33.403088Z","shell.execute_reply":"2024-02-08T14:16:35.685862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFER EFFICIENTNET ON TEST\nif submission:\n    preds = []\n    test_gen = DataGenerator(test, mode='test',specs = spectrograms2, eeg_specs = all_eegs2)\n    test_dataset = tf.data.Dataset.from_generator(generator=test_gen, \n                                               output_signature=(tf.TensorSpec(shape=(IMG_SIZE[0],IMG_SIZE[1],3), dtype=tf.float32),\n                                                                 tf.TensorSpec(shape=(6,), dtype=tf.float32))).batch(64).prefetch(tf.data.AUTOTUNE)\n    model = build_model()\n    \n    for i in range(5):\n        print(f'Fold {i+1}')\n        model.load_weights(f'{LOAD_MODELS_FROM}model_{VER}_{i}.weights.h5')\n        pred = model.predict(test_dataset, verbose=1)\n        preds.append(pred)\n    pred = np.mean(preds,axis=0)\n    print('Test preds shape',pred.shape)","metadata":{"papermill":{"duration":20.951008,"end_time":"2024-02-07T17:27:42.060528","exception":false,"start_time":"2024-02-07T17:27:21.10952","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:35.692319Z","iopub.execute_input":"2024-02-08T14:16:35.693816Z","iopub.status.idle":"2024-02-08T14:16:56.475866Z","shell.execute_reply.started":"2024-02-08T14:16:35.693752Z","shell.execute_reply":"2024-02-08T14:16:56.474809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission:\n    sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\n    sub[TARGETS] = pred\n    sub.to_csv('submission.csv',index=False)\n    print('Submissionn shape',sub.shape)\n    print()\n    print(sub.head().to_string())","metadata":{"papermill":{"duration":0.03493,"end_time":"2024-02-07T17:27:42.105112","exception":false,"start_time":"2024-02-07T17:27:42.070182","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:56.477604Z","iopub.execute_input":"2024-02-08T14:16:56.478723Z","iopub.status.idle":"2024-02-08T14:16:56.497227Z","shell.execute_reply.started":"2024-02-08T14:16:56.478678Z","shell.execute_reply":"2024-02-08T14:16:56.49572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\nif submission:\n    print(sub.iloc[:,-6:].sum(axis=1).to_string())","metadata":{"papermill":{"duration":0.023201,"end_time":"2024-02-07T17:27:42.139259","exception":false,"start_time":"2024-02-07T17:27:42.116058","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-08T14:16:56.498882Z","iopub.execute_input":"2024-02-08T14:16:56.499261Z","iopub.status.idle":"2024-02-08T14:16:56.512055Z","shell.execute_reply.started":"2024-02-08T14:16:56.499225Z","shell.execute_reply":"2024-02-08T14:16:56.511077Z"},"trusted":true},"execution_count":null,"outputs":[]}]}