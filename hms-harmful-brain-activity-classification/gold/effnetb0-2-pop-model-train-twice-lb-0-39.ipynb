{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749},{"sourceId":7392775,"sourceType":"datasetVersion","datasetId":4297782},{"sourceId":7487364,"sourceType":"datasetVersion","datasetId":4359072},{"sourceId":7517324,"sourceType":"datasetVersion","datasetId":4378712},{"sourceId":7621177,"sourceType":"datasetVersion","datasetId":4439202},{"sourceId":7680946,"sourceType":"datasetVersion","datasetId":4476546},{"sourceId":7687292,"sourceType":"datasetVersion","datasetId":4485905},{"sourceId":158958765,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":270.012179,"end_time":"2024-01-14T22:56:02.916427","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-14T22:51:32.904248","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EffNetB0 model trained twice, once for each of two training populations\n\nAs described in @pcjimmmy's [notebook][2], the sum of total votes for each label_id indicates the training data may have been collected from two sources. One population has 1-7 total votes, and the other 10-28 votes. Each population also displays unique class imbalance.\n\nRegardless of the data's source, the two populations exist within the training data. I began to think about the implication of training with data that has few votes. Consider the extreme of only one vote cast. Training on this data would likely result in a model that predicts a class with high probability, while the remaining probabilites are nearly zero. If a pobablility is predicted to be nearly zero, but the true probability is much higher, KL-divergence will be very high due to the division by nearly zero in the logarithm (P(x)log(P(x)/Q(x)). (As evidence of this, I modified my [EfficientNetB0 Noisy Student notebook][0] by replacing the least probable class prediction with 1e-5 and submitting, resulting in worse performance: LB 0.42 -> 0.49).\n\nReasoning how samples with peaked distributions may not be the best for training, I considered only using samples with 10-28 total votes. However, the reduction to the dataset is too extreme. Then, I had the insight to train in 2 stages: first with the 1-7 vote population, then the 10-28 vote population. I thought the first training stage could learn from the peaked distributions (and the seizure samples), then the second stage of training could adjust to have less peaked distributions, addressing the KL-divergence issue. \n\nSpliting the data reduces the amount of training data for each training stage, so I sought to increase the number of samples used. I reduced the size of the `train.csv` to 20,183 rows by dropping duplicates of [`eeg_id`, `seizure_vote`, `lpd_vote`, `gpd_vote`, `lrda_vote`,`grda_vote`, `other_vote`]. That's 3,094 more samples than @cdeotte's [original notebook][1], which uses only unique `eeg_ids`.\n\nIn that notebook, he also used EEG spectrograms he [created][6] for each eeg_id, using data from the central 50 seconds of the EEG. [My approach][4] is to create a unique spectrogram for each label_id, using `eeg_label_offset_seconds` instead of the centered 50-second sample. The result is more training data with slight variations.\n\nTraining with this data in two stages results in CV 0.68 and LB 0.39.\n\n___\nThank you to @cdeotte! This notebook is heavily influenced by his [notebook][1].\n\nThank you to @pcjimmmy! His [notebook][2] and insight into the two dataset sources inspired this notebook.\n\nIf you find the EEG spectrograms useful, please upvote my [dataset][3] and associated [notebook][4]\n\nNote: this EfficientNet model initializes with [noisy student weights][5]\n\n[0]: https://www.kaggle.com/code/seanbearden/efficientnetb0-noisy-student-lb-0-42\n[1]: https://www.kaggle.com/code/cdeotte/efficientnetb0-starter-lb-0-43\n[2]: https://www.kaggle.com/code/pcjimmmy/patient-variation-eda\n[3]: https://www.kaggle.com/datasets/seanbearden/eeg-spectrogram-by-lead-id-unique\n[4]: https://www.kaggle.com/code/seanbearden/spectrogram-from-unique-eeg-votes-combo\n[5]: https://www.kaggle.com/datasets/seanbearden/tf-efficientnet-noisy-student-weights\n[6]: https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg","metadata":{"papermill":{"duration":0.008094,"end_time":"2024-01-14T22:51:36.811998","exception":false,"start_time":"2024-01-14T22:51:36.803904","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Update: Testing the addition of TUSZ seizure data\n\nI've processed [The Temple University Hospital Seizure Detection Corpus][1] for use in this competition, which can be found [here][2]. I'm testing out how the data affects model performance. \n\n~~Since the data only affects the second stage of training, the stage 1 weights from v1 are used to initialize stage 2.~~\n\nAccidentally retrained with pop2 weights on second stage. Submitting to see LB.\n\n[1]: https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml\n[2]: https://www.kaggle.com/datasets/seanbearden/hms-hba-tuh-tusz-seizures","metadata":{}},{"cell_type":"markdown","source":"\n| Details | 1st CV | 2nd CV | Final CV | EffNet LB | Notebook version | Model | Loss | Epochs | Data Augmention |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Split Uniq Votes | 0.81 | 0.29 | 0.68 | 0.39 | 1-6 | EffNetB0 | KL-Div | 5 | none |\n| TUSZ Seizures in Stage 2 (retrain) | ??? | ??? | ??? | ??? | 7-8 | EffNetB0 | KL-Div | 5 | none |","metadata":{}},{"cell_type":"markdown","source":"# Initialize 2xT4 GPUs\nWe will use both Kaggle T4 GPUs and we will use mixed precision.","metadata":{"papermill":{"duration":0.008572,"end_time":"2024-01-14T22:51:36.82846","exception":false,"start_time":"2024-01-14T22:51:36.819888","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os #, gc\nimport time\nimport json\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\nimport tensorflow.keras.backend as K, gc\nimport tensorflow as tf\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\n\n\nprint('TensorFlow version =',tf.__version__)\n\n# USE MULTIPLE GPUS\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus)<=1: \n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    print(f'Using {len(gpus)} GPU')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} GPUs')\n\n","metadata":{"papermill":{"duration":14.80928,"end_time":"2024-01-14T22:51:51.64702","exception":false,"start_time":"2024-01-14T22:51:36.83774","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:23:38.58388Z","iopub.execute_input":"2024-02-23T16:23:38.584188Z","iopub.status.idle":"2024-02-23T16:23:54.532642Z","shell.execute_reply.started":"2024-02-23T16:23:38.58416Z","shell.execute_reply":"2024-02-23T16:23:54.531495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IF THIS EQUALS NONE, THEN WE TRAIN NEW MODELS\n# IF THIS EQUALS DISK PATH, THEN WE LOAD PREVIOUSLY TRAINED MODELS\n\nLOAD_MODELS_FROM_1 = '/kaggle/input/effnetb0-2-pop-model-train-twice-weights-v2'\nLOAD_MODELS_FROM_1_NAME = 'EffNet_pop2_v2'\n\nLOAD_MODELS_FROM_2 = '/kaggle/input/effnetb0-2-pop-model-train-twice-weights-v2' \n\nUSE_KAGGLE_SPECTROGRAMS = True\nUSE_EEG_SPECTROGRAMS = True\n# USE MIXED PRECISION\nMIXED_PRECISION = True\n# READ ALL SPECTROGRAMS\nREAD_SPEC_FILES = False\n# # READ ALL EEG SPECTROGRAMS\nREAD_EEG_SPEC_FILES = False\n\nEVAL_ONLY = True","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:23:54.534318Z","iopub.execute_input":"2024-02-23T16:23:54.534894Z","iopub.status.idle":"2024-02-23T16:23:54.540532Z","shell.execute_reply.started":"2024-02-23T16:23:54.534862Z","shell.execute_reply":"2024-02-23T16:23:54.539467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\nVERSION = 2\n\nMODEL_NAME = f'effnetB0_2_pop_twice_train_v{VERSION}'\nMODEL_LOC = f'models/{MODEL_NAME}'\n\nNOTES = \"\"\"Split training data into 2 populations based on total vote sum.\nEffNetB0 efficientnet package with unique eeg_id and votes.\nUsing GKF and no augmentation.\nAdding TUSZ seizures to pop 2 dataset. Used previous weights in stage 2 and retrained\n\"\"\"\n\nSEED = 2444\nBATCH_SIZE = 32\nFOLDS = 5 # 5-fold cross-validation.\n\n\nLR_START = 1e-4\nLR_MAX = 1e-4\nLR_RAMPUP_EPOCHS = 0\nLR_SUSTAIN_EPOCHS = 2\nLR_STEP_DECAY = 0.5 # 0.5\nLR_EVERY = 1\nEPOCHS = 5\nPATIENCE = 2\nSTART_FROM_EPOCH = 2\n\nTIMESTAMP = pd.Timestamp.now('utc')\n\nmodel_info = {\n    'api': f'TensorFlow version = {tf.__version__}',\n    'datetime': TIMESTAMP.isoformat(),\n    'filename': 'efficientnet_tf_unique_vote.ipynb',\n    'folds': FOLDS,\n    'model': MODEL_NAME,\n    'notes': NOTES,\n    'path': MODEL_LOC,\n    'version': VERSION,\n    'SEED': SEED,\n    'BATCH_SIZE': BATCH_SIZE,\n    'EPOCHS': EPOCHS,\n    'FOLDS': FOLDS,\n    'PATIENCE': PATIENCE,\n    'USE_KAGGLE_SPECTROGRAMS': USE_KAGGLE_SPECTROGRAMS,\n    'USE_EEG_SPECTROGRAMS': USE_EEG_SPECTROGRAMS,\n    'MIXED_PRECISION': MIXED_PRECISION,\n    'READ_SPEC_FILES': READ_SPEC_FILES,\n    'READ_EEG_SPEC_FILES': READ_EEG_SPEC_FILES,\n    'LR_START': LR_START,\n    'LR_MAX': LR_MAX,\n    'LR_RAMPUP_EPOCHS': LR_RAMPUP_EPOCHS,\n    'LR_SUSTAIN_EPOCHS': LR_SUSTAIN_EPOCHS,\n    'LR_STEP_DECAY': LR_STEP_DECAY,\n    'LR_EVERY': LR_EVERY,\n    'START_FROM_EPOCH': START_FROM_EPOCH,\n     }\n\ntf.random.set_seed(\n    SEED\n)\nnp.random.seed(SEED)\n\n# Check if the directory exists\nif not os.path.exists(MODEL_LOC):\n    Path(MODEL_LOC).mkdir(parents=True, exist_ok=True)\n    \nmodel_info_path = os.path.join(MODEL_LOC, 'model_info.json')\nwith open(model_info_path, 'w') as f:\n    json.dump(model_info, f)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:23:54.541783Z","iopub.execute_input":"2024-02-23T16:23:54.542098Z","iopub.status.idle":"2024-02-23T16:23:54.55628Z","shell.execute_reply.started":"2024-02-23T16:23:54.542048Z","shell.execute_reply":"2024-02-23T16:23:54.555514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MIXED PRECISION\nif MIXED_PRECISION:\n    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n    print('Mixed precision enabled')\nelse:\n    print('Using full precision')","metadata":{"papermill":{"duration":0.016556,"end_time":"2024-01-14T22:51:51.671783","exception":false,"start_time":"2024-01-14T22:51:51.655227","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:23:54.558108Z","iopub.execute_input":"2024-02-23T16:23:54.558394Z","iopub.status.idle":"2024-02-23T16:23:54.571209Z","shell.execute_reply.started":"2024-02-23T16:23:54.55837Z","shell.execute_reply":"2024-02-23T16:23:54.570339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train Data","metadata":{"papermill":{"duration":0.007846,"end_time":"2024-01-14T22:51:51.688268","exception":false,"start_time":"2024-01-14T22:51:51.680422","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\nTARGETS = df.columns[-6:]\nprint('Train shape:', df.shape )\nprint('Targets', list(TARGETS))\n\ndf['total_evaluators'] = df[['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']].sum(axis=1)\n\ndf_uniq = df.drop_duplicates(subset=['eeg_id'] + list(TARGETS))\n# df_uniq = df_uniq.sort_values('eeg_id', ascending=True)\nprint(f'There are {df.patient_id.nunique()} patients in the training data.')\nprint(f'There are {df.eeg_id.nunique()} EEG IDs in the training data.')\nprint(f'There are {df_uniq.shape[0]} unique eeg_id + votes in the training data.')\n\ndf_uniq.eeg_id.value_counts().value_counts().plot(kind='bar', title=f'Distribution of Count of EEG w Unique Vote: '\n                                                                    f'{df_uniq.shape[0]} examples');","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.288611,"end_time":"2024-01-14T22:51:51.984993","exception":false,"start_time":"2024-01-14T22:51:51.696382","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:23:54.572279Z","iopub.execute_input":"2024-02-23T16:23:54.572551Z","iopub.status.idle":"2024-02-23T16:23:55.529028Z","shell.execute_reply.started":"2024-02-23T16:23:54.572527Z","shell.execute_reply":"2024-02-23T16:23:55.528136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~90% of eeg_ids have 1 unique distribution of votes.\n\nAs @pcjimmmy has shown in his [notebook](https://www.kaggle.com/code/pcjimmmy/patient-variation-eda), the data breaks into two populations when considering the total number of votes per label_id. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(df['total_evaluators'], bins=10, color='blue', edgecolor='black')\nplt.title('Histogram of Total Evaluators')\nplt.xlabel('Total Evaluators')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:23:55.53015Z","iopub.execute_input":"2024-02-23T16:23:55.530425Z","iopub.status.idle":"2024-02-23T16:23:55.837483Z","shell.execute_reply.started":"2024-02-23T16:23:55.530399Z","shell.execute_reply":"2024-02-23T16:23:55.8366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Train Spectrograms \n\nFirst we need to read in all 11k train spectrogram files. Reading thousands of files takes 11 minutes with Pandas. Instead, we can read 1 file from my [Kaggle dataset here][1] which contains all the 11k spectrograms in less than 1 minute! To use my Kaggle dataset, set variable `READ_SPEC_FILES = False`. Thank you for upvoting my helpful [dataset][1] :-)\n\n[1]: https://www.kaggle.com/datasets/cdeotte/brain-spectrograms","metadata":{"papermill":{"duration":0.00881,"end_time":"2024-01-14T22:51:52.142747","exception":false,"start_time":"2024-01-14T22:51:52.133937","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nif not EVAL_ONLY:\n    spectrograms = np.load('/kaggle/input/brain-spectrograms/specs.npy',allow_pickle=True).item()","metadata":{"papermill":{"duration":55.16894,"end_time":"2024-01-14T22:52:47.320438","exception":false,"start_time":"2024-01-14T22:51:52.151498","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:23:55.838858Z","iopub.execute_input":"2024-02-23T16:23:55.839499Z","iopub.status.idle":"2024-02-23T16:25:01.999674Z","shell.execute_reply.started":"2024-02-23T16:23:55.839461Z","shell.execute_reply":"2024-02-23T16:25:01.998769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read EEG Spectrograms\n\nThese EEG [spectrograms][1] are a modification of @cdeotte's [spectrograms][4]. The dictionary keys are label_ids for unique eeg_id + vote combo.\n\n[1]: https://www.kaggle.com/datasets/seanbearden/eeg-spectrogram-by-lead-id-unique\n[2]: https://www.kaggle.com/code/seanbearden/spectrogram-from-unique-eeg-votes-combo\n[3]: https://www.kaggle.com/datasets/cdeotte/brain-eeg-spectrograms\n[4]: https://www.kaggle.com/code/cdeotte/how-to-make-spectrogram-from-eeg","metadata":{}},{"cell_type":"code","source":"%%time\nif not EVAL_ONLY:\n    all_eegs = np.load('/kaggle/input/eeg-spectrogram-by-lead-id-unique/eeg_specs.npy',allow_pickle=True).item()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:25:02.001035Z","iopub.execute_input":"2024-02-23T16:25:02.00166Z","iopub.status.idle":"2024-02-23T16:26:29.055429Z","shell.execute_reply.started":"2024-02-23T16:25:02.001621Z","shell.execute_reply":"2024-02-23T16:26:29.0545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TUSZ Seizure Data","metadata":{}},{"cell_type":"code","source":"import pywt, librosa\n\nUSE_WAVELET = None \n\nNAMES = ['LL','LP','RP','RR']\n\nFEATS = [['Fp1','F7','T3','T5','O1'],\n         ['Fp1','F3','C3','P3','O1'],\n         ['Fp2','F8','T4','T6','O2'],\n         ['Fp2','F4','C4','P4','O2']]\n\n# DENOISE FUNCTION\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise(x, wavelet='haar', level=1):    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    ret=pywt.waverec(coeff, wavelet, mode='per')\n    \n    return ret\n\ndef spectrogram_from_eeg(parquet_path, display=False, offset=None):\n    \n    # LOAD MIDDLE 50 SECONDS OF EEG SERIES\n    eeg = pd.read_parquet(parquet_path)\n#     print(eeg.shape)\n    if offset is None:\n        middle = (len(eeg)-10_000)//2\n        eeg = eeg.iloc[middle:middle+10_000]\n    else:\n        eeg = eeg.iloc[offset:offset+10_000]\n    \n    # VARIABLE TO HOLD SPECTROGRAM\n    img = np.zeros((128,256,4),dtype='float32')\n    \n    if display: plt.figure(figsize=(10,7))\n    signals = []\n    for k in range(4):\n        COLS = FEATS[k]\n        \n        for kk in range(4):\n        \n            # COMPUTE PAIR DIFFERENCES\n            x = eeg[COLS[kk]].values - eeg[COLS[kk+1]].values\n\n            # FILL NANS\n            m = np.nanmean(x)\n            if np.isnan(x).mean() < 1: \n                x = np.nan_to_num(x,nan=m)\n            else: x[:] = 0\n\n            # DENOISE\n            if USE_WAVELET:\n                x = denoise(x, wavelet=USE_WAVELET)\n            signals.append(x)\n\n            # RAW SPECTROGRAM\n            mel_spec = librosa.feature.melspectrogram(y=x, sr=200, hop_length=len(x)//256, \n                  n_fft=1024, n_mels=128, fmin=0, fmax=20, win_length=128)\n\n            # LOG TRANSFORM\n            width = (mel_spec.shape[1]//32)*32\n            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max).astype(np.float32)[:,:width]\n\n            # STANDARDIZE TO -1 TO 1\n            mel_spec_db = (mel_spec_db+40)/40 \n            img[:,:,k] += mel_spec_db\n                \n        # AVERAGE THE 4 MONTAGE DIFFERENCES\n        img[:,:,k] /= 4.0\n        \n        if display:\n            plt.subplot(2,2,k+1)\n            plt.imshow(img[:,:,k],aspect='auto',origin='lower')\n#             plt.title(f'EEG {eeg_id} - Spectrogram {NAMES[k]}')\n            \n    if display: \n        plt.show()\n        plt.figure(figsize=(10,5))\n        offset = 0\n        for k in range(4):\n            if k>0: offset -= signals[3-k].min()\n            plt.plot(range(10_000),signals[k]+offset,label=NAMES[3-k])\n            offset += signals[3-k].max()\n        plt.legend()\n#         plt.title(f'EEG {eeg_id} Signals')\n        plt.show()\n        print(); print('#'*25); print()\n        \n    return img","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:26:29.056928Z","iopub.execute_input":"2024-02-23T16:26:29.057436Z","iopub.status.idle":"2024-02-23T16:26:29.19571Z","shell.execute_reply.started":"2024-02-23T16:26:29.057401Z","shell.execute_reply":"2024-02-23T16:26:29.19487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not EVAL_ONLY:\ndf_tusz = pd.read_csv('/kaggle/input/hms-hba-tuh-tusz-seizures/seizures.csv')\ndf_tusz.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:26:29.199285Z","iopub.execute_input":"2024-02-23T16:26:29.199576Z","iopub.status.idle":"2024-02-23T16:26:29.253626Z","shell.execute_reply.started":"2024-02-23T16:26:29.199552Z","shell.execute_reply":"2024-02-23T16:26:29.252677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    df_seiz = df_tusz[~df_tusz.class_code.isin(['bckg', 'atsz', 'mysz']) & df_tusz['processed']]\n    df_seiz.class_code.value_counts()\n    df_seiz.groupby('session')['event_time'].max()\n    df_sorted = df_seiz.sort_values(by=['patient_id', 'session', 'class_code', 'event_time'], ascending=[True, True, True, False])\n\n    # Drop duplicates keeping the first occurrence (which has the longest duration due to sorting)\n    df_unique_sessions = df_sorted.drop_duplicates(subset=['patient_id', 'session', 'class_code'], keep='first')\n\n    \n    df_tusz_mod = df_unique_sessions[['id', 'patient_id', 'session', 'class_code']].copy()\n    df_tusz_mod.loc[:, 'eeg_id'] = df_tusz_mod['patient_id'].astype(str) + df_tusz_mod['session']\n    df_tusz_mod.loc[:, 'eeg_sub_id'] = df_unique_sessions['id']\n    df_tusz_mod.loc[:, 'eeg_label_offset_seconds'] = 0.0\n    df_tusz_mod.loc[:, 'spectrogram_id'] = df_unique_sessions['id']\n    df_tusz_mod.loc[:, 'spectrogram_sub_id'] = df_unique_sessions['id']\n    df_tusz_mod.loc[:, 'spectrogram_label_offset_seconds'] = 0.0\n    df_tusz_mod.loc[:, 'label_id'] = df_unique_sessions['id']\n    df_tusz_mod.loc[:, 'patient_id'] += df.patient_id.max() # prevent duplicate patient ids\n    df_tusz_mod.loc[:, 'expert_consensus'] = 'Seizure'\n    df_tusz_mod.loc[:, ['seizure_vote', 'lpd_vote','gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']] = 0\n    df_tusz_mod.loc[:, 'seizure_vote'] = 10 # all are seizures, want to be in pop 2\n    df_tusz_mod.drop(['id', 'session', 'class_code'], axis=1, inplace=True)\n    df_tusz_mod = df_tusz_mod.loc[:, ['eeg_id', 'eeg_sub_id', 'eeg_label_offset_seconds', 'spectrogram_id',\n           'spectrogram_sub_id', 'spectrogram_label_offset_seconds', 'label_id',\n           'patient_id', 'expert_consensus', 'seizure_vote', 'lpd_vote',\n           'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:26:29.254818Z","iopub.execute_input":"2024-02-23T16:26:29.255097Z","iopub.status.idle":"2024-02-23T16:26:29.306367Z","shell.execute_reply.started":"2024-02-23T16:26:29.255073Z","shell.execute_reply":"2024-02-23T16:26:29.305414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    tusz_specs = {}\n    tusz_eegs = {}\n    for file_id in df_unique_sessions['id']:\n        # this is raw EEG signal...neeed\n        tusz_specs[file_id] = pd.read_parquet(f'/kaggle/input/hms-hba-tuh-tusz-seizures/eeg_10min_spec/{file_id}.parquet').iloc[:, 1:].values\n#         tusz_eegs[file_id] = pd.read_parquet()\n        tusz_eegs[file_id] = spectrogram_from_eeg(f'/kaggle/input/hms-hba-tuh-tusz-seizures/eeg_raw/{file_id}.parquet', display=False, offset=0)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:26:29.307569Z","iopub.execute_input":"2024-02-23T16:26:29.307876Z","iopub.status.idle":"2024-02-23T16:31:00.999283Z","shell.execute_reply.started":"2024-02-23T16:26:29.30785Z","shell.execute_reply":"2024-02-23T16:31:00.998129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    df = pd.concat([df, df_tusz_mod], ignore_index=True)\n\n    spectrograms = spectrograms | tusz_specs\n    all_eegs = all_eegs | tusz_eegs","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:31:01.001316Z","iopub.execute_input":"2024-02-23T16:31:01.002312Z","iopub.status.idle":"2024-02-23T16:31:01.125559Z","shell.execute_reply.started":"2024-02-23T16:31:01.00226Z","shell.execute_reply":"2024-02-23T16:31:01.124144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training DataFrame","metadata":{}},{"cell_type":"code","source":"if not EVAL_ONLY:\n    train = df[df['label_id'].isin(all_eegs.keys())].copy()\n    \n    pop_1_idx = train['total_evaluators'] < 10\n    \n    y_data = train[TARGETS].values\n    y_data = y_data / y_data.sum(axis=1,keepdims=True)\n    train[TARGETS] = y_data\n\n    train['target'] = train['expert_consensus']\n    \n    train_pop_1 = train[pop_1_idx].copy().reset_index()\n    train_pop_2 = train[~pop_1_idx].copy().reset_index()\n    # train = train.reset_index()\n    print('Pop 1: train unique eeg_id + votes shape:', train_pop_1.shape )\n    plt.figure(figsize=(10, 6))\n    plt.hist(train['total_evaluators'], bins=10, color='blue', edgecolor='black')\n    plt.title('Histogram of Total Evaluators')\n    plt.xlabel('Total Evaluators')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:31:01.127938Z","iopub.execute_input":"2024-02-23T16:31:01.128289Z","iopub.status.idle":"2024-02-23T16:31:01.508625Z","shell.execute_reply.started":"2024-02-23T16:31:01.128254Z","shell.execute_reply":"2024-02-23T16:31:01.507712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train DataLoader\nThis dataloader outputs 4 spectrogram images as a 4 channel image of size 128x256x4 per train sample. This notebook version is not using data augmention but the code is available below to experiment with albumentations data augmention. Just add `augment = True` when creating the train data loader. And consider adding new transformations to the augment function below.","metadata":{"papermill":{"duration":0.010384,"end_time":"2024-01-14T22:52:47.341581","exception":false,"start_time":"2024-01-14T22:52:47.331197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import albumentations as albu\nTARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\nTARS2 = {x:y for y,x in TARS.items()}\n # 256\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, specs, eeg_specs,\n                 batch_size=32, shuffle=False, augment=False, mode='train'): \n        self.dim_1 = 256\n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.mode = mode\n        self.specs = specs\n        self.eeg_specs = eeg_specs\n        self.on_epoch_end()\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = int( np.ceil( len(self.data) / self.batch_size ) )\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        if self.augment: X = self.__augment_batch(X) \n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange( len(self.data) )\n        if self.shuffle: np.random.shuffle(self.indexes)\n                        \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        \n        X = np.zeros((len(indexes),128,self.dim_1,8),dtype='float32')\n        y = np.zeros((len(indexes),6),dtype='float32')\n        img = np.ones((128,self.dim_1),dtype='float32')\n        \n        for j,i in enumerate(indexes):\n            row = self.data.iloc[i]\n            if self.mode=='test': \n                r = 0\n                spec_id = row.spec_id\n            elif self.mode in ['ensemble', 'granular_train', 'granular_valid']:\n                r = int(row['spectrogram_label_offset_seconds'] // 2)\n                spec_id = row['spectrogram_id']\n            else: \n                r = int( (row['min'] + row['max'])//4 )\n                spec_id = row.spec_id\n\n            for k in range(4):\n                # EXTRACT 300 ROWS OF SPECTROGRAM\n                img = self.specs[spec_id][r:r+300,k*100:(k+1)*100].T\n                \n                # LOG TRANSFORM SPECTROGRAM\n                img = np.clip(img,np.exp(-4),np.exp(8))\n                img = np.log(img)\n                \n                # STANDARDIZE PER IMAGE\n                ep = 1e-6\n                m = np.nanmean(img.flatten())\n                s = np.nanstd(img.flatten())\n                img = (img-m)/(s+ep)\n                img = np.nan_to_num(img, nan=0.0)\n                \n                # CROP TO 256 TIME STEPS\n                X[j,14:-14,:,k] = img[:,22:-22] / 2.0\n            \n            if self.mode in ['ensemble', 'granular_train', 'granular_valid']:\n                # ensemble uses label_id as a unique identifier\n                img = self.eeg_specs[row.label_id]\n            else:\n                # EEG SPECTROGRAMS\n                img = self.eeg_specs[row.eeg_id]\n            X[j,:,:,4:] = img\n\n            if self.mode!='test':\n                y[j,] = row[TARGETS]\n            \n        return X,y\n    \n    def __random_transform(self, img):\n        composition = albu.Compose([\n            albu.HorizontalFlip(p=0.5),\n            #albu.CoarseDropout(max_holes=8,max_height=32,max_width=32,fill_value=0,p=0.5),\n        ])\n        return composition(image=img)['image']\n            \n    def __augment_batch(self, img_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ] = self.__random_transform(img_batch[i, ])\n        return img_batch","metadata":{"papermill":{"duration":2.369789,"end_time":"2024-01-14T22:52:49.721728","exception":false,"start_time":"2024-01-14T22:52:47.351939","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:31:01.510002Z","iopub.execute_input":"2024-02-23T16:31:01.510287Z","iopub.status.idle":"2024-02-23T16:31:02.675445Z","shell.execute_reply.started":"2024-02-23T16:31:01.510262Z","shell.execute_reply":"2024-02-23T16:31:02.674408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Scheduler","metadata":{"papermill":{"duration":0.026741,"end_time":"2024-01-14T22:52:52.251841","exception":false,"start_time":"2024-01-14T22:52:52.2251","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//LR_EVERY)\n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, y, 'o-'); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Step Training Schedule',size=16); plt.show()\n\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.309296,"end_time":"2024-01-14T22:52:52.92271","exception":false,"start_time":"2024-01-14T22:52:52.613414","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:31:02.676871Z","iopub.execute_input":"2024-02-23T16:31:02.677569Z","iopub.status.idle":"2024-02-23T16:31:02.993572Z","shell.execute_reply.started":"2024-02-23T16:31:02.67752Z","shell.execute_reply":"2024-02-23T16:31:02.992668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lrfn2(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//LR_EVERY)\n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn2(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, y, 'o-'); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Step Training Schedule',size=16); plt.show()\n\nLR2 = tf.keras.callbacks.LearningRateScheduler(lrfn2, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:31:02.994949Z","iopub.execute_input":"2024-02-23T16:31:02.995603Z","iopub.status.idle":"2024-02-23T16:31:03.609955Z","shell.execute_reply.started":"2024-02-23T16:31:02.995562Z","shell.execute_reply":"2024-02-23T16:31:03.609054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build EfficientNet Model","metadata":{"papermill":{"duration":0.027228,"end_time":"2024-01-14T22:52:52.97653","exception":false,"start_time":"2024-01-14T22:52:52.949302","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install --no-index --find-links=/kaggle/input/tf-efficientnet-whl-files /kaggle/input/tf-efficientnet-whl-files/efficientnet-1.1.1-py3-none-any.whl","metadata":{"_kg_hide-output":true,"papermill":{"duration":13.587235,"end_time":"2024-01-14T22:53:06.589596","exception":false,"start_time":"2024-01-14T22:52:53.002361","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:31:03.611569Z","iopub.execute_input":"2024-02-23T16:31:03.611934Z","iopub.status.idle":"2024-02-23T16:31:18.026568Z","shell.execute_reply.started":"2024-02-23T16:31:03.6119Z","shell.execute_reply":"2024-02-23T16:31:18.025537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import efficientnet.tfkeras as efn\n\n\ndef build_model():\n    \n    inp = tf.keras.Input(shape=(128,300,10))\n    base_model = efn.EfficientNetB0(include_top=False, weights=None, input_shape=None)\n    base_model.load_weights('/kaggle/input/tf-efficientnet-noisy-student-weights/efficientnet-b0_noisy-student_notop.h5')\n    \n    # RESHAPE INPUT 128x256x8 => 512x512x3 MONOTONE IMAGE\n    # KAGGLE SPECTROGRAMS\n    x1 = [inp[:,:,:,i:i+1] for i in range(4)] #300\n    x1 = tf.keras.layers.Concatenate(axis=1)(x1)\n    # EEG SPECTROGRAMS\n    x2 = [inp[:,:,:,i+4:i+5] for i in range(4)]\n    x2 = tf.keras.layers.Concatenate(axis=1)(x2)\n    # MAKE 512X512X3\n    if USE_KAGGLE_SPECTROGRAMS & USE_EEG_SPECTROGRAMS:\n        x = tf.keras.layers.Concatenate(axis=2)([x1,x2])\n    elif USE_EEG_SPECTROGRAMS: \n        x = x2\n    else: \n        x = x1\n    # possible to change input channel?\n    x = tf.keras.layers.Concatenate(axis=3)([x,x,x])\n\n    # OUTPUT\n    x = base_model(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n#   x = tf.keras.layers.Dense(1024, activation='relu')(x)\n#   x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n\n    # Add your custom layers\n        \n    # COMPILE MODEL\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n#     opt = tf.keras.optimizers.legacy.Adam(learning_rate = 1e-3)\n    loss = tf.keras.losses.KLDivergence()\n\n    model.compile(loss=loss, optimizer = opt) \n        \n    return model","metadata":{"papermill":{"duration":0.057714,"end_time":"2024-01-14T22:53:06.682056","exception":false,"start_time":"2024-01-14T22:53:06.624342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:31:18.028438Z","iopub.execute_input":"2024-02-23T16:31:18.029326Z","iopub.status.idle":"2024-02-23T16:31:18.053915Z","shell.execute_reply.started":"2024-02-23T16:31:18.029279Z","shell.execute_reply":"2024-02-23T16:31:18.053081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model\nWe train using Group KFold on patient id. If `LOAD_MODELS_FROM = None`, then we will train new models in this notebook version. Otherwise we will load saved models from the path `LOAD_MODELS_FROM`.","metadata":{"papermill":{"duration":0.033717,"end_time":"2024-01-14T22:53:06.742557","exception":false,"start_time":"2024-01-14T22:53:06.70884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:31:18.05531Z","iopub.execute_input":"2024-02-23T16:31:18.055677Z","iopub.status.idle":"2024-02-23T16:31:18.060566Z","shell.execute_reply.started":"2024-02-23T16:31:18.055645Z","shell.execute_reply":"2024-02-23T16:31:18.059575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    import sys\n    sys.path.append('/kaggle/input/kaggle-kl-div')\n    from kaggle_kl_div import score\n\n    EARLY_STOP = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        restore_best_weights=True,\n        patience=PATIENCE,\n        start_from_epoch=START_FROM_EPOCH,\n        min_delta=0.0025\n    )\n\n    all_oof = []\n    all_true = []\n    all_histories = []\n \n    gkf = GroupKFold(n_splits=FOLDS)\n    val_indices = {}\n    val_label_ids = {}\n    preds={}\n    for fold_idx, (train_index, valid_index) in enumerate(gkf.split(train_pop_1, train_pop_1.target, train_pop_1.patient_id)):  \n        val_indices[fold_idx] = [str(i) for i in valid_index]\n        print('#'*25)\n        print(f'### Fold {fold_idx+1}')\n        train_valid = train_pop_1.iloc[valid_index]\n        val_label_ids[fold_idx] = [str(i) for i in train_valid['label_id']]\n\n        train_gen = DataGenerator(train_pop_1.iloc[train_index], specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=True, batch_size=BATCH_SIZE, augment=False, mode='granular_train')\n        valid_gen = DataGenerator(train_valid, specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=False, batch_size=64, mode='granular_valid')\n\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n\n        K.clear_session()\n        with strategy.scope():\n            model = build_model()\n        h5_filename = f'EffNet_pop1_v{VERSION}_f{fold_idx}.h5'\n        if LOAD_MODELS_FROM_1 is None:\n            \n            # ModelCheckpoint callback to save the best model\n            h5_path = os.path.join(MODEL_LOC, h5_filename)\n            CHECKPOINT = tf.keras.callbacks.ModelCheckpoint(\n                h5_path,         # Path where to save the model\n                save_best_only=True,     # Only save a model if `val_loss` has improved\n                monitor='val_loss',      # Monitor 'val_loss' for improvement\n                mode='min'               # The smaller the `val_loss`, the better\n            )\n\n\n            history = model.fit(train_gen, verbose=1,\n                  validation_data = valid_gen,\n                  epochs=EPOCHS, callbacks = [EARLY_STOP, LR, CHECKPOINT, \n#                                               CM, TENSORBOARD_CALLBACK\n                                             ])\n            all_histories.append(history)\n            model.load_weights(h5_path)\n\n        else:            \n            # load weights from pop 2 training to get oof \n            h5_filename = f'{LOAD_MODELS_FROM_1_NAME}_f{fold_idx}.h5'\n            model.load_weights(os.path.join(LOAD_MODELS_FROM_1, h5_filename))\n\n        oof = model.predict(valid_gen, verbose=1)\n        all_oof.append(oof)\n        all_true.append(train_valid[TARGETS].values)\n\n        preds[fold_idx] = oof\n\n        del model, oof\n        gc.collect()\n\n\n    all_oof = np.concatenate(all_oof)\n    all_true = np.concatenate(all_true)\n\n\n    if LOAD_MODELS_FROM_1 is None:\n        history_dict = {}\n        for fold, h in enumerate(all_histories):\n            history_dict[fold] = str(h.history)\n\n        with open('histories.json', 'w') as file:\n            json.dump(history_dict, file, indent=4)\n\n        with open('val_indices.json', 'w') as file:\n            json.dump(val_indices, file, indent=4)\n\n        with open('val_label_ids.json', 'w') as file:\n            json.dump(val_label_ids, file, indent=4)\n\n     \n    flattened_list = [item for sublist in val_label_ids.values() for item in sublist]\n    sub = pd.DataFrame({'label_id':flattened_list})\n    sub[TARGETS] = np.vstack(preds.values())\n    sub.to_csv(os.path.join(MODEL_LOC, 'ensemble_data1.csv'), index=False)\n    \n    oof = pd.DataFrame(all_oof.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    model_info['CV Score KL-Div Pop 1'] = cv\n    with open(model_info_path, 'w') as f:\n        json.dump(model_info, f)","metadata":{"papermill":{"duration":161.172,"end_time":"2024-01-14T22:55:47.94776","exception":false,"start_time":"2024-01-14T22:53:06.77576","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:31:18.062084Z","iopub.execute_input":"2024-02-23T16:31:18.062401Z","iopub.status.idle":"2024-02-23T16:33:51.526054Z","shell.execute_reply.started":"2024-02-23T16:31:18.062365Z","shell.execute_reply":"2024-02-23T16:33:51.525166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    import sys\n    sys.path.append('/kaggle/input/kaggle-kl-div')\n    from kaggle_kl_div import score\n    \n    EARLY_STOP = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        restore_best_weights=True,\n        patience=PATIENCE,\n        start_from_epoch=START_FROM_EPOCH,\n        min_delta=0.0025\n    )\n\n    all_oof2 = []\n    all_true2 = []\n    all_histories2 = []\n \n    gkf = GroupKFold(n_splits=FOLDS)\n    val_indices2 = {}\n    val_label_ids2 = {}\n    preds2={}\n    for fold_idx, (train_index, valid_index) in enumerate(gkf.split(train_pop_2, train_pop_2.target, train_pop_2.patient_id)):  \n\n        val_indices2[fold_idx] = [str(i) for i in valid_index]\n        print('#'*25)\n        print(f'### Fold {fold_idx+1}')\n        train_valid = train_pop_2.iloc[valid_index]\n        val_label_ids2[fold_idx] = [str(i) for i in train_valid['label_id']]\n\n        train_gen = DataGenerator(train_pop_2.iloc[train_index], specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=True, batch_size=BATCH_SIZE, augment=False, mode='granular_train')\n        valid_gen = DataGenerator(train_valid, specs=spectrograms, eeg_specs=all_eegs,\n                                  shuffle=False, batch_size=64, mode='granular_valid')\n\n        print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n        print('#'*25)\n\n        K.clear_session()\n        with strategy.scope():\n            model = build_model()\n\n        h5_filename_2 = f'EffNet_pop2_v{VERSION}_f{fold_idx}.h5'\n        if LOAD_MODELS_FROM_2 is None:\n\n            # ModelCheckpoint callback to save the best model\n            h5_path = os.path.join(MODEL_LOC, h5_filename_2)\n            CHECKPOINT = tf.keras.callbacks.ModelCheckpoint(\n                h5_path,         # Path where to save the model\n                save_best_only=True,     # Only save a model if `val_loss` has improved\n                monitor='val_loss',      # Monitor 'val_loss' for improvement\n                mode='min'               # The smaller the `val_loss`, the better\n            )\n            if LOAD_MODELS_FROM_1 is None:\n                h5_filename_1 = f'EffNet_pop1_v{VERSION}_f{fold_idx}.h5'\n                weight_path = os.path.join(MODEL_LOC, h5_filename_1)\n            else:\n                h5_filename_1 = f'{LOAD_MODELS_FROM_1_NAME}_f{fold_idx}.h5'\n                weight_path = os.path.join(LOAD_MODELS_FROM_1, h5_filename_1)\n                \n            model.load_weights(weight_path)\n#             model.load_weights(os.path.join('/kaggle/input/temp-for-correction', h5_filename_1))\n            history = model.fit(train_gen, verbose=1,\n                  validation_data = valid_gen,\n                  epochs=EPOCHS, callbacks = [EARLY_STOP, LR, CHECKPOINT, \n#                                               CM, TENSORBOARD_CALLBACK\n                                             ])\n            all_histories2.append(history)\n            model.load_weights(h5_path)\n\n        else:\n            model.load_weights(os.path.join(LOAD_MODELS_FROM_2, h5_filename))\n\n        oof = model.predict(valid_gen, verbose=1)\n        all_oof2.append(oof)\n        all_true2.append(train_valid[TARGETS].values)\n\n        preds2[fold_idx] = oof\n\n        del model, oof\n        gc.collect()\n\n\n    all_oof2 = np.concatenate(all_oof2)\n    all_true2 = np.concatenate(all_true2)\n\n\n    if LOAD_MODELS_FROM_2 is None:\n        history_dict = {}\n        for fold, h in enumerate(all_histories2):\n            history_dict[fold] = str(h.history)\n\n        with open('histories.json', 'w') as file:\n            json.dump(history_dict, file, indent=4)\n\n        with open('val_indices.json', 'w') as file:\n            json.dump(val_indices2, file, indent=4)\n\n        with open('val_label_ids.json', 'w') as file:\n            json.dump(val_label_ids2, file, indent=4)\n\n     \n    flattened_list = [item for sublist in val_label_ids2.values() for item in sublist]\n    sub = pd.DataFrame({'label_id':flattened_list})\n    sub[TARGETS] = np.vstack(preds2.values())\n    sub.to_csv(os.path.join(MODEL_LOC, 'ensemble_data2.csv'), index=False)\n    \n    oof = pd.DataFrame(all_oof2.copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(all_true2.copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    model_info['CV Score KL-Div Pop 2'] = cv\n    with open(model_info_path, 'w') as f:\n        json.dump(model_info, f)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:37:41.647271Z","iopub.execute_input":"2024-02-23T16:37:41.648093Z","iopub.status.idle":"2024-02-23T16:42:20.846279Z","shell.execute_reply.started":"2024-02-23T16:37:41.648042Z","shell.execute_reply":"2024-02-23T16:42:20.844718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not EVAL_ONLY:\n    oof = pd.DataFrame(np.concatenate([all_oof, all_oof2]).copy())\n    oof['id'] = np.arange(len(oof))\n\n    true = pd.DataFrame(np.concatenate([all_true, all_true2]).copy())\n    true['id'] = np.arange(len(true))\n\n    cv = score(solution=true, submission=oof, row_id_column_name='id')\n    print('CV Score KL-Div =',cv)\n    \n    # ens = pd.read_csv(os.path.join(MODEL_LOC, 'ensemble_data1.csv'))\n    # ens2 = pd.read_csv(os.path.join(MODEL_LOC, 'ensemble_data2.csv'))\n    # pd.concat([ens, ens2], ignore_index=True).to_csv('ensemble_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.94643Z","iopub.status.idle":"2024-02-23T16:33:51.946928Z","shell.execute_reply.started":"2024-02-23T16:33:51.946666Z","shell.execute_reply":"2024-02-23T16:33:51.946689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test and Create Submission CSV\nBelow we use our 5 EfficientNet fold models to infer the test data and create a `submission.csv` file.","metadata":{"papermill":{"duration":0.050491,"end_time":"2024-01-14T22:55:48.321932","exception":false,"start_time":"2024-01-14T22:55:48.271441","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if not EVAL_ONLY:\n    del all_eegs, spectrograms; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.948414Z","iopub.status.idle":"2024-02-23T16:33:51.94879Z","shell.execute_reply.started":"2024-02-23T16:33:51.948591Z","shell.execute_reply":"2024-02-23T16:33:51.948608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\nprint('Test shape',test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.949922Z","iopub.status.idle":"2024-02-23T16:33:51.95028Z","shell.execute_reply.started":"2024-02-23T16:33:51.950113Z","shell.execute_reply":"2024-02-23T16:33:51.950129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL SPECTROGRAMS\nPATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/'\nfiles2 = os.listdir(PATH2)\nprint(f'There are {len(files2)} test spectrogram parquets')\n    \nspectrograms2 = {}\nfor i,f in enumerate(files2):\n    if i%100==0: print(i,', ',end='')\n    tmp = pd.read_parquet(f'{PATH2}{f}')\n    name = int(f.split('.')[0])\n    spectrograms2[name] = tmp.iloc[:,1:].values\n    \n# RENAME FOR DATALOADER\ntest = test.rename({'spectrogram_id':'spec_id'},axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.95179Z","iopub.status.idle":"2024-02-23T16:33:51.952135Z","shell.execute_reply.started":"2024-02-23T16:33:51.951964Z","shell.execute_reply":"2024-02-23T16:33:51.951986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# READ ALL EEG SPECTROGRAMS\nPATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\nDISPLAY = 1\nEEG_IDS2 = test.eeg_id.unique()\nall_eegs2 = {}\n\nprint('Converting Test EEG to Spectrograms...'); print()\nfor i,eeg_id in enumerate(EEG_IDS2):\n        \n    # CREATE SPECTROGRAM FROM EEG PARQUET\n    img = spectrogram_from_eeg(f'{PATH2}{eeg_id}.parquet', i<DISPLAY)\n    all_eegs2[eeg_id] = img","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.953507Z","iopub.status.idle":"2024-02-23T16:33:51.953871Z","shell.execute_reply.started":"2024-02-23T16:33:51.953678Z","shell.execute_reply":"2024-02-23T16:33:51.953694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFER EFFICIENTNET ON TEST\npreds = []\nmodel = build_model()\ntest_gen = DataGenerator(test, spectrograms2, all_eegs2, shuffle=False, batch_size=64, mode='test')\n\nfor i in range(FOLDS):\n    print(f'Fold {i+1}')\n    if LOAD_MODELS_FROM_2:\n        model.load_weights(os.path.join(LOAD_MODELS_FROM_2, f'EffNet_pop2_v{VERSION}_f{i}.h5'))\n    else:\n        model.load_weights(os.path.join(MODEL_LOC, f'EffNet_pop2_v{VERSION}_f{i}.h5'))\n    pred = model.predict(test_gen, verbose=1)\n    preds.append(pred)\npred = np.mean(preds,axis=0)\nprint()\nprint('Test preds shape',pred.shape)","metadata":{"papermill":{"duration":9.827745,"end_time":"2024-01-14T22:55:58.637732","exception":false,"start_time":"2024-01-14T22:55:48.809987","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:33:51.955347Z","iopub.status.idle":"2024-02-23T16:33:51.955689Z","shell.execute_reply.started":"2024-02-23T16:33:51.955521Z","shell.execute_reply":"2024-02-23T16:33:51.955538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\nsub[TARGETS] = pred\nsub.to_csv('submission.csv',index=False)\nprint('Submissionn shape',sub.shape)\nsub.head()","metadata":{"papermill":{"duration":0.071388,"end_time":"2024-01-14T22:55:58.760368","exception":false,"start_time":"2024-01-14T22:55:58.68898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:33:51.957554Z","iopub.status.idle":"2024-02-23T16:33:51.957901Z","shell.execute_reply.started":"2024-02-23T16:33:51.957726Z","shell.execute_reply":"2024-02-23T16:33:51.957749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\nsub.iloc[:,-6:].sum(axis=1)","metadata":{"papermill":{"duration":0.062742,"end_time":"2024-01-14T22:55:58.873394","exception":false,"start_time":"2024-01-14T22:55:58.810652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-02-23T16:33:51.959258Z","iopub.status.idle":"2024-02-23T16:33:51.959861Z","shell.execute_reply.started":"2024-02-23T16:33:51.959628Z","shell.execute_reply":"2024-02-23T16:33:51.959649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-02-23T16:33:51.961167Z","iopub.status.idle":"2024-02-23T16:33:51.961609Z","shell.execute_reply.started":"2024-02-23T16:33:51.961377Z","shell.execute_reply":"2024-02-23T16:33:51.961399Z"},"trusted":true},"execution_count":null,"outputs":[]}]}